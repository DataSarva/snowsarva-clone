{"search_id":"search_399a885d0d2b42bea850a7c441ae7fcf","results":[{"url":"https://docs.snowflake.com/en/user-guide/dynamic-tables-performance","title":"Dynamic table performance and optimization | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nGuides Data engineering Dynamic tables Performance and optimization\nSection Title: Dynamic table performance and optimization ¶\nContent:\nLearn how to optimize and monitor dynamic tables for speed and cost-efficiency. This section\nprovides foundational concepts and links to more detailed topics.\nDynamic table *performance* refers to how quickly and efficiently a dynamic table refresh completes. A\nwell-performing dynamic table refreshes fast enough to meet its target lag without\nconsuming excessive compute resources.\nSection Title: Dynamic table performance and optimization ¶ > Why performance matters ¶\nContent:\nData freshness\nDynamic tables refresh based on a target lag that you specify,\nwhich is the maximum allowed delay between updates to source tables and the dynamic table’s content.\nWhen refreshes take too long, your pipeline might not meet your freshness requirements.\nFor example, setting a target lag of five minutes when your refresh takes eight minutes means your\npipeline can’t maintain the required freshness.\nCost efficiency\nDynamic tables require virtual warehouses for refreshes, which consume credits. Poorly optimized\ndynamic tables might scan more data than necessary, trigger full refreshes when incremental would\nsuffice, or require larger warehouses to complete within target lag windows.\nFor more information about costs, see Understanding costs for dynamic tables .\nSection Title: Dynamic table performance and optimization ¶ > Performance decisions ¶\nContent:\nChanges that affect dynamic table performance fall into two categories based on *when* you\ncan make them:\nSection Title: Dynamic table performance and optimization ¶ > Performance decisions ¶\nContent:\n|  | Design changes | Adjustments |\n| **When** | Before you create a pipeline. | After your pipeline is running. |\n| **Impact** | High | Medium |\n| **Flexibility** | Hard to change; requires recreating tables. | Easy to change; no need to recreate tables. |\n| **Examples** | Query structure, refresh mode, pipeline design. | Warehouse size, clustering keys, target lag. |\nSection Title: Dynamic table performance and optimization ¶ > Performance decisions ¶\nContent:\nFor detailed guidance on both categories, see Optimize dynamic table performance .\nSection Title: Dynamic table performance and optimization ¶ > Get started ¶\nContent:\nTo get started with dynamic table performance optimization, try the hands-on tutorial:\nTutorial: Optimize dynamic table performance for SCD Type 1 workloads\nLearn how to identify and resolve performance bottlenecks in a dynamic table pipeline. This tutorial\nshows how different SQL patterns affect incremental refresh and how to use the `QUALIFY` clause\nto efficiently remove duplicate rows.\nSection Title: Dynamic table performance and optimization ¶ > Topics in this section ¶\nContent:\nMonitor dynamic table performance\nHow to monitor refresh performance, analyze query profiles, and track key metrics.\nOptimize dynamic table performance\nKey concepts and optimization techniques: refresh modes, data locality, warehouse sizing,\ntarget lag, query patterns, and clustering.\nOptimize queries for incremental refresh\nPerformance guide for how SQL operators affect incremental refresh speed.\nUse immutability constraints\nHow to use immutability constraints to mark historical data as unchanging and reduce refresh scope.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nLanguage: **English**\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês"]},{"url":"https://docs.snowflake.com/en/user-guide/dynamic-tables-performance-optimize-query","title":"Optimize queries for incremental refresh | Snowflake Documentation","excerpts":["Use this page when you design a new dynamic table query or want to optimize an existing one for incremental refresh. This guide shows which operators perform ..."]},{"url":"https://docs.snowflake.cn/en/user-guide/dynamic-table-performance-guide.html","title":"Best practices for optimizing dynamic table performance | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.cn)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\nStatus\nGo to https://china-status.snowflake.com to see details of the status\nGuides Data engineering Dynamic Tables Performance considerations Best practices for optimizing performance\nSection Title: Best practices for optimizing dynamic table performance ¶\nContent:\nTo optimize your dynamic tables’ performance, you should understand the system, experiment with ideas, and iterate based on results. For example:\nDevelop ways to improve your data pipeline based on your cost, data lag, and response time needs.\nImplement the following actions:\nStart with a small, fixed dataset to quickly develop queries.\nTest performance with data in motion.\nScale the dataset to verify that it meets your needs.\nAdjust your workload based on findings.\nRepeat as needed, prioritizing tasks with the greatest performance impact.\nAdditionally, use downstream lag to manage refresh dependencies between tables efficiently, ensuring that refreshes happen only when necessary.\nNote\nWhen queried, dynamic tables perform similarly to regular Snowflake tables. For more information, see Optimizing performance in Snowflake .\nSection Title: Best practices for optimizing dynamic table performance ¶ > Full refresh performance ¶\nContent:\nFull refresh dynamic tables perform similarly to CREATE TABLE … AS SELECT (also referred to as CTAS) . They can be optimized like any other Snowflake query.\nSection Title: Best practices for optimizing dynamic table performance ¶ > Incremental refresh performance ¶\nContent:\nTo help achieve optimal incremental refresh performance for your dynamic tables:\nKeep changes between refreshes minimal, ideally less than 5% of the total dataset, for both sources and the dynamic table.\nConsider the number of micro-partitions modified, not just the row count. The amount of work an incremental refresh must do is proportional\nto the size of these micro-partitions, not only the rows that changed.\nMinimize grouping operations like joins, GROUP BYs, and PARTITION BYs in your query. Break down large Common Table Expressions (CTEs) into\nsmaller parts and create a dynamic table for each. Avoid overwhelming a single dynamic table with excessive aggregations or joins.\nEnsure data locality by aligning table changes with query keys; for example, for joins, GROUP BYs, PARTITION BYs. If your tables aren’t naturally\nclustered by these keys, consider enabling automatic clustering .\nSection Title: Best practices for optimizing dynamic table performance ¶ > General best practices ¶\nContent:\nFor common deduplication and top-N use cases, use top-level QUALIFY clauses with RANK or ROW_NUMBER ranking window functions when the rank\nvalue is 1. For example, `QUALIFY ROW_NUMBER() OVER (PARTITION BY ... ORDER BY ...) = 1` .\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.cn)\nShare your feedback\n[Privacy Policy](https://www.snowflake.cn/legal/privacy-policy.pdf) [Terms of Service](https://www.snowflake.cn/legal/terms-of-service.pdf)\nOn this page\nFull refresh performance\nIncremental refresh performance\nGeneral best practices\nLanguage: **English**\nEnglish\n中文"]},{"url":"https://docs.snowflake.com/en/user-guide/dynamic-tables-performance-optimize","title":"Optimize dynamic table performance - Source: Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nGuides Data engineering Dynamic Tables Performance and optimization Optimize dynamic table performance\nSection Title: Optimize dynamic table performance ¶\nContent:\nThis topic covers techniques for optimizing dynamic table performance, organized into\ndesign changes and adjustments.\nBefore you optimize a dynamic table, you might want to diagnose the cause of slow refreshes. See Diagnose slow refreshes for a step-by-step workflow.\nDesign changes\nChoose a refresh mode\nOptimize your queries and pipeline\nMark historical data immutable\nAdjustments\nAdjust your warehouse configuration\nIdentify the right target lag\nImprove data locality\nFor background on performance categories, see Performance decisions .\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶\nContent:\nDesign changes require you to recreate a dynamic table, but have greater impact on performance.\nNote\nWe recommend that you group changes and recreate tables together instead of making incremental modifications.\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶ > Choose a refresh mode ¶\nContent:\nThe refresh mode you choose has a significant impact on performance because it determines\nhow much data Snowflake processes during each refresh. For information about how each mode\nworks, see Dynamic table refresh modes .\nImportant\nDynamic tables with incremental refresh can’t be downstream from dynamic tables that use\nfull refresh.\nUse the following decision process to select a refresh mode:\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶ > Choose a refresh mode ¶\nContent:\nReview your query against the list of supported query constructs . Not all query operators support incremental refresh. For operators that *are* supported,\nsee Optimize queries for incremental refresh to understand how\nthey affect performance. Estimate your change volume, which is the percentage of your data that changes between\nrefreshes. Incremental refresh, for example, works best when less than five percent of data changes. Evaluate your data locality. Check whether your source tables are clustered by the keys\nthat you plan to use in joins, GROUP BY, or PARTITION BY clauses in your dynamic table query. Poor locality reduces incremental refresh efficiency. To improve locality,\nsee Improve data locality .\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶ > Choose a refresh mode ¶\nContent:\nChoose a mode based on the following table:ModeWhen to use**Incremental**Your query uses supported operators, less than five percent of data changes betweenrefreshes, and your source tables have good data locality.NoteIncremental refresh can still scan source tables, not just the rows that changed. For\nexample, a new row in one side of a join must match against all rows in the other table. Even a small number of changes can trigger significant work. |\n|**Full** |A large percentage of data changes, your query uses unsupported operators, or your\ndata lacks locality. |\n|**Auto** |You’re prototyping or testing. Avoid AUTO in production because its behavior might\nchange between Snowflake releases. |\nWhen you create a dynamic table, specify the mode with `REFRESH_MODE = INCREMENTAL` or `REFRESH_MODE = FULL` in your CREATE DYNAMIC TABLE statement.\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶ > Choose a refresh mode ¶\nContent:\nTo check which refresh mode a dynamic table uses, see Refresh mode .\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶ > Optimize your queries and pipeline ¶\nContent:\nThe structure of your dynamic table queries and pipeline directly affects refresh performance. Use the\nfollowing guidelines to reduce the work during each refresh.\n**Simplify individual queries**\nUse inner joins instead of outer joins. Inner joins perform better with incremental\nrefresh. Verify referential integrity in your source data so that you can avoid outer joins.\nAvoid unnecessary operations. Remove redundant DISTINCT clauses and unused columns.\nExclude wide columns (like large JSON blobs) that aren’t frequently queried.\nRemove duplicates efficiently. Use ranking functions instead of DISTINCT where possible.\nFor detailed guidance on how specific SQL operators affect incremental refresh performance,\nsee Optimize queries for incremental refresh .\nNote\nFor a comprehensive example, see Tutorial: Optimize dynamic table performance for SCD Type 1 workloads .\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶ > Optimize your queries and pipeline ¶\nContent:\n**Split transformations across dynamic tables**\nBreaking complex transformations into multiple dynamic tables makes it easier to identify\nbottlenecks and improves debugging. With immutability constraints ,\nyou can also use different refresh modes for different stages.\nAdd filters early. Apply `WHERE` clauses in the dynamic tables closest to your source\ndata so that downstream tables process fewer rows.\nTo avoid repeated `DISTINCT` operations in downstream tables, remove duplicate rows earlier in your pipeline.\nReduce the number of operations per table. Move joins, aggregations, or window functions\ninto intermediate dynamic tables instead of combining them all in one query.\nMaterialize compound expressions (like `DATE_TRUNC('minute', ts)` ) in an intermediate\ntable before grouping by them. For details, see Optimize aggregations .\nNote\nFinding optimal split points requires trial and error.\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶ > Optimize your queries and pipeline ¶\nContent:\nConsider splitting between operations\nthat shuffle data on different keys, such as `GROUP BY` , `DISTINCT` , window functions\nwith `PARTITION BY` , and joins. This allows each dynamic table to maintain better data\nlocality for its key operation. For operator-specific guidance, see Optimize queries for incremental refresh .\nThe following example shows how to split a complex query into intermediate dynamic tables.\nInitial complex query:\n```\nCREATE DYNAMIC TABLE final_result AS \n  SELECT ... \n  FROM large_table a \n  JOIN dimension_table b ON ... \n  JOIN another_table c ON ... \n  GROUP BY ...;\n```\nCopy\nSplit the complex pipeline by adding an intermediate dynamic table:\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶ > Optimize your queries and pipeline ¶\nContent:\n```\nCREATE DYNAMIC TABLE intermediate_joined AS \n  SELECT ... \n  FROM large_table a \n  JOIN dimension_table b ON ...; \n\n CREATE DYNAMIC TABLE final_result AS \n  SELECT ... \n  FROM intermediate_joined \n  JOIN another_table c ON ... \n  GROUP BY ...;\n```\nCopy\nFor detailed information and example of how operators affect performance, see Optimize queries for incremental refresh .\nSection Title: Optimize dynamic table performance ¶ > Design changes ¶ > Mark historical data immutable ¶\nContent:\nUse the `IMMUTABLE WHERE` clause to tell Snowflake that certain rows won’t change. This\nreduces the scope of work during each refresh.\nFor syntax, examples, and detailed guidance, see Use immutability constraints .\nSection Title: Optimize dynamic table performance ¶ > Adjustments ¶\nContent:\nAdjustments don’t require you to recreate dynamic tables. You can make adjustments while your pipeline is running.\nSection Title: Optimize dynamic table performance ¶ > Adjustments ¶ > Adjust your warehouse configuration ¶\nContent:\nThe warehouse that you specify in your CREATE DYNAMIC TABLE statement runs all refreshes for that\ntable. Warehouse size and configuration directly affect refresh duration and cost.\nFor more information about warehouses and dynamic tables, see Understand warehouse usage for dynamic tables .\nFor general warehouse performance optimization strategies, see Optimizing warehouses for performance .\nSection Title: Optimize dynamic table performance ¶ > ... > Use a separate warehouse for initialization ¶\nContent:\nInitial refreshes often process significantly more data than incremental refreshes. Use\nINITIALIZATION_WAREHOUSE to run initializations on a larger warehouse. Reserve a\nsmaller, more cost-effective warehouse for regular refreshes:\n```\nCREATE DYNAMIC TABLE my_dynamic_table \n  TARGET_LAG = 'DOWNSTREAM' \n  WAREHOUSE = 'XS_WAREHOUSE' \n  INITIALIZATION_WAREHOUSE = '4XL_WAREHOUSE' \n  AS < query >;\n```\nCopy\nTo add or change the initialization warehouse for an existing dynamic table:\n```\nALTER DYNAMIC TABLE my_dynamic_table SET INITIALIZATION_WAREHOUSE = '4XL_WAREHOUSE' ;\n```\nCopy\nTo remove the initialization warehouse and use the primary warehouse for all refreshes:\n```\nALTER DYNAMIC TABLE my_dynamic_table UNSET INITIALIZATION_WAREHOUSE ;\n```\nCopy\nTo view the warehouse configuration, use SHOW DYNAMIC TABLES or\ncheck the DYNAMIC_TABLE_REFRESH_HISTORY table function.\nSection Title: Optimize dynamic table performance ¶ > ... > Resize when needed ¶\nContent:\nTo balance cost and performance, choose a warehouse size that prevents bytes from being spilled but doesn’t\nexceed what your workload can use in parallel. When faster refreshes are critical, increase the\nsize slightly beyond the cost-optimal point.\n**Considerations for dynamic table refreshes:**\nSection Title: Optimize dynamic table performance ¶ > ... > Resize when needed ¶\nContent:\n**Bytes spilled** : When query history shows bytes spilled to local or remote storage, the warehouse\nran out of memory during refresh. A larger warehouse provides more memory to prevent spilling.\nFor details, see Queries too large to fit in memory .\n**Slow initial refresh** : When the initial refresh is slow, consider setting INITIALIZATION_WAREHOUSE\nfor the initial creation, or temporarily resize the warehouse and then resize it down after the table\nis created.\n**Saturated parallelism** : Beyond a certain point, additional parallelism provides diminishing\nreturns. Doubling warehouse size might double cost without halving runtime. To check how your\nrefresh uses parallelism, review the query profile .\nTo resize a warehouse, see Increasing warehouse size .\nFor cost considerations, see Virtual warehouse credit usage and Working with warehouses .\nSection Title: ... > Handle concurrent refreshes with multi-cluster warehouses ¶\nContent:\nIf multiple dynamic tables share a warehouse and refreshes frequently queue, consider using a multi-cluster warehouse . Multi-cluster warehouses\nautomatically add clusters when queries queue and remove them when demand drops. This improves\nrefresh latency during peak periods without paying for unused capacity during quiet periods.\nFor guidance on identifying and reducing queues, see Reducing queues .\nMulti-cluster warehouses require Enterprise Edition or higher. For cost considerations, see Setting the scaling policy for a multi-cluster warehouse .\nSection Title: Optimize dynamic table performance ¶ > Adjustments ¶ > Identify the right target lag ¶\nContent:\nTarget lag controls how often your dynamic table refreshes. Shorter target lag means fresher\ndata but more frequent refreshes and higher compute cost. For more information about how target lag works,\nsee Understanding dynamic table target lag .\nUse the following recommendations to optimize target lag for your workload:\n**Use DOWNSTREAM for intermediate tables** that don’t need independent freshness guarantees.\nThese tables refresh only when downstream tables need them.\n**Check the refresh history to find the right lag** : Use DYNAMIC_TABLE_REFRESH_HISTORY or Snowsight to\nanalyze refresh durations and skipped refreshes. Set the target lag slightly higher than your\ntypical refresh duration.\nSection Title: Optimize dynamic table performance ¶ > ... > Identify the right target lag ¶ > Change target lag ¶\nContent:\n```\nALTER DYNAMIC TABLE my_dynamic_table SET TARGET_LAG = '1 hour' ;\n```\nCopy\nTo set a dynamic table to refresh based on downstream demand:\n```\nALTER DYNAMIC TABLE my_dynamic_table SET TARGET_LAG = DOWNSTREAM ;\n```\nCopy\nSection Title: Optimize dynamic table performance ¶ > Adjustments ¶ > Improve data locality ¶\nContent:\n*Locality* describes how closely Snowflake stores rows that share the same key values. When\nrows with matching keys span fewer micro-partitions (good locality), incremental refreshes scan\nless data. When matching keys span many micro-partitions (poor locality), incremental refresh\ncan take longer than full refresh.\nFor more information about how Snowflake stores data, see Micro-partitions & Data Clustering .\n ... \nSection Title: Optimize dynamic table performance ¶ > ... > Factors that affect locality ¶\nContent:\nBeyond source table clustering, two other factors affect locality. These depend on your data\npatterns and are harder to change directly:\nSection Title: Optimize dynamic table performance ¶ > ... > Factors that affect locality ¶\nContent:\n**How new data aligns with partition keys** : Incremental refresh is faster when new rows\naffect only a small portion of the table. This depends on your data ingestion patterns, not\nyour query structure.For example, time-series data grouped by hour has good locality because\nnew rows share recent timestamps. Data grouped by a column with values spread across the\nentire table has poor locality.\n**How changes align with dynamic table clustering** : When Snowflake applies updates or\ndeletions to a dynamic table, it must locate the affected rows. This is faster when the changed rows are stored\nclose together.For example, updates to recent rows perform well when the dynamic table is\nnaturally ordered by time. Updates scattered across the entire table are slower. This factor\ndepends on your workload patterns, including which rows change and how often.\nSection Title: Optimize dynamic table performance ¶ > ... > Factors that affect locality ¶\nContent:\nWhen you experience poor locality because of these factors, consider whether you can adjust your data model or\ningestion patterns upstream.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nLanguage: **English**\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês"]},{"url":"https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh","title":"Understanding dynamic table initialization and refresh | Snowflake Documentation","excerpts":["Guides Data engineering Dynamic tables Key concepts Understanding initialization and refresh\nSection Title: Understanding dynamic table initialization and refresh ¶\nContent:\nA dynamic table’s content is defined by a query and automatically updates — called a refresh — when the underlying data changes.\nThis process analyzes the query to keep the table current.\nThe following sections explain dynamic table refresh in more detail:\nSection Title: Understanding dynamic table initialization and refresh ¶\nContent:\n| Section | Description |\n| Understanding dynamic table initialization | Introduces initialization, or in other words, the initial data population when you create a dynamic table. You can specify when the |\n| initial refresh occurs. |  |\n| Understanding manual and scheduled refresh options | An overview of dynamic table refresh. Dynamic tables refresh on a schedule unless manually refreshed. |\n| Dynamic table refresh modes | Dynamic tables support different refresh modes: incremental, full, and AUTO. |\n| How data is refreshed when a dynamic table depends on other dynamic tables | Learn how dynamic tables refresh in relation to their dependencies. |\n| Understanding the effects of changes to columns in base tables |  |\nSection Title: ... > Understanding dynamic table initialization ¶\nContent:\nWhen you create a dynamic table , its initial refresh takes place either synchronously at creation\nor at a scheduled time. The initial data population, or initialization , depends on when this initial refresh occurs.\nDynamic tables refresh based on the specified target lag , which sets the maximum allowed delay\nbetween updates to the base tables and the dynamic table’s content. If you set `INITIALIZE = ON_CREATE` (default), the table is initialized\nimmediately. If you set `INITIALIZE = ON_SCHEDULE` , initialization happens within the specified target lag timeframe.\nFor example, consider a dynamic table, `DT1` , with a target lag of 30 minutes. The initial data population for `DT1` can occur as follows:\nIf `DT1` is set to refresh synchronously at creation ( `ON_CREATE` ), it initializes at creation.\nIf `DT1` is set to refresh at a scheduled time ( `ON_SCHEDULE` ), it initializes within 30 minutes.\nSection Title: ... > Understanding dynamic table initialization ¶\nContent:\nIn scenarios with downstream dependencies, refresh behavior depends on the dependencies. For example, if dynamic table `DT1` has a downstream target lag and `DT2` , which depends on `DT1` , has a 30-minute target lag, `DT1` refreshes only when `DT2` refreshes.\nFor `DT1` :\nIf set to refresh synchronously at creation, it initializes immediately. If initialization fails, the creation process stops, providing\nimmediate feedback on any errors.\nIf set to refresh at a scheduled time, initialization depends on when `DT2` refreshes.\nInitialization can take some time, depending on how much data is scanned. To track progress, see Troubleshoot dynamic table creation .\nSection Title: ... > Understanding manual and scheduled refresh options ¶\nContent:\nDynamic tables are refreshed on a schedule that’s determined by the target lag . Every time a\ndynamic table is read, the data freshness is within the time period defined by the target lag.\nYou can manually refresh your dynamic tables to get the latest data using the ALTER DYNAMIC TABLE … REFRESH command or Snowsight.\nFor more information, see Manually refresh dynamic tables .\nDynamic table refresh timeouts are controlled by the STATEMENT_TIMEOUT_IN_SECONDS parameter, which sets the maximum allowed\nduration at the account or warehouse level before a refresh is automatically canceled.\nSection Title: ... > How target lag affects scheduled refreshes ¶\nContent:\nTarget lag controls the frequency of scheduled refreshes. To manually manage refreshes, set your dynamic table’s target lag to DOWNSTREAM and\nensure that all downstream dynamic tables are also set to DOWNSTREAM.\nSetting the entire Directed Acyclic Graph (DAG)’s target lag to DOWNSTREAM essentially disables scheduled refreshes because the final dynamic\ntable controls the refresh schedule. If no dynamic table has a time-based target lag, the pipeline is suspended for scheduled refreshes. In\nthis case, manually refreshing the most downstream table automatically refreshes any upstream dependencies.\nSetting the target lag to DOWNSTREAM doesn’t specify exact times. Instead, Snowflake picks a refresh cadence to attempt to keep the lag under\nthe target value. For example, a dynamic table with a target lag of 4 hours might refresh every 3.5 hours.\nSection Title: ... > How target lag affects scheduled refreshes ¶\nContent:\nTo specify exact times, you can use a task with a CRON schedule. For more information, see Manually refresh dynamic tables .\nSection Title: Understanding dynamic table initialization and refresh ¶ > Dynamic table refresh modes ¶\nContent:\nDynamic tables support three refresh modes: auto, incremental, and full.\nYou can either set the refresh mode to AUTO or set it explicitly:\nSection Title: Understanding dynamic table initialization and refresh ¶ > Dynamic table refresh modes ¶\nContent:\n**AUTO refresh mode:** When using the `AUTO` parameter, Snowflake automatically selects the most cost- and time-effective refresh mode\nbased on query complexity, supported constructs, operators, functions, and expected performance. This decision is made only once at the time\nof table creation. If incremental refresh is unsupported or inefficient , Snowflake chooses full refresh instead.For example, if a dynamic table references a view and the view’s definition changes asynchronously, the refresh mode remains unchanged. If\nthe original decision was incremental but becomes unsupported (for example, due to an upstream view change), the refresh will fail with an\nerror like `Dynamic table can no longer be refreshed incrementally because an upstream view changed.`To change the refresh mode, recreate the dynamic table using the CREATE OR REPLACE DYNAMIC TABLE command.\nSection Title: Understanding dynamic table initialization and refresh ¶ > Dynamic table refresh modes ¶\nContent:\n**Incremental refresh mode:** This mode analyzes the dynamic table’s query and calculates changes since the last refresh. It then merges these\nchanges into the table. **Full refresh mode:** This mode executes the dynamic table’s query and completely replaces the previously materialized results.\nSection Title: Understanding dynamic table initialization and refresh ¶ > Dynamic table refresh modes ¶\nContent:\nFor guidance on when to use incremental refresh versus full refresh, see Choose a refresh mode .\nTo check which refresh mode an existing dynamic table uses, see Refresh mode .\nImportant\nDynamic tables in incremental refresh mode can’t be downstream from dynamic tables with full refresh mode. This is because\nincremental refresh mode is incompatible with the complete row changes that occur during each refresh of an upstream full\nrefresh table.\nSection Title: ... > How data is refreshed when a dynamic table depends on other dynamic tables ¶\nContent:\nWhen a dynamic table’s lag is set as a time measure, the automated refresh process schedules refreshes to best meet the target lag times.\nIn order to keep data consistent in cases when one dynamic table depends on another , the\nprocess refreshes all dynamic tables in an account at compatible times. The timing of less frequent refreshes coincides with the timing of\nmore frequent refreshes. If refreshes take too long, the scheduler may skip refreshes to try to stay up to date. However, snapshot isolation\nis preserved.\nFor example, suppose that dynamic table `DT1` has a target lag of two minutes and queries dynamic table `DT2` , which has a target lag of\none minute. The process might determine that `DT1` should be refreshed every 96 seconds, and `DT2` every 48 seconds. As a result, the\nprocess might apply the following schedule:\nSection Title: ... > How data is refreshed when a dynamic table depends on other dynamic tables ¶\nContent:\n| Specific Point in Time | Dynamic Tables Refreshed |\n| 2022-12-01 00:00:00 | DT1, DT2 |\n| 2022-12-01 00:00:48 | DT2 |\n| 2022-12-01 00:01:36 | DT1, DT2 |\n| 2022-12-01 00:02:24 | DT2 |\nSection Title: ... > How data is refreshed when a dynamic table depends on other dynamic tables ¶\nContent:\nThe target lag of a dynamic table can’t be shorter than the target lag of the dynamic tables it depends on. For example, suppose that:\n`DT1` queries dynamic tables `DT2` and `DT3` .\n`DT2` has a target lag of five minutes.\n`DT3` has a target lag of one minute.\nThis means that the target lag time for `DT1` must not be shorter than five minutes (that is, not shorter than the longer of the lag times\nfor `DT2` and `DT3` ).\nIf you set the lag for `DT1` to five minutes, the process sets up a refresh schedule with these goals:\nRefresh `DT3` often enough to keep its lag below one minute.\nRefresh `DT1` and `DT2` together and often enough to keep their lags below five minutes.\nEnsure that the refresh for `DT1` and `DT2` coincides with a refresh of `DT3` to ensure snapshot isolation.\nImportant\nSection Title: ... > How data is refreshed when a dynamic table depends on other dynamic tables ¶\nContent:\nDynamic tables in incremental refresh mode can’t be downstream from dynamic tables with full refresh mode. This is because\nincremental refresh mode is incompatible with the complete row changes that occur during each refresh of an upstream full\nrefresh table.\nSection Title: Understanding dynamic table initialization and refresh ¶ > ... > Snapshot isolation ¶\nContent:\nWhen a dynamic table refreshes, it ensures a consistent state by Time Traveling to the same data timestamp across all upstream dependencies.\nFor non-dynamic base tables, Time Travel works as usual, where it looks at the “wall-clock” commit time. This means that the contents of a\ndynamic table are always consistent with a “snapshot” of the data in the base tables.\nFor upstream dynamic tables, Snowflake looks up the specific table version tagged with that data timestamp. This ensures that downstream tables\nare always consistent with their ancestors. You don’t need to coordinate refresh schedules or worry about different lags; Snowflake automatically\naligns the snapshots to ensure data integrity across the pipeline.\nSection Title: Understanding dynamic table initialization and refresh ¶ > ... > Snapshot isolation ¶\nContent:\nSnapshot isolation isn’t guaranteed when you join multiple dynamic tables using a manual SELECT statement because ad hoc queries use the current\nversion of each table. Because each dynamic table commits its refresh independently, a manual join might capture different refresh states, even\nif the dynamic tables share the same target lag or an upstream refresh is delayed. This means the results might not reflect a single, consistent\nsnapshot of the base data.\nSection Title: ... > Understanding the effects of changes to columns in base tables ¶\nContent:\nWhen the underlying objects associated with a dynamic table change, the following behaviors apply:\nSection Title: ... > Understanding the effects of changes to columns in base tables ¶\nContent:\n| Change | Impact |\n| * New column added to the base table. |  |\nSection Title: ... > Understanding the effects of changes to columns in base tables ¶\nContent:\nExisting unused column removed in the base table. |None. If a new column is added to the base table or an unused column is deleted, no action occurs and refreshes continue as before. |\n|* Underlying base table is recreated with identical column names and types.\nUnderlying base table column is recreated with the same name and type.\nChanges to the policies on underlying base tables of dynamic tables with incremental refresh. |Reinitialization: The first refresh after recreation is initialization . |\n|* Changes to underlying base table for dynamic tables created with `SELECT *` from base table. |The dynamic table fails to refresh and must be recreated to respond to the change. |\n|* Changes to underlying base table for dynamic tables created with a column definition. |No impact to the dynamic table. |\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\nSection Title: ... > Understanding the effects of changes to columns in base tables ¶\nContent:\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nUnderstanding dynamic table initialization\nUnderstanding manual and scheduled refresh options\nDynamic table refresh modes\nHow data is refreshed when a dynamic table depends on other dynamic tables\nUnderstanding the effects of changes to columns in base tables"]},{"url":"https://medium.com/@pascalpfffle/optimizing-data-pipelines-the-strategic-use-of-snowflakes-dynamic-table-initialization-warehouse-66c712764219","title":"Optimizing Data Pipelines: The Strategic Use of Snowflake’s Dynamic Table INITIALIZATION_WAREHOUSE | by Pascal Pfäffle | Dec, 2025 | Medium","publish_date":"2025-12-15","excerpts":["Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nSearch\nSection Title: Optimizing Data Pipelines: The Strategic Use of Snowflake’s Dynamic Table `INITIALIZATION_WAREHOUSE`\nContent:\nPascal Pfäffle\n3 min read\n·\n1 day ago\n--\nListen\nShare\nPress enter or click to view image in full size\nSection Title: ... > Introduction\nContent:\nSnowflake’s Dynamic Tables (DTs) have emerged as a cornerstone for modern, automatically managed data pipelines. They materially simplify extract, transform, load (ETL) and extract, load, transform (ELT) processes by maintaining the result of a declarative SQL query in a continuously updated form. While the primary compute resource for routine updates is managed by the mandatory `WAREHOUSE` property, a critical, often underutilized feature exists for performance and cost control: the `**INITIALIZATION_WAREHOUSE**` . This optional property allows data architects to designate a separate, typically more powerful Virtual Warehouse specifically for the initial, and most resource-intensive, population of the Dynamic Table. Understanding its strategic application is key to building resilient and cost-optimized data solutions on Snowflake.\nSection Title: ... > The Strategic Imperative: Decoupling Initial Load from Routine Updates\nContent:\nThe lifecycle of a Dynamic Table presents a distinct challenge: the initial load, or **initialization** , is fundamentally different from subsequent incremental refreshes.\nSection Title: ... > 1. The Resource-Heavy Nature of Initialization\nContent:\nWhen a Dynamic Table is first created, it must perform a full computation based on its defining query, often involving extensive scans, complex joins, and aggregations across large source datasets. This operation is typically **I/O and CPU-intensive** , demanding significant compute power and potentially running for hours. Conversely, routine, incremental refreshes are generally lightweight, re-calculating only the data that has changed upstream.\nSection Title: ... > 2. The Solution: Dedicated Compute for High-Demand Tasks\nContent:\nThe `INITIALIZATION_WAREHOUSE` property directly addresses this dichotomy. By setting this property, you instruct Snowflake to use a designated warehouse only for the following scenarios:\n**Initial Population:** The very first time the DT is built.\n**Full Reinitialization:** Any time a full refresh is triggered, either manually ( `ALTER DYNAMIC TABLE ... REFRESH` ) or automatically (e.g., following a non-compatible schema change in an upstream object).\nSection Title: ... > Example Syntax for Optimal Sizing\nContent:\nThis syntax highlights the separation of concerns:\n```\nCREATE DYNAMIC TABLE sales_agg_dt  \nTARGET_LAG = '10 MINUTES'  \nWAREHOUSE = 'S_INCREMENTAL_WH' -- Small, cost-effective for routine updates  \nINITIALIZATION_WAREHOUSE = 'XL_FULL_LOAD_WH' -- Large, dedicated for fast initial completion  \nAS  \nSELECT ... <complex query on large base tables>\n```\nSection Title: ... > Key Benefits of Isolation\nContent:\n**Performance and Time-to-Value:** A larger warehouse (e.g., `XL` or `2XL` ) can dramatically reduce the duration of the initial load, accelerating time-to-value for the data consumer.\n**Cost Efficiency:** Once the initialization is complete, the dedicated, expensive warehouse is automatically suspended, and all future activity switches to the smaller, cheaper `WAREHOUSE` specified for incremental refreshes. This ensures the organization pays for premium compute only when the high-demand task requires it.\n**Contention Mitigation:** Separating the initialization ensures that the resource-heavy operation does not contend with or starve other critical workloads that rely on the smaller, standard warehouse. This improves the overall stability and predictability of the data platform.\nSection Title: ... > Conclusion\nContent:\nThe `INITIALIZATION_WAREHOUSE` property is more than just a configuration setting; it is a vital architectural tool for optimizing the performance and economy of Dynamic Tables in Snowflake. By recognizing that the initial build phase has distinct compute requirements, engineers can leverage a temporary boost of power without incurring the ongoing cost of an oversized default warehouse. Implementing this strategy leads to faster initial deployment, lower total cost of ownership (TCO), and a more robust, stable data pipeline ecosystem.\nSnowflake\nData Engineering\n--\n--\nSection Title: ... > Written by Pascal Pfäffle\nContent:\n270 followers\n· 26 following\nAs a data enthusiast, I am primarily involved in projects focused on Data Management and Data Engineering with a strong emphasis on Snowflake.\nSection Title: ... > No responses yet\nContent:\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--66c712764219---------------------------------------)\n[Help](https://help.medium.com/hc/en-us?source=post_page-----66c712764219---------------------------------------)\n[Status](https://status.medium.com/?source=post_page-----66c712764219---------------------------------------)\nAbout\nCareers\nPress\n[Blog](https://blog.medium.com/?source=post_page-----66c712764219---------------------------------------)\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----66c712764219---------------------------------------)\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----66c712764219---------------------------------------)\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----66c712764219---------------------------------------)\nSection Title: ... > No responses yet\nContent:\n[Text to speech](https://speechify.com/medium?source=post_page-----66c712764219---------------------------------------)"]},{"url":"https://www.montecarlodata.com/blog-snowflake-cost-optimization/","title":"5 Snowflake Cost Optimization Techniques You Should Know","publish_date":"2025-07-06","excerpts":["Section Title: ... > Snowflake pricing: how the model works\nContent:\nTo understand Snowflake cost optimization strategies and best practices, you first need to know how the consumption based pricing model works.\n[Actual pricing](https://www.snowflake.com/pricing/) depends on a few different variables such as the cloud provider, region, plan type, services, and more. Since we’re not getting too crazy, we will oversimplify a bit.\nEssentially, your Snowflake cost is based on the actual monthly usage of three separate items: storage, compute, and cloud services. You will be charged for any [Snowflake serverless features](https://docs.snowflake.com/en/user-guide/admin-serverless-billing.html) you use as well.\nExample Snowflake pricing in the AWS – US East region. Image from Snowflake.com.\nSection Title: ... > Snowflake pricing: how the model works\nContent:\nMost customers pay on-demand with Snowflake credits, but you can pre-purchase capacity as well. For example, the on-demand pricing in the AWS-US East region as of April 2022 is $40 per terabyte per month with Snowflake credits priced at $2.00, $3.00, or $4.00 respectively depending on which plan (Standard, Enterprise, Business Critical) was selected.\nSnowflake also offers dynamic pricing. Snowflake dynamic pricing will **adjust prices in real time to help retailers maximize the profitability of each unit sold** , based on inventory levels, competitors’ pricing, and current trends in consumer demand.\nThis can seem a bit abstract and intimidating at first glance. Converting your average workload into expected Snowflake credit spend takes a bit of math (and what is the conversion of Snowflake credits to Shrute bucks?).\n ... \nSection Title: ... > Is there a Snowflake object that does not consume any storage costs?\nContent:\nYou can get started with Snowflake for free with a [trial account](https://docs.snowflake.com/en/user-guide/admin-trial-account) , and the balance of your free usage decreases as you consume credits to use compute and accrue storage costs.\nIn addition, temporary and transient tables do not incur the same fees as standard permanent tables. This helps to manage the storage costs associated with time travel and fail-safe.\nCourtesy of [Snowflake.](https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs)\nAll of this is to say that Snowflake pricing is mainly driven by credit consumption, which is mainly driven by storage and compute; **which is mainly driven by the amount of tables in your environment, the SQL queries being run on those tables, and the sizes of your data warehouses** .\nSection Title: ... > Is there a Snowflake object that does not consume any storage costs?\nContent:\nThat is why our Snowflake cost optimization strategy will focus on optimizing those three areas by leveraging native features, best practices, and other solutions.\nSection Title: ... > Step 1: Snowflake warehouse size optimization\nContent:\nSnowflake virtual warehouses come in 10 sizes. The larger the warehouse the more credits it consumes per hour of actively running queries.\nImage from Snowflake [pricing guide](https://www.snowflake.com/pricing-page-registration-page/) .\nYou might assume the best Snowflake cost optimization strategy would be to keep your warehouse as small as possible, but that’s not necessarily true. That’s because the larger data warehouses also run queries faster.\nThere isn’t a magical formula for how to optimize warehouse size for your typical workloads– **it’s a process of trial and error** . That being said, there are some Snowflake cost optimization best practices related to rightsizing your warehouse.\n ... \nSection Title: ... > Identifying most expensive, deteriorating, and heavy queries\nContent:\nThis includes helpful Insight Reports such as **deteriorating queries** which automatically identifies queries taking longer and longer to complete indicating scale or performance issues that may precede an incident and **heavy queries** which automatically identifies the top 5 queries with the longest runtime and most bytes scanned. Advanced [SQL anomaly detection](https://www.montecarlodata.com/blog-how-to-build-your-own-data-anomaly-detectors-using-sql/) can also flag queries that deviate from their historical performance patterns, helping you catch optimization opportunities before they become cost problems.\nSection Title: ... > Step 3: Snowflake table optimization\nContent:\nQuerying large tables can get expensive. Deprecating unused tables is not only a helpful Snowflake cost optimization strategy, it can make it easier for your data consumers to find what they need.\nSection Title: ... > Search optimization service vs table clustering, and materialized views\nContent:\nAll three of these native Snowflake features can make querying large tables more efficient in both time and credit consumption. They all work slightly differently and are best used for different use cases.\nMicro-partitions happen naturally in the order data is ingested into Snowflake, but over time that might not occur in a way that is optimized to how the data is routinely queried. [**Table clustering**](https://docs.snowflake.com/en/user-guide/tables-clustering-keys.html) solves that by defining a cluster key that co-locates similar rows in the same micro-partitions.\nThis is another area where you don’t want to get crazy and cluster all, or even most, of your tables–it won’t be cost efficient. Large tables (say 3 terabytes or more) that are not updated often, but have frequent queries selecting a smaller set of rows or that sort the data, are primary candidates for table clustering.\nSection Title: ... > Search optimization service vs table clustering, and materialized views\nContent:\nIf you are [migrating to Snowflake](https://www.montecarlodata.com/blog-how-to-migrate-to-snowflake-like-a-boss/) , you will want to use a tool like Monte Carlo to document and analyze your current schema and lineage to make sure its structure and corresponding upstream sources and downstream consumers make sense for how your data will be partitioned and used once migrated to Snowflake. Optimizing your [data warehouse schema](https://www.montecarlodata.com/blog-data-warehouse-schemas/) design upfront can prevent costly restructuring later and ensure your clustering keys align with actual query patterns. It can also help ensure you are selecting the appropriate cluster keys.\nSection Title: ... > Search optimization service vs table clustering, and materialized views\nContent:\nNow let’s say you have a large table with billions of rows where the entire table gets queried often for specific information (or at least parts that aren’t clustered). Think of this use case as needing to get all the associated information around one particular object–say all the financial transactions for a given user or all the coupon usage on a specific product.\nSearch optimization service use cases for Snowflake cost optimization. [Source](https://www.snowflake.com/blog/now-generally-available-snowflakes-search-optimization-service-accelerates-queries-dramatically/) .\nThis is where the newer [**search optimization service**](https://docs.snowflake.com/en/user-guide/search-optimization-service.html) can come in handy, especially for queries running more than tens of seconds and returning at least 100k-200k values.\nSection Title: ... > Search optimization service vs table clustering, and materialized views\nContent:\nBefore you ask, yes this is technically Snowflake query optimization, but we’ve placed it in this section because the action is taken on the table. When you add this serverless feature to the table, the maintenance service creates the optimal search path across the data needed to perform the lookups.\n**A materialized view,** available in the Enterprise Edition, is a pre-computed data set stored for later use. This feature is also an effective Snowflake cost optimization strategy when running frequent or complex queries on a subsection of large data sets.\n[Snowflake recommends leveraging a materialized view when:](https://docs.snowflake.com/en/user-guide/views-materialized.html)\nSection Title: ... > Search optimization service vs table clustering, and materialized views\nContent:\nThe query results from the view don’t change often. This almost always means that the underlying/base table for the view doesn’t change often, or at least that the subset of base table rows used in the materialized view don’t change often.\nThe results of the view are used often (typically significantly more often than the query results change).\nThe query consumes a lot of resources. Typically, this means that the query consumes a lot of processing time or credits, but it could also mean that the query consumes a lot of storage space for intermediate results.\nMonte Carlo has recently **introduced key field** and **unused field reports** that can also be used to help prune overly large tables with significant query history.\nSection Title: 5 Simple Steps For Snowflake Cost Optimization Without Getting Too Crazy > ... > Unused tables\nContent:\nDisposing of unused data tables would seem to be an easy Snowflake cost optimization win, but data engineers tend to be data hoarders.\nPart of it is superstition that the minute a table is deleted, a data consumer will demand access to it. But the biggest reason is that table importance and **usage is often hazy** as a result of the all too common chasm between data engineer and data consumer.\nWith Monte Carlo Insights Reports you can export a CSV report for table clean up suggestions or surface them directly in Snowflake.\nAutomated discovery and lineage features in Monte Carlo can help add the clarity the data team needs to gather the courage to dispose. Monte Carlo also has a number of Insight reports such as **key assets** , **table read/write activity** , and **clean up suggestions** that can identify your most important tables based on usage.\n ... \nSection Title: ... > Step 4: Snowflake data lifecycle and retention optimization\nContent:\nHere’s where many teams miss significant cost savings opportunities. By default, Snowflake retains 1-day time-travel and 7-day fail-safe for all tables, which can dramatically increase storage costs, sometimes up to 90× more than the base table size for historical copies.\nUnderstanding the cost implications of Snowflake’s data retention features is crucial for cost optimization without getting too crazy about data recovery capabilities. Standard tables in Snowflake include both [Time Travel](https://docs.snowflake.com/en/user-guide/data-time-travel) , which can range from 1-90 days, and [Fail-safe](https://docs.snowflake.com/en/user-guide/data-failsafe) at 7 days, which means you’re paying for multiple copies of your data. But not every table needs this level of protection.\nSection Title: ... > Time-travel vs transient vs temporary tables\nContent:\n[Transient tables](https://docs.snowflake.com/en/user-guide/tables-temp-transient) include Time Travel but no Fail-safe, cutting storage costs significantly. These are perfect for staging tables, temporary analytics, or frequently refreshed data where you don’t need extensive recovery options. You can create new tables as transient or convert existing tables to transient status.\n[Temporary tables](https://docs.snowflake.com/en/user-guide/tables-temp-transient) also include Time Travel but no Fail-safe, and are automatically dropped at session end. These are ideal for intermediate processing steps within ETL pipelines where the data is only needed temporarily.\nSection Title: ... > Time-travel vs transient vs temporary tables\nContent:\nYou can also [adjust your Time Travel retention](https://docs.snowflake.com/en/user-guide/data-time-travel) based on actual recovery needs rather than accepting the default. For frequently changing staging tables, you might reduce retention to just 1 day, while critical business tables might warrant 30 days or more depending on your enterprise plan.\n ... \nSection Title: ... > Data observability for proactive cost control\nContent:\nTraditional monitoring can still miss emerging cost issues until they’ve already impacted your budget. Schema changes, new data sources, or pipeline modifications can gradually increase costs without triggering immediate alerts. This is where data observability tools like Monte Carlo provide an early warning system by monitoring data patterns, volume trends, and quality metrics that often precede cost spikes.\nThe platform can detect when data volumes unexpectedly increase, when new tables consume more storage than anticipated, or when query patterns shift in ways that drive up compute costs. This proactive approach helps teams address cost issues at their source rather than just managing the symptoms.\n ... \nSection Title: ... > Take your Snowflake cost optimization journey one step at a time\nContent:\nThe business adage goes, “Do you want it done fast, good, or cheap? Pick two.”\nAs a data professional moving slowly is rarely an option, and there is no compromising on data quality–bad data is too expensive no matter the price.\nAs a result Snowflake cost optimization sometimes gets the short shrift–and that’s OK. Snowflake is an efficient technology and getting more cost effective every year.\nHowever, following these Snowflake cost optimization best practices can ensure you minimize waste and put your resources to good use–just don’t get too crazy about it.\n***Interested in how data observability can help you spot inefficiencies in your data stack? Book a time to speak with us in the form below.***\nOur promise: we will show you the product.\nSearch\nSearch\nSection Title: 5 Simple Steps For Snowflake Cost Optimization Without Getting Too Crazy > Recommended for you\nContent:\n[### Snowflake Observability and 4 Reasons Data Teams Should Invest In It](https://www.montecarlodata.com/blog-snowflake-observability/) Read more\n[### Delivering End-to-End Data Trust with Snowflake and Monte Carlo](https://www.montecarlodata.com/blog-snowflake/) Read more\n[### The Top Snowflake Integrations Every Data Team Should Know](https://www.montecarlodata.com/blog-snowflake-integrations/) Read more\n[### The Best Snowflake Orchestration Tools](https://www.montecarlodata.com/blog-snowflake-orchestration-tools/) Read more\n[### 4 Snowflake Data Replication Best Practices That Will Save You Time and Money](https://www.montecarlodata.com/blog-4-snowflake-data-replication-best-practices-that-will-save-you-time-and-money/) Read more\n[### A New Horizon for Data Reliability With Monte Carlo and Snowflake](https://www.montecarlodata.com/blog-snowflake-horizon-partner) Read more"]},{"url":"https://docs.snowflake.com/en/user-guide/dynamic-tables-about","title":"Dynamic tables - Source: Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nGuides Data engineering Dynamic Tables\nSection Title: Dynamic tables ¶\nContent:\nDynamic tables are tables that automatically refresh based on a defined query and target freshness, simplifying data transformation and\npipeline management without requiring manual updates or custom scheduling.\nWhen you create a dynamic table, you define a query that specifies how data should be transformed from base objects. Snowflake handles the\nrefresh schedule of the dynamic table and updates the table automatically to reflect the changes made to the base objects based on the query.\nSection Title: Dynamic tables ¶ > Key considerations and general best practices ¶\nContent:\n**Immutability constraints** : Use immutability constraints to let you control dynamic table updates. The constraints keep specific rows static\nwhile enabling incremental updates to the rest of the table. They prevent unwanted changes to marked data while they let normal refreshes occur\nfor other parts of the table. For more information, see Use immutability constraints on dynamic tables .\n**Performance considerations:** Dynamic tables use incremental processing for workloads that support it, which can improve performance by\nrecomputing only the data that has changed, rather than performing a full refresh. For more information, see Best practices for optimizing dynamic table performance .\n**Break down complex dynamic tables:** Break your pipeline into smaller, focused dynamic tables to improve performance and simplify\ntroubleshooting. For more information, see Best practices for creating dynamic tables .\nSection Title: Dynamic tables ¶ > How dynamic tables work ¶\nContent:\nSnowflake runs the definition query specified in your CREATE DYNAMIC TABLE statement and your dynamic tables are updated through an automated\nrefresh process.\nThe following diagram shows how this process computes the changes made to the base objects and merges them into the dynamic table by using\ncompute resources associated with the table.\nSection Title: Dynamic tables ¶ > How dynamic tables work ¶ > Target lag ¶\nContent:\nUse *target lag* to set how fresh you want your data to be. Usually, the table data freshness won’t be more than that far behind the base table\ndata freshness. With target lag, you control how often the table refreshes and how up-to-date the data stays.\nFor more information, see Understanding dynamic table target lag .\nSection Title: Dynamic tables ¶ > How dynamic tables work ¶ > Dynamic table refresh ¶\nContent:\nDynamic tables aim to refresh within the target lag you specify. For example, a target lag of five minutes ensures that the data in the dynamic\ntable is no more than five minutes behind data updates to the base table. You set the refresh mode when you create the table and, afterward,\nrefreshes can happen on a schedule or manually.\nFor more information, see Understanding dynamic table initialization and refresh and Manually refresh dynamic tables .\nSection Title: Dynamic tables ¶ > When to use dynamic tables ¶\nContent:\nDynamic tables are ideal for the following scenarios:\nYou want to materialize query results without writing custom code.\nYou want to avoid manually tracking data dependencies and managing refresh schedules. Dynamic tables enable you to define pipeline outcomes\ndeclaratively, without managing transformation steps manually.\nYou want to chain together multiple tables for data transformations in a pipeline.\nYou don’t need fine-grained control over refresh schedules, and you only need to specify a target freshness for the pipeline. Snowflake\nhandles the orchestration of data refreshes, including scheduling and execution, based on your target freshness requirements.\nSection Title: Dynamic tables ¶ > When to use dynamic tables ¶ > Example use cases ¶\nContent:\n**Slowly changing dimensions (SCDs):** Dynamic tables can be used to implement Type 1 and Type 2 SCDs by reading from a change stream and\nusing window functions over per-record keys ordered by a change timestamp. This method handles insertions, deletions, and updates that occur\nout of order, simplifying the creation of SCDs. For more information, see [Slowly Changing Dimensions with Dynamic Tables](https://medium.com/snowflake/slowly-changing-dimensions-with-dynamic-tables-d0d76582ff31) .\n**Joins and aggregations:** You can use dynamic tables to incrementally precompute slow joins and aggregations to enable fast queries.\n**Batch to streaming transitions** : Dynamic tables support seamless transitions from batch to streaming with a single ALTER DYNAMIC TABLE\ncommand. You can control the refresh frequency in your pipeline to balance cost and data freshness.\nWas this page helpful?\nYes No\nSection Title: Dynamic tables ¶ > When to use dynamic tables ¶ > Example use cases ¶\nContent:\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nKey considerations and general best practices\nHow dynamic tables work\nWhen to use dynamic tables\nLanguage: **English**\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês\nSection Title: Dynamic tables ¶ > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\nSection Title: Dynamic tables ¶ > Privacy Preference Center > Your Privacy\nContent:\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nSection Title: Dynamic tables ¶ > Privacy Preference Center > Your Privacy > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Details‎\nSection Title: Dynamic tables ¶ > Privacy Preference Center > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details‎\nSection Title: Dynamic tables ¶ > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details‎\nSection Title: Dynamic tables ¶ > Privacy Preference Center > Your Privacy > Targeting Cookies\nContent:\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nCookies Details‎\nSection Title: Dynamic tables ¶ > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"]},{"url":"https://docs.snowflake.com/En/user-guide/tutorials/optimize-dynamic-table-performance","title":"Optimize dynamic table performance for SCD Type 1 workloads","excerpts":["Create a sample source table with product change data. · Build two SCD Type 1 dynamic tables: one with a suboptimal SQL pattern and one with an optimized pattern ..."]},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization","excerpts":["Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Business Impact > Overview > Consider cost as a design constraint\nContent:\nAt the ingestion layer, best practices include balancing latency versus\ncost by selecting appropriate services (e.g., Snowpipe, Snowpipe\nStreaming, or third-party tools) and choosing the right storage format\n(e.g., native tables, Iceberg). For transformations, design with\nfrequency versus SLA in mind to ensure data freshness matches the\nbusiness need. For analytics, apply schema design best practices such as\nthoughtful clustering key choices and pruning strategies to reduce\nconsumed credits. In distribution, optimize data transfer by monitoring\negress patterns and applying cost-saving practices like the [Snowflake Data Transfer Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) .\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n| Metric Category | Description | Key Metrics | Primary Data Sources |\n| Compute & query metrics | Understand the cost of query execution, warehouse consumption, and overall compute health. These are often the most dynamic and largest portion of your spend. | - Credits used: total credits by warehouse |  |\n| - Query performance: execution time, bytes scanned, compilation time, parameterized query hash |  |  |  |\n| - Warehouse health: % idle time, queueing, spilling, concurrency | - `ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY` (hourly warehouse credit usage) |  |  |\n| - `ACCOUNT_USAGE.QUERY_HISTORY` (detailed query metrics and associated warehouses) |  |  |  |\n| - Table access (stale/unused) | - `ACCOUNT_USAGE.TABLE_STORAGE_METRICS` |  |  |\n| - `ACCOUNT_USAGE.DATABASE_STORAGE_USAGE_HISTORY` |  |  |  |\n| - `ACCOUNT_USAGE.ACCESS_HISTORY` |  |  |  |\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n| Metric Category | Description | Key Metrics | Primary Data Sources |\n| Compute & query metrics | Understand the cost of query execution, warehouse consumption, and overall compute health. These are often the most dynamic and largest portion of your spend. | - Credits used: total credits by warehouse |  |\n| - Query performance: execution time, bytes scanned, compilation time, parameterized query hash |  |  |  |\n| - Warehouse health: % idle time, queueing, spilling, concurrency | - `ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY` (hourly warehouse credit usage) |  |  |\n| - `ACCOUNT_USAGE.QUERY_HISTORY` (detailed query metrics and associated warehouses) |  |  |  |\n| - `ORGANIZATION_USAGE.METERING_DAILY_HISTORY` |  |  |  |\n| - AI views such as `CORTEX_FUNCTIONS_USAGE_HISTORY` , `CORTEX_ANALYST_USAGE_HISTORY` , `DOCUMENT_AI_USAGE_HISTORY` |  |  |  |\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n| Metric Category | Description | Key Metrics | Primary Data Sources |\n| Compute & query metrics | Understand the cost of query execution, warehouse consumption, and overall compute health. These are often the most dynamic and largest portion of your spend. | - Credits used: total credits by warehouse |  |\n| - Query performance: execution time, bytes scanned, compilation time, parameterized query hash |  |  |  |\n| - Warehouse health: % idle time, queueing, spilling, concurrency | - `ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY` (hourly warehouse credit usage) |  |  |\n| - `ACCOUNT_USAGE.QUERY_HISTORY` (detailed query metrics and associated warehouses) |  |  |  |\n| - `ORGANIZATION_USAGE.RATE_SHEET_DAILY` |  |  |  |\n ... \nSection Title: Cost Optimization > Optimize > Overview > Leverage Managed Services\nContent:\n**Automatic Clustering** is a background process in Snowflake that\norganizes data within a table by sorting it according to predefined\ncolumns. This process is critical for optimizing query performance and\nreducing costs. Benefits include:\n**Improved query pruning:** By sorting data, automatic clustering\nenables more effective pruning in SQL WHERE clauses, meaning less data\nneeds to be scanned for a given query.\n**Faster joins:** Clustering also results in quicker and more\nefficient join operations.\n**Cost-efficient queries:** These benefits ultimately result in faster\nand more cost-effective query execution.\n**Considerations and Best Practices:**\n ... \nSection Title: Cost Optimization > Optimize > Overview > Leverage Managed Services\nContent:\n**Cost efficiency:** It is often cheaper to update a materialized view\nthan to repeatedly execute full scans of large, complex base tables.\n**Alternative table order:** MVs can provide an alternative table\norder for queries that do not align with your existing clustering\ndesign.\n**Considerations for MVs:**\n**Infrequently queried, frequently updated tables:** Avoid creating\nmaterialized views on tables that are frequently updated but rarely\nqueried, as automated maintenance costs will negate any potential\nquery cost savings.\n**Clustering differences:** Be aware of clustering differences between\nthe base table and the materialized view, as this can lead to high\nmaintenance costs.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Data storage types & lifecycle management\nContent:\nYou do not always need to define cluster keys for all tables (unlike\nmany other relational database management systems) if Snowflake's\nnatural data loading maintains consistent micro-partition min/max\nvalues relative to your query patterns. Additionally, you can disable\nAuto-Clustering on a table while keeping its cluster key definition\nwithout incurring extra costs. **Infrequently used materialized views and search optimization paths:** Materialized Views and search optimization paths can incur\nunnecessary storage and compute costs if they are no longer actively\nutilized. Materialized Views are most effective for stable data tables\nwith repeated complex aggregations or joins, while search optimization\nis designed for high-speed point lookup queries.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Limit data transfer\nContent:\n**Targeted replication:** If a full database replica is not\nrequired, use [replication groups](https://docs.snowflake.com/en/user-guide/account-replication-intro) to replicate only the necessary databases or schemas. This granular\napproach ensures you only pay for the data you absolutely need to\nmove. **Consider refresh cadence:** your frequency of refresh will affect\nthe amount of data that is replicated, since it acts like a snapshot\nrather than every incremental change. Incremental ETL practices are\nrecommended even more with data that is being replicated vs full\ntable reloads.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nSnowflake pipeline optimization is about designing and managing data\ningestion and transformation processes that are efficient,\ncost-effective, scalable, and low-maintenance, while balancing business\nvalue and SLAs (service levels for freshness and responsiveness). Key\nlevers include architecture patterns (truncate & load versus incremental\nloads), use of serverless managed services (e.g., Snowpipe, Dynamic\nTables), and auditing loading practices to maximize cost and performance\nbenefits.\n**Batch loading**\nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nThe COPY INTO\n( [table](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table) or [location](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location) )\ncommand is a foundational and flexible method for bulk data loading from\nan external stage into a Snowflake table. Its importance lies in its\nrole as a powerful, built-in tool for migrating large volumes of\nhistorical data or loading scheduled batch files. The best practice is\nto use COPY INTO for one-time or large batch data loading jobs, which\ncan then be supplemented with more continuous ingestion methods like\nSnowpipe for incremental data. Additional information regarding COPY\nINTO and general data loading best practices can be found in our\ndocumentation [here](https://docs.snowflake.com/user-guide/data-load-considerations) .\nSome additional best practices are outlined below.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\n**Best practices:** Schedule loads during off-peak or agreed “quiet”\nwindows. Consider transient or temporary tables to stage incoming\ndata, maximize parallelism during the load, and then do an atomic\nswap/rename to minimize downtime. Ensure that you carefully manage\nobject dependencies, constraints, and statistical metadata refresh\nafter reloads to ensure proper performance once data is loaded\nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\n**Incremental loads:** Incremental loads are best when datasets are\nlarge and a full reload is too costly, slow, or would create\nunacceptable latency. Change-data-capture (CDC), event streaming, or\nother means are available to reliably identify deltas (inserts,\nupdates, deletes). Additionally, in these cases, downstream consumers\nrequire near-continuous data availability or very low data latency and\nfreshness. **Best practices:** Design pipelines to deduplicate, merge, and\ncorrectly apply CDC deltas using staging tables, Streams, and merge\noperations. Ideally, you use Stream objects on source tables to\ntrack changes efficiently if utilizing Streams or Tasks, and use\nTasks to automate processing. Dynamic tables are also an option;\nplease see below. Additionally, for batch files: organize files by\npartitioned paths and use file loading patterns that enable max\nparallelism (see COPY INTO guidance below).\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nA great example of truncate & load versus incremental can be seen in\nrefresh strategies for [Dynamic Tables (DTs)](https://docs.snowflake.com/en/user-guide/dynamic-tables-about) .\nThey are also a cost-effective and low-maintenance way to maintain data\npipelines. Dynamic tables provide a powerful, automated way to build\ncontinuous data transformation pipelines with SQL, eliminating the need\nfor manual task orchestration that was historically architected with\nstreams & tasks in Snowflake. [Streams & tasks](https://docs.snowflake.com/en/user-guide/data-pipelines-intro) still have their uses, but general guidance and ease of use see more\nSnowflake users leaning towards DTs for automated data pipelines since\nthe pipeline definitions are defined in one object or in a chain of\nobjects.\nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nThe [key concepts of dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh) are defined in our documentation. However, best practices and\ndetermining when to use DTs versus other methods of pipeline tooling in\nSnowflake still warrant discussion, and are compared in Snowflake’s [documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison) .\nIn addition to Snowflake’s published [best practices](https://docs.snowflake.com/en/user-guide/dynamic-table-performance-guide) ,\nconsider the following\nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\n**Default to AUTO refresh** : Override only as needed for predictable\nSLAs. **Keep incremental refresh-friendly queries simple:** Avoid complex\nnested joins, CTEs, and limit the number of outer joins per DT. The\nintroduction of complexity for incremental refresh may result in\nlonger times for execution, which in turn could force Snowflake in\nAUTO to perform a full refresh rather than incremental. **Incremental refresh is optimal** when less than 5% of the rows\nchange between refresh cycles, and source tables are well clustered by\nrelevant keys. **For very complex/large transformations:** Chain multiple DTs for\nbetter incrementalization, rather than building one massive DT. **Monitor actual lag and refresh metrics** to adjust lag or warehouse\nsizing as cost and response time needs evolve.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nMore information on dynamic tables versus streams & tasks versus\nmaterialized views can be found in the Snowflake documentation [here](https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison) .\n**Table pruning optimization**\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nTo determine tables that can most benefit from re-ordering how data is\nstored, you can review Snowflake’s [best practice](https://medium.com/snowflake/supercharging-snowflake-pruning-using-new-account-usage-views-52530b24bf2e) on how to analyze the TABLE_QUERY_PRUNING_HISTORY and\nCOLUMN_QUERY_PRUNING_HISTORY account usage views. Fundamentally,\nreducing the percentage of partitions in each table pruned to the\npercentage of rows returned in a query will lead to the most optimized\ncost and performance for any given workload.\nA table’s Ideal pruning state is scanning the same % of rows matched as\npartitions read, minimizing unused read rows.\n**Warehouse optimization**\nWarehouse concurrency, type, and sizing can impact the execution\nperformance and cost of queries within Snowflake. Review the compute\noptimization section for more information into the tuning of the\nwarehouse and its effect on cost and performance."]}],"usage":[{"name":"sku_search","count":1}]}
{
  "search_id": "search_73ddec59b6c5488e837e5b4e02152316",
  "results": [
    {
      "url": "http://docs.snowflake.com",
      "title": "Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nSection Title: Snowflake Documentation\nContent:\nStreamline your Snowflake journey with comprehensive documentation and learning resources\nSection Title: Snowflake Documentation > Featured Resources\nContent:\nDive into our top picks\nuser guide Connecting to Snowflake Learn about the applications and tools that you can use to access Snowflake\nuser guide Working with Virtual Warehouses Learn how to create and manage warehouses, which are used to process queries\nuser guide Databases, Tables and Views Learn how to create and manage databases, tables, and views for storing and accessing your data\nuser guide Load Data into Snowflake Learn about the different options for getting data into Snowflake and setting up a pipeline to transform your data\nSection Title: Snowflake Documentation > Getting Started\nContent:\nBasic information and instructions for first-time users of Snowflake.\nGetting a Trial Account\nSnowflake in 20 Minutes\nKey Concepts and Architecture\nConnecting to Snowflake\nSee all\nSection Title: Snowflake Documentation > User Guides\nContent:\nInstructions on performing various Snowflake operations.\nUnderstanding & Using Time Travel\nWorking with Temporary and Transient Tables\nWorking with Materialized Views\nAbout Snowflake Openflow\nSee all\nSection Title: Snowflake Documentation > Developer Guides\nContent:\nWrite applications that extend Snowflake, act as a client, or act as an integrating component.\nSnowflake Native App Framework\nSnowpark API\nUser-defined Functions (UDFs) and Stored Procedures\nDrivers\nSee all\nSection Title: Snowflake Documentation > Reference\nContent:\nReference for SQL data types, SQL commands, SQL functions, SQL classes, scripting, views, and other areas\nSQL Data Types\nSQL Commands\nSQL Functions\nSQL Classes\nSnowflake Scripting\nSee all\nSection Title: Snowflake Documentation > Tutorials\nContent:\nTutorials to help you learn the basics of using Snowflake\nSnowflake in 20 Minutes\nBulk Loading from a Local File System\nBulk Loading from Amazon S3 Using COPY\nSee all\nSection Title: Snowflake Documentation > Releases\nContent:\nOverview of the new features, enhancements, and important fixes introduced in the most recent releases of Snowflake.\nNew features\nBehavior change log\nSnowflake connector, driver, and library monthly releases\nSee all\nSection Title: Snowflake Documentation > What's New\nContent:\nNew features, enhancements, and important behavior changes introduced in the most recent Snowflake releases.\nSee all\nSection Title: Snowflake Documentation > What's New > Recent Release Highlights\nContent:\nOn September 30th, 2025 Declarative Sharing - Preview Snowflake is pleased to announce the preview of Declarative Sharing. Declarative Sharing in the Snowflake Native App Framework enables providers to share and sell data products, and to enhance those apps by including notebooks that help Snowflake consumers visualize data.\nOn June 2nd, 2025 Snowpipe Streaming with High-Performance Architecture Snowflake is pleased to announce the preview of the Snowpipe Streaming high-performance architecture. This next-generation implementation delivers significantly enhanced throughput and optimized streaming performance with a predictable, throughput-based pricing model.\nOn May 5th, 2025 Release channels for Snowflake Native Apps\nOn May 23rd, 2025 Search optimization: Support for Apache Iceberg\u2122 tables\nOn May 23rd, 2025 Query Acceleration Service: Support for Apache Iceberg\u2122 tables\nSection Title: Snowflake Documentation > What's New > Recent Release Highlights\nContent:\nOn May 23rd, 2025 Triggered tasks: Support for streams hosted on directory tables and data shares\nSection Title: Snowflake Documentation > What's New > Announcements\nContent:\nStay informed and discover the latest updates, improvements, and enhancements\nBehavior Changes Behavior changes that may impact your usage\nPerformance Improvements All features that can improve performance and make queries run faster\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-understanding-compute",
      "title": "Understanding compute cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nEN\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic tables\nStreams and tasks\nRow timestamps\ndbt Projects on Snowflake\nData Unloading\nStorage lifecycle policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Understanding cost Understanding compute cost\nSection Title: Understanding compute cost \u00b6\nContent:\nCompute costs represent credits used for:\nVirtual Warehouse compute \u2014 Virtual warehouses consume credits as they execute queries, load\ndata and perform other DML operations. Virtual Warehouses are user-managed , which means you can directly\ncontrol credit consumption of these resources.\nServerless compute \u2014\nServerless features use compute resources that are managed by Snowflake instead of using virtual warehouses.\nCompute pools \u2014 Compute pools provide the compute resources for Snowpark Container Services.\nCloud Services compute \u2014 Cloud Services is the layer of the Snowflake architecture that\nperforms services that tie together all the different components of Snowflake to process user requests, login, query display, and more.\nCloud Services compute resources are managed by Snowflake.\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nA virtual warehouse is one or more clusters of compute resources that enable executing queries, loading data, and performing other DML\noperations. The web interface and other features use warehouses, such as Cross-Cloud Auto-Fulfillment or display information in dashboards.\nSnowflake credits are used to pay for the processing time used by each virtual warehouse.\nSnowflake credits are charged based on the number of virtual warehouses you use, how long they run, and their size.\nWarehouses come in many sizes. In this table, the size specifies the compute resources per cluster available to the warehouse.\nEach increase in size to the next larger warehouse approximately doubles the computing power and the number of credits billed per\nfull hour that the warehouse runs.\nFor information on credit consumption, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nImportant\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nWarehouses are only billed for credit usage while running. When a warehouse is suspended, it does not use any credits.\nThe credit numbers shown above are for a full hour of usage; however, credits are billed per-second, with a 60-second (i.e. 1-minute)\nminimum:\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nEach time a warehouse is started or resumed , the warehouse is billed for 1 minute\u2019s worth of usage based on the hourly\nrate shown above. Each time a warehouse is resized to a larger size, the warehouse is billed for 1 minute\u2019s worth of usage; however, the number\nof credits billed are only for the additional compute resources that are provisioned. For example, resizing from Small\n(2 credits/hour) to Medium (4 credits/hour) results in billing charges for 1 minute\u2019s worth of 2 additional credits. After 1 minute, all subsequent billing is per-second as long as the warehouse runs continuously. Suspending and then resuming a warehouse within the first minute results in multiple charges because the 1-minute minimum starts\nover each time a warehouse is resumed.\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nResizing a warehouse from 5X-Large or 6X-Large to 4X-Large (or smaller) results in a brief period during which the warehouse is\nbilled for both the new compute resources and the old resources while the old resources are quiesced.\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nFor more information on warehouses in general, see Overview of warehouses and Warehouse considerations .\nTo learn how to view the historical cost of consuming compute resources with virtual warehouses, see Exploring compute cost .\nSection Title: Understanding compute cost \u00b6 > Serverless credit usage \u00b6\nContent:\nServerless credit usage is the result of features relying on compute resources provided by Snowflake rather than user-managed\nvirtual warehouses. These compute resources are automatically resized and scaled up or down by Snowflake as required for each workload.\nFor these serverless features, which usually require continuous and/or maintenance operations, this model is more efficient, allowing\nSnowflake to charge based on the time spent using the resources. In contrast, user-managed virtual warehouses consume credits while running,\nregardless of whether they are performing any work, which may cause them to be overutilized or sit idle.\nSection Title: Understanding compute cost \u00b6 > Serverless credit usage \u00b6\nContent:\nCharges for serverless features are calculated based on total usage of snowflake-managed compute resources measured in *compute-hours* .\nCompute-Hours are calculated on a per second basis, rounded up to the nearest whole second. The number of credits consumed per compute\nhour varies depending on the serverless feature.\nTo learn how many credits are consumed by a serverless feature, refer to the \u201cServerless Feature Credit Table\u201d in the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nCharges for the use of a serverless feature appear on your bill as an individual line item. Charges for both Snowflake-managed compute\nresources and Cloud Services appear as a single line item for that serverless feature.\nTo learn how to view the historical cost of using serverless compute resources, see Exploring compute cost .\nSection Title: Understanding compute cost \u00b6 > Compute pool credit usage \u00b6\nContent:\nSnowpark Container Services uses compute pools to run its jobs and services.\nA compute pool is a collection of one or more virtual machine (VM) nodes. The number and type of these nodes determine how many credits the\njob or service consumes as it uses the compute pool.\nFor more information about the cost of compute pools, including how to monitor these costs, see Compute pool cost .\nSection Title: Understanding compute cost \u00b6 > Cloud service credit usage \u00b6\nContent:\nThe cloud services layer of the Snowflake architecture is a collection of services that coordinate activities across Snowflake.\nThis layer authenticates users, enforces security, performs query compilation and optimization, handles request query caching, and more.\nCloud services tie together all of the different components of Snowflake, including supporting the use of virtual warehouses.\nThe cloud services layer is constructed of stateless compute resources, running across multiple availability zones and using a highly\navailable, distributed metadata store for global state management. The cloud services layer runs on compute instances provisioned by\nSnowflake from the cloud provider.\nSimilar to virtual warehouse usage, Snowflake credits are used to pay for the usage of the cloud\nservices.\nSection Title: Understanding compute cost \u00b6 > Cloud service credit usage \u00b6\nContent:\nSnowflake Marketplace calculates compute costs for listing auto-fulfillment to VPS regions by using VPS rates. For details on VPS rates, see [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nSection Title: Understanding compute cost \u00b6 > ... > Understanding billing for cloud services usage \u00b6\nContent:\nUsage for cloud services is charged only if the daily consumption of cloud services exceeds 10% of the daily usage of virtual warehouses.\nThe charge is calculated daily (in the UTC time zone). This ensures that the 10% adjustment is accurately applied each day, at the credit\nprice for that day.\nKeep the following in mind:\nSection Title: Understanding compute cost \u00b6 > ... > Understanding billing for cloud services usage \u00b6\nContent:\nServerless compute does not factor into the 10% adjustment for cloud services.\nThe 10% adjustment for cloud services is calculated daily (in the UTC time zone) by multiplying daily warehouse usage by 10%.\nThe adjustment on the monthly usage statement is equal to the sum of these daily calculations.\nIf cloud services consumption is less than 10% of warehouse compute credits on a given day, then the adjustment for that day is equal to\nthe cloud services used by your account. The daily adjustment never exceeds actual cloud services usage for that day. Thus, the total\nmonthly adjustment may be significantly less than 10%.\nFor example:\nSection Title: Understanding compute cost \u00b6 > ... > Understanding billing for cloud services usage \u00b6\nContent:\n| Date | Compute Credits Used (Warehouses only) | Cloud Services Credits Used | Credit Adjustment for Cloud Services (Lesser of 10% of Compute or Cloud Services) | Credits Billed (Sum of Compute, Cloud Services, and Adjustment) |\n| Nov 1 | 100 | 20 | -10 | 110 |\n| Nov 2 | 120 | 10 | -10 | 120 |\n| Nov 3 | 80 | 5 | -5 | 80 |\n| Nov 4 | 100 | 13 | -10 | 103 |\n| **Total** | **400** | **48** | **-35** | **413** |\nSection Title: Understanding compute cost \u00b6 > Cloud service credit usage \u00b6 > More about cloud services \u00b6\nContent:\nTo learn how to view the historical cost of consuming cloud services resources, see Exploring compute cost , which\nincludes sample queries you can run to see how much of cloud services consumption was\nactually billed and which queries and warehouses have the highest cloud services usage.\nTo learn about patterns that drive cloud services consumption and ways that you might be able to reduce that consumption, see Optimizing cloud services for cost .\nSection Title: Understanding compute cost \u00b6 > What are credits? \u00b6\nContent:\nSnowflake credits are used to pay for the consumption of resources on Snowflake. A Snowflake credit is a unit of measure, and it is\nconsumed only when a customer is using resources, such as when a virtual warehouse is running, the cloud services layer is performing work,\nor serverless features are used.\n**Next Topic**\n* Exploring compute cost\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nVirtual warehouse credit usage\nServerless credit usage\nCompute pool credit usage\nCloud service credit usage\nWhat are credits?\nRelated content\nUnderstanding overall cost\nExploring compute cost"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-exploring-compute",
      "title": "Exploring compute cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Visibility Exploring cost Exploring compute cost\nSection Title: Exploring compute cost \u00b6\nContent:\nTotal compute cost consists of the overall use of:\nVirtual warehouses (user-managed compute resources)\nServerless features such as Automatic Clustering and Snowpipe that use Snowflake-managed compute resources\nCloud services layer of the Snowflake architecture\nvCPU usage for Openflow BYOC cost and scaling considerations and Openflow Snowflake Deployment cost and scaling considerations .\nSee Openflow components for more information about Openflow components including runtimes.\nThis topic describes how to gain insight into historical compute costs using Snowsight , or by writing queries against views in\nthe ACCOUNT_USAGE and ORGANIZATION_USAGE schemas.\nSnowsight allows you to quickly and easily obtain information about cost from a visual dashboard. Queries against the usage views\nallow you to drill down into cost data and can help generate custom reports and dashboards.\nSection Title: Exploring compute cost \u00b6\nContent:\nIf you need more information about how compute costs are incurred, refer to Understanding compute cost .\nNote\nThe cloud services layer consumes credits, but not all of those credits are actually billed. Usage for cloud services is charged only if\nthe daily consumption of cloud services exceeds 10% of the daily usage of virtual warehouses. Snowsight and a majority of views\nshow the total number of credits consumed by warehouses, serverless features, and cloud services without accounting for this daily\nadjustment to cloud services.\nTo determine how many credits were actually billed for compute costs, run queries against the METERING_DAILY_HISTORY view .\n ... \nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > General cost views \u00b6\nContent:\nThe following views contain information related to the compute costs of all Snowflake features. You can focus on a particular feature by filtering on the `service_type` column.\nFor additional views that focus on the cost of a specific feature, see Feature-specific cost views .\nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > General cost views \u00b6\nContent:\n| View | Compute resource | Description | Schema |\n| METERING_DAILY_HISTORY | Warehouses |  |  |\n ... \nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > Feature-specific cost views \u00b6\nContent:\nThe following views that are dedicated to the usage and cost information for a specific feature.\nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > Feature-specific cost views \u00b6\nContent:\n| View | Compute resource | Description |\n| APPLICATION_DAILY_USAGE_HISTORY | Warehouses |  |\nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > Feature-specific cost views \u00b6\nContent:\nServerless\n ... \nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > Feature-specific cost views \u00b6\nContent:\n|\n|REPLICATION_USAGE_HISTORY |Serverless |Credits consumed and number of bytes transferred during database replication. If possible, use the DATABASE_REPLICATION_USAGE_HISTORY view instead. |\n|REPLICATION_GROUP_USAGE_\nHISTORY |Serverless |Credits consumed and number of bytes transferred during replication for a specific replication group. |\n|SEARCH_OPTIMIZATION_HISTORY |Serverless |Credits consumed by the search optimization service. |\n|SERVERLESS_ALERT_HISTORY |Serverless |Credits consumed by serverless alerts. |\n|SERVERLESS_TASK_HISTORY |Serverless |Credits consumed by serverless tasks. |\n|SNOWPIPE_STREAMING_FILE_\nMIGRATION_HISTORY |Serverless |Credits consumed by Snowpipe Streaming compute (does not include client costs). |\n|WAREHOUSE_METERING_HISTORY |Warehouses\n ... \nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > Example queries \u00b6\nContent:\nQuery: Total cloud services cost by type of query\nQuery: Cloud services cost for queries of a given type\nQuery: Warehouses with high cloud services usage\nQuery: Cloud services cost sorted by portion of query time\nCompute for Automatic Clustering :\n* Query: Automatic Clustering cost history (by day, by object)\nQuery: Automatic Clustering History & m-day average\nCompute for Search Optimization :\n* Query: Search Optimization cost history (by day, by object)\nQuery: Search Optimization History & m-day average\nCompute for Materialized Views :\n* Query: Materialized Views cost history (by day, by object)\nQuery: Materialized Views History & m-day average\nCompute for Query Acceleration Service :\n* Query: Query Acceleration Service cost by warehouse\nCompute for Snowpipe :\n* Query: Cumulative usage of data ingest (Snowpipe and \u201cCopy\u201d)\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for cloud services \u00b6\nContent:\nQuery: Warehouses with high cloud services usage\nThis query shows the warehouses that are not using enough warehouse time to cover the cloud services portion of compute. This provides a\nlaunching point for additional investigation by isolating warehouses with a high ratio of cloud service use (>10% of overall credits).\nInvestigation candidates include issues around cloning, listing files in S3, partner tools, setting session parameters, etc.\n```\nSELECT \n  warehouse_name , \n  SUM ( credits_used ) AS credits_used , \n  SUM ( credits_used_cloud_services ) AS credits_used_cloud_services , \n  SUM ( credits_used_cloud_services )/ SUM ( credits_used ) AS percent_cloud_services \n FROM snowflake . account_usage . warehouse_metering_history \n WHERE TO_DATE ( start_time ) >= DATEADD ( month ,- 1 , CURRENT_TIMESTAMP ()) \n    AND credits_used_cloud_services > 0 \n GROUP BY 1 \n ORDER BY 4 DESC ;\n```\nCopy\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Automatic Clustering \u00b6\nContent:\nQuery: Automatic Clustering cost history (by day, by object)\nThis query provides a list of tables with Automatic Clustering and the volume of credits consumed via the service over the last 30 days,\nbroken out by day. Any irregularities in the credit consumption or consistently high consumption are flags for additional investigation.\n```\nSELECT TO_DATE ( start_time ) AS date , \n  database_name , \n  schema_name , \n  table_name , \n  SUM ( credits_used ) AS credits_used \n FROM snowflake . account_usage . automatic_clustering_history \n WHERE start_time >= DATEADD ( month ,- 1 , CURRENT_TIMESTAMP ()) \n GROUP BY 1 , 2 , 3 , 4 \n ORDER BY 5 DESC ;\n```\nCopy\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Search Optimization \u00b6\nContent:\nQuery: Search Optimization cost history (by day, by object)\nThis query provides a full list of tables with Search Optimization and the volume of credits consumed via the service over the last 30\ndays, broken out by day. Any irregularities in the credit consumption or consistently high consumption are flags for additional\ninvestigation.\n```\nSELECT TO_DATE ( start_time ) AS date , \n  database_name , \n  schema_name , \n  table_name , \n  SUM ( credits_used ) AS credits_used \n FROM snowflake . account_usage . search_optimization_history \n WHERE start_time >= DATEADD ( month ,- 1 , CURRENT_TIMESTAMP ()) \n GROUP BY 1 , 2 , 3 , 4 \n ORDER BY 5 DESC ;\n```\nCopy\nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Search Optimization \u00b6\nContent:\nQuery: Search Optimization History & m-day average\nThis query shows the average daily credits consumed by Search Optimization grouped by week over the last year. It can help identify\nanomalies in daily averages over the year so you can investigate spikes or unexpected changes in\nconsumption.\n```\nWITH credits_by_day AS ( \n  SELECT TO_DATE ( start_time ) AS date , \n    SUM ( credits_used ) AS credits_used \n  FROM snowflake . account_usage . search_optimization_history \n  WHERE start_time >= DATEADD ( year ,- 1 , CURRENT_TIMESTAMP ()) \n  GROUP BY 1 \n  ORDER BY 2 DESC \n ) \n\n SELECT DATE_TRUNC ( 'week' , date ), \n  AVG ( credits_used ) as avg_daily_credits \n FROM credits_by_day \n GROUP BY 1 \n ORDER BY 1 ;\n```\nCopy\nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Materialized Views \u00b6\nContent:\nQuery: Materialized Views cost history (by day, by object)\nThis query provides a full list of materialized views and the volume of credits consumed via the service over the last 30 days, broken\nout by day. Any irregularities in the credit consumption or consistently high consumption are flags for additional investigation.\n```\nSELECT TO_DATE ( start_time ) AS date , \n  database_name , \n  schema_name , \n  table_name , \n  SUM ( credits_used ) AS credits_used \n FROM snowflake . account_usage . materialized_view_refresh_history \n WHERE start_time >= DATEADD ( month ,- 1 , CURRENT_TIMESTAMP ()) \n GROUP BY 1 , 2 , 3 , 4 \n ORDER BY 5 DESC ;\n```\nCopy\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Snowpipe and Snowpipe Streaming \u00b6\nContent:\nQuery: Cumulative usage of data ingest (Snowpipe and \u201cCopy\u201d)\nThis query returns an aggregated daily summary of all loads for each table in Snowflake showing average file size, total rows, total\nvolume and the ingest method (copy or Snowpipe). If file sizes are too small or big for optimal ingest, additional\ninvestigation/optimization may be required. By mapping the volume to credit consumption, it is possible to determine which tables are\nconsuming more credits per TB loaded.\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Snowpipe and Snowpipe Streaming \u00b6\nContent:\nQuery: Snowpipe cost history (by day, by object)\nThis query provides a full list of pipes and the volume of credits consumed via the service over the last 30 days, broken out by day.\nAny irregularities in the credit consumption or consistently high consumption are flags for additional investigation.\n```\nSELECT TO_DATE ( start_time ) AS date , \n  pipe_name , \n  SUM ( credits_used ) AS credits_used \n FROM snowflake . account_usage . pipe_usage_history \n WHERE start_time >= DATEADD ( month ,- 1 , CURRENT_TIMESTAMP ()) \n GROUP BY 1 , 2 \n ORDER BY 3 DESC ;\n```\nCopy\nQuery: Snowpipe History & m-day average\nThis query shows the average daily credits consumed by Snowpipe grouped by week over the last year. It can help identify anomalies in\ndaily averages over the year so you can investigate spikes or unexpected changes in consumption.\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for replication \u00b6\nContent:\nQuery: Account replication cost\nThis query lists the credits used by a replication or failover group for account replication in the current month:\n```\nSELECT start_time , \n  end_time , \n  replication_group_name , \n  credits_used , \n  bytes_transferred \n FROM snowflake . account_usage . replication_group_usage_history \n WHERE start_time >= DATE_TRUNC ( 'month' , CURRENT_DATE ());\n```\nCopy\nQuery: Database replication cost history (by day, by object)\nThis query provides a full list of replicated databases and the volume of credits consumed via the replication service over the last 30\ndays, broken out by day. Any irregularities in the credit consumption or consistently high consumption are flags for additional\ninvestigation.\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for partner tools \u00b6\nContent:\nQuery: Credit consumption by partner tools\nThis query identifies which of Snowflake\u2019s partner tools/solutions (e.g. BI, ETL, etc.) are consuming the most credits. This can help\nidentify partner solutions that are consuming more credits than anticipated, which can be a starting point for additional investigation.\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for partner tools \u00b6\nContent:\nCopy\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Cortex Analyst \u00b6\nContent:\nQuery: Credit consumption by Cortex Analyst.\nThis query shows the credit consumption for Cortex Analyst.\n```\nSELECT * \n  FROM SNOWFLAKE . ACCOUNT_USAGE . CORTEX_ANALYST_USAGE_HISTORY ;\n```\nCopy\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Cortex functions \u00b6\nContent:\n```\nSELECT * \n  FROM SNOWFLAKE . ACCOUNT_USAGE . CORTEX_FUNCTIONS_QUERY_USAGE_HISTORY \n  WHERE query_id = 'query-id' ;\n```\nCopy\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Snowflake Notebooks \u00b6\nContent:\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nViewing credit usage\nQuerying data for compute cost\nRelated content\nUnderstanding compute cost\nExploring overall cost\nAttributing cost\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-optimize",
      "title": "Optimizing cost - Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nEN\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic tables\nStreams and tasks\nRow timestamps\ndbt Projects on Snowflake\nData Unloading\nStorage lifecycle policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Optimization\nSection Title: Optimizing cost \u00b6\nContent:\nThis topic summarizes the features and strategies you can use to optimize Snowflake to reduce costs and maximize your spend.\nUsing cost insights to save\nLearn how to use cost insights to optimize Snowflake for cost within a particular account.\nOptimizing cloud services for cost\nLearn how to adjust your cloud services usage to reduce costs.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nRelated content\nManaging cost in Snowflake\nUnderstanding overall cost"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-understanding-overall",
      "title": "Understanding overall cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nEN\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic tables\nStreams and tasks\nRow timestamps\ndbt Projects on Snowflake\nData Unloading\nStorage lifecycle policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Understanding cost\nSection Title: Understanding overall cost \u00b6\nContent:\nNote\nThis topic describes foundational costs associated with using Snowflake (compute costs, storage costs, and data transfer costs).\nSpecific Snowflake features (for example, Snowflake Cortex and Snowpark Container Services) incur costs in unique ways, and are not\ndiscussed in this topic.\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\nThe total cost of using Snowflake is the aggregate of the cost of using data transfer, storage, and compute resources. Snowflake\u2019s\ninnovative cloud architecture separates the cost of accomplishing any task into one of these\nusage types.\nCompute Resources\nUsing compute resources within Snowflake consumes Snowflake credits. The billed cost of using compute resources is\ncalculated by multiplying the number of consumed credits by the price of a credit. For the current price of a credit, see the [Snowflake Pricing Guide](https://www.snowflake.com/pricing/pricing-guide/) .\nThere are three types of compute resources that consume credits within Snowflake:\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\n**Virtual Warehouse Compute** : Virtual warehouses are user-managed compute resources that consume\ncredits when loading data, executing queries, and performing other DML operations. Because Snowflake utilizes per-second billing (with a\n60-second minimum each time the warehouse starts), warehouses are billed only for the credits they actually consume when they are\nactively working. **Serverless Compute** : There are Snowflake features such as Search Optimization and Snowpipe that use Snowflake-managed compute\nresources rather than virtual warehouses. To minimize cost, these serverless compute resources are automatically resized and scaled\nup or down by Snowflake as required for each workload. **Cloud Services Compute** : The cloud services layer of the Snowflake architecture consumes credits as it performs behind-the-scenes\ntasks such as authentication, metadata management, and access control.\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\nUsage of the cloud services layer is charged only if the daily\nconsumption of cloud services resources exceeds 10% of the daily warehouse usage.\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\nFor more details about compute costs, see Understanding compute cost .\nStorage Resources\nThe monthly cost for storing data in Snowflake is based on a flat rate per terabyte (TB). For the current rate, which\nvaries depending on your type of account (Capacity or On Demand) and region (US or EU), see the [Snowflake Pricing Guide](https://www.snowflake.com/pricing/pricing-guide/) .\nStorage is calculated monthly based on the average number of on-disk bytes stored each day in your Snowflake account.\nFor more details about storage costs, see Understanding storage cost .\nData Transfer Resources\nSnowflake does not charge data ingress fees to bring data into your account, but does charge for data egress.\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\nSnowflake charges a per-terabyte fee when you transfer data from a Snowflake account into a different region on the same cloud platform or into a completely different cloud platform. This fee for data egress depends on the region where your Snowflake account is hosted. For details,\nsee the [Snowflake Pricing Guide](https://www.snowflake.com/pricing/pricing-guide/) .\nFor more details about data transfer costs, see Understanding data transfer cost .\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\nThe following example provides insight into the total cost in Snowflake to load and query data.\nSuppose an organization loads data constantly, 24x7. It has two different groups of users (Finance and Sales) using the database in\noverlapping, but different times of the day. It also runs a weekly batch report. This organization:\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\nUses the Standard Edition of Snowflake.\nStores an average of 65 TBs of compressed data (compare with 325 TB without compression).\nLoads data 24x7x365. They use a Small Standard virtual warehouse for this purpose.\nEnables seven finance users to work 5 days a week from 8am until 5pm using a Large Standard virtual warehouse.\nEnables twelve sales users in different geographies to work a total of 16 hours a day (across Europe and the Americas), 5 days a\nweek using a Medium Standard virtual warehouse.\nRuns a complex weekly report every Friday. This report takes approximately 2 hours to run on a 2X-Large standard warehouse.\n**Data Loading Requirements**\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n| Parameter | Customer Requirement | Configuration | Cost |\n| Loading Window | 24 x 7 x 365 | Small Standard Virtual Warehouse (2 credits/hr) | 1,488 credits |\n| (2 credits/hr x 24 hours per day x 31 days per month) |  |  |  |\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n**Storage Requirements**\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n| Data set size (per month) | 65 TB (after compression) |\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n**Compute Requirements**\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n| Parameter | Customer Requirement | Configuration | Cost |\n| Finance Users | 5 Users, 8am-5pm (9 hours) | Large Standard Virtual Warehouse (8 credits/hr) | 1,440 credits (8 credits/hr x 9 hours per day x 20 days per month) |\n| Sales Users | 12 Users, 16 hour time slot | Medium Standard Virtual Warehouse (4 credits/hr) | 1,280 (4 credits/hr x 16 hours per day x 20 days per month) |\n| Complex Query Users | 1 User, 2 hours/day | 2X Standard Virtual Warehouse (32 credits/hr) | 256 (32 credits/hr x 2 hours per day x 4 days per month) |\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n**Total Cost**\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n| Usage Type | Monthly Cost | Total Billed Cost |\n| Compute Cost | 4,464 credits (@ $2/credit) | $8928 |\n| Storage Cost | 65 TB (@ $23/TB) | $1495 |\n| $10,423 |  |  |\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n**Next Topics**\nUnderstanding compute cost\nUnderstanding storage cost\nUnderstanding data transfer cost\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nHow are costs incurred?\nTotal cost example\nRelated content\nExploring overall cost\nManaging cost in Snowflake"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/guides-overview-cost",
      "title": "Cost & billing - Source: Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nGuides Cost & Billing\nSection Title: Cost & billing \u00b6\nContent:\nSnowflake provides a robust framework to manage costs. You can also obtain monthly usage statements and reconcile those statements with\nusage data in views.\nSection Title: Cost & billing \u00b6 > Cost management \u00b6\nContent:\nUnderstanding overall cost\nThe total cost of using Snowflake is the aggregate of the cost of using data transfer, storage, and compute resources.\nLearn about how overall cost is calculated.\nExploring overall cost\nSnowsight allows you to quickly and easily obtain information about cost from a visual dashboard.\nQueries against the usage views allow you to drill down into cost data and can help generate custom reports and dashboards.\nLearn about exploring your spend using various queries to return cost information.\nOptimizing cost\nLearn how to optimize Snowflake in order to reduce costs and maximize your spend.\nAttributing cost\nGain insight into Snowflake cost by attributing those costs to logical units within the organization such as departments, environments or\nother entities.\nSection Title: Cost & billing \u00b6 > Cost management \u00b6\nContent:\nLearn how to attribute cost to differing entities within your organization.\nControlling cost\nCost controls allow you to limit how much is spent on various services such as virtual warehouses.\nBudgets allow you to monitor the credit usage of supported objects and serverless features in your account. Resource monitors allow you to monitor credit usage by user-managed virtual warehouses and the\ncloud services layer of the Snowflake architecture.\nSection Title: Cost & billing \u00b6 > Billing \u00b6\nContent:\nAccess a billing usage statement\nLearn how to use Snowsight to view and download monthly usage statements.\nReconcile a billing usage statement\nLearn how to execute queries to reconcile usage data shown on a usage statement with data in the billing views of the Organization Usage\nschema.\nUpdate billing contact information\nLearn how to use Snowsight to update billing contact information.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nCost management\nBilling\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/pricing-options/",
      "title": "Pricing Options - Snowflake",
      "excerpts": [
        "Data for Breakfast Around the World\nDrive impact across your organization with data and agentic intelligence.\n[register now](https://www.snowflake.com/en/data-for-breakfast/)\npricing options\noverview\ncost & performance optimization\npricing calculator\nSection Title: Resources\nContent:\n[Pricing calculator overview](https://www.snowflake.com/en/pricing-options/calculator/overview/) [Pricing calculator FAQs](https://www.snowflake.com/en/pricing-options/calculator/feedback-faqs/) [Snowflake Performance Index](https://www.snowflake.com/en/pricing-options/performance-index/)\nSection Title: Snowflake Pricing\nContent:\nWe keep pricing simple with a consumption-based pricing model. Explore the details of our pricing model and how you can easily understand costs. For a view of all specific pricing related information, view the [consumption table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nSection Title: Snowflake Pricing > Platform:\nContent:\nSelect a platform\nSection Title: Snowflake Pricing > Platform: > Region:\nContent:\nSelect a region\nSection Title: Snowflake Pricing > Platform: > Region: > Standard\nContent:\nAn entry-level, introductory offering providing access to core functionality.\n[get started](https://signup.snowflake.com/?utm_cta=website-pricing-page-get-started-trial)\nIncludes\nAll core platform functionality with fully managed elastic compute\nSecurity with automatic encryption of all data\nSnowpark\nData sharing\nOptimized storage with compression and Time Travel\n+ more\n[View All Features](https://docs.snowflake.com/en/user-guide/intro-editions)\nSection Title: Snowflake Pricing > Platform: > Region: > Enterprise\nContent:\nMost popular and addresses the needs of high-growth, large-scale customers.\n[get started](https://signup.snowflake.com/?utm_cta=website-pricing-page-get-started-trial)\nALL STANDARD EDITION FEATURES +\nAbility to use multi-cluster compute\nGranular governance and privacy controls\nExtended Time Travel windows\n+ more\n[View All Features](https://docs.snowflake.com/en/user-guide/intro-editions)\nMost Popular\nSection Title: Snowflake Pricing > Platform: > Region: > Business Critical\nContent:\nOffers functionality for highly regulated industries with regulations of sensitive data.\n[get started](https://signup.snowflake.com/?utm_cta=website-pricing-page-get-started-trial)\nALL ENTERPRISE EDITION FEATURES +\nTri-Secret Secure\nAccess to private connectivity\nFailover and failback for backup and disaster recovery\n+ more\n[View All Features](https://docs.snowflake.com/en/user-guide/intro-editions)\nSection Title: Snowflake Pricing > Platform: > Region: > Virtual Private Snowflake\nContent:\nVirtual Private Snowflake (VPS) includes all the features of Business Critical Edition, but in a completely separate Snowflake environment, isolated from all other Snowflake accounts.\ntalk to sales\nSection Title: Snowflake Pricing > Optimized Storage\nContent:\nA monthly fee for data stored in Snowflake is calculated using the average amount of storage used per month, after compression, for data ingested into Snowflake. There are two ways to buy the Snowflake Service: On-demand or pre-paid capacity.\n[download snowflake pricing guide](https://www.snowflake.com/resource/the-simple-guide-to-snowflake-pricing/?utm_cta=website-pricing-page-optimized-storage-simple-pricing-guide)\nSection Title: Snowflake Pricing > Optimized Storage > On-Demand Storage\nContent:\nPay for usage month-to-month.\nPrice shown above reflects the list price. To view capacity storage discounts, please refer to [Table 3(a) in the Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nSection Title: Snowflake Pricing > A fully managed product delivers simplicity and better TCO\nContent:\nSnowflake takes care of infrastructure management and provides the ease-of-use that only a unified, fully managed product can deliver.\nOver 7,200 brands mobilize their data with Snowflake\nSection Title: Snowflake Pricing > Drive more efficiency with Snowflake\nContent:\nWhether you\u2019re just getting started with Snowflake or looking to get the most out of your existing deployment, check out these resources to help you forecast, understand and optimize costs.\nModernize your data ecosystem\nSection Title: Snowflake Pricing > Migrate to the AI Data Cloud\nContent:\nMoving from on-premises to the cloud or between clouds, Snowflake helps you have a seamless journey with low-risk and cost-effective AI-powered solutions. * Free code conversion tools\nEcosystem of migration partners\nMost critical data workloads enabled\nvisit the migration hub\nWhite Paper ##### Definitive Guide to Managing Spend in Snowflake Read now\n[Course ##### Cost Management Bootcamp - AI Data Cloud Academy Register now](https://www.snowflake.com/data-cloud-academy-cost-management-bootcamp/?utm_cta=website-pricing-drive-more-efficiency-dca-cost-management-bootcamp)\n[Docs ##### Manage Costs in Snowflake Read now](https://docs.snowflake.com/user-guide/cost-understanding-overall)\n[Docs ##### Optimizing Performance on Snowflake Read now](https://docs.snowflake.com/en/guides-overview-performance)\nSection Title: Snowflake Pricing > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\n[start for free](https://signup.snowflake.com/)\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\nLearn * [Resource Library](https://snowflake.com/en/resources/)\nSection Title: Snowflake Pricing > Where Data Does More\nContent:\nLive Demos\n[Fundamentals](https://www.snowflake.com/en/fundamentals/)\n[Training](https://www.snowflake.com/en/resources/learn/training/)\n[Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n[Snowflake University](https://learn.snowflake.com/en/)\n[Developer Guides](https://www.snowflake.com/en/developers/guides)\n[Documentation](https://docs.snowflake.com/)\n[Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n[Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\nCookie Settings\n[Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Legal](https://www.snowflake.com/en/legal/)\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\nSection Title: Snowflake Pricing > Where Data Does More\nContent:\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/",
      "title": "Cost Optimization - Snowflake",
      "excerpts": [
        "Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake\u2019s resource billing models\nContent:\n**Compute (Virtual Warehouses, Snowpark Container Services, Openflow):** This is often the most dynamic and largest portion of\nSnowflake spend. Virtual Warehouses are billed per-second after an\ninitial 60-second minimum when active, with credit consumption\ndirectly proportional to warehouse size (e.g., an \u201cX-Small\u201d Gen1\nwarehouse consumes one credit per hour, a 'Small' consumes two credits\nper hour, doubling with each size increase) while SPCS (via [compute pools](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) )\nare billed for all uptime with a minimum of five minutes. [Openflow](https://docs.snowflake.com/en/user-guide/data-integration/openflow/cost) is billed per second of runtime with a 60-second minimum.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake\u2019s resource billing models\nContent:\nUsage\nfor cloud services is only charged if the daily consumption of cloud\nservices exceeds 10% of the daily usage of virtual warehouses. **AI features:** Snowflake additionally offers artificial intelligence\nfeatures that run on Snowflake-managed compute resources, including\nCortex AISQL functions (e.g. COMPLETE, CLASSIFY, etc. ), Cortex\nAnalyst, Cortex Search, Fine Tuning, and Document AI. The usage of\nthese features, often with tokens, are converted to credits to unify\nwith the rest of Snowflake\u2019s billing model. Details are listed in the\nCredit Consumption Document. **Data transfer:** Data transfer is the process of moving data into\n(ingress) and out of (egress) Snowflake.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Embed cost accountability into your organization's DNA\nContent:\nIf cost accountability models have not been implemented previously,\nconsider a showback model. This involves transparently reporting\nSnowflake costs to different departments or projects to raise awareness\nof their costs. By showing each team their monthly consumption (broken\ndown by warehouse usage, query costs, and storage, etc.), it encourages\na cost-conscious culture. This initial step helps teams understand the\nfinancial impact of their actions without the immediate pressure of\nbudget cuts. Tools like Snowflake's built-in [Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) & [budget](https://docs.snowflake.com/en/user-guide/budgets) views, third-party cost management platforms, or custom dashboards can\nbe used to provide these reports.\n**Chargeback**\n ... \nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\nForecasting Snowflake consumption should be a strategic business\nfunction, not a mere technical prediction. The goal is to establish a\ntransparent basis for budgeting and optimizing ROI by linking\nconsumption directly to measurable business outcomes. In a dynamic,\nusage-based environment where compute costs are the most volatile\nelement of the bill, a robust framework must integrate quantitative\nanalysis of historical usage with qualitative insights into future\nbusiness drivers. The following framework outlines how to build and\nmaintain a comprehensive consumption forecast.\n**Establish the Baseline**\nThis phase focuses on understanding the source of spend and establishing\ngranular cost accountability.\n ... \nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\nTo effectively manage Snowflake expenditure and prevent unforeseen\ncosts, it is crucial to implement a robust framework of resource\ncontrols. These controls act as automated guardrails, ensuring that\nresource consumption for compute, storage, and other services aligns\nwith your financial governance policies. By proactively setting policies\nand remediating inefficiencies, you can maintain budget predictability\nand maximize the value of your investment in the platform.\n**Compute controls**\nControlling compute consumption is often the most critical aspect of\nSnowflake cost management, as it typically represents the largest\nportion of spend. Snowflake offers several features to manage warehouse\nusage and prevent excessive costs.\n ... \nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\n**Storage Controls**\nWhile storage costs are generally lower than compute costs, they can\ngrow significantly over time. Understanding the different components of\nstorage cost and implementing policies to manage the types of storage is\nkey to keeping these costs in check.\n**Staged files:** [Staged](https://docs.snowflake.com/en/user-guide/data-load-considerations-stage) files are files that have been prepped for bulk data loading/unloading\n(stored in compressed or uncompressed format). They can be stored in\nan [external stage](https://docs.snowflake.com/en/user-guide/data-load-s3-create-stage) using cloud provider\u2019s blob storage (such as Amazon S3) or an [internal stage](https://docs.snowflake.com/en/user-guide/data-load-local-file-system-create-stage) within the Snowflake platform. You are only charged for data stored in\ninternal stages.\n ... \nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\nFor serverless features, which do not use warehouse compute and\ntherefore cannot leverage the Resource Monitor feature, we recommend\nsetting up a budget. [Budgets](https://docs.snowflake.com/en/user-guide/budgets) allow\nyou to define a monthly spending limit on the [compute costs](https://docs.snowflake.com/en/user-guide/cost-understanding-compute) for a Snowflake account or a custom group of Snowflake objects. When the\nspending limit is projected to be hit, a notification is sent. While\nBudgets do not explicitly allow you to suspend serverless features upon\nreaching a limit (the way that Resource Monitors do), Budgets can be\nconfigured to not only send emails, but also send [notifications](https://docs.snowflake.com/en/user-guide/budgets/notifications) to a cloud message queue or other webhooks (such as Microsoft Teams,\nSlack, or PagerDuty).\n ... \nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nCompute is the most significant part of any organization\u2019s Snowflake\nspend, typically accounting for 80% or more of spend. A good warehouse\ndesign should incorporate the principles below:\nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nSeparate warehouses by workload (e.g., ELT versus analytics versus\ndata science)\nWorkload size in bytes should match the t-shirt size of the warehouse\nin the majority of the workloads\u2013larger warehouse size doesn\u2019t always\nmean faster\nAlign warehouse size for optimal cost-performance settings\n[Utilize Multi-Cluster Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-considerations) configuration to solve for high concurrency\nUtilize [Query Acceleration Services](https://docs.snowflake.com/en/user-guide/query-acceleration-service) to help with infrequent, large data scans\nFor memory-intensive workloads, use a warehouse type of Snowpark\nOptimized or higher memory resource constraint configurations as\nappropriate\nSet appropriate auto-suspend settings - longer for high cache use,\nlower for no cache reuse\nSet appropriate warehouse query timeout settings for the workload and\nthe use cases it supports.\nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nUsing the principles above ensures that your compute costs are well\nmanaged and balanced with optimal benefits.\n**Separate warehouses by workload**\nDifferent workloads (e.g., data engineering, analytics, AI and\napplications) have varying characteristics. [Separating these to be serviced by different virtual warehouses](https://docs.snowflake.com/en/user-guide/warehouses-considerations) can help ensure relevant features in Snowflake can be utilized.\nSome examples of this include:\nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nOptimize dashboards and reports by [reusing the warehouse SSD cache](https://docs.snowflake.com/en/user-guide/warehouses-considerations) for repeated select queries. This can be achieved by configuring a\nlonger warehouse autosuspend setting.\nLoading large volumes of data using [optimal file sizes](https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare) and utilizing all available threads in a virtual warehouse\nProcessing large data sets for AI workloads using memory-optimized\nwarehouses\n**Warehouse sizing**\nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nMapping the workload to the [right warehouse size](https://docs.snowflake.com/en/user-guide/performance-query-warehouse-size) and configuration is an important consideration of warehouse design.\nThis should consider several factors like query completion time,\ncomplexity, data size, query volume, SLAs, queuing, and balancing\noverall cost objectives. Warehouse sizing involves a cost-benefit\nanalysis that balances performance, cost, and human expectations. Humans\noften have expectations that their queries will not be queued or take a\nlong time to complete, so it is recommended to have dedicated warehouses\nfor teams.\nRecommendations for choosing the right-sized warehouse include:\nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nFollow the principles outlined above and understand that this is a\ncontinuous improvement process.\nChoose a size based on the estimated or actual workload size and\nmonitor.\nUtilize Snowflake's extensive telemetry data, such as [QUERY_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) and [WAREHOUSE_METERING_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history) ,\nto validate that the warehouse size is impacting the metrics you care\nabout in the direction you intend.\n**Optimal warehouse settings**\nWhile Snowflake strives for minimal knobs and self-managed tuning, there\nare situations where selecting the right settings for warehouses can\nhelp with optimal cost and/or performance. Some of the key [warehouse settings](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse) include\n ... \nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nThe [WAREHOUSE_LOAD_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_load_history) view can help you assess the average number of queries running on a\nwarehouse over a specific period. A useful benchmark is to aim for a\nwarehouse running queries 80% of the time it's active. Continuously\nmonitor your key metrics to ensure they still meet SLA goals and adjust\nwarehouse settings as needed.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Leverage Managed Services\nContent:\n**[Query Acceleration Service](https://docs.snowflake.com/en/user-guide/query-acceleration-service) (QAS):** QAS is a serverless feature that provides a burst of additional\ncompute resources to accelerate specific parts of a query, rather than\nreplacing an appropriately sized warehouse. It's particularly beneficial\nfor large I/O operation queries, eliminating the need to manually scale\nwarehouses up or down. QAS also helps speed up query execution when\ntable clustering cannot be altered due to other workload dependencies. A\ncost-benefit analysis should always be performed to ensure that the\ncredit consumption from QAS is justified by the performance improvement\nand the avoided cost of a larger warehouse.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nThe COPY INTO\n( [table](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table) or [location](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location) )\ncommand is a foundational and flexible method for bulk data loading from\nan external stage into a Snowflake table. Its importance lies in its\nrole as a powerful, built-in tool for migrating large volumes of\nhistorical data or loading scheduled batch files. The best practice is\nto use COPY INTO for one-time or large batch data loading jobs, which\ncan then be supplemented with more continuous ingestion methods like\nSnowpipe for incremental data. Additional information regarding COPY\nINTO and general data loading best practices can be found in our\ndocumentation [here](https://docs.snowflake.com/user-guide/data-load-considerations) .\nSome additional best practices are outlined below.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nThe [key concepts of dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh) are defined in our documentation. However, best practices and\ndetermining when to use DTs versus other methods of pipeline tooling in\nSnowflake still warrant discussion, and are compared in Snowflake\u2019s [documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison) .\nIn addition to Snowflake\u2019s published [best practices](https://docs.snowflake.com/en/user-guide/dynamic-table-performance-guide) ,\nconsider the following"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/getting-started-cost-performance-optimization/",
      "title": "Getting Started with Cost and Performance Optimization - Snowflake",
      "excerpts": [
        "Data for Breakfast Around the World\nDrive impact across your organization with data and agentic intelligence.\n[register now](https://www.snowflake.com/en/data-for-breakfast/)\n[Snowflake for Developers](https://www.snowflake.com/content/snowflake-site/global/en/developers) [Guides](https://www.snowflake.com/content/snowflake-site/global/en/developers/guides) Getting Started with Cost and Performance Optimization\nSection Title: Getting Started with Cost and Performance Optimization\nContent:\nPraveen Purushothaman\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/getting-started-cost-performance-optimization)\nSection Title: Getting Started with Cost and Performance Optimization > **Overview**\nContent:\nBy completing this guide, you will be able to understand and implement various optimization features on Snowflake.\n**Setup Environment** : Use correct roles and sample datasets to use the optimization features\n**Account Usage** : Understand purpose of Account Usage schema and use it to uncover savings opportunities\n**Warehouse Controls** : Leverage settings on a virtual warehouse to optimize usage\n**Storage** : Determine cost savings with high-churn and short-lived tables\n**Optimization Features** : Utilize Snowflake optimization features to achieve cost or performance savings\nSection Title: Getting Started with Cost and Performance Optimization > **Overview** > **Prerequisites**\nContent:\nFamiliarity with [Snowflake platform](https://docs.snowflake.com/en/user-guide/intro-key-concepts)\nBasic understanding of [micro-partitions](https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions)\n[Accountadmin](https://docs.snowflake.com/en/user-guide/security-access-control-considerations) access on a Snowflake account\nIf you do not have access to a Snowflake account, you can sign up for a [free trial](https://signup.snowflake.com/?utm_source=snowflake-devrel&utm_medium=developer-guides&utm_cta=developer-guides)\nSection Title: Getting Started with Cost and Performance Optimization > **Overview** > **What You\u2019ll Learn**\nContent:\nHow to identify optimization patterns in your Snowflake account\nHow to implement performance improvements in your Snowflake environment\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Warehouse Controls**\nContent:\nThis section covers the code for controls that can be enforced on virtual warehouses.\nSection Title: Getting Started with Cost and Performance Optimization > **Warehouse Controls** > SQL\nContent:\n-- SETTING CONTEXT FOR THE SESSION --\nUSE ROLE ACCOUNTADMIN;\n-- Check what parameters or settings are being used in a warehouse\nshow parameters for warehouse hol_compute_wh;\n-- Setting Auto suspend for a warehouse, value in seconds\nalter warehouse hol_compute_wh set auto_suspend=60;\n-- Setting Auto Resume for a warehouse\nalter warehouse hol_compute_wh set auto_resume=TRUE;\n-- Changing Statement Timeout at account level\nalter account set statement_timeout_in_seconds=7200;\n-- Changing Statement Timeout at warehouse level\nalter warehouse hol_compute_wh set statement_timeout_in_seconds=3600;\n-- Create a resource monitor\nCREATE OR REPLACE RESOURCE MONITOR Credits_Quota_Monitoring\nWITH CREDIT_QUOTA = 5000\nNOTIFY_USERS = (JDOE, \"Jane Smith\", \"John Doe\")\nTRIGGERS ON 75 PERCENT DO NOTIFY\nON 100 PERCENT DO SUSPEND\nON 110 PERCENT DO SUSPEND_IMMEDIATE;\nSection Title: Getting Started with Cost and Performance Optimization > **Warehouse Controls** > SQL\nContent:\n-- Activating a resource monitor\nalter warehouse hol_compute_wh set resource_monitor=Credits_Quota_Monitoring;\n```\n\nCopy\n```\nSection Title: Getting Started with Cost and Performance Optimization > **Account Usage Queries**\nContent:\n[Account Usage](https://docs.snowflake.com/en/sql-reference/account-usage) is a powerful tool in an administrator's toolbox to identify optimization opportunites. Apart from metadata about objects in the Snowflake account, it contains usage metrics related to all services consumed in the account - Credits, Storage, Data Transfer.\nIn addition, [Query History](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) contains all information related to a query - metrics for each operation in a query, rows scanned, warehouse used, query text etc. The below queries are examples for viewing results of different views in Account Usage schema.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\ndatabases\nAND\n(\nchurn_pct>=40\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Automatic Clustering**\nContent:\nThis section covers [Automatic Clustering](https://docs.snowflake.com/en/user-guide/tables-auto-reclustering) . Automatic Clustering is a Snowflake managed service that manages reclustering (as needed) of clustered tables. Reclustering is the process of reordering data in tables to colocate rows that have same cluster key values, which reduces the number of micro-partitions that need to be scanned during execution of a query thereby reducing execution times and help with efficient query execution on smaller sized warehouses.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > ... > SQL > Results Screenshot\nContent:\nUnclustered table\nClustered table\nSection Title: Getting Started with Cost and Performance Optimization > **Automatic Clustering** > SQL > Outcome\nContent:\nAutomatic clustering improves query performance by scanning less data (less micropartitions). Refer to [Clustering Considerations](https://docs.snowflake.com/en/user-guide/tables-clustering-keys) for clustering considerations and choosing the right clustering key for a table.\nSection Title: Getting Started with Cost and Performance Optimization > **Materialized Views**\nContent:\nThis section covers use of [Materialized Views](https://docs.snowflake.com/en/user-guide/views-materialized) as an option to optimize Snowflake workloads. A materialized view is a pre-computed data set derived from a query specification and stored for later use. Because the data is pre-computed, querying a materialized view is faster than executing a query against the base table of the view. This performance difference can be significant when a query is run frequently or is sufficiently complex. As a result, materialized views can speed up expensive aggregation, projection, and selection operations, especially those that run frequently and that run on large data sets.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Materialized Views** > SQL > Outcome\nContent:\nRefer to [Materialized Views Best Practices](https://docs.snowflake.com/en/user-guide/views-materialized) for considerations on choosing materialized views.\nSection Title: Getting Started with Cost and Performance Optimization > **Query Acceleration**\nContent:\nThis section covers use of [Query Acceleration Service](https://docs.snowflake.com/en/user-guide/query-acceleration-service) which can accelerate parts of a query workload in a warehouse. When it is enabled for a warehouse, it can improve overall warehouse performance by reducing the impact of outlier queries, which are queries that use more resources than the typical query. The query acceleration service does this by offloading portions of the query processing work to shared compute resources that are provided by the service.\nExamples of the types of workloads that might benefit from the query acceleration service include:\nAd hoc analytics.\nWorkloads with unpredictable data volume per query.\nQueries with large scans and selective filters.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Query Acceleration** > SQL\nContent:\n-- Find Queries that could be accelerated (for cost consistency, best to find an application workload with consistent query \"templates\").\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Query Acceleration** > SQL > Outcome\nContent:\nRefer to [Evaluating Cost and Performance](https://docs.snowflake.com/en/user-guide/query-acceleration-service) to understand impact of Query Acceleration on workloads in your account.\nSection Title: Getting Started with Cost and Performance Optimization > **Search Optimization**\nContent:\nThis section covers use of [Search Optimization](https://docs.snowflake.com/en/user-guide/search-optimization-service) which can significantly improve the performance of certain types of lookup and analytical queries. The search optimization service aims to significantly improve the performance of certain types of queries on tables, such as:\nSelective point lookup queries on tables\nSubstring and regular expression searches\nQueries on fields in VARIANT, OBJECT, and ARRAY (semi-structured) columns that use the following types of predicates: EQUALITY, IN, ARRAY_CONTAINS, ARRAYS_OVERLAP etc.\nQueries that use selected geospatial functions with GEOGRAPHY values\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Search Optimization** > SQL > Outcome\nContent:\nRefer to [Search Optimization Cost Estimation and Management](https://docs.snowflake.com/en/user-guide/search-optimization/cost-estimation) for Search Optimization cost management considerations.\nSection Title: Getting Started with Cost and Performance Optimization > Conclusion And Resources\nContent:\nCongratulations! You have learned about optimization features and tools to assist in your quest to optimize workloads on your Snowflake account. Apart from the features and options discussed in this guide, the below mentioned resources are worth taking a look to get guidance to optimize workloads on Snowflake.\nSection Title: Getting Started with Cost and Performance Optimization > Conclusion And Resources > What You Learned\nContent:\nHow to set up warehouse controls to optimize warehouse usage\nHow to identify savings opportunities for table storage\nHow to implement automatic clustering, materialized views, query acceleration or search optimization service to improve performance of Snowflake workloads\nSection Title: Getting Started with Cost and Performance Optimization > ... > What You Learned > Call to Action\nContent:\nDefinitive Guide to managing spend in Snowflake .\n[Snowflake Education](https://learn.snowflake.com/en/)\nProfessional Services\nUpdated Dec 20, 2025\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\n*\n*"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-optimize-cloud-services",
      "title": "Optimizing cloud services for cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Optimization Optimizing cloud services\nSection Title: Optimizing cloud services for cost \u00b6\nContent:\nIf you find that your cloud services usage is higher than expected, check if your use of\nSnowflake follows any of the following patterns. Each pattern includes a recommendation that might help you reduce costs associated with\ncloud services.\nPattern: Copy commands with poor selectivity\nPattern: High-frequency DDL operations and cloning\nPattern: High-frequency, simple queries\nPattern: High-frequency INFORMATION_SCHEMA queries\nPattern: High-frequency SHOW commands (by data applications and third-party tools)\nPattern: Single-row inserts and fragmented schemas (by data applications)\nPattern: Complex SQL queries\nPattern: Copy commands with poor selectivity\nExecuting copy commands involves listing files from Amazon Simple Storage Service (S3). Because listing files uses only cloud services\ncompute, executing copy commands with poor selectivity can result in high cloud services usage.\nSection Title: Optimizing cloud services for cost \u00b6\nContent:\n**Recommendation:** Consider changing the structure of your S3 bucket to include some kind of date prefix, so you list only the targeted\nfiles you need.\nPattern: High-frequency DDL operations and cloning\nData Definition Language (DDL) operations, particularly cloning, are entirely metadata operations, meaning they use only cloud services\ncompute. Frequently creating or dropping large schemas or tables, or cloning databases for backup, can result in significant cloud\nservices usage.\n**Recommendation:** Cloning uses only a fraction of the resources needed to do deep copies, so you should continue to clone. Review your\ncloning patterns to ensure they are as granular as possible, and aren\u2019t being executed too frequently. For example, you might want to\nclone only individual tables rather than an entire schema.\nSection Title: Optimizing cloud services for cost \u00b6\nContent:\nPattern: High-frequency, simple queries\nThe consumption of cloud services by a single simple query is negligible, but running queries such as `SELECT 1` , `SELECT sequence1.NEXTVAL` , or `SELECT CURRENT_SESSION()` at an extremely high frequency (tens of thousands per day) can result in\nsignificant cloud services usage.\n**Recommendation:** Review your query frequency and determine whether the frequency is appropriately set for your use case. If you\nobserve a high frequency of `SELECT CURRENT_SESSION()` queries originating from partner tools using the JDBC driver, confirm that\nthe partner has updated their code to use the `getSessionId()` method in the SnowflakeConnection interface . This takes advantage of caching and reduces cloud services usage.\nSection Title: Optimizing cloud services for cost \u00b6\nContent:\nPattern: High-frequency INFORMATION_SCHEMA queries\nQueries against the Snowflake Information Schema consume only cloud services resources. The consumption of cloud services by a single\nquery against INFORMATION_SCHEMA views might be negligible, but running these queries at extremely high frequency (tens of thousands per\nday) can result in significant cloud services usage.\n**Recommendation:** Review your query frequency and determine whether the frequency is appropriately set for your use case.\nAlternatively, you can query a view in the ACCOUNT_USAGE schema instead of an INFORMATION_SCHEMA\nview. Querying the ACCOUNT_USAGE schema uses a virtual warehouse rather than cloud services.\nSection Title: Optimizing cloud services for cost \u00b6\nContent:\nPattern: High-frequency SHOW commands (by data applications and third-party tools)\nSHOW commands are entirely metadata operations, meaning they consume only cloud services resources. This pattern typically occurs when\nyou have created an application built on top of Snowflake that executes SHOW commands at a high frequency. These commands might also be\ninitiated by third-party tools.\n**Recommendation:** Review your query frequency and determine whether the frequency is appropriately set for your use case. In the case of partner tools,\nreach out to your partner to see if they have any plans to adjust their usage.\nPattern: Single-row inserts and fragmented schemas (by data applications)\nSnowflake is not an OLTP system, so single-row inserts are suboptimal, and can consume significant cloud services resources.\nSection Title: Optimizing cloud services for cost \u00b6\nContent:\nBuilding a data application that defines one schema per customer might result in several data loads in a given time period, which can\nresult in high cloud services consumption.\nThis pattern also results in a lot more metadata that Snowflake needs to maintain, and metadata operations consume cloud services\nresources. Each metadata operation individually consumes minimal resources, but consumption might be significant in aggregate.\n**Recommendation:** In general, do batch or bulk loads rather than single-row inserts.\nUsing a shared schema is significantly more efficient, which saves costs. You\u2019ll likely want to cluster all tables on `customer_ID` and\nuse secure views .\nPattern: Complex SQL queries\nQueries can consume significant cloud services compute if they include a lot of joins/Cartesian products, use the IN operator with large\nlists, or are very large queries. These types of queries all have high compilation times.\nSection Title: Optimizing cloud services for cost \u00b6\nContent:\n**Recommendation:** Review your queries to confirm they are doing what you intend them to do. Snowflake supports these queries and will\ncharge you only for the resources consumed.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nRelated content\nManaging cost in Snowflake\nUnderstanding compute cost\nOptimizing cost\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}

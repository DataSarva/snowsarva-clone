{
  "search_id": "search_8c2cbdc272f64394b85c32b3232ad372",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-attributing",
      "title": "Attributing cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Visibility Attributing cost\nSection Title: Attributing cost \u00b6\nContent:\nAn organization can apportion the cost of using Snowflake to logical units within the organization (for example, to different\ndepartments, environments, or projects). This chargeback or showback model is useful for accounting purposes and pinpoints\nareas of the organization that could benefit from controls and optimizations that can reduce costs.\nTo attribute costs to different groups like departments or projects, use the following recommended approach:\nUse object tags to associate resources and users with departments or projects.\nUse query tags to associate individual queries with departments or projects when the queries are\nmade by the same application on behalf of users belonging to multiple departments.\nSection Title: Attributing cost \u00b6 > Types of cost attribution scenarios \u00b6\nContent:\nThe following cost attribution scenarios are the most commonly encountered. In these scenarios, warehouses are used as an\nexample of a resource that incurs costs.\nSection Title: Attributing cost \u00b6 > Types of cost attribution scenarios \u00b6\nContent:\n**Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate\nwarehouses with a department. You can use these object tags to attribute the costs incurred by those warehouses to that\ndepartment entirely. **Resources that are shared by users from multiple departments:** An example of this is a warehouse shared by users from\ndifferent departments. In this case, you use object tags to associate each user with a department. The costs of queries are\nattributed to the users. Using the object tags assigned to users, you can break down the costs by department. **Applications or workflows shared by users from different departments:** An example of this is an application that issues\nqueries on behalf of its users.\n ... \nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6\nContent:\n**Attributing costs within an account**You can attribute costs within an account by querying the following views in the ACCOUNT_USAGE schema:\nTAG_REFERENCES view : Identifies objects (for example, warehouses and users) that have tags. WAREHOUSE_METERING_HISTORY view : Provides credit usage for warehouses. QUERY_ATTRIBUTION_HISTORY view : Provides the compute costs for queries. The cost per query is\nthe warehouse credit usage for executing the query.For more information on using this view, see About the QUERY_ATTRIBUTION_HISTORY view . **Attributing costs across accounts in an organization**Within an organization, you can also attribute costs for resources that are used **exclusively by a single department** by\nquerying views in the ORGANIZATION_USAGE schema from the organization account .Note\nIn the ORGANIZATION_USAGE schema, the TAG_REFERENCES view is only available in the organization account.\nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6\nContent:\nThe QUERY_ATTRIBUTION_HISTORY view is only available in the ACCOUNT_USAGE schema for an account. There is no\norganization-wide equivalent of the view.\nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6\nContent:\nThe next sections explain how to attribute costs for some of the common cost-attribution scenarios :\nResources not shared by departments\nResources shared by users from different departments\nResources used by applications that need to attribute costs to different departments\nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6 > Resources not shared by departments \u00b6\nContent:\nSuppose that you want to attribute costs by department and that each department uses a set of dedicated warehouses.\nIf you tag warehouses with a `cost_center` tag to identify the department that owns the warehouse, you can join the\nACCOUNT_USAGE TAG_REFERENCES view with the WAREHOUSE_METERING_HISTORY view on the `object_id` and `warehouse_id` columns to get usage\ninformation by warehouse, and you can use the `tag_value` column to identify the departments that own those warehouses.\nThe following SQL statement performs this join:\nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6 > Resources not shared by departments \u00b6\nContent:\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6 > Resources not shared by departments \u00b6\nContent:\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ORGANIZATION_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ORGANIZATION_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n          AND tag_database = 'COST_MANAGEMENT' AND tag_schema = 'TAGS' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of user queries for the last month \u00b6\nContent:\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n  ), \n  user_credits AS ( \n    SELECT user_name , SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n      GROUP BY user_name \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n    FROM user_credits \n  ) \n SELECT \n    u . user_name , \n    u . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM user_credits u , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of user queries by department without idle time \u00b6\nContent:\nThe following example attributes the compute cost to each department through the queries executed by users in that department.\nThis query depends on the user objects having a tag that identifies their department.\n```\nWITH joined_data AS ( \n  SELECT \n      tr . tag_name , \n      tr . tag_value , \n      qah . credits_attributed_compute , \n      qah . start_time \n    FROM SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES tr \n      JOIN SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n        ON tr . domain = 'USER' AND tr . object_name = qah . user_name \n ) \n SELECT \n    tag_name , \n    tag_value , \n    SUM ( credits_attributed_compute ) AS total_credits \n  FROM joined_data \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY tag_name , tag_value \n  ORDER BY tag_name , tag_value ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries by users without tags \u00b6\nContent:\nThe following example calculates the cost of queries by users who are not tagged. You can use this to verify that tags are\nbeing applied consistently to users.\n```\nSELECT qah . user_name , SUM ( qah . credits_attributed_compute ) as total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n    LEFT JOIN snowflake . account_usage . tag_references tr \n    ON qah . user_name = tr . object_name AND tr . DOMAIN = 'USER' \n  WHERE \n    start_time >= dateadd ( month , - 1 , current_date ) \n    AND qah . user_name IS NULL OR tr . object_name IS NULL \n  GROUP BY qah . user_name \n  ORDER BY total_credits DESC ;\n```\nCopy\n```\n+------------+---------------+ \n | USER_NAME  | TOTAL_CREDITS | \n |------------+---------------| \n | RSMITH     |  0.1830555556 | \n +------------+---------------+\n```\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries by department \u00b6\nContent:\nThe following example calculates the compute credits and the credits used for the query acceleration service for the finance department. This depends on the `COST_CENTER=finance` query tag being applied to the original queries that were executed.\nNote that the costs exclude idle time.\n```\nSELECT \n    query_tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_tag = 'COST_CENTER=finance' \n  GROUP BY query_tag ;\n```\nCopy\n```\n+---------------------+-----------------+------+ \n | QUERY_TAG           | COMPUTE_CREDITS | QAS  | \n |---------------------+-----------------|------| \n | COST_CENTER=finance |      0.00576115 | null | \n +---------------------+-----------------+------+\n```\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (including idle time) by query tag \u00b6\nContent:\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n      AND start_time < CURRENT_DATE \n  ), \n  tag_credits AS ( \n    SELECT \n        COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n        SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n      GROUP BY tag \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n      FROM tag_credits \n  ) \n SELECT \n    tc . tag , \n    tc . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM tag_credits tc , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\nCopy"
      ]
    },
    {
      "url": "https://medium.com/snowflake/granular-cost-attribution-and-chargeback-for-warehouse-costs-on-snowflake-cf96fb690967",
      "title": "Granular cost attribution and chargeback for warehouse costs on Snowflake | by Kaushal Jain | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium",
      "publish_date": "2024-09-03",
      "excerpts": [
        "Section Title: Granular cost attribution and chargeback for warehouse costs on Snowflake\nContent:\nKaushal Jain\n6 min read\n\u00b7\nSep 3, 2024\n--\nListen\nShare\nPress enter or click to view image in full size\nImagine a Snowflake warehouse as a virtual processing facility. While Snowflake has a separate storage layer for data, the virtual [warehouse](https://docs.snowflake.com/en/user-guide/warehouses) is primarily responsible for the compute tasks. The size and activity level of the warehouse determine the costs, similar to how a busier processing facility would incur higher expenses.\nA common request by Snowflake customers is how to attribute costs to individual queries at a finer grain than a whole warehouse. This blog post will explore various methods to allocate compute costs and introduce the [per query cost attribution](https://docs.snowflake.com/LIMITEDACCESS/query_attribution_history) feature for more granular cost attribution and analysis.\nSection Title: ... > Warehouse-Based Cost Allocation\nContent:\nWhen warehouses are dedicated to a specific business unit, cost allocation is straightforward. There are two common scenarios:\n**Scenario 1: Object Tagging** ( *recommended* ) Warehouses are tagged using [object tagging](https://docs.snowflake.com/en/user-guide/object-tagging) to logically group cost centers. The cost attribution to a business unit for a given month can be computed using the following query. Note that this requires compliance, and a unique tag attributed to every warehouse. For reconciliation, the following example query also groups the untagged warehouses into the *untagged* bucket.\nSection Title: ... > Warehouse-Based Cost Allocation\nContent:\n```\nSELECT COALESCE(tag_references.tag_value, 'untagged') AS tag_value,   \n       SUM(warehouse_metering_history.credits_used_compute) AS total_credits  \nFROM snowflake.account_usage.warehouse_metering_history  \nLEFT JOIN snowflake.account_usage.tag_references  \nON warehouse_metering_history.warehouse_id = tag_references.object_id  \nWHERE warehouse_metering_history.start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))   \n  AND warehouse_metering_history.start_time < DATE_TRUNC('MONTH',  CURRENT_DATE)  \nGROUP BY COALESCE(tag_references.tag_value, 'untagged')  \nORDER BY total_credits DESC;\n```\nPress enter or click to view image in full size\nSection Title: ... > Warehouse-Based Cost Allocation\nContent:\n**Scenario 2: Warehouse Naming Convention** Warehouses follow a naming convention such as *teamname_details* or *workload_details* . This allows for direct mapping of costs to the respective business units or workloads. Here is an example query that gives a split of the costs across teams assuming they follow a naming convention. The *others* bucket captures the ones not following the convention for reconciliation.\nSection Title: ... > Warehouse-Based Cost Allocation\nContent:\n```\nSELECT   \n    CASE   \n        WHEN POSITION('_' IN warehouse_metering_history.warehouse_name) > 0   \n             THEN SPLIT_PART(warehouse_metering_history.warehouse_name, '_', 1)  \n        ELSE 'others'  \n    END AS team_name,  \n    SUM(warehouse_metering_history.credits_used_compute) AS total_credits  \nFROM snowflake.account_usage.warehouse_metering_history  \nWHERE warehouse_metering_history.start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))   \n  AND warehouse_metering_history.start_time < DATE_TRUNC('MONTH',  CURRENT_DATE)  \nGROUP BY team_name  \nORDER BY total_credits DESC;\n```\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\nWhen multiple business units or workloads share the same warehouse, cost allocation becomes more complex as the warehouse-level granularity is not sufficient enough. Even in cases where a warehouse may be tied to a given business unit, the unit may want to understand the costs in a more granular manner. The per query cost attribution [feature](https://docs.snowflake.com/LIMITEDACCESS/query_attribution_history) , now **generally available** , helps in these scenarios. This feature allows for warehouse cost attribution at the query level.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\nThe per query cost attribution feature provides the portion of the warehouse cost that can be attributed to a given query. If there is only a single query running in a warehouse, the entire cost of the warehouse is attributed to that query. If multiple simultaneous queries are running, the cost of the warehouse during the overlapping periods is attributed to the queries based on their relative resource consumption. During the times when there is no query running and the warehouse is idle (not suspended), the costs are not attributed to any particular query but can be easily determined and spread across the queries as needed for reconciliation purposes.\n**Common Methods to Allocate Costs Using Per Query Cost Attribution**\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n**1. User-Based Costs** Understanding costs associated with a given user can be useful for accountability, budgeting and more granular forecasting. It can also provide hints on which users are most effective in their use of Snowflake. Chargeback of Snowflake costs at a user level can be achieved using the USER_NAME column in the QUERY_ATTRIBUTION_HISTORY view. The following sample query provides a way to attribute monthly warehouse costs across users. The idle time costs are proportionately attributed based on the relative spend by users.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n```\nWITH wh_bill AS (  \n   SELECT SUM(credits_used_compute) AS compute_credits  \n   FROM snowflake.account_usage.warehouse_metering_history  \n   WHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n   AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \n),  \nuser_credits AS (  \n   SELECT user_name, SUM(credits_attributed_compute) AS credits  \n   FROM snowflake.account_usage.query_attribution_history  \n   WHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n   AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \n   GROUP BY user_name  \n),  \ntotal_credit AS (  \n   SELECT SUM(credits) AS sum_all_credits  \n   FROM user_credits  \n)  \nSELECT u.user_name, u.credits / t.sum_all_credits * w.compute_credits AS attributed_credits  \nFROM user_credits u, total_credit t, wh_bill w;\n```\nPress enter or click to view image in full size\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\nUsers can be mapped to business units / cost centers in a custom view, which can be used along with the user-based costs to roll up the spend across business units. Alternatively, the ROLE_NAME (available in QUERY_HISTORY view) can be used to allocate costs for a given project, if several developers use a given role for a specific project. This method does not require manual tagging, thereby reducing the operational overhead to enforce tagging policies.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n**2. Query Tag-Based Costs** [Query tags](https://docs.snowflake.com/en/sql-reference/parameters) can be used to allocate costs across different workloads. Customers can assign query tags to specific workloads or a set of queries to track them together. These can be set at a session level (Account->User->Session) and can also be configured when issuing snowflake queries from tools such as [dbt](https://docs.getdbt.com/reference/resource-configs/snowflake-configs) , [airflow](https://airflow.apache.org/docs/apache-airflow-providers-snowflake/stable/_api/airflow/providers/snowflake/hooks/snowflake_sql_api/index.html) , etc. The following example query helps attribute the warehouse compute costs from the previous month to different workloads, assuming that workloads are tagged, and also allocates the idle costs proportionately across workloads for reconciliation purposes.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n```\nWITH wh_bill AS (  \n   SELECT SUM(credits_used_compute) AS compute_credits  \n   FROM snowflake.account_usage.warehouse_metering_history  \n   WHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n   AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \n),  \ntag_credits AS (  \n   SELECT COALESCE(NULLIF(query_tag, ''), 'untagged') AS tag, SUM(credits_attributed_compute) AS credits  \n   FROM snowflake.account_usage.query_attribution_history  \n   WHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n   AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \n   GROUP BY tag  \n),  \ntotal_credit AS (  \n   SELECT SUM(credits) AS sum_all_credits  \n   FROM tag_credits  \n)  \nSELECT tc.tag, tc.credits / t.sum_all_credits * w.compute_credits AS attributed_credits  \nFROM tag_credits tc, total_credit t, wh_bill w;\n```\nPress enter or click to view image in full size\n ... \nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n```\nSELECT query_parameterized_hash,   \n       COUNT(*) AS query_count,   \n       SUM(credits_attributed_compute) AS total_credits  \nFROM snowflake.account_usage.query_attribution_history  \nWHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n  AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \nGROUP BY query_parameterized_hash  \nORDER BY total_credits DESC  \nLIMIT 20;\n```\nPress enter or click to view image in full size\nAdditionally, for stored procedures that often issue multiple hierarchical queries, you can compute the attributed costs for the entire procedure using the following example query, as the parent and the root query ids are available for such procedures in the view.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n```\nSET query_id = '<query_id>'; // root query id for the stored procedure  \n  \nSELECT SUM(credits_attributed_compute) AS total_attributed_credits  \n  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY  \n  WHERE (root_query_id = $query_id  \n         OR query_id = $query_id);\n```\nSection Title: Granular cost attribution and chargeback for warehouse costs on Snowflake > ... > Conclusion\nContent:\nSnowflake\u2019s per query cost attribution feature, along with existing cost allocation mechanisms, provides powerful tools for granular cost management. By leveraging these features, organizations can enhance transparency, optimize resource usage, and ensure fair cost allocation. For more details, visit Snowflake\u2019s [per query cost attribution](https://docs.snowflake.com/LIMITEDACCESS/query_attribution_history) page.\nMaximize your cost efficiency with Snowflake and take control of your data platform expenses today!\nOptimization\nCost Management\nSnowflake\nWarehouse Management\n--\n--\n[](https://medium.com/snowflake?source=post_page---post_publication_info--cf96fb690967---------------------------------------)\n[](https://medium.com/snowflake?source=post_page---post_publication_info--cf96fb690967---------------------------------------)\nSection Title: Granular cost attribution and chargeback for warehouse costs on Snowflake > ... > Conclusion\nContent:\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--cf96fb690967---------------------------------------)\n11K followers\n\u00b7 Last published 2 days ago\nBest practices, tips & tricks from Snowflake experts and community\nSection Title: Granular cost attribution and chargeback for warehouse costs on Snowflake > Written by Kaushal Jain\nContent:\n27 followers\n\u00b7 7 following\n ... \nSection Title: Granular cost attribution and chargeback for warehouse costs on Snowflake > No responses yet\nContent:\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----cf96fb690967---------------------------------------)\n[Text to speech](https://speechify.com/medium?source=post_page-----cf96fb690967---------------------------------------)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history",
      "title": "WAREHOUSE_METERING_HISTORY view | Snowflake Documentation",
      "excerpts": [
        "Reference General reference SNOWFLAKE database Account Usage WAREHOUSE\\_METERING\\_HISTORY\n\nSchemas:\n    ACCOUNT\\_USAGE , READER\\_ACCOUNT\\_USAGE\n\n# WAREHOUSE\\_ METERING\\_ HISTORY view \u00b6\n\nThis Account Usage view can be used to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within the last 365 days (1 year).\n\n## Columns \u00b6\n\n|Column Name |Data Type |Description |\n| --- | --- | --- |\n|READER\\_ACCOUNT\\_NAME |VARCHAR |Name of the reader account where the warehouse usage took place. Column only included in view in READER\\_ACCOUNT\\_USAGE schema. |\n|START\\_TIME |TIMESTAMP\\_LTZ |The date and beginning of the hour (in the local time zone) in which the warehouse usage took place. |\n|END\\_TIME |TIMESTAMP\\_LTZ |The date and end of the hour (in the local time zone) in which the warehouse usage took place. |\n|WAREHOUSE\\_ID |NUMBER |Internal/system-generated identifier for the warehouse. |\n|WAREHOUSE\\_NAME |VARCHAR |Name of the warehouse. |\n|CREDITS\\_USED |NUMBER |Total number of credits used for the warehouse in the hour. This is a sum of CREDITS\\_USED\\_COMPUTE and CREDITS\\_USED\\_CLOUD\\_SERVICES. This value does not take into account the adjustment for cloud services , and may therefore be greater than the credits that are billed. To determine how many credits were actually billed, run queries against the METERING\\_DAILY\\_HISTORY view . |\n|CREDITS\\_USED\\_COMPUTE |NUMBER |Number of credits used for the warehouse in the hour. |\n|CREDITS\\_USED\\_CLOUD\\_SERVICES |NUMBER |Number of credits used for cloud services in the hour. |\n|CREDITS\\_ATTRIBUTED\\_COMPUTE\\_QUERIES |NUMBER |Number of credits attributed to queries in the hour. . . Includes only the credit usage for query execution and doesn\u2019t include warehouse idle time usage. |\n\n## Usage notes \u00b6\n\n* In the ACCOUNT\\_USAGE schema, latency for the view is up to 180 minutes (3 hours), except for the CREDITS\\_USED\\_CLOUD\\_SERVICES column. Latency for\n  CREDITS\\_USED\\_CLOUD\\_SERVICES is up to 6 hours.\n* In the READER\\_ACCOUNT\\_USAGE schema, latency for the view is up to 24 hours.\n* Warehouse idle time is not included in the CREDITS\\_ATTRIBUTED\\_COMPUTE\\_QUERIES column.\n  \n  See Examples for a query that calculates the cost of idle time.\n\n* If you want to reconcile the data in this view with a corresponding view in the ORGANIZATION USAGE schema , you must first set the timezone of the session to UTC. Before querying the Account Usage view, execute:\n  \n  > ```\n  > ALTER SESSION SET TIMEZONE = UTC ;\n  > ```\n  > \n  > Copy\n  > \n  >\n\n## Examples \u00b6\n\nFor example, to determine the cost of idle time for each warehouse for the last 10 days, execute the following statement:\n\n```\nSELECT \n  ( SUM ( credits_used_compute ) - \n    SUM ( credits_attributed_compute_queries )) AS idle_cost , \n  warehouse_name \n FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n WHERE start_time >= DATEADD ( 'days' , - 10 , CURRENT_DATE ()) \n  AND end_time < CURRENT_DATE () \n GROUP BY warehouse_name ;\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Columns\n2. Usage notes\n3. Examples\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://blog.greybeam.ai/snowflake-cost-per-query/",
      "title": "Deep Dive: Snowflake's Query Cost and Idle Time Attribution",
      "publish_date": "2024-10-22",
      "excerpts": [
        "[](https://www.greybeam.ai/)\n[Blog](https://blog.greybeam.ai/)\n[Waitlist](https://greybeam.ai)\n[Customer Stories](https://blog.greybeam.ai/tag/customer-story/)\nSubscribe\nSep 9, 2024 13 min read How-To\nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query\nContent:\nSnowflake's new QUERY_ATTRIBUTION_HISTORY view\nSnowflake recently released a new feature for granular cost attribution down to individual queries through the `QUERY_ATTRIBUTION_HISTORY` view in `ACCOUNT_USAGE` . As a company focused on SQL optimization, we at Greybeam were eager to dive in and see how this new capability compares to our own custom cost attribution logic. What we found was surprising - and it led us down a rabbit hole of query cost analysis.\nSection Title: ... > The Promise and Limitations of QUERY_ATTRIBUTION_HISTORY\nContent:\nThe new view aims to provide visibility into the compute costs associated with each query. Some key things to note:\nData is only available from July 1, 2024 onwards\nShort queries (<100ms) are excluded\nIdle time is not included in the attributed costs\nThere can be up to a 6 hour delay in data appearing\nThere's also a `WAREHOUSE_UTILIZATION` view that displays cost of idle time. At the time of writing, this must be enabled by your Snowflake support team.\n ... \nSection Title: ... > Our Initial Findings\nContent:\n```\nWITH query_execution AS (\n    SELECT\n        qa.query_id\n        , TIMEADD(\n                'millisecond',\n                qh.queued_overload_time + qh.compilation_time +\n                qh.queued_provisioning_time + qh.queued_repair_time +\n                qh.list_external_files_time,\n                qh.start_time\n            ) AS execution_start_time\n        , qh.end_time::timestamp AS end_time\n        , DATEDIFF('MILLISECOND', execution_start_time, qh.end_time)*0.001 as execution_time_secs\n        , qa.credits_attributed_compute\n        , DATE_TRUNC('HOUR', execution_start_time) as execution_start_hour\n        , w.credits_used_compute\n    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY AS qa\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS qh\n        ON qa.query_id = qh.query_id\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS w\n        ON execution_start_hour = w.start_time\n        AND qh.warehouse_id = w.warehouse_id\n    WHERE\n ... \nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > ... > Potential Issues\nContent:\nAt the time of writing, we\u2019ve identified a few potential problems with the new view:\nWarehouse ID mismatch\u200a\u2014\u200aThe `warehouse_id` in `QUERY_ATTRIBUTION_HISTORY` doesn't match the actual `warehouse_id` from `QUERY_HISTORY` .\nInflated query costs\u200a\u2014\u200aThe credits attributed to short queries seem disproportionately high in some cases.\nIdle time accounting\u200a\u2014\u200aIt\u2019s unclear how idle time factors into the attribution, if at all.\nWe\u2019ve raised these concerns with Snowflake, and they\u2019ve recommended filing a support ticket for further investigation. In the meantime, we\u2019ll continue to rely on our custom attribution logic for accuracy.\nSection Title: ... > Our Approach to Query Cost Attribution\nContent:\nGiven the discrepancies we\u2019ve found, we wanted to share our methodology for calculating per-query costs, including idle time. Here\u2019s an overview of our process:\nGather warehouse suspend events\nEnrich query data with execution times and idle periods\nCreate a timeline of all events (queries and idle periods)\nJoin with `WAREHOUSE_METERING_HISTORY` to attribute costs\nBefore we dive in, let\u2019s cover a few basics:\nSnippet of WAREHOUSE_METERING_HISTORY\nSection Title: ... > Our Approach to Query Cost Attribution\nContent:\nWe use `WAREHOUSE_METERING_HISTORY` as our source of truth for warehouse compute credits. The credits billed here will reconcile with Snowflake\u2019s cost management dashboards.\nCredits here are represented on an hourly grain. We like to refer to this as *credits metered* , analogous to how most homes in North America are metered for their electricity. In our solution, we\u2019ll need to allocate queries and idle times into their metered hours.\nWe use a weighted time-based approach to attribute costs within the metered hour. In reality, Snowflake\u2019s credit attribution is likely much more complex, especially in situations with more clusters or warehouse scaling.\nHow we need to break down our queries and idle times.\nThe full SQL query will be available at the end of this blog.\n ... \nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\nIn addition, warehouse suspension doesn\u2019t actually occur during the `SUSPEND_WAREHOUSE` event. Technically, it happens when the `WAREHOUSE_CONSISTENT` event is logged. The `WAREHOUSE_CONSISTENT` event indicates that all compute resources associated with the warehouse have been fully released. You can find more information about this event in the [Snowflake documentation](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_events_history?ref=blog.greybeam.ai) .\nFor the sake of simplicity (and because the time difference is usually negligible), we\u2019re sticking with the `SUSPEND_WAREHOUSE` event in our analysis. This approach gives us a good balance between accuracy and complexity in our cost attribution model.\nBefore moving onto enriching query data, we want to apply filters to reduce the load from table scans. Feel free to adjust the dates as you see fit.\n ... \nSection Title: ... > Step 3: Create Timeline of All Events\nContent:\nWe now need to create an hourly timeline of all events so that we can reconcile our credits with `WAREHOUSE_METERING_HISTORY` . The timeline of all events can be broken down into 4 components:\nA query executed and ended in the same hour\nIdle time started and ended in the same hour\nA query executed and ended in a different hour\nIdle time started and ended in a different hour\n1 and 2 are straight forward since they don\u2019t cross any hourly boundaries we can simply select from the dataset and join directly to `WAREHOUSE_METERING_HISTORY` .\n ... \nSection Title: ... > Step 3: Create Timeline of All Events\nContent:\nFor 3 and 4, we need a record for each hour that the queries and idle times ran within. For example, if a query ran from 7:55PM to 10:40PM, we\u2019d need a record for 7, 8, 9, and 10PM.\nA query that executed across 4 hourly slots (including 0).\nOriginally we used a slightly more complicated join:\n```\nFROM queries_enriched AS q\nJOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS m\n    ON q.warehouse_id = m.warehouse_id\n    AND m.start_time >= q.meter_start_time\n    AND m.start_time < q.end_time\n```\nThis took forever to run on a large account. Instead, we first create records for each hour so that the join to `WAREHOUSE_METERING_HISTORY` is a direct join in the next step.\n ... \nSection Title: ... > Step 4: Attribute Costs\nContent:\nFinally, with each query and idle period properly allocated to their hourly slots, we can directly join to `WAREHOUSE_METERING_HISTORY` and calculate our credits used.\n```\nmetered AS (\n    SELECT\n        m.query_id\n        , m.warehouse_id\n        , m.type\n        , m.event_start_at\n        , m.event_end_at\n        , m.meter_start_hour\n        , m.meter_start_at\n        , m.meter_end_at\n        , m.meter_time_secs\n        , SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour) AS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN snowflake.account_usage.warehouse_metering_history AS w -- inner join because both tables have different delays\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time -- we can directly join now since we used our numgen method\n)\n```\nSection Title: ... > Step 4: Attribute Costs\nContent:\nIn this approach we allocate credits based on the proportion of the total execution time in that hour:\n**Time-based Weighting** : We use the duration of each event (query or idle period) as the basis for our weighting. This is represented by `m.meter_time_secs` .\n**Hourly Totals** : We calculate the total time for all events within each hour for each warehouse `SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour)` .\n**Credit Allocation** : We then allocate credits to each event based on its proportion of the total time in that hour `(m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute` .\nSection Title: ... > Step 4: Attribute Costs\nContent:\nOne important note: This approach assumes that all time within an hour is equally valuable in terms of credit consumption. In reality, Snowflake may have more complex internal algorithms for credit attribution, especially for multi-cluster warehouses or warehouses that change size within an hour. However, this weighted time-based approach provides a reasonable and transparent method for cost attribution that aligns well with Snowflake\u2019s consumption-based billing model.\n ... \nSection Title: ... > Full SQL Cost Attribution\nContent:\n```\nSET startDate = DATEADD('DAY', -15, current_date);\nWITH warehouse_list AS (\n    SELECT \n        DISTINCT warehouse_name,\n        warehouse_id\n    FROM warehouse_metering_history\n    WHERE \n        warehouse_name IS NOT NULL\n        AND start_time >= $startDate\n),\n\nwarehouse_events AS (\n    SELECT\n        weh.warehouse_id\n        , weh.timestamp\n    FROM warehouse_events_history as weh\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'        \n),\n\nqueries_filtered AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.warehouse_name\n        , q.warehouse_size\n        , q.role_name\n        , q.user_name\n        , q.query_text\n        , q.query_hash\n        , q.queued_overload_time\n        , q.compilation_time\n        , q.queued_provisioning_time\n        , q.queued_repair_time\n        , q.list_external_files_time\n        , q.start_time\n        , TIMEADD(\n                'millisecond',\n                q.queued_overload_time +\nSection Title: ... > Full SQL Cost Attribution\nContent:\nq.compilation_time +\n                q.queued_provisioning_time + q.queued_repair_time +\n                q.list_external_files_time,\n                q.start_time\n            ) AS execution_start_time\n        , q.end_time::timestamp AS end_time\n        , w.timestamp AS suspended_at\n        , MAX(q.end_time) OVER (PARTITION BY q.warehouse_id, w.timestamp ORDER BY execution_start_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as end_time_max\n        , LEAD(execution_start_time) OVER (PARTITION BY q.warehouse_id ORDER BY execution_start_time ASC) as next_query_at\n    FROM query_history AS q\n    ASOF JOIN warehouse_events AS w\n        MATCH_CONDITION (q.end_time::timestamp <= w.timestamp)\n        ON q.warehouse_id = w.warehouse_id\n    WHERE\n        q.warehouse_size IS NOT NULL\n        AND q.execution_status = 'SUCCESS'\n        AND start_time >= $startDate\n        AND EXISTS (\n            SELECT 1\n            FROM warehouse_list AS wl\n ... \nSection Title: ... > Full SQL Cost Attribution\nContent:\nAS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN warehouse_metering_history AS w\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time\n),\n\nfinal AS (\n    SELECT\n        m.* EXCLUDE total_meter_time_secs, meter_end_at, original_query_id\n        , q.query_text\n        , q.query_hash\n        , q.warehouse_size\n        , q.warehouse_name\n        , q.role_name\n        , q.user_name\n    FROM metered AS m\n    JOIN queries_filtered AS q\n        ON m.original_query_id = q.query_id\n)\nSELECT\n    *\nFROM final\n;\n```"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/guides-overview-cost",
      "title": "Cost & billing | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nGuides Cost & Billing\n\n# Cost & billing \u00b6\n\nSnowflake provides a robust framework to manage costs. You can also obtain monthly usage statements and reconcile those statements with\nusage data in views.\n\n## Cost management \u00b6\n\nUnderstanding overall cost\n    The total cost of using Snowflake is the aggregate of the cost of using data transfer, storage, and compute resources.\n\nLearn about how overall cost is calculated.\nExploring overall cost\n    Snowsight allows you to quickly and easily obtain information about cost from a visual dashboard.\nQueries against the usage views allow you to drill down into cost data and can help generate custom reports and dashboards.\n\nLearn about exploring your spend using various queries to return cost information.\nOptimizing cost\n    Learn how to optimize Snowflake in order to reduce costs and maximize your spend.\nAttributing cost\n    Gain insight into Snowflake cost by attributing those costs to logical units within the organization such as departments, environments or\nother entities.\n\nLearn how to attribute cost to differing entities within your organization.\nControlling cost\n    Cost controls allow you to limit how much is spent on various services such as virtual warehouses.\n\nBudgets allow you to monitor the credit usage of supported objects and serverless features in your account. Resource monitors allow you to monitor credit usage by user-managed virtual warehouses and the\ncloud services layer of the Snowflake architecture.\n\n## Billing \u00b6\n\nAccess a billing usage statement\n    Learn how to use Snowsight to view and download monthly usage statements.\nReconcile a billing usage statement\n    Learn how to execute queries to reconcile usage data shown on a usage statement with data in the billing views of the Organization Usage\nschema.\nUpdate billing contact information\n    Learn how to use Snowsight to update billing contact information.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Cost management\n2. Billing\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/resource-optimization-billing-metrics/",
      "title": "Resource Optimization: Billing Metrics - Snowflake",
      "excerpts": [
        "Section Title: Resource Optimization: Billing Metrics > Overview\nContent:\nThis resource optimization guide represents one module of the four contained in the series. These guides are meant to help customers better monitor and manage their credit consumption. Helping our customers build confidence that their credits are being used efficiently is key to an ongoing successful partnership. In addition to this set of Snowflake Quickstarts for Resource Optimization, Snowflake also offers community support as well as Training and Professional Services offerings. To learn more about the paid offerings, take a look at upcoming education and training .\nThis blog post can provide you with a better understanding of Snowflake's Resource Optimization capabilities.\nSection Title: Resource Optimization: Billing Metrics > Overview > Billing Metrics\nContent:\nBilling queries are responsible for identifying total costs associated with the high level functions of the Snowflake Cloud Data Platform, which includes warehouse compute, snowpipe compute, and storage costs. If costs are noticeably higher in one category versus the others, you may want to evaluate what might be causing that.\nThese metrics also seek to identify those queries that are consuming the most amount of credits. From there, each of these queries can be analyze for their importance (do they need to be run as frequently, if at all) and explore if additional controls need to be in place to prevent excessive consumption (i.e. resource monitors, statement timeouts, etc.).\nSection Title: Resource Optimization: Billing Metrics > Overview > What You\u2019ll Learn\nContent:\nhow to identify and analyze Snowflake consumption across all services\nhow to analyze most resource-intensive queries\nhow to analyze serverless consumption\n ... \nSection Title: Resource Optimization: Billing Metrics > Billing Metrics (T1) > Tier 1 > Description:\nContent:\nIdentify key metrics as it pertains to total compute costs from warehouses,\nserverless features, and total storage costs.\n ... \nSection Title: Resource Optimization: Billing Metrics > Billing Metrics (T1) > Tier 1 > SQL\nContent:\n/* These queries can be used to measure where costs have been incurred by\nthe different cost vectors within a Snowflake account including:\nWarehouse Costs\nServerless Costs\nStorage Costs\nTo accurately report the dollar amounts, make changes to the variables\ndefined on lines 17 to 20 to properly reflect your credit price, the initial\ncapacity purchased, when your contract started and the term (default 12 months)\nIf unsure, ask your Sales Engineer or Account Executive\n*/\nUSE DATABASE SNOWFLAKE;\nUSE SCHEMA ACCOUNT_USAGE;\nSET CREDIT_PRICE = 4.00; --edit this number to reflect credit price\nSET TERM_LENGTH = 12; --integer value in months\nSET TERM_START_DATE = '2019-01-01';\nSET TERM_AMOUNT = 100000.00; --number(10,2) value in dollars\nWITH CONTRACT_VALUES AS (\n ... \nSection Title: Resource Optimization: Billing Metrics > Average Cost per Query by Warehouse (T2) > Tier 2 > SQL\nContent:\nset credit_price = 4;  --edit this value to reflect your credit price\nSELECT\nCOALESCE(WC.WAREHOUSE_NAME,QC.WAREHOUSE_NAME) AS WAREHOUSE_NAME\n,QC.QUERY_COUNT_LAST_MONTH\n,WC.CREDITS_USED_LAST_MONTH\n,WC.CREDIT_COST_LAST_MONTH\n,CAST((WC.CREDIT_COST_LAST_MONTH / QC.QUERY_COUNT_LAST_MONTH) AS decimal(10,2) ) AS COST_PER_QUERY\nFROM (\nSELECT\nWAREHOUSE_NAME\n,COUNT(QUERY_ID) as QUERY_COUNT_LAST_MONTH\nFROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\nWHERE TO_DATE(START_TIME) >= TO_DATE(DATEADD(month,-1,CURRENT_TIMESTAMP()))\nGROUP BY WAREHOUSE_NAME\n) QC\nJOIN (\n```\nSELECT\n    WAREHOUSE_NAME\n    ,SUM(CREDITS_USED) as CREDITS_USED_LAST_MONTH\n    ,SUM(CREDITS_USED)*($CREDIT_PRICE) as CREDIT_COST_LAST_MONTH\nFROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\nWHERE TO_DATE(START_TIME) >= TO_DATE(DATEADD(month,-1,CURRENT_TIMESTAMP()))\nGROUP BY WAREHOUSE_NAME\n```\n) WC\nON WC.WAREHOUSE_NAME = QC.WAREHOUSE_NAME\nSection Title: Resource Optimization: Billing Metrics > Average Cost per Query by Warehouse (T2) > Tier 2 > SQL\nContent:\nORDER BY COST_PER_QUERY DESC\n;"
      ]
    },
    {
      "url": "https://medium.com/@kenny.nagano/cost-attribution-tagging-in-snowflake-6eceeea1c4b6",
      "title": "Cost Attribution Tagging in Snowflake | by Kenny Nagano | Medium",
      "publish_date": "2024-04-12",
      "excerpts": [
        "Section Title: Cost Attribution Tagging in Snowflake > What is a Tag?\nContent:\nAfter you have tagged your objects, you can now create a view leveraging your snowflake data share and account_usage view. This example query below will give you a summary of the past 7 days . These next 2 queries might have to be slightly adjusted but is a great starting off point.\n```\nCREATE OR  REPLACE  VIEW  TAGGING_DB.TAGGING_SCHEMA.V_COMPUTE_CREDITS_PER_COST_CENTER   \nAS  \nSELECT  \n    TAG_VALUE  AS  COST_CENTER,  \nSUM (NVL(CREDITS_USED,  0 ))  AS  CREDITS   \nFROM  \n    SNOWFLAKE.ACCOUNT_USAGE.TAG_REFERENCES   \nLEFT JOIN  \n    SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY  ON  WAREHOUSE_NAME = OBJECT_NAME   \nWHERE  \n    TAG_NAME = 'COST_CENTER'  \nAND  TAG_DATABASE = 'TAGGING_DB'  \nAND  TAG_SCHEMA = 'TAGGING_SCHEMA'  \nAND  START_TIME >= DATEADD( 'DAYS' , -7 , CURRENT_DATE ())  \nGROUP BY 1  \nORDER BY 2 DESC ; and  you should  get  an output that looks  like  this\n```\nSection Title: Cost Attribution Tagging in Snowflake > What is a Tag?\nContent:\nAs you know, Snowflake spend is based on computer and storage. We will need to create a view to see how much storage each department is consuming.\n```\nCREATE OR  REPLACE  VIEW  TAGGING_DB.TAGGING_SCHEMA.V_STORAGE_USAGE_PER_COST_CENTER   \nAS  \nSELECT  \n    TAG_VALUE  AS  COST_CENTER,  \nSUM ((ACTIVE_BYTES  +  FAILSAFE_BYTES  +  TIME_TRAVEL_BYTES  +  RETAINED_FOR_CLONE_BYTES) / POWER ( 1024 , 3 ))::NUMBER( 8 , 2 )  AS  TOTAL_TB  \nFROM  \n    SNOWFLAKE.ACCOUNT_USAGE.TAG_REFERENCES   \nLEFT JOIN  \n    SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS  ON  TABLE_SCHEMA = OBJECT_NAME   \nWHERE  \n    TAG_NAME = 'COST_CENTER'  \nGROUP BY ALL  \nORDER BY 2 DESC ;\n```\nYou\u2019ll receive storage output by tag in terabytes (TB) and will need to convert it into Snowflake Credits. An estimated storage cost is $23 per TB per month. Next, calculate the number of credits consumed based on the price per credit.\nSection Title: Cost Attribution Tagging in Snowflake > What is a Tag?\nContent:\nAbove are two basic queries to create 2 different views, but you can explore more detailed historical cost using by writing queries against views in the [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) and [ORGANIZATION_USAGE](https://docs.snowflake.com/en/sql-reference/organization-usage) schemas. You may not immediately receive data from the views you created earlier. The metadata loading into account_usage can be delayed, depending on the tagged objects. Depending on the view you used, there could be a delay of up to three hours. For the most accurate timeframe, please refer to the documentation.\nSection Title: Cost Attribution Tagging in Snowflake > What is a Tag?\nContent:\nWith the data collected through object tagging, organizations can build comprehensive reports and dashboards that provide insights into cost attribution and usage trends. These reports can help stakeholders make informed decisions about resource allocation and cost optimization strategies, leading to more efficient use of resources and cost savings. There are many different options. You can create simple dashboards in Snowsight or create a complete data app using Streamlit in Snowflake.\nSnowsight allows you to quickly and easily obtain information about cost from a visual dashboard. Queries against the usage views allow you to drill down into cost data and can help generate custom reports and dashboards."
      ]
    },
    {
      "url": "https://www.linkedin.com/posts/rajiv-gupta-618b0228_optimizing-cost-attribution-in-snowflake-activity-7354005174287269888-DYCb",
      "title": "How to Build a Chargeback Model for Snowflake: A Guide - LinkedIn",
      "publish_date": "2025-07-23",
      "excerpts": [
        "How to Build a Chargeback Model for Snowflake: A Guide ... Snowflake Documentation and best practices, Query History and Snowflake Intelligence!"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-exploring-compute",
      "title": "Exploring compute cost - Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Visibility Exploring cost Exploring compute cost\nSection Title: Exploring compute cost \u00b6\nContent:\nTotal compute cost consists of the overall use of:\nVirtual warehouses (user-managed compute resources)\nServerless features such as Automatic Clustering and Snowpipe that use Snowflake-managed compute resources\nCloud services layer of the Snowflake architecture\nvCPU usage for Openflow BYOC cost and scaling considerations and Openflow Snowflake Deployment cost and scaling considerations .\nSee Openflow components for more information about Openflow components including runtimes.\nThis topic describes how to gain insight into historical compute costs using Snowsight , or by writing queries against views in\nthe ACCOUNT_USAGE and ORGANIZATION_USAGE schemas.\nSnowsight allows you to quickly and easily obtain information about cost from a visual dashboard. Queries against the usage views\nallow you to drill down into cost data and can help generate custom reports and dashboards.\nSection Title: Exploring compute cost \u00b6\nContent:\nIf you need more information about how compute costs are incurred, refer to Understanding compute cost .\nNote\nThe cloud services layer consumes credits, but not all of those credits are actually billed. Usage for cloud services is charged only if\nthe daily consumption of cloud services exceeds 10% of the daily usage of virtual warehouses. Snowsight and a majority of views\nshow the total number of credits consumed by warehouses, serverless features, and cloud services without accounting for this daily\nadjustment to cloud services.\nTo determine how many credits were actually billed for compute costs, run queries against the METERING_DAILY_HISTORY view .\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Cortex functions \u00b6\nContent:\nQuery: Credit consumption by Cortex functions.\nThis query shows the credit consumption for each Cortex function call, aggregated in one hour increments based on\nfunction and model.\n```\nSELECT * \n  FROM SNOWFLAKE . ACCOUNT_USAGE . CORTEX_FUNCTIONS_USAGE_HISTORY ;\n```\nCopy\nQuery: Credit consumption by Cortex function called with the `mistral-large` model.\nThis query shows the credit consumption for each Cortex function called with the `mistral-large` model, aggregated in one\nhour increments based on function and model.\n```\nSELECT * \n  FROM SNOWFLAKE . ACCOUNT_USAGE . CORTEX_FUNCTIONS_USAGE_HISTORY \n  WHERE model_name = 'mistral-large' ;\n```\nCopy\nQuery: Credit consumption by Cortex functions query.\nThis query shows the credit consumption for each Cortex functions query, aggregated in one hour increments based on\nfunction and model.\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Cortex REST API \u00b6\nContent:\nQuery: Credit consumption by Cortex REST API.\nThis query shows the credit consumption for Cortex REST API calls, including the number of tokens processed\nand the model used for each request.\n```\nSELECT * \n  FROM SNOWFLAKE . ACCOUNT_USAGE . CORTEX_REST_API_USAGE_HISTORY ;\n```\nCopy"
      ]
    },
    {
      "url": "https://www.kipi.ai/insights/optimizing-cost-attribution-in-snowflake-a-chargeback-model-guide/",
      "title": "Optimizing Cost Attribution in Snowflake: A Chargeback Model Guide",
      "excerpts": [
        "Jun 20, 2025 \u2014 Cost attribution in Snowflake involves assigning usage costs to specific departments, projects, or teams. This process is vital for\u00a0..."
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}

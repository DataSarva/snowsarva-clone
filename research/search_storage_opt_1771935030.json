{
  "search_id": "search_3d0904873f62463eadf383fe61eb2f46",
  "results": [
    {
      "url": "https://www.unraveldata.com/insights/snowflake-costs/",
      "title": "How Can I Reduce My Snowflake Costs? - Unravel Data",
      "publish_date": "2025-07-22",
      "excerpts": [
        "WHY OUR AI\nPLATFORMDATA OBSERVABILITY FORPLATFORM & PRICINGUSE CASESFeaturedGet a Free Health Check ReportFor Databricks For SnowflakeTour Unravel\u2019s Key Product Features for YourselfExplore Now\nDatabricks\nSnowflake\nGoogle Cloud BigQuery\nAmazon EMR\nCloudera\nPlatform Overview\nAll Integrations & APIs\nPlans & Pricing\nCloud Cost Management & FinOps\nOperations & Troubleshooting\nPipeline & App Optimization\nAutofix & Prevention\nData Quality & Reliability\nCloud Migration\nRESOURCESTOP TOPICSTOP TECHNOLOGIESTOP CONTENT TYPESFeaturedAI Agents: Empower Data Teams With Actionability TMRead MoreData Actionability TM : Empower Your TeamRead More\nAI & Automation\nCost Optimization & FinOps\nTroubleshooting & DataOps\nPerformance & Data Eng\nCloud Migration\nCI/CD\nExplore All\nDatabricks\nSnowflake\nBigQuery\nExplore All\nCustomer Stories\nProduct Docs\nResearch & Reports\nWebinars & Podcasts\nExplore All\nCUSTOMERS\nCOMPANY\nAbout Unravel\nCustomers\nCareers\nPartners\nEvents\nNews & Press\nSUPPORT\nFAQ\n[Customer\n ... \nCloud Cost Optimization\nSection Title: How can I reduce my Snowflake costs?\nContent:\nHow to Cut Snowflake Costs: Optimize Query Performance, Right-Size Warehouses, and Implement Smart Scaling for 30-60% Savings Worried about rising Snowflake costs? You open the dashboard, see hundreds or thousands wasted on credits, and need real [\u2026]\n5 min read\nSection Title: ... > How to Cut Snowflake Costs: Optimize Query Performance, Right-Size Warehouses, and Implemen...\nContent:\nWorried about rising **Snowflake costs** ? You open the dashboard, see hundreds or thousands wasted on credits, and need real answers now. If your Snowflake bill jumped from $5,000 to $15,000, you\u2019re not alone\u2014inefficient queries, oversized warehouses, and poor resource management are the real culprits behind surging Snowflake charges.\nHere\u2019s the good news: Optimizing a few key factors can slash your **Snowflake cost** 30-60%, often in your very next billing cycle. It\u2019s not rocket science, but knowing what really drives those Snowflake expenses is a game-changer for cloud cost management.\nSection Title: ... > How to Cut Snowflake Costs: Optimize Query Performance, Right-Size Warehouses, and Implemen...\nContent:\n**TL;DR:** The fastest way to reduce your Snowflake costs: Optimize slow or data-hungry queries, right-size your virtual warehouses, and enforce auto-suspend. Kill off unused or zombie warehouses and stop paying for compute you don\u2019t use. Most companies see Snowflake savings of 30-60% in the first two months.\nWhat really drives your Snowflake costs? Storage is usually small potatoes. For 80-90% of organizations, **compute costs** (warehouse credits) cause the big bills, especially when queries are unoptimized or warehouses are oversized.\nThink of Snowflake charges like an electric bill. Sure, baseline usage is predictable. But leave compute \u201con\u201d for inefficient queries, and Snowflake costs explode. A single rogue query running multiple times a day can burn thousands in unnecessary compute spend every month.\nSection Title: How can I reduce my Snowflake costs? > Snowflake Costs: Where Your Money Is Really Going\nContent:\nMost organizations focus on the wrong line items\u2014worrying about storage when compute charges (credits) are really spiking their Snowflake bills.\n**Major Snowflake cost drains:**\nPoorly optimized queries that scan excessive data\nOversized warehouses using more credits than needed\nAuto-suspend settings that keep warehouses active long after use\nUntracked background processes consuming compute\nData modeling or tables that make every query less efficient\nSnowflake\u2019s billing tells you how many credits you\u2019re burning, but finding the exact cause of high Snowflake costs often takes detective work with query and warehouse reporting.\nSection Title: How can I reduce my Snowflake costs? > ... > When Inefficient Queries Spike Snowflake Costs\nContent:\nA single badly written query can use 10x more credits than it should\u2014especially if it runs repeatedly. These Snowflake cost hogs often go unnoticed until the bill arrives. Identifying and optimizing them can yield immediate reductions in Snowflake costs.\n ... \nSection Title: How can I reduce my Snowflake costs? > Quick Wins for Lowering Snowflake Costs\nContent:\nGet Snowflake savings quickly with these high-impact actions:\n ... \nSection Title: ... > Unlocking the Biggest Snowflake Cost Savings: Query Optimization\nContent:\nQuery optimization is the best path to slashing Snowflake costs. Fine-tune your SQL for maximum efficiency, and watch your compute credits drop.\nSection Title: How can I reduce my Snowflake costs? > ... > How to Optimize Queries to Minimize Snowflake Costs\nContent:\nApply clustering keys to large tables commonly filtered on the same columns\nWrite WHERE clauses that let Snowflake skip unnecessary data scans\nOptimize JOIN order (smallest to largest tables first)\nCreate materialized views for repeated, resource-intensive aggregations\nLeverage result caching for high-frequency queries to minimize compute\nEfficient queries scan fewer micro-partitions\u2014finishing faster and incurring less cost per job. It\u2019s the simplest Snowflake optimization with the highest ROI.\nSection Title: ... > Stop wasting Snowflake spend\u2014act now with a free Snowflake cost health check.\nContent:\nRequest Your Snowflake Cost Health Check Report\nSection Title: How can I reduce my Snowflake costs? > ... > Improve Data Organization to Reduce Snowflake Costs\nContent:\nSmart clustering and data modeling can reduce per-query Snowflake expenses by 70-90% for large tables. Use these methods judiciously for the highest ROI.\nSection Title: How can I reduce my Snowflake costs? > ... > Maximize Snowflake Result Cache\nContent:\nStructure queries for repeatability to benefit from Snowflake\u2019s result cache\u2014returning results at no extra cost and making a dramatic impact on your monthly Snowflake bill.\n ... \nSection Title: How can I reduce my Snowflake costs? > ... > Configure Smart Scaling in Snowflake\nContent:\nScale up automatically based on workload, then scale down rapidly when demand drops. Snowflake\u2019s multi-cluster warehouses can save money when tuned for real usage\u2014but aggressive settings can backfire, so monitor carefully.\n ... \nSection Title: ... > ETL Processing Disaster Turned Into $25,000/Month in Snowflake Cost Savings\nContent:\nAfter a schema change, a retailer\u2019s ETL jobs scanned entire tables, increasing Snowflake costs by 10x. Once they reclustered and fixed problem queries, daily costs dropped 85%\u2014saving $25K per month.\n ... \nSection Title: ... > Materialized Views Power $100K in Annual Snowflake Cost Reductions\nContent:\nBy re-architecting dashboards to use materialized views and summarized tables, a tech company reduced compute usage by 75% while improving refresh speed, driving major reductions in Snowflake charges.\nSection Title: How can I reduce my Snowflake costs? > ... > Real Accountability for Spending\nContent:\nImplement chargeback so business teams see their actual Snowflake usage. When each group is responsible for their own warehouses, they prioritize cost controls and optimization.\nSection Title: How can I reduce my Snowflake costs? > ... > Treat Snowflake Cost Optimization as an Ongoing Process\nContent:\nReview Snowflake costs monthly and hunt for new optimization wins\nAudit warehouse configuration quarterly\nRe-test performance as workloads evolve\nProvide training in cost-efficient Snowflake practices\nOngoing monitoring and optimization prevent small leaks from becoming massive Snowflake cost problems. Top performers treat this as a continuous discipline\u2014not a one-off project.\n ... \nSection Title: How can I reduce my Snowflake costs? > ... > Snowflake Storage Optimization\nContent:\nKeep Time Travel periods tight to avoid high storage bills\nSet up lifecycle policies to purge stale data as soon as business rules allow\nArchive cold data to external storage when appropriate\nTrack storage growth and implement governance policies\nSection Title: How can I reduce my Snowflake costs? > ... > Leverage Advanced Query Techniques\nContent:\nBig Snowflake deployments can unlock cost savings using result materialization, scoped clustering, and workload-specific pipeline optimization.\nSection Title: How can I reduce my Snowflake costs? > Track and Measure Snowflake Cost Optimization Effectively\nContent:\nCredit and dollar consumption per query, user, and team\nQuery execution and warehouse idle time trends\nWarehouse utilization and compute efficiency\nBusiness function-level cost tracking\nThe best metric for Snowflake success: trend to lower credits and lower cost-per-insight over time with stable or improved performance.\nMost organizations unlock 30-60% cost savings very quickly. The right mix of warehouse right-sizing, query optimization, storage management, and monitoring usually pays for itself in under six months.\nSection Title: How can I reduce my Snowflake costs? > Snowflake Cost Optimization: Mistakes to Avoid\nContent:\nDon\u2019t over-optimize for tiny savings\u2014slow performance can negate any reduction in Snowflake costs.\nMonitor results: Validate that cost-reduction changes don\u2019t break functionality.\nEngage business users: Sometimes expensive queries serve real, irreplaceable needs.\nSection Title: How can I reduce my Snowflake costs? > Your Snowflake Cost Action Plan\nContent:\n**Week 1:** Focus on warehouse right-sizing and tightening auto-suspend.\n**Weeks 2-4:** Identify and optimize the most expensive queries using Snowflake\u2019s query history.\n**Months 2-3:** Deploy clustering keys, run advanced warehouse and storage optimizations, and refine query patterns.\n**Ongoing:** Institute ongoing cost governance and regular Snowflake optimization reviews.\nStart with the quick wins, then advance to strategic optimization. Combination of immediate and long-term improvements typically delivers 30-60% reduced Snowflake costs while maintaining or improving performance.\nTurn those Snowflake cost savings into investments that grow your business\u2014rather than throwing money at inefficient queries and idle compute.\nSection Title: How can I reduce my Snowflake costs? > Your Snowflake Cost Action Plan\nContent:\nPublishedJuly 16 2025AuthorUnravel DataRelated PostsExplore Other Insights By\n[What Snowflake cost management tool features are essential for automation?](https://www.unraveldata.com/insights/snowflake-cost-management-tool-features/)\n[How can I tie Snowflake cost monitoring to budget forecasting and planning?](https://www.unraveldata.com/insights/snowflake-cost-monitoring/)\n[What are the top Snowflake cost management tool evaluation criteria?](https://www.unraveldata.com/insights/snowflake-cost-management-tool-evaluation-criteria/)\n[Cloud Cost Optimization](https://www.unraveldata.com/insight/cloud-cost-optimization-insights/)\n[FinOps](https://www.unraveldata.com/insight/finops-insights/)\n[Snowflake](https://www.unraveldata.com/insight/snowflake-data-observability-insights/)\n ... \nSection Title: How can I reduce my Snowflake costs? > Your Snowflake Cost Action Plan\nContent:\nWhy Our AI\nDatabricks\nSnowflake\nGoogle Cloud BigQuery\nCloudera\nCloud Cost Management\nOperations & Troubleshooting\nPipeline & App Optimization\nAutofix & Prevention\nData Quality & Reliability\nCloud Migration\nPlatform Overview\nAll Integrations & APIs\nPlans & Pricing\nRESOURCES\nAI & Automation\nCost Optimization & FinOps\nTroubleshooting & DataOps\nPerformance & Data Eng\nCloud Migration\nCI/CD\nDatabricks\nSnowflake\nBigQuery\nCustomer Stories\nProduct Docs\nResearch & Reports\nWebinars & Podcasts\nInsights\nExplore All\nCOMPANY\nAbout Unravel\nCustomers\nCareers\nPartners\nEvents\nNews & Press\nFAQ\n[Customer Portal](https://customers.unraveldata.com/login?ec=302&startURL=%2Fs%2F)\n[Documentation](https://docs.unraveldata.com/?lang=en)\nContact Us\nDetect, fix, and prevent data issues with the most advanced AI-native data observability and FinOps platform on the market. Powerful automated action you fully control."
      ]
    },
    {
      "url": "https://seemoredata.io/blog/performance-tuning-in-snowflake/",
      "title": "Snowflake Performance Tuning in 2025: Pro Tips and Common Mistakes in Snowflake: Best Practices, Common Mistakes, and Pro Tips for Data Teams | Seemore Data",
      "publish_date": "2025-09-01",
      "excerpts": [
        "Solution\n[Continuous Cost Control](https://seemoredata.io/data-cost-control/)\n[Warehouse Optimization](https://seemoredata.io/data-warehouse-optimization/)\n[Usage-Based Data Pipeline Optimization](https://seemoredata.io/usage-based-optimization/)\n[Data efficiency AI Agent](https://seemoredata.io/data-efficiency-ai-agent/)\nProduct\n[Root Cause Anomalies Detection](https://seemoredata.io/snowflake-root-cause-anomalies-detection/)\n[SmartPulse](https://seemoredata.io/smart-pulse-hourly-autonomous-snowflake-warehouse-optimization/)\n[AI Auto-Clustering](https://seemoredata.io/intelligent-snowflake-auto-clustering/)\n[Integrations](https://seemoredata.io/integrations/)\n[Snowflake optimization](https://seemoredata.io/integrations/snowflake/)\n[ETL / ELT optimization](https://seemoredata.io/integrations/etl/)\n[Orchestration optimization](https://seemoredata.io/integrations/orchestration/)\n[BI optimization](https://seemoredata.io/integrations/bitools-2/)\n[All\n ... \nSection Title: Snowflake Performance Tuning in 2025: Pro Tips and Common Mistakes in Snowflake: Best Practices, ...\nContent:\nIdan Birnboim\nFeb 13, 2025\nSection Title: Snowflake Performance Tuning in 2025: Pro Tips and Common Mistakes in Snowflake: Best Practices, ...\nContent:\n[](https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fseemoredata.io%2Fblog%2Fperformance-tuning-in-snowflake%2F&linkname=Snowflake%20Performance%20Tuning%20in%202025%3A%20Pro%20Tips%20and%20Common%20Mistakes%20in%20Snowflake%3A%20Best%20Practices%2C%20Common%20Mistakes%2C%20and%20Pro%20Tips%20for%20Data%20Teams \"Email\") [](https://www.addtoany.com/add_to/facebook_messenger?linkurl=https%3A%2F%2Fseemoredata.io%2Fblog%2Fperformance-tuning-in-snowflake%2F&linkname=Snowflake%20Performance%20Tuning%20in%202025%3A%20Pro%20Tips%20and%20Common%20Mistakes%20in%20Snowflake%3A%20Best%20Practices%2C%20Common%20Mistakes%2C%20and%20Pro%20Tips%20for%20Data%20Teams \"Messenger\")\nSection Title: Snowflake Performance Tuning in 2025: Pro Tips and Common Mistakes in Snowflake: Best Practices, ...\nContent:\n[](https://www.addtoany.com/add_to/whatsapp?linkurl=https%3A%2F%2Fseemoredata.io%2Fblog%2Fperformance-tuning-in-snowflake%2F&linkname=Snowflake%20Performance%20Tuning%20in%202025%3A%20Pro%20Tips%20and%20Common%20Mistakes%20in%20Snowflake%3A%20Best%20Practices%2C%20Common%20Mistakes%2C%20and%20Pro%20Tips%20for%20Data%20Teams \"WhatsApp\") [](https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fseemoredata.io%2Fblog%2Fperformance-tuning-in-snowflake%2F&linkname=Snowflake%20Performance%20Tuning%20in%202025%3A%20Pro%20Tips%20and%20Common%20Mistakes%20in%20Snowflake%3A%20Best%20Practices%2C%20Common%20Mistakes%2C%20and%20Pro%20Tips%20for%20Data%20Teams \"LinkedIn\")\nSection Title: Snowflake Performance Tuning in 2025: Pro Tips and Common Mistakes in Snowflake: Best Practices, ...\nContent:\n[](https://www.addtoany.com/add_to/tumblr?linkurl=https%3A%2F%2Fseemoredata.io%2Fblog%2Fperformance-tuning-in-snowflake%2F&linkname=Snowflake%20Performance%20Tuning%20in%202025%3A%20Pro%20Tips%20and%20Common%20Mistakes%20in%20Snowflake%3A%20Best%20Practices%2C%20Common%20Mistakes%2C%20and%20Pro%20Tips%20for%20Data%20Teams \"Tumblr\") [](https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fseemoredata.io%2Fblog%2Fperformance-tuning-in-snowflake%2F&linkname=Snowflake%20Performance%20Tuning%20in%202025%3A%20Pro%20Tips%20and%20Common%20Mistakes%20in%20Snowflake%3A%20Best%20Practices%2C%20Common%20Mistakes%2C%20and%20Pro%20Tips%20for%20Data%20Teams \"Facebook\")\nSection Title: Snowflake Performance Tuning in 2025: Pro Tips and Common Mistakes in Snowflake: Best Practices, ...\nContent:\n[](https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fseemoredata.io%2Fblog%2Fperformance-tuning-in-snowflake%2F&linkname=Snowflake%20Performance%20Tuning%20in%202025%3A%20Pro%20Tips%20and%20Common%20Mistakes%20in%20Snowflake%3A%20Best%20Practices%2C%20Common%20Mistakes%2C%20and%20Pro%20Tips%20for%20Data%20Teams \"Twitter\")\n ... \nSection Title: ... > **Why Snowflake Performance Tuning Matters?**\nContent:\nPerformance tuning in Snowflake is not just about speeding up queries. It directly impacts cost efficiency and user experience. Slow-running queries can delay critical business insights, frustrate analysts, and increase compute costs unnecessarily. Since Snowflake operates on a pay-as-you-go model, every second a query runs consumes compute credits, making [optimization essential for cost control](https://seemoredata.io/blog/best-snowflake-tools-for-cost-optimization-management/) .\nSnowflake\u2019s pricing structure is based on compute and storage usage. Compute costs are tied to virtual warehouses, which scale dynamically based on workload demand. Poorly optimized queries often lead to excessive resource consumption, forcing organizations to run larger warehouses for longer periods. Additionally, inefficient data storage, such as unoptimized micro-partitions, can cause queries to scan more data than needed, further increasing costs.\n ... \nSection Title: ... > **7 Reasons Your Snowflake Queries Might Be Running Slo** w\nContent:\n**3. Missing or Poor Clustering** **\n** Snowflake automatically organizes data into micro-partitions, but without proper clustering on high-cardinality columns (like timestamps or user IDs), performance can degrade over time. Especially for large datasets, the lack of clustering can lead to full table scans.\n**4. Inefficient Joins and Lack of Statistics** **\n** Joining large tables without filters or broadcasting small tables to large ones can overload compute resources. Additionally, if Snowflake lacks updated statistics, it may choose suboptimal join strategies.\n**5. Warehouse Misconfiguration** **\n** Running queries on under-provisioned warehouses can result in slower performance, especially for resource-intensive workloads. Conversely, oversizing warehouses can lead to high costs without much performance gain.\n ... \nSection Title: ... > **Separation of Storage and Compute**\nContent:\nIn Snowflake, data is stored in a centralized location, while compute power comes from virtual warehouses that process queries. This separation provides flexibility, allowing users to scale compute resources up or down without affecting data storage. However, improper warehouse sizing or inefficient query execution can lead to wasted compute credits. To optimize performance, teams should:\nChoose the right warehouse size based on workload complexity.\nEnable auto-suspend and auto-resume to avoid idle compute costs.\nRun queries in the smallest possible warehouse that meets performance needs.\nSection Title: ... > **Micro-Partitions and Clustering**\nContent:\nSnowflake automatically organizes data into [micro-partitions](https://docs.snowflake.com/en/user-guide/tables-micro-partitions) , which improve query speed by reducing the amount of data scanned. However, if a table\u2019s natural data order doesn\u2019t align with query patterns, performance can suffer. Clustering can help by logically grouping related data, minimizing unnecessary scans. Best practices include:\nUsing clustering keys on frequently filtered columns.\nMonitoring partition pruning efficiency with query profiles.\nAvoiding excessive manual clustering, which can increase costs.\nSection Title: ... > **Snowflake Query Optimizer**\nContent:\nThe Snowflake query optimizer automatically determines the best execution plan based on available statistics and table metadata. While Snowflake handles many optimizations internally, manual tuning can help in cases where performance issues persist. Users should:\nAnalyze query plans using EXPLAIN to identify bottlenecks.\nOptimize JOIN conditions and avoid unnecessary cross-joins.\nLeverage materialized views and result caching when applicable.\nBy leveraging Snowflake\u2019s architecture effectively, teams can maximize performance while keeping costs under control.\nSection Title: ... > **4 Critical Best Practices for Performance Tuning in Snowflake**\nContent:\nOptimizing Snowflake\u2019s performance requires a combination of efficient query design, proper storage management, and strategic resource scaling. By implementing the following best practices, data teams can enhance query speed, reduce costs, and maximize the platform\u2019s capabilities.\nSection Title: ... > **1. Optimizing Query Design**\nContent:\nPoorly structured queries can slow down performance and lead to excessive compute costs. Snowflake\u2019s query engine is designed to optimize execution plans automatically, but following these best practices ensures optimal performance:\n ... \nSection Title: ... > **2. Using Clustering and Micro-Partitions Effectively**\nContent:\nUse automatic clustering for dynamic datasets \u2013 Snowflake\u2019s automatic clustering continuously reorganizes data, improving query efficiency without manual intervention.\nManually define clustering keys for large, frequently queried tables \u2013 If queries consistently filter by specific columns (e.g., date or customer_id ), defining a clustering key ensures Snowflake physically groups related data together, reducing scan time.\nCheck partition pruning effectiveness \u2013 Use EXPLAIN and query profiles to verify that Snowflake is scanning the expected number of partitions. If excessive partitions are scanned, re-evaluate clustering strategies.\nAvoid excessive clustering \u2013 While clustering can improve performance, unnecessary clustering can increase compute costs. Regularly analyze whether clustering benefits outweigh its cost.\n ... \nSection Title: ... > **4. Scaling Compute Resources Wisely**\nContent:\nBy applying these best practices, data teams can enhance Snowflake\u2019s performance, reduce query execution time, and optimize cost efficiency.\n ... \nSection Title: ... > **3. Failing to Use Materialized Views or Clustering Effectively**\nContent:\nMaterialized views and clustering can significantly improve query performance, but many teams either misuse them or fail to implement them at all.\n**How to avoid this mistake:**\nUse Materialized Views for frequently executed aggregation queries to reduce computation time.\nDefine clustering keys on large tables with predictable filtering patterns, such as date-based columns.\nRegularly review clustering performance and avoid unnecessary reclustering operations that increase costs.\nSection Title: ... > **4. Inefficient Data Storage Leading to Poor Pruning Performance**\nContent:\nSnowflake relies on micro-partitions to improve query efficiency, but poor data storage practices can prevent effective partition pruning. If queries scan more partitions than necessary, performance suffers.\n**How to avoid this mistake:**\nEnsure date columns and high-cardinality fields are used effectively for partition pruning.\nAvoid transforming date fields in WHERE clauses (e.g., TO_CHAR(created_at, \u2018YYYY-MM\u2019) prevents pruning).\nUse EXPLAIN to check if queries are scanning excessive partitions and optimize filtering conditions accordingly.\n ... \nSection Title: ... > AWS Outage: When \u201cNot My Cloud\u201d Still Becomes Your Outage\nContent:\nAriel Utnik\nOct 28, 2025\n[](https://seemoredata.io/blog/snowflake-data-types/)\n15 min read"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/getting-started-cost-performance-optimization/",
      "title": "Getting Started with Cost and Performance Optimization - Snowflake",
      "excerpts": [
        "Section Title: Getting Started with Cost and Performance Optimization > **Overview**\nContent:\nBy completing this guide, you will be able to understand and implement various optimization features on Snowflake.\n**Setup Environment** : Use correct roles and sample datasets to use the optimization features\n**Account Usage** : Understand purpose of Account Usage schema and use it to uncover savings opportunities\n**Warehouse Controls** : Leverage settings on a virtual warehouse to optimize usage\n**Storage** : Determine cost savings with high-churn and short-lived tables\n**Optimization Features** : Utilize Snowflake optimization features to achieve cost or performance savings\nSection Title: Getting Started with Cost and Performance Optimization > **Overview** > **Prerequisites**\nContent:\nFamiliarity with [Snowflake platform](https://docs.snowflake.com/en/user-guide/intro-key-concepts)\nBasic understanding of [micro-partitions](https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions)\n[Accountadmin](https://docs.snowflake.com/en/user-guide/security-access-control-considerations) access on a Snowflake account\nIf you do not have access to a Snowflake account, you can sign up for a [free trial](https://signup.snowflake.com/?utm_source=snowflake-devrel&utm_medium=developer-guides&utm_cta=developer-guides)\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Setup**\nContent:\nThis section contains the code that needs to be executed in your Snowflake account to enable understanding of content in this guide.\n-- WAREHOUSE CREATION --\nUSE ROLE ACCOUNTADMIN;\ncreate warehouse if not exists hol_compute_wh\nwith warehouse_size='SMALL'\nwarehouse_type='STANDARD'\ninitially_suspended=TRUE\nauto_resume=FALSE\n;\nuse warehouse hol_compute_wh;\n-- DATABASE SCHEMA CREATION --\ncreate database if not exists OPT_HOL;\nuse database OPT_HOL;\ncreate schema if not exists DEMO;\nuse schema OPT_HOL.DEMO;\ncreate or replace table lineitem as select * from snowflake_sample_data.tpch_sf100.lineitem order by L_PARTKEY;\ncreate or replace table orders as select * from snowflake_sample_data.tpch_sf100.orders;\ncreate or replace table part as select * from snowflake_sample_data.tpch_sf100.part;\ncreate or replace table lineitem_cl as select * from lineitem;\nalter table lineitem_cl cluster by linear(l_shipdate);\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Account Usage Queries** > SQL\nContent:\n-- SETTING CONTEXT FOR THE SESSION --\nUSE ROLE ACCOUNTADMIN;\nUSE SCHEMA SNOWFLAKE.ACCOUNT_USAGE;\nuse warehouse hol_compute_wh;\n-- SAMPLE ACCOUNT USAGE QUERIES\n-- Warehouse Usage Metrics\nselect * from snowflake.account_usage.warehouse_metering_history limit 10;\n-- Access History for objects used in queries\nselect * from snowflake.account_usage.access_history limit 10;\n-- Snowpipe Usage Metrics\nselect * from snowflake.account_usage.pipe_usage_history limit 10;\n-- Storage Metrics\nselect * from snowflake.account_usage.storage_usage limit 10;\n-- Table Storage Detailed Metrics\nselect * from snowflake.account_usage.table_storage_metrics limit 10;\n```\n\nCopy\n```\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring**\nContent:\nThis section covers the code to identify high churn tables - significant DML, short lived tables - tables truncated and reloaded everyday and tables not active in the past 90 days.\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\n-- SETTING CONTEXT FOR THE SESSION ----\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE hol_compute_wh;\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\n-- Identify high churn tables or short lived tables\nSELECT\nt.table_catalog||'.'||t.table_schema||'.\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\n'||t.table_name as fq_table_name\n,t.active_bytes/power(1024,3) as active_size_gb\n,t.time_travel_bytes/power(1024,3) as time_travel_gb\n,t.failsafe_bytes/power(1024,3) as failsafe_gb\n,t.retained_for_clone_bytes/power(1024,3) as clone_retain_gb\n,active_size_gb+time_travel_gb+failsafe_gb+clone_retain_gb as total_size_gb\n,(t.time_travel_bytes + t.failsafe_bytes + t.retained_for_clone_bytes)/power(1024,3) as non_active_size_gb\n,div0(non_active_size_gb,active_size_gb)*100 as churn_pct\n,t.deleted\n,timediff('hour',t.table_created,t.table_dropped) as table_life_duration_hours\n,t1.is_transient\n,t1.table_type\n,t1.retention_time\n,t1.auto_clustering_on\n,t1.clustering_key\n,t1.last_altered\n,t1.last_ddl\nFROM\nsnowflake.account_usage.table_storage_metrics t\nJOIN snowflake.account_usage.tables t1\nON t.id=t1.table_id\nWHERE\n1=1\n--AND t1.table_catalog in ('','') -- use this to filter on specific\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\ndatabases\nAND\n(\nchurn_pct>=40\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\n```\n         table_life_duration_hours<=24  -- short lived tables\n        )\n```\nORDER BY total_size_gb desc;\n-- Unused tables\n-- Identify Table sizes and Last DDL/DML Timestamps\nSELECT TABLE_CATALOG || '.' || TABLE_SCHEMA || '.' || TABLE_NAME AS TABLE_PATH\n,TABLE_NAME\n,TABLE_SCHEMA AS SCHEMA\n,TABLE_CATALOG AS DATABASE\n,BYTES\n,TO_NUMBER(BYTES / POWER(1024,3),10,2) AS GB\n,LAST_ALTERED AS LAST_USE\n,DATEDIFF('Day',LAST_USE,CURRENT_DATE) AS DAYS_SINCE_LAST_USE\nFROM INFORMATION_SCHEMA.TABLES\nWHERE DAYS_SINCE_LAST_USE > 90 --Use your Days Threshold\nORDER BY BYTES DESC;\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\n-- Tables not used in any query in the last 90 days\nWITH access_history as\n(\nSELECT\ndistinct\nsplit(base.value:objectName, '.')[0]::string as DATABASE_NAME\n,split(base.value:objectName, '.')[1]::string as SCHEMA_NAME\n,split(base.value:objectName, '.')[2]::string as TABLE_NAME\nFROM snowflake.account_usage.access_history\n,lateral flatten (base_objects_accessed) base\nwhere query_start_time between current_date()-90 and current_date()\n)\nSELECT  tbl.table_catalog||'.'||tbl.table_schema||'.'||tbl.table_name as FQ_table_name\nFROM    snowflake.account_usage.tables tbl\nLEFT JOIN access_history ah\nON tbl.table_name=ah.table_name\nAND tbl.table_schema=ah.schema_name\nAND tbl.table_catalog=ah.database_name\nWHERE   ah.table_name is NULL\nAND tbl.deleted is null\n;\n```\n\nCopy\n```\nSection Title: Getting Started with Cost and Performance Optimization > ... > SQL > Actions from query results\nContent:\nDecide on time travel setting for high churn tables\nConsider using transient table type for high churn tables and short lived tables\nInvestigate the business value of tables that haven't been used in the last 90 days\nSection Title: Getting Started with Cost and Performance Optimization > **Automatic Clustering**\nContent:\nThis section covers [Automatic Clustering](https://docs.snowflake.com/en/user-guide/tables-auto-reclustering) . Automatic Clustering is a Snowflake managed service that manages reclustering (as needed) of clustered tables. Reclustering is the process of reordering data in tables to colocate rows that have same cluster key values, which reduces the number of micro-partitions that need to be scanned during execution of a query thereby reducing execution times and help with efficient query execution on smaller sized warehouses.\nSection Title: Getting Started with Cost and Performance Optimization > **Automatic Clustering** > SQL\nContent:\n---- SETTING CONTEXT FOR THE SESSION ----\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE hol_compute_wh;\nUSE SCHEMA OPT_HOL.DEMO;\n-- Query to show Clustering information on a non-clustered table\nSELECT SYSTEM$CLUSTERING_INFORMATION('LINEITEM','LINEAR(L_SHIPDATE)');\n-- Executing a query on an non-clustered table\n-- Ensuring that we are not using cached results\nalter session set USE_CACHED_RESULT=false;\nSELECT  *\nFROM    lineitem\nWHERE   l_shipdate BETWEEN '1995-01-01' AND '1995-03-01'\n;\n-- uery to show Clustering information on a clustered table\nSELECT SYSTEM$CLUSTERING_INFORMATION('LINEITEM_CL');\n-- Executing a query on an clustered table\nSELECT  *\nFROM    lineitem_cl\nWHERE   l_shipdate BETWEEN '1995-01-01' AND '1995-03-01'\n;\n```\n\nCopy\n```\nSection Title: Getting Started with Cost and Performance Optimization > ... > SQL > Results Screenshot\nContent:\nUnclustered table\nClustered table\nSection Title: Getting Started with Cost and Performance Optimization > **Automatic Clustering** > SQL > Outcome\nContent:\nAutomatic clustering improves query performance by scanning less data (less micropartitions). Refer to [Clustering Considerations](https://docs.snowflake.com/en/user-guide/tables-clustering-keys) for clustering considerations and choosing the right clustering key for a table.\nSection Title: Getting Started with Cost and Performance Optimization > **Materialized Views**\nContent:\nThis section covers use of [Materialized Views](https://docs.snowflake.com/en/user-guide/views-materialized) as an option to optimize Snowflake workloads. A materialized view is a pre-computed data set derived from a query specification and stored for later use. Because the data is pre-computed, querying a materialized view is faster than executing a query against the base table of the view. This performance difference can be significant when a query is run frequently or is sufficiently complex. As a result, materialized views can speed up expensive aggregation, projection, and selection operations, especially those that run frequently and that run on large data sets.\nSection Title: Getting Started with Cost and Performance Optimization > **Materialized Views** > SQL\nContent:\n---- SETTING CONTEXT FOR THE SESSION ----\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE hol_compute_wh;\nUSE SCHEMA OPT_HOL.DEMO;\n-- Let's say a user ran this query without knowing a Materialized View exists\n-- After execution of the query, check the query profile\nSELECT\nto_char(l_shipdate,'YYYYMM') as ship_month\n,l_orderkey\n,sum(l_quantity*l_extendedprice) as order_price\n,sum(l_quantity*l_discount) as order_discount\nFROM\nlineitem_cl\nWHERE\nl_orderkey between 1000000 and 2000000\nGROUP BY\nALL;\n```\n\nCopy\n```\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Materialized Views** > SQL > Outcome\nContent:\nRefer to [Materialized Views Best Practices](https://docs.snowflake.com/en/user-guide/views-materialized) for considerations on choosing materialized views.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Query Acceleration** > SQL\nContent:\n-- Find Queries that could be accelerated (for cost consistency, best to find an application workload with consistent query \"templates\").\n ... \nSection Title: Getting Started with Cost and Performance Optimization > Conclusion And Resources > What You Learned\nContent:\nHow to set up warehouse controls to optimize warehouse usage\nHow to identify savings opportunities for table storage\nHow to implement automatic clustering, materialized views, query acceleration or search optimization service to improve performance of Snowflake workloads\nSection Title: Getting Started with Cost and Performance Optimization > ... > What You Learned > Call to Action\nContent:\nDefinitive Guide to managing spend in Snowflake .\n[Snowflake Education](https://learn.snowflake.com/en/)\nProfessional Services\nUpdated Dec 20, 2025\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\n*\n*"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/tables-clustering-keys",
      "title": "Clustering Keys & Clustered Tables - Snowflake Documentation",
      "excerpts": [
        "Section Title: Clustering Keys & Clustered Tables \u00b6\nContent:\nIn general, Snowflake produces well-clustered data in tables; however, over time, particularly as DML occurs on very large tables (as defined by the amount of data in the table,\nnot the number of rows), the data in some table rows might no longer cluster optimally on desired dimensions.\nTo improve the clustering of the underlying table micro-partitions, you can always manually sort rows on key table columns and re-insert them into the table; however, performing\nthese tasks could be cumbersome and expensive.\nInstead, Snowflake supports automating these tasks by designating one or more table columns/expressions as a *clustering key* for the table. A table with a clustering key defined\nis considered to be *clustered* .\nSection Title: Clustering Keys & Clustered Tables \u00b6\nContent:\nYou can cluster materialized views , as well as tables. The rules for\nclustering tables and materialized views are generally the same. For a few additional tips specific to materialized\nviews, see Materialized Views and Clustering and Best Practices for Materialized Views .\nAttention\nClustering keys are not intended for all tables due to the costs of initially clustering the data and\nmaintaining the clustering. Clustering is optimal when either:\nYou require the fastest possible response times, regardless of cost.\nYour improved query performance offsets the credits required to cluster and maintain the table.\nFor more information about choosing which tables to cluster, see: Considerations for Choosing Clustering for a Table .\n ... \nSection Title: ... > Benefits of Defining Clustering Keys (for Very Large Tables) \u00b6\nContent:\nUsing a clustering key to co-locate similar rows in the same micro-partitions enables several benefits for very large tables, including:\nImproved scan efficiency in queries by skipping data that does not match filtering predicates.\nBetter column compression than in tables with no clustering. This is especially true when other columns are strongly correlated with the columns that comprise the clustering key.\nAfter a key has been defined on a table, no additional administration is required, unless you chose to drop or modify the key. All future maintenance on the rows in the table\n(to ensure optimal clustering) is performed automatically by Snowflake.\nAlthough clustering can substantially improve the performance and reduce the cost of some queries, the compute resources used to perform clustering consume credits. As such, you\nshould cluster only when queries will benefit substantially from the clustering.\nSection Title: ... > Benefits of Defining Clustering Keys (for Very Large Tables) \u00b6\nContent:\nTypically, queries benefit from clustering when the queries filter or sort on the clustering key for the table. Sorting is commonly done for `ORDER BY` operations,\nfor `GROUP BY` operations, and for some joins. For example, the following join would likely cause Snowflake to perform a sort operation:\nCopy\nIn this pseudo-example, Snowflake is likely to sort the values in either `my_materialized_view.col1` or `my_table.col1` . For example, if the values in `my_table.col1` are\nsorted, then as the materialized view is being scanned, Snowflake can quickly find the corresponding row in `my_table` .\nThe more frequently a table is queried, the more benefit clustering provides. However, the more frequently a table changes, the more expensive it will be to keep it\nclustered. Therefore, clustering is generally most cost-effective for tables that are queried frequently and do not change frequently.\nNote\nSection Title: ... > Benefits of Defining Clustering Keys (for Very Large Tables) \u00b6\nContent:\nAfter you define a clustering key for a table, the rows are not necessarily updated immediately. Snowflake only performs automated maintenance if the table will benefit from\nthe operation. For more details, see Reclustering (in this topic) and Automatic Clustering .\nSection Title: Clustering Keys & Clustered Tables \u00b6 > Considerations for Choosing Clustering for a Table \u00b6\nContent:\nWhether you want faster response times or lower overall costs, clustering is best for a table that meets all of\nthe following criteria:\nThe table contains a large number of micro-partitions . Typically, this means that\nthe table contains multiple terabytes (TB) of data.\nThe queries can take advantage of clustering. Typically, this means that one or both of the following are true:\nThe queries are selective. In other words, the queries need to read only a small percentage of rows (and thus usually a small\npercentage of micro-partitions) in the table.\nThe queries sort the data. (For example, the query contains an ORDER BY clause on the table.)\nA high percentage of the queries can benefit from the same clustering key(s). In other words, many/most queries select on,\nor sort on, the same few column(s).\nSection Title: Clustering Keys & Clustered Tables \u00b6 > Considerations for Choosing Clustering for a Table \u00b6\nContent:\nIf your goal is primarily to reduce overall costs, then each clustered table should have a high ratio of queries to DML operations\n(INSERT/UPDATE/DELETE). This typically means that the table is queried frequently and updated infrequently. If you want to\ncluster a table that experiences a lot of DML, then consider grouping DML statements in large, infrequent batches.\nAlso, before choosing to cluster a table, Snowflake strongly recommends that you test a representative set of queries on\nthe table to establish some performance baselines.\nSection Title: Clustering Keys & Clustered Tables \u00b6 > Strategies for Selecting Clustering Keys \u00b6\nContent:\nA single clustering key can contain one or more columns or expressions. For most tables, Snowflake recommends a\nmaximum of 3 or 4 columns (or expressions) per key. Adding more than 3-4 columns tends to increase costs more than\nbenefits.\nSelecting the right columns/expressions for a clustering key can dramatically impact query performance. Analysis of\nyour workload will usually yield good clustering key candidates.\nSnowflake recommends prioritizing keys in the order below:\n ... \nSection Title: Clustering Keys & Clustered Tables \u00b6 > Strategies for Selecting Clustering Keys \u00b6\nContent:\nThe cost of clustering on a unique key might be more than the benefit of clustering on that key,\nespecially if point lookups are not the primary use case for that table.\nIf you want to use a column with very high cardinality as a clustering key, Snowflake recommends defining the key as an\nexpression on the column, rather than on the column directly, to reduce the number of distinct values. The\nexpression should preserve the original ordering of the column so that the minimum and maximum values in each\npartition still enable pruning.\n ... \nSection Title: Clustering Keys & Clustered Tables \u00b6 > Reclustering \u00b6\nContent:\nAs DML operations (INSERT, UPDATE, DELETE, MERGE, COPY) are performed on a clustered table, the data in the table might become less clustered. Periodic/regular reclustering of the table is required to\nmaintain optimal clustering.\nDuring reclustering, Snowflake uses the clustering key for a clustered table to reorganize the column data, so that related records are relocated to the same micro-partition. This DML operation deletes the\naffected records and re-inserts them, grouped according to the clustering key.\nNote\nReclustering in Snowflake is automatic; no maintenance is needed. For more details, see Automatic Clustering .\nHowever, for certain accounts, manual reclustering has been deprecated, but is still allowed. For more details see Manual Reclustering .\nSection Title: Clustering Keys & Clustered Tables \u00b6 > Reclustering \u00b6 > Credit and Storage Impact of Reclustering \u00b6\nContent:\nSimilar to all DML operations in Snowflake, reclustering consumes credits. The number of credits consumed depends on the size of the table and the amount of data that needs to be reclustered.\nReclustering also results in storage costs. Each time data is reclustered, the rows are physically grouped based on the clustering key for the table, which results in Snowflake generating new micro-partitions for the table. Adding even a small number of rows to a table can cause all micro-partitions that contain those values to be recreated.\nSection Title: Clustering Keys & Clustered Tables \u00b6 > Reclustering \u00b6 > Credit and Storage Impact of Reclustering \u00b6\nContent:\nThis process can create significant data turnover because the original micro-partitions are marked as deleted, but retained in the system to enable Time Travel and Fail-safe. The original micro-partitions\nare purged only after both the Time Travel retention period and the subsequent Fail-safe period have passed (i.e. minimum of 8 days and up to 97 days for extended Time Travel, if you are using Snowflake\nEnterprise Edition (or higher)). This typically results in increased storage costs. For more information, see Snowflake Time Travel & Fail-safe .\nImportant\nBefore defining a clustering key for a table, you should consider the associated credit and storage costs.\nSection Title: Clustering Keys & Clustered Tables \u00b6 > Reclustering \u00b6 > Reclustering Example \u00b6\nContent:\nBuilding on the clustering diagram from the previous topic, this diagram illustrates how reclustering a table can help reduce scanning of micro-partitions to improve\nquery performance:\nTo start, table `t1` is naturally clustered by `date` across micro-partitions 1-4.\nThe query (in the diagram) requires scanning micro-partitions 1, 2, and 3.\n`date` and `type` are defined as the clustering key. When the table is reclustered, new micro-partitions (5-8) are created.\nAfter reclustering, the same query only scans micro-partition 5.\nIn addition, after reclustering:\nSection Title: Clustering Keys & Clustered Tables \u00b6 > Reclustering \u00b6 > Reclustering Example \u00b6\nContent:\nMicro-partition 5 has reached a *constant state* (i.e. it cannot be improved by reclustering) and is therefore excluded when computing depth and overlap for future maintenance. In a well-clustered\nlarge table, most micro-partitions will fall into this category.\nThe original micro-partitions (1-4) are marked as deleted, but are not purged from the system; they are retained for Time Travel and Fail-safe .\nNote\nThis example illustrates the impact of reclustering on an extremely small scale. Extrapolated to a very large table (i.e. consisting of millions of micro-partitions or more), reclustering can have a\nsignificant impact on scanning and, therefore, query performance.\n ... \nSection Title: Clustering Keys & Clustered Tables \u00b6 > ... > Important Usage Notes \u00b6\nContent:\nFor each VARCHAR column, the current implementation of clustering uses only the first 5 bytes.If the first N characters are the same for every row, or do not provide sufficient cardinality, then consider clustering on a\nsubstring that starts after the characters that are identical, and that has optimal cardinality. (For more information about\noptimal cardinality, see Strategies for Selecting Clustering Keys .) For example:Copy\nIf you define two or more columns/expressions as the clustering key for a table, the order has an impact on how the data is clustered in micro-partitions.For more details, see Strategies for Selecting Clustering Keys (in this topic). An existing clustering key is copied when a table is created using CREATE TABLE \u2026 CLONE. However, Automatic Clustering is suspended for the cloned table and must be resumed."
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/getting-started-with-snowflake-cluster-key-selection/",
      "title": "A Data-Driven Methodology for Choosing a Snowflake Clustering Key",
      "excerpts": [
        "Section Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Overview > What You Will Learn\nContent:\nProperties that make better or worse choices for clustering keys\nHow to use data to make clustering key decisions\nHow to simulate a clustered table for testing purposes\nHow to estimate the cost of a clustering key\n ... \nSection Title: ... > Clustering Key Cardinality Targets\nContent:\nChoosing a clustering key with an appropriate cardinality (number of distinct values) balances better performance with the increased cost of clustering data. Rows are not ordered within micro-partitions, so there is no performance advantage to be gained from a clustering key that produces a cardinality larger than the micro-partition count for the table. Furthermore, a high-cardinality clustering key is often more expensive to maintain than a low-cardinality clustering key. Since the performance advantage of clustering is gained through having a narrow range between the minimum and maximum values on each micro-partition, we might ideally have only one value for the minimum and maximum values of the clustering key on a micro-partition. When the minimum and maximum values are the same, we call it a constant micro-partition.\n ... \nSection Title: ... > The Process of Selecting a Clustering Key\nContent:\nWe will follow those steps in the following sections, with an additional step at the beginning to define the data and structure of our experimentation environment.\nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Data and Structure\nContent:\nTo keep things simple, we'll use data from the [Snowflake sample data](https://docs.snowflake.com/en/user-guide/sample-data-tpcds) . The database `SNOWFLAKE_SAMPLE_DATA` is shared with all Snowflake accounts, so should be easily accessible. We'll be making copies of this data so we can play with different clustering options, and querying it in order to test performance. For the purposes of this quickstart, we'll be using data from the tpcds_sf10tcl schema. This schema represents a 10 TB scale of data following the data model for the [TPC Benchmark\u2122 DS (TPC-DS)](https://www.tpc.org/TPC_Documents_Current_Versions/pdf/TPC-DS_v2.5.0.pdf) .\nWe will also need dedicated warehouses to work with. While this work can be done on shared warehouses, when testing query performance, we really need to isolate the execution of queries to eliminate the variable of concurrent work.\nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Data and Structure\nContent:\nHere's the syntax to create the objects we will need. We're using 2XL for the warehouse size here to speed up the creation of the largest testing table.\nCREATE OR REPLACE TRANSIENT DATABASE clustering_exp;\nCREATE OR REPLACE SCHEMA clustering_exp.tpcds_clustering_test;\nCREATE OR REPLACE WAREHOUSE clustering_qs_2xl_wh WAREHOUSE_SIZE='XXLARGE' AUTO_SUSPEND=60;\nCREATE OR REPLACE WAREHOUSE clustering_qs_l_wh WAREHOUSE_SIZE='LARGE' AUTO_SUSPEND=60;\n ... \nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Data and Structure\nContent:\n```\n\nCopy\n```\nNote that we are doing a bit of change for the date storage and handling for the `CATALOG_SALES` table. This is mostly because this data model uses a highly normalized approach to dates and times that is a bit out of the ordinary. Tweaking this lets us handle dates in a more common manner throughout this tutorial. We're also defining the entire database as transient so as not to incur charges for Fail-safe for this purely experimental work.\nThe `ORDER BY RANDOM()` ensures we don't get strange effects from data that happens to be naturally clustered. In a real world scenario, we might choose not to order by random, because we often take advantage of natural clustering and want to see the effects, but using an order by random gives us a clearer baseline to compare to for this example.\nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Identify Workload\nContent:\nIdentifying the workload to optimize for is one of the hardest parts of choosing a clustering key. It is also one of the steps that truly requires human involvement, and mostly shouldn't be automated. On the one hand, the workload could be considered every query that accesses a table. However, we can only cluster a table in one way, and it is unusual for a workload to be so simple that every single query can be helped by a single choice for a clustering key.\n ... \nSection Title: ... > sold_timestamp AND s_item_sk\nContent:\nThe cardinality is pretty close to our estimate at 73,807,578. This is far too high for our clustering key target. We have different choices on how to reduce the cardinality here. We can apply a function to either column to reduce cardinality or both of them. Let's start by reducing the cardinality on cs_item_sk.\nSELECT\nDATE_TRUNC('DAY', sold_timestamp) AS sold_ckey\n,TRUNC(cs_item_sk,-5) AS item_ckey\n,COUNT(*)\nFROM catalog_sales\nGROUP BY sold_ckey, item_ckey\n;\n```\n\nCopy\n```\nThe cardinality is reasonable here, but we are actually down to a single digit for the item number, so let's look at a combination of cardinality reduction between the two columns.\nSELECT\nDATE_TRUNC('WEEK', sold_timestamp) AS sold_ckey\n,TRUNC(cs_item_sk,-4) AS item_ckey\n,COUNT(*)\nFROM catalog_sales\nGROUP BY sold_ckey, item_ckey\n;\n```\n\nCopy\n```\nThe cardinality here is within our target range.\n ... \nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Test Query Workload\nContent:\nClustering is a physical state of the data, where data is grouped into micro-partitions in such a way that the minimum/maximum ranges of a clustering key are narrow. It can be achieved and maintained long-term through something like [automatic clustering](https://docs.snowflake.com/en/user-guide/tables-auto-reclustering) . It can also be achieved naturally - a table with data that is loaded regularly with time-based data often achieves some natural time- or date-based clustering. Data engineering processes can also sort data as a table is built to achieve clustering. While we might want to use auto-clustering long-term, using a simple ORDER BY works well for testing the impact of clustering. We will use `CREATE TABLE ... AS ... ORDER BY ...` statements to create copies of the data in a clustered form and test performance to compare against the baseline.\n ... \nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Compare Test Results\nContent:\nThere are some interesting things we see in this data. The first thing is that ANY of the clustering keys improve the execution time of query1. If we want the best performance for query1, we would choose to cluster on DATE_TRUNC('week', sold_timestamp), TRUNC(cs_item_sk,-4). If we want the best performance for query2, we would choose to cluster on TRUNC(cs_item_sk,-1). If we want the best performance for query3, It is pretty much a tie between three of the four choices we tried.\nWe're looking at query duration here because that is the metric we care about optimizing for in this case. We might also look at the number of micro-partitions scanned and look for the clustering key choice that minimizes that.\n ... \nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Compare Test Results\nContent:\nThere are techniques that might work when there are truely two different ways data needs to be clustered. They are mostly beyond the scope of this tutorial. One that bears mentioning is the use of various materializations, including materialized views, to cluster the same data in multiple ways. Caution should be used when clustering materialized views. If both the table and materialized views based on it are clustered, then automatic clustering can incur some additional costs. It is best to choose one level within the table/view hierarchy for clustering, and not cluster at multiple levels. If you are using automatic clustering for materialized views, do not use automatic clustering for the table the materialized view is based on.\nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Implement Clustering\nContent:\nImplementing clustering for the first time or changing a clustering key can be an expensive operation. It is valid to modify a data engineering pipeline to introduce natural clustering, or to to use Snowflake's Automatic Clustering Service to introduce or just maintain clustering. It makes sense to use the function [SYSTEM$ESTIMATE_AUTOMATIC_CLUSTERING_COSTS](https://docs.snowflake.com/en/sql-reference/functions/system_estimate_automatic_clustering_costs) to estimate the initial costs of defining a clustering key. This function can also be used to estimate ongoing clustering costs for certain patterns, but the nature of our test scenario will not work with this functionality. This syntax can be used to run the function for the sample scenario:\nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Implement Clustering\nContent:\nSELECT SYSTEM$ESTIMATE_AUTOMATIC_CLUSTERING_COSTS(\n'clustering_exp.tpcds_clustering_test.catalog_sales'\n, '(DATE_TRUNC('week', sold_timestamp), TRUNC(cs_item_sk,-4))'\n);\n```\n\nCopy\n```\nRunning this function may take several minutes. The output will look something like this:\n{\n\"reportTime\": \"Mon, 28 Apr 2025 15:02:26 GMT\",\n\"clusteringKey\": \"LINEAR(DATE_TRUNC(\\u0027week\\u0027, sold_timestamp), TRUNC(cs_item_sk,-4))\",\n\"initial\": {\n\"unit\": \"Credits\",\n\"value\": 36.580923038,\n\"comment\": \"Total upper bound of one time cost\"\n},\n\"maintenance\": {\n\"comment\": \"Unable to produce maintenance cost estimate because table does not have enough substantial DMLs in the past 7 days.\"\n}\n}\n```\n\nCopy\n```\nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > Implement Clustering\nContent:\nThis output tells us that if we were to just turn on automatic clustering using an Alter Table statement, it would cost approximately 36.6 credits to achieve a well-clustered state. One method for reducing the initial cost of clustering is to first use either a CTAS (Create Table AS) or `INSERT OVERWRITE ...` to manually place a table in order first, and then enable automatic clustering to keep up with clustering as our data changes over time.\nIn this case, we'll use this statement to replace our table with a clustered version of the same data:\nINSERT OVERWRITE INTO catalog_sales\nSELECT *\nFROM catalog_sales\nORDER BY DATE_TRUNC('week', sold_timestamp), TRUNC(cs_item_sk,-4);\n```\n\nCopy\n```\n ... \nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > ... > What You Learned\nContent:\nProperties that make better or worse choices for clustering keys\nHow to use data to make clustering key decisions\nHow to simulate a clustered table for testing purposes\nHow to estimate the cost of a clustering key\nSection Title: A Data-Driven Methodology for Choosing a Snowflake Clustering Key > ... > Resources\nContent:\nTo read more about clustering in Snowflake, check out the [official documentation](https://docs.snowflake.com/en/user-guide/tables-clustering-keys) and these medium articles with more information:\n[Snowflake Clustering Demystified](https://medium.com/snowflake/snowflake-clustering-demystified-8042fa81289e)\n[A Data-Driven Approach to Choosing a Clustering Key in Snowflake](https://medium.com/snowflake/a-data-driven-approach-to-choosing-a-clustering-key-in-snowflake-4b3400704778)\n[An Example of Choosing a Clustering Key in Snowflake](https://medium.com/snowflake/an-example-of-choosing-a-clustering-key-in-snowflake-3b23a35cd9b4)\nUpdated Dec 20, 2025\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/blog/storage-lifecycle-policies-ga/",
      "title": "Optimize Storage Costs and Simplify Compliance with Storage Lifecycle Policies, Now Generally Available",
      "publish_date": "2025-11-15",
      "excerpts": [
        "Section Title: Category\nContent:\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\nOct 31, 2025 | 4 min read\nSection Title: Optimize Storage Costs and Simplify Compliance with Storage Lifecycle Policies, Now Generally Ava...\nContent:\nEvery organization today faces the same fundamental challenge: how to balance the need to retain vast amounts of data with the growing pressure to control costs. A financial services firm may need to keep years of model outputs for regulatory audits. A media company might accumulate terabytes of log data that\u2019s rarely touched but must be preserved. Security teams across industries generate endless logs that are essential to keep but seldom accessed. Whether it\u2019s compliance, analytics or operational traceability, the story is the same \u2014 data that was once critical for daily operations eventually becomes cold, yet remains too valuable or mandated to delete outright. At Snowflake, we believe in making the complex simple. To address these challenges, we are excited to announce the general availability of Storage Lifecycle Policies.\nSection Title: Optimize Storage Costs and Simplify Compliance with Storage Lifecycle Policies, Now Generally Ava...\nContent:\nThis feature provides a simple, automated way to manage your data lifecycle, helping you dramatically reduce storage costs (by 55%-90%) for dormant data and streamline compliance with minimal operational overhead. In this blog post, we will show how you can use Storage Lifecycle Policies to automatically manage your data lifecycle \u2014 from archiving cold data to deleting expired records \u2014 so you can save costs, maintain compliance and focus on innovation instead of infrastructure. ## What is a Storage Lifecycle Policy? A Storage Lifecycle Policy is a schema-level object that lets you automatically archive or delete data from standard Snowflake tables. These policies are applied at a row level, providing fine-grained control over which specific data is archived or deleted based on a defined timeline.\nSection Title: Optimize Storage Costs and Simplify Compliance with Storage Lifecycle Policies, Now Generally Ava...\nContent:\nYou define a simple policy expression, and Snowflake takes care of the rest, automatically running the policy every day on shared compute resources. Getting started involves just two steps, with a new set of privileges enabling you to control who manages and applies these policies: 1. **Create a policy** that specifies which rows to archive or delete. 2. **Apply the policy** to one or more tables. Here\u2019s a quick example: ### Step 1: Create the policy Create a storage lifecycle policy that archives data older than 360 days into the COLD tier for five years before deletion.\n ... \nSection Title: ... > FinOps for Snowflake\nContent:\nThe FinOps for Snowflake on-demand course provides participants with a high-level overview of the FinOps framework within Snowflake.\n[enroll now](https://learn.snowflake.com/en/courses/OD-FINOPS/)\n ... \nSection Title: ... > Share Article\nContent:\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fstorage-lifecycle-policies-ga&title=Optimize+Storage+Costs+and+Simplify+Compliance+with+Storage+Lifecycle+Policies%2C+Now+Generally+Available)\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fstorage-lifecycle-policies-ga&text=Optimize+Storage+Costs+and+Simplify+Compliance+with+Storage+Lifecycle+Policies%2C+Now+Generally+Available)\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fstorage-lifecycle-policies-ga)\nSubscribe to our blog newsletter\nGet the best, coolest and latest delivered to your inbox each week\n*\nSubscribe Now\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice."
      ]
    },
    {
      "url": "https://ternary.app/blog/snowflake-cost-optimization/",
      "title": "Snowflake cost optimization: 8 proven strategies for reducing costs",
      "publish_date": "2025-09-22",
      "excerpts": [
        "Ternary named a Leader in the 2025 ISG Provider Lens\u00ae for FinOps Platforms . [Download the report](https://ternary.app/isg-names-ternary-a-leader/) .\n\u2715\n[](https://ternary.app/)\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs\nContent:\n**Last updated:** September 22, 2025\nTernary Team\nSnowflake has quickly become a favorite as a data warehousing platform, and for good reasons. This makes many teams jump in, only to realize later that their [Snowflake spend](https://ternary.app/blog/manage-your-snowflake-costs/) is climbing faster than expected.\nThis is because managing Snowflake costs often becomes a sticking point despite the platform being impressive from a tech POV. That\u2019s where Snowflake cost optimization comes in.\nIn this guide, we\u2019ll explore how Snowflake pricing works and how you can stay in control without giving up performance or flexibility.\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Storage\nContent:\nEvery file, every table, every backup, all adds up in Snowflake. Snowflake charges a monthly fee based on the average amount of storage used over the month. The data is stored in compressed format. Depending on what kind of data you\u2019re working with, like if you\u2019re pulling in a bunch of raw CSVs versus more compact file types, the compression can significantly lower Snowflake storage costs.\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Total annual cost\nContent:\n| **Storage** | $1,104 |\n| **Virtual warehouse** | $21,774 |\n| **Grand Total** | $22,878 per year |\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\nWhile automatic clustering can improve query performance, it runs on serverless compute. This means it racks up Snowflake credits whether anyone\u2019s actually using the table or not.\nIf the table is only getting hit a few times a week, that background compute activity is just silently chipping away at your budget. This is where smart Snowflake cost optimization begins.\nLook for tables with automatic clustering enabled that barely get queried, say, fewer than 100 times per week. Ask yourself if these tables are part of a disaster recovery setup or being shared with another account. If not, it\u2019s probably safe to hit pause.\nTo suspend automatic clustering, run:\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\n| **ALTER** **TABLE** your_table_name **SUSPEND** RECLUSTER; |\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\nThis one step alone can help reduce Snowflake costs tied to unnecessary background compute.\n ... \nSection Title: ... > 4. Clean out large tables that haven\u2019t been touched in a week\nContent:\nThe massive tables that sit there eating up storage and haven\u2019t been queried at all in the past week not only inflate your Snowflake storage costs, but they also clutter up your environment and slow down everything from data discovery to data warehouse optimization and management.\nIf a table isn\u2019t serving any purpose (besides reminding you of a project from six months ago), drop it. But again, always check if it\u2019s being used for recovery or data sharing before swinging the axe.\nTo delete a table, run:\n ... \nSection Title: ... > 6. Allow multi-cluster warehouses to scale down\nContent:\nMulti-cluster warehouses can be incredibly powerful, especially when you\u2019ve got a high volume of concurrent queries.\nBut if you\u2019ve locked the cluster count at a fixed number, say, 3 minimum and 3 maximum, you\u2019re forcing Snowflake to keep all clusters running at all times, even when demand doesn\u2019t justify it.\nThat\u2019s wasted compute and wasted credits. Snowflake is built to scale, so let it.\nLower the minimum cluster count so the warehouse can scale down during slower periods. It won\u2019t affect performance during peak hours, but it\u2019ll quietly reduce credit consumption when traffic drops.\nTo adjust the scaling behavior, run:"
      ]
    },
    {
      "url": "https://yukidata.com/snowflake-clustering-costs-guide/",
      "title": "Snowflake Clustering Costs: Complete Beginner\u2019s Guide to Clusters | Yuki",
      "publish_date": "2025-06-17",
      "excerpts": [
        "Section Title: Snowflake Clustering Costs: Complete Beginner\u2019s Guide to Clusters\nContent:\nBy Ido Arieli Noga\nJune 17, 2025 | 5 min read\nSnowflake is a widely customizable platform \u2013 which also means adjusting it to best perform and not break your bank can be more than a little difficult.\nEven after you\u2019ve started with warehouse right-sizing and [query optimization](https://yukidata.com/blog/snowflake-cost-per-query/) , there\u2019s still more work that can be done with Snowflake clustering.\nAfter reading this article, you\u2019ll walk away knowing:\nWhat Snowflake clustering is\nWhy use clustering\nWhat clustering keys are\nHow exactly clustering works\nHow Snowflake clustering works\nAnd you\u2019ll know fully if you should introduce Snowflake clustering to your tech stack. Let\u2019s dive in.\n ... \nSection Title: ... > How Exactly Does Snowflake Clustering Work?\nContent:\nLet\u2019s start from the beginning:\nFirst, your data is loaded into Snowflake and stored in a \u201cload table\u201d. This table is a temporary table used for staging data before it\u2019s loaded into an actual table. Snowflake will automatically reorganize this table into micro-partitions based on any clustering keys defined for that table.\nApply your clustering key based on how you want to group your data. If you decide to use automatic clustering to cluster or recluster your tables, you won\u2019t have to bother with manual adjustments, but you will have to cough up some extra Snowflake credits.\nAutomatic clustering uses Snowflake credits without requiring you to provide a dedicated virtual warehouse. Snowflake will internally manage resource allocation for reclustering your tables, and you\u2019ll be billed for the credits consumed by the automatic clustering.\nThe amount of credit consumed varies based on:\nTable size\nNumber of clustered or reclustered rows\nSection Title: ... > How Exactly Does Snowflake Clustering Work?\nContent:\nIf you enable Snowflake automatic clustering and haven\u2019t clustered in some time, you\u2019ll probably experience some higher charges as Snowflake brings your tables to the optimally-clustered state.\nSection Title: ... > Should I Try Snowflake Clustering?\nContent:\nClustering can significantly improve query performance and reduce costs, but it requires careful planning. While automatic clustering simplifies management, the additional credits can add up quickly for large, frequently-changing tables.\nBut automatic clustering can be an additional charge atop of an already opaque bill, and if you\u2019re already struggling to get your Snowflake platform performance into shape, you may be in need of a different kind of optimization.\nIf managing clustering manually feels overwhelming, or if you\u2019re looking for comprehensive Snowflake optimization beyond just clustering, specialized tools can help automate the entire process.\nThird-party tools like Yuki are designed to cluster tables, right-size warehouses, and optimize your queries automatically, no additional dev lift needed. Yuki\u2019s plug-and-play tool means you can start saving up to 40% on your Snowflake spend at the click of a button.\nSection Title: ... > Should I Try Snowflake Clustering?\nContent:\nCurious how much Yuki can help you start to save? [Reach out now for your free demo](https://yukidata.com/request-demo/) .\nBy Ido Arieli Noga\nIdo Arieli Noga is the CEO and Co-Founder of Yuki, where he helps businesses cut Snowflake spend through smart warehouse scaling and DevOps-driven optimization. He brings over 12 years of experience across data storage, BI, and FinOps, including nearly four years as Head of Data at Lightico and five years managing large-scale virtual environments in the government sector. Ido holds a degree in Computer Science and is passionate about building scalable, cost-efficient data infrastructures. Since founding Yuki in 2023, he\u2019s focused on helping teams reduce costs without changing queries or code. Find more of his insights on Medium or LinkedIn."
      ]
    },
    {
      "url": "https://www.flexera.com/blog/finops/snowflake-clustering/",
      "title": "Snowflake clustering 101: A beginner's guide (2026) - Flexera",
      "excerpts": [
        "Find out the benefits of combining optimized query design ..."
      ]
    },
    {
      "url": "https://yukidata.com/snowflake-storage-cost-optimization-guide/",
      "title": "Snowflake Cost Per Query: Query-Level Cost Attribution Guide | Yuki",
      "publish_date": "2025-07-30",
      "excerpts": [
        "Section Title: Snowflake Storage Costs: 2025 Guide to Optimizing Your Bill > How Snowflake\u2019s Storage Works\nContent:\nBefore we get into the good stuff (a.k.a., cost savings), first you\u2019ll need to understand how Snowflake\u2019s storage works.\nUnlike traditional data warehouses, Snowflake uses a patented \u201cmulti-cluster, shared data architecture\u201d with three distinct but integrated layers:\n**Storage layer:** Automatically compresses and organizes your data into optimized columnar format micro-partitions, stored in your cloud provider\u2019s blob storage (S3, Azure Blob, or GCP Cloud Storage)\n**Compute layer (virtual warehouses):** Independent clusters that process queries against the storage layer, scaling automatically based on workload\n**Cloud services layer:** Handles infrastructure management, metadata, security, [query optimization](https://yukidata.com/snowflake-query-optimization/) , and transaction control\n**NOTE:** Snowflake can usually achieve 3-5x compression, which directly improves storage costs.\n ... \nSection Title: Snowflake Storage Costs: 2025 Guide to Optimizing Your Bill > ... > Duration of storage\nContent:\nThough you\u2019ll be charged for your data storage on a monthly basis, your final fee will be based on your data retention settings for Time Travel and Fail-safe settings \u2013 which we\u2019ll discuss in more detail in a moment.\nSnowflake does minimize storage prices here the best it can by only maintaining changes needed to restore data versions, not the full copies of tables.\n ... \nSection Title: Snowflake Storage Costs: 2025 Guide to Optimizing Your Bill > ... > Fail-safe storage costs\nContent:\nSimilar to time travel storage, fail-safe storage gives you an additional seven days of data retention on top of Time Travel for absolute peace of mind.\nThe cost of fail-safe storage depends on the amount of data stored in multiple copies. You won\u2019t get charged for fail-safe directly, but instead you\u2019ll pay for the additional overall storage volume. Snowflake will only maintain full copies of tables after a table is dropped or truncated.\n ... \nSection Title: ... > Storage optimization tip #4: Use SQL queries\nContent:\nRun these queries to identify your largest tables and optimization opportunities:\nFind your largest tables and use this SQL code:\nSELECT\nTABLE_NAME,\nROUND(ACTIVE_BYTES/1024/1024/1024,2) AS ACTIVE_GB,\nROUND(TIME_TRAVEL_BYTES/1024/1024/1024,2) AS TIME_TRAVEL_GB,\nROUND(FAILSAFE_BYTES/1024/1024/1024,2) AS FAILSAFE_GB\nFROM SNOWFLAKE.ACCOUNT_USAGE.TABLE_STORAGE_METRICS\nORDER BY ACTIVE_BYTES DESC\nLIMIT 20;\nOnce you\u2019ve identified large tables, consider:\nImplementing column-level clustering keys on frequently filtered columns\nEvaluating data retention policies for historical data\nConverting rarely-accessed data to lower-cost storage tiers\n ... \nSection Title: ... > Maximize Snowflake storage cost ROI with Yuki\nContent:\nIdo Arieli Noga is the CEO and Co-Founder of Yuki, where he helps businesses cut Snowflake spend through smart warehouse scaling and DevOps-driven optimization. He brings over 12 years of experience across data storage, BI, and FinOps, including nearly four years as Head of Data at Lightico and five years managing large-scale virtual environments in the government sector. Ido holds a degree in Computer Science and is passionate about building scalable, cost-efficient data infrastructures. Since founding Yuki in 2023, he\u2019s focused on helping teams reduce costs without changing queries or code. Find more of his insights on Medium or LinkedIn.\nSection Title: Snowflake Storage Costs: 2025 Guide to Optimizing Your Bill > ... > Free cost analysis\nContent:\nTake 5 minutes to learn how much money you can save on your Snowflake account.\nBy clicking Submit you\u2019re confirming that you agree with our Terms and Conditions.\n[### Follow us on LinkedIn](https://www.linkedin.com/company/yukidata)\nSection Title: Snowflake Storage Costs: 2025 Guide to Optimizing Your Bill > ... > Related posts\nContent:\n[](https://yukidata.com/nowflake-cybersecurity-guide/)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/tables-auto-reclustering",
      "title": "Automatic Clustering | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Databases, Tables, & Views Table Structures Automatic Clustering\n ... \nSection Title: Automatic Clustering \u00b6 > Resuming Automatic Clustering for a table \u00b6\nContent:\nTo resume Automatic Clustering for a clustered table, use the ALTER TABLE command with a `RESUME RECLUSTER` clause. For example:\nCopy\nTip\nBefore you resume Automatic Clustering on a clustered table, consider the following conditions, which may cause reclustering activity (and corresponding credit charges):\nThe table is not optimally-clustered (e.g. significant DML has been performed on the table since it was last reclustered).\nThe clustering key on the table has changed.\nFor more details, see Micro-partitions & Data Clustering and Clustering Keys & Clustered Tables .\nSection Title: Automatic Clustering \u00b6 > Automatic Clustering costs \u00b6\nContent:\nThe cost of enabling Automatic Clustering can be broken down into compute costs and storage costs.\nCompute costs\nSnowflake uses serverless compute resources to cluster a table for the first time. It also uses compute resources to maintain that table in a well-clustered state as new data is added to the table. The more changes to a table, the higher the\nmaintenance costs.\nStorage Costs\nBecause Automatic Clustering reorganizes existing data rather than creating additional storage, in many cases there are no additional\nstorage costs. However, reclustering can incur additional storage costs if it increases the size of Fail-safe storage. For more information, see Credit and Storage Impact of Reclustering .\nSection Title: Automatic Clustering \u00b6 > ... > Credit usage and warehouses for Automatic Clustering \u00b6\nContent:\nAutomatic Clustering consumes Snowflake credits, but does not require you to provide a virtual warehouse. Instead, Snowflake internally\nmanages and achieves efficient resource utilization for reclustering the tables.\nYour account is billed only for the actual credits consumed by automatic clustering operations on your clustered tables.\nImportant\nAfter enabling or resuming Automatic Clustering on a clustered table, if it has been a while since the table was reclustered, you may\nexperience reclustering activity (and corresponding credit charges) as Snowflake brings the table to an optimally-clustered state. Once\nthe table is optimally-clustered, the reclustering activity will drop off.\nLikewise, defining a clustering key on an existing table or changing the clustering key on a clustered table may trigger reclustering and\ncredit charges.\n ... \nSection Title: Automatic Clustering \u00b6 > Automatic Clustering costs \u00b6 > Estimating Automatic Clustering cost \u00b6\nContent:\nYou can call the SYSTEM$ESTIMATE_AUTOMATIC_CLUSTERING_COSTS function to help estimate the compute cost of\nenabling Automatic Clustering for a table and maintaining the table in a well-clustered state. You can also call the function to help predict\nthe compute cost of changing the cluster key of a table.\nImportant\nThe cost estimates returned by the SYSTEM$ESTIMATE_AUTOMATIC_CLUSTERING_COSTS function are best efforts. The actual realized costs can vary by up to 100% (or, in rare cases, several times) from the estimated costs."
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/well-architected-framework-performance/",
      "title": "Performance - Snowflake",
      "excerpts": [
        "Section Title: Performance > Principles and recommendations > Recommendations\nContent:\n**Leveraging Elasticity** Dynamically adjust resources based on workload. Utilize horizontal\nscaling (multi-cluster warehouses) and vertical sizing to match query\ncomplexity. Employ manual scaling for predictable peaks and\nauto-scaling for real-time fluctuations. **Peak Load Planning** Proactively plan for high-demand periods. Use manual scaling for\npredictable surges; enable auto-scaling and predictive scaling for\nless predictable demand. **Continuous Performance Improvement** Monitor key performance indicators like latency, throughput, and\nutilization. Foster continuous tuning through training, feedback, and\nmetric review. **Test-First Design** Integrate performance testing into the design phase. Define KPIs,\nestablish baselines, and simulate workloads early. Use load,\nscalability, and stress testing throughout development to prevent\nbottlenecks. **Effective Data Shaping** Leverage Snowflake micro-partition clustering.\nSection Title: Performance > Principles and recommendations > Recommendations\nContent:\nDefine clustering keys\nfor frequently filtered/joined columns to enable efficient pruning and\nreduce I/O. Regularly monitor clustering depth. **Well-Designed SQL** Write efficient queries by avoiding `SELECT \\*` , specifying needed\ncolumns, and using efficient joins. Use query profiling tools to\nrefine performance and minimize resource consumption. **Optimizing Warehouses** Reduce queues and spillage, scale appropriately, and use query\nacceleration. Right-size warehouses and manage concurrency to reduce\ncontention and optimize resource use. **Optimizing Storage** Use automatic clustering, Search Optimization Service, and\nmaterialized views strategically. Match the technique to your\nworkload. **High-Level Practices** To optimize query performance, select fewer columns, leverage query\npruning with clustered columns, and use pre-aggregated tables.\nSection Title: Performance > Principles and recommendations > Recommendations\nContent:\nSimplify SQL by reducing unnecessary sorts, preferring window\nfunctions, and avoiding OR conditions in joins. Minimize view\ncomplexity, maximize cache usage, scale warehouses, and tune cluster\nscaling policies for balanced performance and cost.\n ... \nSection Title: Performance > Utilize Snowflake's serverless features > Automatic Clustering Service\nContent:\n[Automatic clustering](https://docs.snowflake.com/en/user-guide/tables-auto-reclustering) in Snowflake is a background service that continuously manages data\nreclustering for tables with a defined and enabled cluster key.\nClustering table data effectively, based on columns or expressions, is a\nhighly effective performance optimization. It directly supports pruning,\nwhere specific micro-partitions are eliminated from a query's table\nscan. This micro-partition elimination provides direct performance\nbenefits, as I/O operations, especially across a network to remote\nstorage, can be expensive. By clustering data to align with common\naccess paths, MIN-MAX ranges for columns become narrower, reducing the\nnumber of micro-partitions scanned for a query.\n ... \nSection Title: ... > [Best practices for Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-table-...\nContent:\n**Monitor refresh history:** Regularly monitor the refresh history of\nyour Dynamic Tables using the DYNAMIC_TABLE_REFRESH_HISTORY view. This\nprovides insights into the refresh performance, latency, and costs,\nallowing you to fine-tune your definitions and warehouse sizes for\ncontinuous optimization."
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/",
      "title": "Cost Optimization - Snowflake",
      "excerpts": [
        "Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Business Impact > Overview > Consider cost as a design constraint\nContent:\nAt the ingestion layer, best practices include balancing latency versus\ncost by selecting appropriate services (e.g., Snowpipe, Snowpipe\nStreaming, or third-party tools) and choosing the right storage format\n(e.g., native tables, Iceberg). For transformations, design with\nfrequency versus SLA in mind to ensure data freshness matches the\nbusiness need. For analytics, apply schema design best practices such as\nthoughtful clustering key choices and pruning strategies to reduce\nconsumed credits. In distribution, optimize data transfer by monitoring\negress patterns and applying cost-saving practices like the [Snowflake Data Transfer Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) .\n ... \nSection Title: Cost Optimization > Optimize > Overview > Leverage Managed Services\nContent:\n**Automatic Clustering** is a background process in Snowflake that\norganizes data within a table by sorting it according to predefined\ncolumns. This process is critical for optimizing query performance and\nreducing costs. Benefits include:\n**Improved query pruning:** By sorting data, automatic clustering\nenables more effective pruning in SQL WHERE clauses, meaning less data\nneeds to be scanned for a given query.\n**Faster joins:** Clustering also results in quicker and more\nefficient join operations.\n**Cost-efficient queries:** These benefits ultimately result in faster\nand more cost-effective query execution.\n**Considerations and Best Practices:**\n ... \nSection Title: Cost Optimization > Optimize > Overview > Data storage types & lifecycle management\nContent:\nYou do not always need to define cluster keys for all tables (unlike\nmany other relational database management systems) if Snowflake's\nnatural data loading maintains consistent micro-partition min/max\nvalues relative to your query patterns. Additionally, you can disable\nAuto-Clustering on a table while keeping its cluster key definition\nwithout incurring extra costs. **Infrequently used materialized views and search optimization paths:** Materialized Views and search optimization paths can incur\nunnecessary storage and compute costs if they are no longer actively\nutilized. Materialized Views are most effective for stable data tables\nwith repeated complex aggregations or joins, while search optimization\nis designed for high-speed point lookup queries.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Improve continually\nContent:\nThis continual improvement framework is the culmination of all subtopics\nwithin the Cost Optimization Pillar and provides a consistent way for\nyou to grow healthily on Snowflake.\nUpdated Nov 24, 2025\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances"
      ]
    },
    {
      "url": "https://keebo.ai/2025/11/03/snowflake-clustering-keys-optimization/",
      "title": "Why Your Snowflake Clustering Strategy Is Costing You Money",
      "publish_date": "2025-11-03",
      "excerpts": [
        "Most teams I work with see 30-50% cost reductions on their biggest tables just from proper clustering. Some see 10x improvements or better ..."
      ]
    },
    {
      "url": "https://articles.analytics.today/snowflake-cluster-keys-and-micro-partition-elimination-best-practices",
      "title": "Snowflake Cluster Keys: How to Improve Query Performance with Partitio",
      "excerpts": [
        "Snowflake Cluster Keys: How to Improve Query Performance with Partitio\n![Image ![Analytics Today](https://cdn.hashnode.com/res/hashnode/image/upload/v1720697358908/bcd77308-e6bf-4169-abcd-72ac4b7553b9.png?w=1000&h=250&auto=compress,format&format=webp)](/?source=top_nav_blog_home)\nFollow\n![Image ![Analytics Today](https://cdn.hashnode.com/res/hashnode/image/upload/v1720697358908/bcd77308-e6bf-4169-abcd-72ac4b7553b9.png?w=1000&h=250&auto=compress,format&format=webp)](/?source=top_nav_blog_home)\nFollow\n[](https://x.com/TodayAnalytics) [](https://Analytics.Today) [](https://hashnode.com/@JohnRyan) [](https://www.linkedin.com/in/johnryanuk/)\nSection Title: Snowflake Cluster Keys and Partition Elimination: Best Practices to Maximize Query Performance (2...\nContent:\n![Image !John Ryan's photo ![John Ryan's photo](https://cdn.hashnode.com/res/hashnode/image/upload/v1719920035886/157ec7c1-829b-4148-8cee-a162b9451a57.png?w=200&h=200&fit=crop&crop=faces&auto=compress,format&format=webp)](https://hashnode.com/@JohnRyan)\n[John Ryan](https://hashnode.com/@JohnRyan)\n\u00b7 [Jun 7, 2025](https://analytics-today.hashnode.dev/snowflake-cluster-keys-and-micro-partition-elimination-best-practices) \u00b7\n21 min read\nSection Title: Snowflake Cluster Keys and Partition Elimination: Best Practices to Maximize Query Performance (2...\nContent:\n!Snowflake Cluster Keys and Partition Elimination: Best Practices to Maximize Query Performance (2025) ![Snowflake Cluster Keys and Partition Elimination: Best Practices to Maximize Query Performance (2025)](https://cdn.hashnode.com/res/hashnode/image/stock/unsplash/BilMxu-yNdM/upload/a5752ef07e963e45df4b132b7b07d223.jpeg?w=1600&h=840&fit=crop&crop=entropy&auto=compress,format&format=webp) ![Image](https://views.unsplash.com/v?app_id=261036&photo_id=BilMxu-yNdM)\nPhoto by [Riccardo Pierri](https://unsplash.com/@rieppi?utm_source=Hashnode&utm_medium=referral) on [Unsplash](https://unsplash.com/?utm_source=Hashnode&utm_medium=referral)\n**Last Updated:** 9th June 2025\n ... \nSection Title: Prefer to listen to a Podcast? > Best Practices: Frequent Updates\nContent:\nAs mentioned above, clustering is typically unsuited for tables with frequent UPDATEs as [Snowflake records data modification events](https://patents.justia.com/patent/10963443) (Inserts, Updates, or Deletes) and automatically triggers re-clustering as required.\nHowever, customers sometimes have frequent `Update` or `Delete` operations against clustered tables, which risks a very high re-clustering cost.\nIn extreme cases, Snowflake may constantly re-cluster data, which is then disrupted by subsequent update operations, leading to further costly reclustering.\nOne way around this is to suspend clustering during peak update times manually and then resume clustering during quiet times.\nMany customers find that suspending clustering during the week and resuming at the weekend (when there are fewer updates) restores the balance between keeping the tables well clustered and reclustering the cost. For example:\n ... \nSection Title: Prefer to listen to a Podcast? > Further Reading\nContent:\n[Boost Your Snowflake Query Performance With These 10 Tips](https://articles.analytics.today/boost-your-snowflake-query-performance-with-these-10-tips)\n[Improve Snowflake Query Speed By Preventing Spilling to Storage](https://articles.analytics.today/improve-snowflake-query-speed-by-preventing-spilling-to-storage)\n[Best Practices for Reducing Snowflake Costs: Top 10 Strategies](https://articles.analytics.today/best-practices-for-reducing-snowflake-costs-top-10-strategies)\n[Monitor Snowflake Usage & Cost](https://articles.analytics.today/monitor-snowflake-usage-cost)\n[Data Engineer\u2019s Guide to Snowflake ETL Best Practices](https://articles.analytics.today/data-engineers-guide-to-snowflake-etl-best-practices)\n[How to Cut Snowflake Data Storage Costs with Zero Copy Clones](https://articles.analytics.today/how-to-cut-snowflake-data-storage-costs-with-zero-copy-clones)"
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    },
    {
      "name": "sku_extract_excerpts",
      "count": 5
    }
  ]
}

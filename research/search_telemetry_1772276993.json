{"search_id":"search_18ca4c75c7e6458bb006260c10e66a71","results":[{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/ui-consumer-enable-logging","title":"Set up event tracing for an app - Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg™\nApache Iceberg™ Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Listings Snowflake Marketplace listings Use applications as a consumer Set up event tracing for an app\nSection Title: Set up event tracing for an app ¶\nContent:\nFeature — Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nThis topic describes how to set up use event tracing to capture the log messages and trace events\nemitted by an app. It also describes how to enable event sharing to share log messages and trace events\nwith providers.\nSection Title: Set up event tracing for an app ¶ > About event tracing in the Snowflake Native App Framework ¶\nContent:\nEvent tracing allows an app to emit information related to its performance and behavior. The Snowflake Native App Framework\nsupports using the Snowflake logging and tracing .\nfunctionality to gather this information. An app can emit the following:\nLog messages that are independent, detailed messages with information about the state of a specific\nfeature within the app.\nTrace events with structured data you can use to get information spanning and grouping multiple\nparts of an app.\nMetrics data that includes the CPU and memory metrics that Snowflake generates.\n ... \nSection Title: Set up event tracing for an app ¶ > About event sharing ¶\nContent:\nConsumers can also enable event sharing to share event data with providers. When a provider enables\nevent sharing, the log messages and trace events that are inserted into the event table in\nthe consumer account are also inserted into an event table in provider account.\nEvent sharing allows the provider to collect information about the app’s performance and behavior. See About event sharing for an app for more information.\nSection Title: Set up event tracing for an app ¶ > About event sharing ¶ > About event definitions ¶\nContent:\nEvent definitions specify how an app shares log messages and trace events with the provider.\nEvent definitions act as filters on the log message and trace event levels set by the provider.\nA provider specifies the event definitions for an app when a new version or patch is published.\nNote\nEvent definitions are not required. If a provider does not specify event definitions for an app\nconsumers can enable or disable event sharing as required.\nProviders can set an event definition to be required or optional:\n ... \nSection Title: Set up event tracing for an app ¶ > About event sharing ¶ > Supported event definitions ¶\nContent:\nThe following table lists the event definitions that are currently supported:\nTypeNameDescriptionFilterAllSNOWFLAKE$ALLShares all log messages and trace events that the app emits.`*`Errors and warningsSNOWFLAKE$ERRORS_AND_WARNINGSShares logs related to errors, warnings, and fatal events.`RECORD_TYPE = ‘LOG’ AND RECORD:severity_text in (‘FATAL’, ‘ERROR’, ‘WARN’)`MetricsSNOWFLAKE$METRICSShares the CPU and memory metrics that Snowflake generates.`RECORD_TYPE = in ('METRIC')`TracesSNOWFLAKE$TRACESShares detailed traces of user activities and journeys in the application.`RECORD_TYPE in (‘SPAN’, ‘SPAN_EVENT’)`Usage logsSNOWFLAKE$USAGE_LOGSShares high-level logs related to user actions and app events.`RECORD_TYPE = LOG AND RECORD:severity_text = ‘INFO’`Debug logsSNOWFLAKE$DEBUG_LOGSShares technical logs used to troubleshoot the app.`RECORD_TYPE = ‘LOG’ AND RECORD:severity_text in (‘DEBUG’, ‘TRACE’)`\nNote\nSection Title: Set up event tracing for an app ¶ > About event sharing ¶ > Supported event definitions ¶\nContent:\nIf a provider does not configure the app to use event definitions, Snowsight displays only the All type.\nSection Title: ... > Considerations for consumers when using event definitions ¶\nContent:\nConsumers can continue to use the existing SHARE_EVENTS_WITH_PROVIDER property, however there\nare limitations:\nIf an app only uses the OPTIONAL ALL event definition, setting the SHARE_EVENTS_WITH_PROVIDER property\nto `true` enables event sharing and setting it to `false` disables event sharing.This is applicable when a provider explicitly adds the OPTIONAL ALL event definition to the manifest\nfile or an app was migrated from the existing event sharing functionality.\nIf a provider adds mandatory and optional event definitions to the manifest file, setting the\nSHARE_EVENTS_WITH_PROVIDER property to `true` enables all event definitions. In contrast, the\nSHARE_EVENTS_WITH_PROVIDER property can only be set to `false` if the provider adds only\noptional event definitions.SHARE_EVENTS_WITH_PROVIDER is TRUE only when all event definitions are enabled, otherwise it is FALSE.\n ... \nSection Title: Set up event tracing for an app ¶ > Considerations when using event tracing ¶\nContent:\nThis feature requires you to set up an event table in\nyour account. After you enable event sharing , a masked and redacted\ncopy of the trace events and logs messages is automatically inserted in the event table of the designated\nprovider account. Snowflake does not charge you to enable event sharing. However, you are responsible for the\ncost of ingesting trace events and log message in the event table as well as storage\ncosts for the event table. After enabling event sharing with a provider, you cannot revoke access to shared\ntrace events and log messages. You cannot share historical events using event sharing. Snowflake sends the shared events to a designated provider account within the same region as your account. This feature does not share data across different regions. You cannot change the logging or tracing levels for an app. The app provider sets these levels\nwhen publishing the app.\n ... \nSection Title: Set up event tracing for an app ¶ > Enable event sharing for an app ¶\nContent:\nThe Snowflake Native App Framework supports sharing log messages and trace events stored in the consumer event table with the\napp provider. To share logs and event information with a provider, the consumer must enable event\nsharing for an app.\n ... \nSection Title: Set up event tracing for an app ¶ > ... > Enable event sharing by using SQL ¶\nContent:\nUse the SHOW TELEMETRY EVENT DEFINITIONS command to determine the event definitions for the app:CopyIf the provider did not configure the app to use event definitions, the `type` column\ndisplays `ALL` . Otherwise, this command lists the optional event definitions specified\nfor the app. If the app contains required event definitions, use the ALTER APPLICATION command to\nenable them:CopyThis command enables all of the require event definitions, but does not enable optional event\ndefinitions.NoteAfter enabling the required event definitions for an app, event sharing cannot be disabled. If the app contains options event definitions, use the use the ALTER APPLICATION to enable them as shown in the following example:CopyThis example enables the `SNOWFLAKE$TRACES` and `SNOWFLAKE$DEBUG_LOGS` based on the output of the SHOW TELEMETRY EVENT DEFINITIONS command.\n ... \nSection Title: ... > Determine if a log message or trace event is shared with the provider ¶\nContent:\nThe RECORD_ATTRIBUTES column contains the `snow.application.shared` field. If the value of\nthis field is TRUE, the log message or trace event is shared with the provider. Otherwise,\nthe log message or event is not shared.\nSection Title: Set up event tracing for an app ¶ > View the log and trace levels for an app ¶\nContent:\nThe log and trace level of an app are defined by the provider before publishing an app.\nConsumers cannot change the log and trace levels for an app.\nHowever, before setting up event tracing or enabling event sharing for an app, Snowflake\nrecommends verifying the log level to understand the type of information that collected\nand shared with the provider.\nTo view the log and trace level of an app, run the following command:\n```\nDESC APPLICATION HelloSnowflake ;\n```\nCopy\nThis command displays information about the `HelloSnowflake` app, including the following information\nabout the log and trace level set for the app:\nlog_level: The log level set by the provider.\ntrace_level: The trace level set by the provider.\nmetric_level: The metric data level set by the provider.\neffective_log_level: The log level set for the app.\neffective_trace_level: The trace level enabled for the app.\nSection Title: Set up event tracing for an app ¶ > View the log and trace levels for an app ¶\nContent:\nThe effective log and trace levels are determined by the event definitions the consumer enables for the app.\nFor example, if the provider defines the log level as OFF, but consumer enables the ERROR_AND_WARNING event\ndefinition, the app dynamically changes log level to WARN so that ERROR_AND_WARNING events can be collected. The app\nemits events that are less or equally as verbose as WARN and shares those error and warning events with the provider.\nThe values of `log_level` would be OFF and the value of `effective_log_level` would be WARN.\nIn contrast, if the provider defines the log level as TRACE, but the consumer enables the ERROR_AND_WARNING\nevent definition, the app emits events that are less or equally as verbose as trace, but only error and warning\nmessages are shared with the provider. The value of both log_level and effective_log_level would be TRACE.\nWas this page helpful?\nYes No\n ... \nSection Title: Set up event tracing for an app ¶ > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details‎"]},{"url":"https://docs.snowflake.com/en/developer-guide/builders/observability","title":"Observability in Snowflake apps | Snowflake Documentation","excerpts":["Section Title: Observability in Snowflake apps ¶ > What is observability? ¶\nContent:\nIn an observable system, you can understand what’s happening internally through external evidence generated by the system—evidence\nthat includes telemetry data, alerts, and notifications.\nThrough the evidence of internal functioning it provides, observability makes it easier for you to troubleshoot hard-to-understand behaviors\non a production system. This is especially true in a distributed system, where evidence collected from observation provides a view of\nbehavior across multiple components. Rather than disrupting a production environment to diagnose issues, you can analyze the collected\ndata from it.\nWith an observable system, you can start to answer questions such as the following:\nHow well is the system performing?\nWhere is there latency and what’s causing it?\nWhy is a particular component or process not working as it should?\nWhat improvements can be made?\nSection Title: Observability in Snowflake apps ¶ > Observability in Snowflake ¶\nContent:\nSnowflake supports a model that provides built-in observable data while also giving you ways to add more instrumentation where you need it.\nWhile Snowflake provides support for telemetry data such as logs, metrics, and traces (which are typical of observability), it also\nincludes other features you can use to keep track of system usage and performance.\nThe following lists features you can use to receive and analyze system performance and usage.\n|Collected telemetry data |As your application generates logs, metrics, and traces, Snowflake collects that telemetry data in an event table. Using\nSnowsight, you can explore the data, looking for patterns.\nYou can emit custom telemetry into the event table to provide contextual, domain-specific information to expedite debugging. |\n| --- | --- |\n|History Tables |Use the following views and their associated tables to monitor all usage in your account.\nSection Title: Observability in Snowflake apps ¶ > Observability in Snowflake ¶\nContent:\nQuery History\nCopy History\nTasks |\n|Alerts and notifications |Alerts allow for customizable triggering conditions, actions, and a schedule, in combination with notification integrations for proactive monitoring. |\n|Extensibility with third-party tools |The Snowflake event table adopts [OpenTelemetry](https://opentelemetry.io/docs/) standards, so your\nSnowflake telemetry can easily be consumed by other ecosystem tools. |\nSection Title: Observability in Snowflake apps ¶ > Telemetry data collected for analysis ¶\nContent:\nAs code in your application executes, you can have Snowflake collect data from the code that tells you about the application’s internal\nstate. Using this telemetry data—collected in a Snowflake event table (your account has one by default )—you can look for bottlenecks and other opportunities to optimize.\nTelemetry data must be emitted as your code executes. Snowflake emits some of this data on your code’s behalf without\nyou needing to instrument your code. You can use also APIs included with Snowflake to emit telemetry data from specific parts of your code.\nAs described below, you can analyze the collected data by querying the event table or using the visualizations that capture the data\nin Snowsight.\nSection Title: Observability in Snowflake apps ¶ > ... > Types of telemetry data ¶\nContent:\nTo ensure that the telemetry data you collect is broadly useful, Snowflake telemetry is built on the standard [OpenTelemetry](https://opentelemetry.io/docs/) (sometimes called OTel) framework, an incubating project of the Cloud Native Compute Foundation. Through this framework (and APIs and\ntools designed for it), you can reuse collected data with tools besides Snowflake .\nThrough OpenTelemetry, you can instrument application code to add observability where you want it.\nSnowflake event tables collect log, span, and metrics data in the OpenTelemetry data model. The following describes each type of telemetry\ndata collected in an event table.\n|Logs |Logs record individual operations performed by code. Each log message is generated at\na discrete point during the execution of the code.\nSection Title: Observability in Snowflake apps ¶ > ... > Types of telemetry data ¶\nContent:\n**Instrumenting code** You can log from your code using libraries standard for the language you’re using, as listed in Logging from handler code .\n**Viewing data** You can view log messages for analysis\neither by querying the event table or looking at the visualizations provided in Snowsight.\nThe following image from Snowsight shows a list of collected log messages for a two-hour period in a single database.\n|\n| --- | --- |\n|Metrics |Metrics are measurements calculated over a time period. These values include CPU and memory measurements.\n**Instrumenting code** Snowflake emits metrics data automatically as your code executes, so you don’t need to instrument your code.\n**Viewing data** You can view metrics data for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\nSection Title: Observability in Snowflake apps ¶ > ... > Types of telemetry data ¶\nContent:\nThe following image from Snowsight shows changes in collected metrics data for the execution of a user-defined function.\n|\n|Traces |Traces show distributed events as data flows through a system. In a trace, you can see where time is spent as processing flows\nfrom component to component.\nYou can emit trace events—both within the default span Snowflake creates or from a custom span you create—using libraries\nstandard for the language you’re using, as listed in Logging from handler code .\n**Instrumenting code** You can emit trace events from your code using libraries standard for the language you’re using, as listed in Event tracing from handler code .\n**Viewing data** You can view trace events for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\nThe following image from Snowsight shows the spans resulting as a UDF executes.\n|\nSection Title: Observability in Snowflake apps ¶ > Telemetry best practices ¶\nContent:\nUse the following best practices to get the most out of observablity in Snowflake.\nSet up your environment to capture telemetry data before you need it\nOptimize procedures with query telemetry\nCache redundant DataFrame operations\nManage the amount of telemetry data received for UDFs\nOptimize user-defined functions with query telemetry\nSection Title: ... > Set up your environment to capture telemetry data before you need it ¶\nContent:\nYou can’t analyze data that you haven’t collected, so it’s best to start collecting telemetry data so you’ll have it when you need it.\nAs your deployment grows, your need to understand how your code is performing grows.\nUse the following best practices:\nSection Title: ... > Set up your environment to capture telemetry data before you need it ¶\nContent:\nEnable telemetry data collection for your Snowflake environment.To collect the data you’ll need, ensure that you have an active event table. To ensure you’re collecting telemetry data you want, set telemetry levels to\nuseful thresholds.At first, you’ll want to set these levels to ensure that you’re collecting data. For example, set log levels to at least WARN for any\nproduction or business critical jobs. Over time, you might adjust these levels to meet changing needs.Organize your production stored procedures, UDFs, and other objects under a database or schema so you can simply enable warning logs\nfor that database or schema. This saves the trouble of managing settings for separate objects.\n ... \nSection Title: Observability in Snowflake apps ¶ > ... > Manage the amount of telemetry data received for UDFs ¶\nContent:\nWhen adding code to collect telemetry data with UDFs, remember that the UDF execution model can mean many more rows in the event table\nthan for a procedure.\nWhen a UDF is called on every input row, your handler code emits logging statements or span events for every row of the input dataset.\nFor example, a dataset of 10 million rows passed to a UDF would emit 10 million log entries.\nConsider using the following patterns when adding logs and span events to UDFs:\n ... \nSection Title: Observability in Snowflake apps ¶ > ... > Optimize user-defined functions with query telemetry ¶\nContent:\nThe following image shows a span for each row passed to a UDF, where one span’s longer duration suggests that the row might have larger\ndata than the others.\n ... \nSection Title: Observability in Snowflake apps ¶ > Tools for analysis and visualization ¶\nContent:\nYou can use the telemetry data collected in your event table with other tools that support the OpenTelemetry data model.\nThrough Snowflake support of OpenTelemetry, you can use APIs, SDKs, and other tools to instrument, generate, collect, and export telemetry\ndata. Using these tools, you can more thoroughly analyze software performance and behavior. Because a Snowflake event table uses this\nwidely-adopted standard, you might also be able to integrate your organization’s observability tools with event tables with little overhead.\nConsider integrating your external tools in one of the following ways:\nIf your observability tools can read from external sources, point them to the event table.\nIf your tools use a push model—in which telemetry data must be sent to the tool—consider using a stored procedure with external access to regularly read telemetry data from\nthe event table and emit it to your tool.\nSection Title: Observability in Snowflake apps ¶ > Tools for analysis and visualization ¶\nContent:\nThe following lists tools you might integrate with Snowflake event tables:\n[Snowflake integration for Datadog](https://docs.datadoghq.com/integrations/snowflake_web/)\nSnowflake integration for Grafana dashboardFor an introduction to using Grafana with Snowflake, see [How to monitor Snowflake with Grafana Cloud](https://grafana.com/blog/2023/05/24/how-to-monitor-snowflake-with-grafana-cloud/) .\n[Snowflake data source for Grafana](https://grafana.com/docs/plugins/grafana-snowflake-datasource/latest/)\n[Snowflake integration for Grafana Cloud](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/integrations/integration-reference/integration-snowflake/)\n[Observe for Snowflake](https://app.snowflake.com/marketplace/listing/GZTYZY3AR0U/observe-inc-observe-for-snowflake) , Observe’s native app for observability\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n ... \nSection Title: Observability in Snowflake apps ¶ > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details‎"]},{"url":"https://docs.snowflake.com/en/sql-reference/account-usage","title":"Account Usage | Snowflake Documentation","excerpts":["Reference General reference SNOWFLAKE database Account Usage\nSection Title: Account Usage ¶\nContent:\nIn the SNOWFLAKE database, the ACCOUNT_USAGE and READER_ACCOUNT_USAGE schemas enable querying object metadata, as well as historical\nusage data, for your account and all reader accounts (if any) associated with the account.\nSection Title: Account Usage ¶ > Overview of Account Usage schemas ¶\nContent:\nACCOUNT_USAGE :\nViews that display object metadata and usage metrics for your account.\nIn general, these views mirror the corresponding views and table functions in the Snowflake Snowflake Information Schema , but\nwith the following differences:\nRecords for dropped objects included in each view.\nLonger retention time for historical usage data.\nData latency.\nFor more details, see Differences Between Account Usage and Information Schema (in this topic). For more details about each\nview, see ACCOUNT_USAGE Views (in this topic).\nREADER_ACCOUNT_USAGE :\nViews that display object metadata and usage metrics for all the reader accounts that have been created for\nyour account (as a Secure Data Sharing provider).\nThese views are a small subset of the ACCOUNT_USAGE views that apply to reader accounts. Also, each view in this schema contains an\nadditional `READER_ACCOUNT_NAME` column for filtering results by reader account.\n ... \nSection Title: Account Usage ¶ > ... > Historical data retention ¶\nContent:\nCertain account usage views provide historical usage metrics. The retention period for these views is 1 year (365 days).\nIn contrast, the corresponding views and table functions in the Snowflake Information Schema have much shorter retention periods,\nranging from 7 days to 6 months, depending on the view.\nSection Title: Account Usage ¶ > ACCOUNT_USAGE views ¶\nContent:\nThe ACCOUNT_USAGE schema contains the following views:\n ... \nSection Title: Account Usage ¶ > ACCOUNT_USAGE views ¶\nContent:\n| View | Type | Latency [1] | Edition [3] | Notes |\n| ACCESS_HISTORY | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| AGGREGATE_ACCESS_HISTORY | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| AGGREGATE_QUERY_HISTORY | Historical | 3 hours |  |  |\n| AGGREGATION_POLICIES | Object | 2 hours |  |  |\n| APPLICATION_SPECIFICATIONS | Historical | 1 hour |  | Data for deleted app specifications is retained for 1 year. |\n| ARCHIVE_STORAGE_DATA_RETRIEVAL_USAGE_HISTORY | Historical | 1 hour |  | Data retained for 1 year. |\n| AUTOMATIC_CLUSTERING_HISTORY | Historical | 3 hours |  | Data retained for 1 year. |\n| BACKUP_OPERATION_HISTORY | Historical | 6 hours |  | Data retained for 1 year. |\n| BACKUP_POLICIES | Object | 6 hours |  |  |\n| BACKUP_SETS | Object | 6 hours |  |  |\n| BACKUP_STORAGE_USAGE | Historical | 6 hours |  | Data retained for 1 year. |\n ... \nSection Title: Account Usage ¶ > ACCOUNT_USAGE views ¶\nContent:\n| View | Type | Latency [1] | Edition [3] | Notes |\n| ACCESS_HISTORY | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| AGGREGATE_ACCESS_HISTORY | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| AGGREGATE_QUERY_HISTORY | Historical | 3 hours |  |  |\n| AGGREGATION_POLICIES | Object | 2 hours |  |  |\n| WAREHOUSE_EVENTS_HISTORY | Historical | 3 hours |  | Data retained for 1 year. |\n| WAREHOUSE_LOAD_HISTORY | Historical | 3 hours |  | Data retained for 1 year. |\n| WAREHOUSE_METERING_HISTORY | Historical | 3 hours |  | Data retained for 1 year. |\nSection Title: Account Usage ¶ > ACCOUNT_USAGE views ¶\nContent:\n[1] All latency times are approximate; in some instances, the actual latency may be lower.\n[2] The latency of the views for a given table may be up to 2 days if both of the following conditions are true: 1. Fewer than 32 DML statements have been added to the given table since it was last updated in LOAD_HISTORY or COPY_HISTORY. 2. Fewer than 100 rows have been added to the given table since it was last updated in LOAD_HISTORY or COPY_HISTORY.\n[3] Unless otherwise noted, the Account Usage view is available to all accounts.\nSection Title: Account Usage ¶ > ACCOUNT_USAGE views ¶ > Account Usage table functions ¶\nContent:\nCurrently, Snowflake supports one ACCOUNT_USAGE table function:\nSection Title: Account Usage ¶ > ACCOUNT_USAGE views ¶ > Account Usage table functions ¶\nContent:\n| Table Function | Data Retention | Notes |\n| TAG_REFERENCES_WITH_LINEAGE | N/A | Results are only returned for the role that has access to the specified object. |\nSection Title: Account Usage ¶ > ACCOUNT_USAGE views ¶ > Account Usage table functions ¶\nContent:\nNote\nSimilar to the Account Usage views, please account for latency when calling this table function. The expected latency for this table\nfunction is similar to the latency for the TAG_REFERENCES view.\n ... \nSection Title: Account Usage ¶ > ... > ACCOUNT_ USAGE schema SNOWFLAKE database roles ¶\nContent:\nIn addition, you can grant finer control to accounts using SNOWFLAKE Database roles.\nFor more information on database roles, see database roles .\nACCOUNT_USAGE schemas have four defined SNOWFLAKE database roles, each granted the SELECT privilege on specific views.\nSection Title: Account Usage ¶ > ... > ACCOUNT_ USAGE schema SNOWFLAKE database roles ¶\nContent:\n| Role | Purpose and Description |\n| OBJECT_VIEWER | The OBJECT_VIEWER role provides visibility into object metadata. |\n| USAGE_VIEWER | The USAGE_VIEWER role provides visibility into historical usage information. |\n| GOVERNANCE_VIEWER | The GOVERNANCE_VIEWER role provides visibility into data governance related information. |\n| SECURITY_VIEWER | The SECURITY_VIEWER role provides visibility into security based information. |\n ... \nSection Title: Account Usage ¶ > Querying the Account Usage views ¶ > Examples ¶\nContent:\nThe following examples show some typical/useful queries using the views in the ACCOUNT_USAGE schema.\nNote\nThese examples assume the SNOWFLAKE database and the ACCOUNT_USAGE schema are in use for the current session. The examples also\nassume the ACCOUNTADMIN role (or a role granted IMPORTED PRIVILEGES on the database) is in use. If they are not in use, execute\nthe following commands before running the queries in the examples:Copy\n ... \nSection Title: Account Usage ¶ > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details‎"]},{"url":"https://www.flexera.com/blog/finops/snowflake-native-apps/","title":"Snowflake Native Apps 101: Build and monetize data apps (2026)","publish_date":"2026-01-27","excerpts":["Section Title: ... > 1) **Native Integration with Snowflake Services**\nContent:\nSnowflake Native Apps work directly with Snowflake’s core services. They use stored procedures , user-defined functions (Snowflake UDFs UDFs) and the Snowpark API, making them efficient and seamless.\nSection Title: ... > 2) **Simplified Development and Testing**\nContent:\nYou can build Snowflake Native Applications using the Snowflake Native App Framework. Snowflake Native App Framework streamlines development and testing. You can create, test and deploy Snowflake apps within Snowflake, reducing development time.\nSection Title: ... > 3) **Monetization via Snowflake Marketplace**\nContent:\nProviders can list and sell Snowflake apps in the Snowflake Marketplace . Consumers install these apps directly into their Snowflake accounts, simplifying deployment and making app monetization straightforward.\n ... \nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > ... > 8) **Streamlit Integration**\nContent:\nYou can integrate your apps with Streamlit , which allows you to create interactive dashboards within Snowflake. While the integration is still evolving, it supports embedding visualizations in your apps for end-user analytics.\n ... \nSection Title: ... > What Are the Benefits of Snowflake Native Apps — For Consumers?\nContent:\nSnowflake Native Apps offer significant advantages for end-users, enhancing their ability to integrate, utilize and manage data applications seamlessly within the Snowflake ecosystem. Here are some of the key benefits of Native Snowflake apps for consumers:\n ... \nSection Title: ... > How Do Snowflake Native Applications Work?\nContent:\nSnowflake Native Applications leverage the *Snowflake Native App Framework* to build and deploy data-driven applications directly within the Snowflake ecosystem. These Snowflake apps harness Snowflake’s core features—secure data sharing, analytics, compute and governance—enabling seamless integration and monetization, without requiring data to move outside the platform. The framework supports applications ranging from analytical tools to fully containerized services.\nSnowflake Native App Framework allows:\nSection Title: ... > How Do Snowflake Native Applications Work?\nContent:\nProviders to share data, business logic and application interfaces (e.g., Streamlit apps, stored procedures) using [Snowpark API](https://docs.snowflake.com/en/developer-guide/snowpark/index) , [Python](https://www.python.org/) , [SQL](https://www.w3schools.com/sql/) and [JavaScript](https://www.w3schools.com/js/) .\nApplications to be listed as free or paid offerings on the Snowflake Marketplace or shared privately with select accounts.\nDevelopers to benefit from streamlined testing environments, version control via external repositories and detailed logging for troubleshooting.\nBuilt-in support for structured and unstructured event logging to streamline troubleshooting and performance tracking.\nIntegration with Streamlit to build interactive, user-friendly visual interfaces.\nOn top of that, the Snowflake Native Framework also provides an enhanced developer experience, including:\n ... \nSection Title: ... > Architecture of the Snowflake Native App Framework\nContent:\nThe architecture of the Snowflake Native App Framework operates on a provider-consumer model:\n**Provider** — Creates and shares data and application logic using the framework.\n**Consumer** — Installs and interacts with applications shared by providers.\nSnowflake Native Applications are packaged as **Application Packages** , which contains the necessary logic, metadata and configuration to deploy a Snowflake Native App. This includes:\n**Manifest file** : Configuration details, including setup script locations and versioning.\n**Setup script** : Contains SQL commands for installation and updates.\nThe provider publishes the Snowflake Native app via:\n**Marketplace Listings** — Accessible to all Snowflake users for broad distribution.\n**Private Listings** — Targeted sharing with specific accounts across regions.\n ... \nSection Title: ... > How do Snowflake Native Applications work with Snowpark Container Services?\nContent:\nFor advanced use cases, Snowflake Native Applications can utilize [**Snowpark Container Services**](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) , which enable Snowflake apps to manage containerized workloads within Snowflake. This approach supports high-performance applications, such as machine learning and AI-driven analytics, without externalizing data.\nComponents unique to containerized Snowflake apps:\n**Services specification file** — Applications reference container images stored in the provider’s repository.\n**Compute pool** — A collection of virtual machine nodes where containerized workloads execute.\nSnowflake Native App with Snowpark Container Architecture\nFeatures of Snowpark Container Services include:\n ... \nSection Title: ... > **Step 6** —Adding Data Content to Your Snowflake Native App\nContent:\n```\ndefinition_version: 2\nentities:\n\t snowflake_native_app_demo_package:\n\t\t\t type: application package\n\t\t\tstage: stage_content.hello_snowflake_stage\n\t\t\tmanifest: app/manifest.yml\n\t\t\tidentifier: snowflake_native_app_demo_package\n\t\t\t artifacts:\n\t\t\t\t\t- src: app/*\n\t\t\t\t\t dest: ./\n\t\t\t meta:\n\t\t\t\t post_deploy:\n\t\t\t\t\t\t - sql_script: scripts/shared_content.sql\n\t\tdemo_snowflake_native_app:\n\t\t\ttype: application\n\t\t\t from:\n\t\t\t\t target: snowflake_native_app_demo_package\n\t\t\tdebug: false\n```\nNext, modify the **setup_script.sql** file to create a view that app consumers can use to query the data.\n**Create a Versioned Schema:**\n```\nCREATE OR ALTER VERSIONED SCHEMA code_schema;\nGRANT USAGE ON SCHEMA code_schema TO APPLICATION ROLE snowflake_native_app_public;\n```\n**Create the View:**\n ... \nSection Title: ... > **Step 7** —Adding Python Code to Your Snowflake Native App\nContent:\nLet’s enhance our Snowflake Native App by incorporating Python-based logic using User-Defined Functions (Snowflake UDFs). We’ll add both an inline Python UDF and one that references an external Python module.\nInline Python UDFs enable you to embed Python logic directly within your setup script. To do so, update your setup script to include the following code:\n```\nCREATE OR REPLACE FUNCTION code_schema.squareroot(i INT)\nRETURNS INT\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.11'\nHANDLER = 'squareroot_py'\nAS\n$$\ndef squareroot_py(i):\n\t return i * i\n$$;\n\nGRANT USAGE ON FUNCTION code_schema.squareroot(INT) TO APPLICATION ROLE snowflake_native_app_public;\n```\nAs you can see, this:\nCreates a Python UDF named squareroot in the code_schema schema.\nUses Python 3.11 runtime.\nGrants the necessary usage privilege to the snowflake_native_app_public role.\n ... \nSection Title: ... > Monetization and Distribution of Snowflake Native App\nContent:\nRaw datasets\nRefined and enriched data\nHistorical datasets for forecasting and machine learning\nReal-time data streams (like weather or traffic updates)\nSpecialized identity or audience data for analytics\nSnowflake Native Applications\nPre-built data pipelines and transformations\nSnowflake Marketplace leverages Snowflake’s architecture to facilitate the secure sharing of data and applications. Transactions are managed natively, eliminating the need for third-party billing systems. Vendors can offer their products through various pricing models, such as pay-as-you-go, one-time payment, usage-based payment, or subscription-based plans, while benefiting from Snowflake’s built-in analytics to track customer engagement.\nLet’s jump right into the juicy part of the article: a step-by-step guide to monetizing Snowflake Native Applications via Snowflake Marketplace.\n ... \nSection Title: ... > **Step 3** —Create and Configure a Private Listing on the Snowflake Marketplace\nContent:\nTo share your app via the Snowflake Marketplace, start by signing in to Snowsight and navigating to **Data Products > Provider Studio** .\nNavigating to Provider Studio in Snowflake\nClick **+ Listing** to create a new listing and proceed with configuration. Enter a name for the listing and specify the discovery permissions, choosing whether the listing will be public or restricted to specific consumers (e.g., select “ **Only specified consumers** ” for private sharing and select “ **Anyone on the Marketplace** ” for public listing).\nCreating a private listing for only specified consumers – Snowflake Native App\nSection Title: ... > **Step 3** —Create and Configure a Private Listing on the Snowflake Marketplace\nContent:\nAttach the application package you prepared earlier as the core data content for the listing. Provide a detailed description outlining your app’s features and usage scenarios. If creating a private listing, add the account identifiers of intended consumers in the “ **Add** **Consumer accounts** ” section. Finally, publish your listing for approval.\nCreating a private listing for only specified consumers – Snowflake Native App\n ... \nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > FAQs\nContent:\n**What are Native Apps in Snowflake?**\nSnowflake Native Apps are designed specifically to operate within the Snowflake ecosystem without requiring external access or movement of sensitive data outside its environment.\n**How can I develop and test a Snowflake Native App locally?**\nDevelopers can set up their environments using tools like VSCode along with necessary extensions provided by Snowflakes such as CLI support.\n**Can I share my Snowflake Native App with other users?**\nYes! Once published on the marketplace after meeting compliance requirements.\n**Does the Snowflake Native App framework support logging and monitoring?**\nYes! Snowflake Native App framework includes telemetry tools that allow developers to monitor application performance post-deployment.\n**What is Streamlit’s role in Snowflake Native apps?**"]},{"url":"https://www.snowflake.com/en/blog/collect-logs-traces-snowflake-apps/","title":"Collect Logs and Traces From Your Snowflake Applications","publish_date":"2024-08-21","excerpts":["blog\nSection Title: Category\nContent:\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\nData Engineering\nOct 30, 2023 | 4 min read\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables\nContent:\nWe are excited to announce the general availability of Snowflake\nEvent Tables\nfor logging and tracing, an essential feature to boost application observability and supportability for Snowflake developers.\nIn our conversations with developers over the last year, we’ve heard that monitoring and observability are paramount to effectively develop and monitor applications. But previously, developers didn’t have a centralized, straightforward way to capture application logs and traces.\nEnter the new Event Tables feature, which helps developers and data engineers easily instrument their code to capture and analyze logs and traces for all languages: Java, Scala, JavaScript, Python and Snowflake Scripting.\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables\nContent:\nWith Event Tables, developers can instrument logs and traces from their UDFs, UDTFs, stored procedures, Snowflake Native Apps and Snowpark Container Services, then seamlessly route them to a secure, customer-owned Event Table. Developers can then query Event Tables to troubleshoot their applications or gain insights into performance and code behavior.\nLogs and traces are collected and propagated via Snowflake’s telemetry APIs, then automatically ingested into your Snowflake Event Table.\nSection Title: ... > Simplify troubleshooting in Snowflake Native Apps\nContent:\nEvent Tables are also supported for Snowflake Native Apps. When a Snowflake Native App runs, it is running in the consumer’s account, generating telemetry data that’s ingested into their active Event Table.\nOnce the consumer enables event sharing, new telemetry data will be ingested into both the consumer and provider Event Tables. Now the provider has the ability to debug the application that’s running in the consumer’s account. The provider only sees the telemetry data that is being shared from this data application—nothing else.\nSection Title: ... > Improve reliability across a variety of use cases\nContent:\nYou can use Event Tables to capture and analyze logs for various use cases: * As a data engineer building UDFs and stored procedures within queries and tasks, you can instrument your code to analyze its behavior based on input data.\nAs a Snowpark developer, you can instrument logs and traces for your Snowflake applications to troubleshoot and improve their performance and reliability.\nAs a Snowflake Native App provider, you can analyze logs and traces from various consumers of your applications to troubleshoot and improve performance.\nSection Title: ... > Improve reliability across a variety of use cases\nContent:\nSnowflake customers ranging from Capital One to phData are already using Event Tables to unlock value in their organization. “The Event Tables feature simplifies capturing logs in the observability solution we built to monitor the quality and performance of Snowflake data pipelines in Capital One Slingshot,” says Yudhish Batra, Distinguished Engineer, Capital One Software. “Event Tables has abstracted the complexity associated with logging from our data pipelines—specifically, the central Event Table gives us the ability to monitor and alert from a single location.”\nAs phData migrates its Spark and Hadoop applications to Snowpark, the Event Tables feature has helped architects save time and hassle.\n“When working with Snowpark UDFs, some of the logic can become quite complex. In some instances, we had thousands of lines of Java code that needed to be monitored and debugged,” says Nick Pileggi,\nPrincipal Solutions Architect at phData\nSection Title: ... > Improve reliability across a variety of use cases\nContent:\n. “Before Event Tables, we had almost no way to see what was happening inside the UDF and correct issues. Once we rolled out Event Tables, the amount of time we spent testing dropped significantly and allowed us to have debug and info-level access to the logs we were generating in Java.”\nOne large communications service provider also uses logs in Event Tables to capture and analyze failed records during data ingestion from various external services to Snowflake. And a Snowflake Native App provider offering geolocation data services uses Event Tables to capture logs and traces from their UDFs to improve application reliability and performance.\nWith Event Tables, you now have a built-in place to easily and consistently manage logging and tracing for your Snowflake applications. And in conjunction with other features such as Snowflake Alerts and Email Notifications, you can be notified of new events and errors in your applications.\nSection Title: ... > Try Event Tables today\nContent:\nTo learn more about Event Tables, join us at BUILD\n,\nSnowflake’s developer conference. Or get started with Event Tables today with a tutorial\nand quickstarts for logging\nand tracing\n. For further information about how Event Tables work, visit Snowflake product documentation\n.\nSection Title: ... > The Data Engineer’s Guide to Python for Snowflake\nContent:\n[download now](https://www.snowflake.com/resource/the-data-engineers-guide-to-python-for-snowflake/)\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables > ... > Author\nContent:\nAshwin Kamath\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables > ... > Share Article\nContent:\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps&title=Collect+Logs+and+Traces+From+Your+Snowflake+Applications+With+Event+Tables)\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps&text=Collect+Logs+and+Traces+From+Your+Snowflake+Applications+With+Event+Tables)\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps)\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables > ... > Just For You\nContent:\nProduct and Technology\nSection Title: ... > Alerts and Observability for Pipeline Monitoring and Cost Management\nContent:\nShiyi Gu\nNov 23, 2022 | 5 min read\nSubscribe to our blog newsletter\nGet the best, coolest and latest delivered to your inbox each week\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice."]},{"url":"https://docs.snowflake.com/en/collaboration/provider-listings-monitor-studio","title":"Monitor listing use - Source: Snowflake Documentation","excerpts":["Section Title: Monitor listing use ¶ > Which metrics are tracked? ¶\nContent:\nDepending on whether you offer your listing privately or on the Snowflake Marketplace, you see different usage analytics.\nTo see who is using your listings, you can use Provider Studio or the database views provided by Snowflake.\nSnowflake tracks many metrics for listings, including the following:\nDaily telemetry usage for your listing, such as the daily consumer query history.\nEvents when consumers get or request your listing.\nEvents when consumers view or click your listing detail page on the Snowflake Marketplace.\nUse of your listing, such as number of jobs run on the data product in your listing.\nAccess details for your listing, such as viewing the tables in your listing.\nThe company name and account name of consumers accessing your listing.\nInformation consumers submit when requesting unlimited access to a limited trial listing.\nAnd more.\nSection Title: Monitor listing use ¶ > Which metrics are tracked? ¶\nContent:\nSee Data Sharing Usage and Organization Usage for more details about viewing this information in SQL.\nFor a full list of metrics tracked for listings, and details for viewing this data using SQL, see Data Sharing Usage and Organization Usage .\nFor paid listings, you can see additional data on a per-listing basis or for all paid listings in your organization:\nEarnings history for your listings.\nCharges accumulated by type of charge.\nNumber of queries included in the charge to a consumer.\nNumber of consumers trialing your listing.\nNumber of purchases of your listing.\nAnd more.\nSee Monetization usage views for more details about viewing this information in SQL.\nSection Title: Monitor listing use ¶ > Monitor consumer usage metrics in Provider Studio ¶\nContent:\nTo help you manage the performance and usage of your listings, Provider Studio provides overview analytics on the Home tab and\naggregated and detailed analytics on the Analytics tab.\nNote\nProviders receive usage data and other details as described in Monitor listing use . Consumers accessing\na data product containing a Snowflake Native App will receive consumer-related monitoring data\nonly if the app emits events specifically for the consumer.\nSection Title: Monitor listing use ¶ > ... > Prerequisites for viewing usage data in Provider Studio ¶\nContent:\nBefore you can view usage data, make sure that you meet the prerequisites.\nTo view Provider Studio and the data on the Home and Analytics tabs, you must use the ACCOUNTADMIN role\nor a custom role granted the CREATE LISTING privilege and IMPORTED PRIVILEGES on the SNOWFLAKE database.\nSee Enabling other roles to use schemas in the SNOWFLAKE database .\nYou must select a warehouse that Snowflake can use to bill your account for the queries that generate these usage analytics.\nYou are billed at the normal rate for these queries. For more information, see Understanding compute cost .\nSection Title: Monitor listing use ¶ > ... > View usage data in Provider Studio ¶\nContent:\nTo view usage data in Provider Studio , do the following:\nIn the navigation menu, select Marketplace » Provider Studio .\nSelect a warehouse. In the upper-right corner of the page, click Select Warehouse .\nQueries to show analytics data on the Home tab and Analytics tab run successfully and display results.\nYou can see trends from the last 28 days on the Home tab, including the following:\nThe number of queries executed by consumers against your data products, including how those numbers are trending compared to the previous\n28 day period.\nThe number of unique consumers that have queried your listings, including how those numbers are trending compared to the previous\n28 day period.\nThe name of your most-queried listing.\nThe consumer who has run the most queries against your listings.\nOn the Analytics tab, you can see additional metrics, both aggregated across all your listings or a detailed view for a specific listing.\nSection Title: Monitor listing use ¶ > ... > View overview analytics for your listings ¶\nContent:\nPreview Feature — Open\nThis feature is currently in preview and is available only to Snowflake data providers.\nYou can see the following overview metrics for your listings:\nSection Title: Monitor listing use ¶ > ... > View overview analytics for your listings ¶\nContent:\nThe reach of your listings on the Snowflake Marketplace, such as views of all of your listings, and a list of the most-viewed listings.\nEngagement with your listings, based on the number of queries executed. You can see the following engagement overview metrics:\nThe number of queries executed across all your listings.\nYour listings ordered by usage, determined by number of queries executed.\nThe number of consumer accounts actively using your listings per day.\nA ranking of the most active consumers, based on query execution.\nA list of the regions in which consumers execute the most queries.\nFor your free listings on the Snowflake Marketplace, you can see consumer conversion from viewing, mounting the share, and querying\nyour listings. You can only see consumer conversion over the last 28 days.\nSection Title: Monitor listing use ¶ > ... > View detailed metrics for your listings ¶\nContent:\nOn the Analytics tab, if you select More for a specific metric or select Detailed Metrics , you can see the following\nanalytics:\n ... \nSection Title: Monitor listing use ¶ > ... > View consumer details for your listings ¶\nContent:\nYou can see consumer details, including the company name and Snowflake account name for the consumer, in several places:\nIn the Usage Trends on the Home tab that lists the most active consumer.\nOn the Overview tab in the Most active consumers tile.\nIn the detailed metrics for Listings Installed , Consumer Requests , and Queries Executed when viewing a specific listing.\nTo see which consumers are installing and using your listings, do the following:\nSection Title: Monitor listing use ¶ > ... > View consumer details for your listings ¶\nContent:\nIn the navigation menu, select Marketplace » Provider Studio .\nIf you do not have a default warehouse set, select a warehouse. In the upper-right corner of the page, choose Select Warehouse .\nSelect Analytics » Detailed Metrics .\nSelect the Queries Executed drop-down and select Listings Installed .\nReview the table of listings and select a specific listing that you want to see details for.\nIn the table of All consumers for the selected time period, review the company name, account name, first name, last name,\nemail address, and region of the consumer that installed the selected listing. If the company name is not available for a consumer,\nyou instead see the Snowflake organization and account names.\nSection Title: Monitor listing use ¶ > ... > View consumer details for your listings ¶\nContent:\nTo see other details about consumers using your listings, review the Most active consumers tile on the Overview tab. It shows\na list of consumer company names ordered by how many queries they executed against your data product in the selected time frame.\nSelect a consumer to see detailed metrics such as the dates of usage and Snowflake region of the consumer account.\nSection Title: Monitor listing use ¶ > ... > View usage data by using SQL ¶\nContent:\nThe overview metrics on the Analytics tab are derived from the metrics in the DATA_SHARING_USAGE schema. Some metrics, including\nmost active consumers, conversion rate, most viewed listings, and most used listings, are derived from the metrics and might not exactly\nmatch what you can see in the schema.\nThe detailed metrics on the Analytics tab are obtained by querying the views in the Data Sharing Usage schema.\nIf you want, you can query the views directly. See Data Sharing Usage .\nBecause the views are part of the SNOWFLAKE database, only account administrators (users with the ACCOUNTADMIN role) can perform queries on the data sharing usage for the listings published from that account. Privileges can be granted to other roles in your account to allow other users access. For more details, see Enabling other roles to use schemas in the SNOWFLAKE database .\nNote\n ... \nSection Title: Monitor listing use ¶ > Improve listing performance in the Snowflake Marketplace ¶\nContent:\nIf you publish your listing on the Snowflake Marketplace and want to improve the listing performance with consumers, review the [Snowflake Marketplace Provider Best Practices](https://www.snowflake.com/provider-best-practices/) .\nFor a video on how to use Provider Studio to review the performance of your listings,\nsee [Provider Studio Analytics - Understanding Listing Performance](https://www.snowflake.com/wp-content/uploads/2024/11/Provider_Studio_Analytics_2024-11.mp4) .\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nSupplemental Documentation\nAdditional terms of use may apply to features listed on this page.\nSection Title: Monitor listing use ¶ > Improve listing performance in the Snowflake Marketplace ¶\nContent:\nOn this page\nWhich metrics are tracked?\nMonitor consumer usage metrics in Provider Studio\nImprove listing performance in the Snowflake Marketplace\nRelated content\nCreate and publish a listing\nManage listing requests as a provider\nModify published listings\nRemove listings as a provider"]},{"url":"https://www.snowflake.com/en/developers/guides/resource-optimization-usage-monitoring/","title":"Resource Optimization: Usage Monitoring","excerpts":["Section Title: Resource Optimization: Usage Monitoring\nContent:\nMatt Meredith\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/resource-optimization-usage-monitoring)\nSection Title: Resource Optimization: Usage Monitoring > Overview\nContent:\nThis resource optimization guide represents one module of the four contained in the series. These guides are meant to help customers better monitor and manage their credit consumption. Helping our customers build confidence that their credits are being used efficiently is key to an ongoing successful partnership. In addition to this set of Snowflake Quickstarts for Resource Optimization, Snowflake also offers community support as well as Training and Professional Services offerings. To learn more about the paid offerings, take a look at upcoming education and training .\nThis blog post can provide you with a better understanding of Snowflake's Resource Optimization capabilities.\nContact our team at marketing@snowflake.com , we appreciate your feedback.\nSection Title: Resource Optimization: Usage Monitoring > Overview > Usage Monitoring\nContent:\nUsage Monitoring queries are designed to identify the warehouses, queries, tools, and users that are responsible for consuming the most credits over a specified period of time. These queries can be used to determine which of those resources are consuming more credits than anticipated and take the necessary steps to reduce their consumption.\n ... \nSection Title: Resource Optimization: Usage Monitoring > ... > Tier 1 > How to Interpret Results:\nContent:\nAre there specific warehouses that are consuming more credits than the others? Should they be? Are there specific warehouses that are consuming more credits than anticipated for that warehouse?\n ... \nSection Title: Resource Optimization: Usage Monitoring > Forecasting Usage/Billing (T1) > Tier 1 > Description:\nContent:\nThis query provides three distinct consumption metrics for each day of the contract term. (1) the contracted consumption is the dollar amount consumed if usage was flat for the entire term. (2) the actual consumption pulls from the various usage views and aggregates dollars at a day level. (3) the forecasted consumption creates a straight line regression from the actuals to project go-forward consumption.\n ... \nSection Title: Resource Optimization: Usage Monitoring > ... > Tier 1 > Description:\nContent:\nIdentifies which of Snowflake's partner tools/solutions (BI, ETL, etc.) are consuming the most credits.\nSection Title: Resource Optimization: Usage Monitoring > ... > Tier 1 > How to Interpret Results:\nContent:\nAre there certain partner solutions that are consuming more credits than anticipated? What is the reasoning for this?\n ... \nSection Title: Resource Optimization: Usage Monitoring > Partner Tools Consuming Credits (T1) > Tier 1 > SQL\nContent:\n--THIS IS APPROXIMATE CREDIT CONSUMPTION BY CLIENT APPLICATION\nWITH CLIENT_HOUR_EXECUTION_CTE AS (\nSELECT  CASE\nWHEN CLIENT_APPLICATION_ID LIKE 'Go %' THEN 'Go'\nWHEN CLIENT_APPLICATION_ID LIKE 'Snowflake UI %' THEN 'Snowflake UI'\nWHEN CLIENT_APPLICATION_ID LIKE 'SnowSQL %' THEN 'SnowSQL'\nWHEN CLIENT_APPLICATION_ID LIKE 'JDBC %' THEN 'JDBC'\nWHEN CLIENT_APPLICATION_ID LIKE 'PythonConnector %' THEN 'Python'\nWHEN CLIENT_APPLICATION_ID LIKE 'ODBC %' THEN 'ODBC'\nELSE 'NOT YET MAPPED: ' || CLIENT_APPLICATION_ID\nEND AS CLIENT_APPLICATION_NAME\n,WAREHOUSE_NAME\n,DATE_TRUNC('hour',START_TIME) as START_TIME_HOUR\n,SUM(EXECUTION_TIME)  as CLIENT_HOUR_EXECUTION_TIME\nFROM \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"QUERY_HISTORY\" QH\nJOIN \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"SESSIONS\" SE ON SE.SESSION_ID = QH.SESSION_ID\nWHERE WAREHOUSE_NAME IS NOT NULL\nAND EXECUTION_TIME > 0\nSection Title: Resource Optimization: Usage Monitoring > Partner Tools Consuming Credits (T1) > Tier 1 > SQL\nContent:\n--Change the below filter if you want to look at a longer range than the last 1 month\nAND START_TIME > DATEADD(Month,-1,CURRENT_TIMESTAMP())\ngroup by 1,2,3\n)\n, HOUR_EXECUTION_CTE AS (\nSELECT  START_TIME_HOUR\n,WAREHOUSE_NAME\n,SUM(CLIENT_HOUR_EXECUTION_TIME) AS HOUR_EXECUTION_TIME\nFROM CLIENT_HOUR_EXECUTION_CTE\ngroup by 1,2\n)\n, APPROXIMATE_CREDITS AS (\nSELECT\nA.CLIENT_APPLICATION_NAME\n,C.WAREHOUSE_NAME\n,(A.CLIENT_HOUR_EXECUTION_TIME/B.HOUR_EXECUTION_TIME)*C.CREDITS_USED AS APPROXIMATE_CREDITS_USED\n```\nFROM CLIENT_HOUR_EXECUTION_CTE A\nJOIN HOUR_EXECUTION_CTE B  ON A.START_TIME_HOUR = B.START_TIME_HOUR and B.WAREHOUSE_NAME = A.WAREHOUSE_NAME\nJOIN \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"WAREHOUSE_METERING_HISTORY\" C ON C.WAREHOUSE_NAME = A.WAREHOUSE_NAME AND C.START_TIME = A.START_TIME_HOUR\n```\n)\nSection Title: Resource Optimization: Usage Monitoring > Partner Tools Consuming Credits (T1) > Tier 1 > SQL\nContent:\nSELECT\nCLIENT_APPLICATION_NAME\n,WAREHOUSE_NAME\n,SUM(APPROXIMATE_CREDITS_USED) AS APPROXIMATE_CREDITS_USED\nFROM APPROXIMATE_CREDITS\nGROUP BY 1,2\nORDER BY 3 DESC\n;\n```\n\nCopy\n```\nSection Title: Resource Optimization: Usage Monitoring > Credit Consumption by User (T1) > Tier 1 > Description:\nContent:\nIdentifies which users are consuming the most credits within your Snowflake environment.\nSection Title: Resource Optimization: Usage Monitoring > ... > Tier 1 > How to Interpret Results:\nContent:\nAre there certain users that are consuming more credits than they should? What is the purpose behind this additional usage?\n ... \nSection Title: Resource Optimization: Usage Monitoring > Warehouse Utilization (T2) > Tier 2 > Description:\nContent:\nThis query is designed to give a rough idea of how busy Warehouses are compared to the credit consumption per hour. It will show the end user the number of credits consumed, the number of queries executed and the total execution time of those queries in each hour window.\n ... \nSection Title: Resource Optimization: Usage Monitoring > Warehouse Utilization (T2) > Tier 2 > Screenshot\nContent:\nUpdated Feb 25, 2023\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n*\n*"]},{"url":"https://www.youtube.com/watch?v=TZyt9bn42aw","title":"Snowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector - YouTube","publish_date":"2023-03-09","excerpts":["Back\nSkip navigation\nSearch\nSearch with your voice\nSnowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector\nTap to unmute\n2x\nSection Title: Snowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector\nContent:\nSnowflake Developers 1,529 views 2 years ago\nSearch Copy link Info Shopping\nIf playback doesn't begin shortly, try restarting your device.\n•\nShare\nInclude playlist\nAn error occurred while retrieving sharing information. Please try again later.\n0:00\n[](https://www.youtube.com/watch?v=dd2Xg7ECFlk \"Next (SHIFT+n)\")\n0:00 / 20:42\nLive\n• Watch full video\n•\nIntro\n•\nVideo playback paused with a verification dialog. To continue, complete the task below\nSnowflake Developers\nSnowflake Developers 34.6K subscribers\nSubscribe Subscribed\n25 Share\nSave Download Download\n1.5K views 2 years ago\nSection Title: Snowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector\nContent:\n1,529 views • Mar 9, 2023 \"Learn how Snowflake instrumented telemetry for Snowflake native connectors. Telemetry signals from the connector are ingested using Snowflake's Telemetry Pipeline into an EVENT TABLE. These signals are used for debugging, performance analysis, product usage, and configuring alerts. This video provides a deep dive into the process of generating data using the telemetry APIs and creating a Snowflake Alert. These concepts can also be applied to developing native applications, UDFs, UDTFs, and Stored Procedures. … ...more\n...more Show less\n1,529 views 1.5K views\nMar 9, 2023\n25 Share\nSave Download Download\nNaN / NaN\nSection Title: Snowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector > Comments\nContent:\nTop\nShow featured comments Newest\nShow recent comments, including potential spam ## In this video\nChapters\nTranscript ## Description\nSnowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector\nSection Title: Snowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector > Comments\nContent:\n25 Likes 1,529 Views 2023 Mar 9 \"Learn how Snowflake instrumented telemetry for Snowflake native connectors. Telemetry signals from the connector are ingested using Snowflake's Telemetry Pipeline into an EVENT TABLE. These signals are used for debugging, performance analysis, product usage, and configuring alerts. This video provides a deep dive into the process of generating data using the telemetry APIs and creating a Snowflake Alert. These concepts can also be applied to developing native applications, UDFs, UDTFs, and Stored Procedures.\nSection Title: Snowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector > Comments\nContent:\nConnect with Tyson Hamilton, Engineering Manager, Snowflake LinkedIn: /www.linkedin.com/in/tysonh Learn how to build your application on Snowflake: [https://developers.snowflake.com](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqazRQNGpsX2tTMXpVNmU0UlNCMDlHNmdVUGIzZ3xBQ3Jtc0trUVRyMFlReUhtOW9Ca1ZJdnhVSG1KQTBQWDBWUUJIUWwxcS03OXd2clgxcmJ0aEFTcVVUUXhLUHZoYW55TDdmMExKeUhSdG5YZ2lmSEg3dldpSGVaQzNFcmhSaEtQU19UclF0VlQ5SFh0eWpCWmRNUQ&q=https%3A%2F%2Fdevelopers.snowflake.com%2F&v=TZyt9bn42aw) Continue the conversation by joining the Snowflake Community:\nSection Title: Snowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector > Comments\nContent:\n[https://community.snowflake.com](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbXdrVE11MUhDcWpWQnJDX0dDUXgybG9IWVJwUXxBQ3Jtc0tuTXFlaHIyejFVNDM4T3BTS1l3cklkcWhWaXBhQm1fTEx6bXR0Vk5XNnFPYXNVX25YazZXUXJaUlpfZkg3bW5jN1NrZTY3dnFrOUIxaThKc0U1WVJmdUdybVR3TW84UjMtOFNqOEdaRy1jeHVHSzREVQ&q=https%3A%2F%2Fcommunity.snowflake.com%2F&v=TZyt9bn42aw) ❄Join our YouTube community❄ [https://bit.ly/3lzfeeB](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbmxCa3NDTGZUVjJRTHBPcGpmVjlYTks2X3NxUXxBQ3Jtc0tsNTc5MU9YLWNUa2dTUU1TRk9INTFBMGhnRVl3ZmdCakpqaUtJZzhHY2NWZ3ZiR0lFZ3YyTDFNY0tHVzVxNXVHNjNISGw0c29UUGt6TjlOUVhaMHVjbk9JUmE0WFp2M1Q4YXRXQXRwUlNwaXJuQ1lLNA&q=https%3A%2F%2Fbit.ly%2F3lzfeeB&v=TZyt9bn42aw) \" … ...more\nSection Title: Snowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector > Comments\nContent:\n...more Show less ## Chapters\nView all\nSection Title: Snowflake Shares How It Instrumented Telemetry For The Snowflake ServiceNow Connector > ... > Intro\nContent:\n0:00 #### Telemetry at Snowflake #### Telemetry at Snowflake 1:32\nSection Title: ... > Telemetry at Snowflake\nContent:\n1:32 #### Native Connectors #### Native Connectors 2:36\nSection Title: ... > Native Connectors\nContent:\n2:36 #### Native Application Framework #### Native Application Framework 3:15\n ... \nSection Title: ... > How to deploy Managed, Connected, or Snowflake Native Applications on Snowflake\nContent:\nSnowflake Developers\n1.9K views • 3 years ago 0:50\n ... \nSection Title: ... > Unpacking The App Building Benefits Of Unistore And Native Apps\nContent:\nSnowflake Developers\n295 views • 2 years ago 27:11\n ... \nSection Title: ... > [[LIVE] Vibe Code Data Apps with Replit + Snowflake](/watch?v=QrkbCM9bZ-I&pp=0gcJCZEKAYcqIYzv)\nContent:\nSnowflake Developers\n3.7K views • Streamed 8 days ago 12:11"]},{"url":"https://www.snowflake.com/en/product/features/snowflake-trail/","title":"Snowflake Trail for Observability","excerpts":["Data for Breakfast Around the World\nDrive impact across your organization with data and agentic intelligence.\nregister now\nSection Title: Snowflake Trail for Observability\nContent:\nIntroducing Snowflake Trail: a set of Snowflake capabilities for developers to better monitor, troubleshoot, debug and take actions on pipelines, apps, user code and compute utilizations.\nstart for free\nSection Title: Snowflake Trail for Observability > NEW VIDEOS > Data Engineering School\nContent:\nLearn about the latest trends and skill sets in data engineering by completing 3 free instructional tracks presented by industry experts.\nget started\nBlog ##### Observability in Snowflake Read now\n[Blog ##### Enhanced Tracing, Log, and Metrics Read now](https://medium.com/snowflake/new-in-snowflake-trail-enhanced-logs-tracing-and-metrics-for-snowpark-a2476198e14e)\nQuickstart ##### Snowpark Observability (Public Preview) Get started\nBlog ##### Observability in Snowflake Read now\n[Blog ##### Enhanced Tracing, Log, and Metrics Read now](https://medium.com/snowflake/new-in-snowflake-trail-enhanced-logs-tracing-and-metrics-for-snowpark-a2476198e14e)\nQuickstart ##### Snowpark Observability (Public Preview) Get started\nBlog ##### Observability in Snowflake Read now\nWhat's new\nSection Title: Snowflake Trail for Observability > Observability in Snowflake\nContent:\nObservability shouldn’t be an afterthought. Snowflake Trail allows you to monitor, diagnose and troubleshoot, and gain insight into your apps, pipelines and compute.\ntry snowpark observability\nFast Insights\nSection Title: Snowflake Trail for Observability > Effortless telemetry with one simple setting\nContent:\nGetting started with telemetry traditionally is a tedious process. Snowflake Trail eliminates the need for any agent installation, time-intensive setup or data export tasks, providing fast insights into application and pipeline performance. With just one simple setting, you can gain visibility into the performance of your Snowpark code and its resource usage, so you can quickly diagnose and debug your apps and pipeline development. Events are all within Snowflake with no need for additional data transfer.\nSection Title: Snowflake Trail for Observability > Reduce time to detect (TTD) and time to resolution (TTR)\nContent:\nSnowflake Trail provides a comprehensive set of telemetry signals, including metrics, logs and span events, to help developers better understand their applications and pipelines. These signals unite in Snowsight, Snowflake’s user interface, to help developers debug and detect issues quickly.\nstart for free\nSection Title: Snowflake Trail for Observability > Bring Your Own Tools (BYOT) or use Snowsight\nContent:\nBuilt with OpenTelemetry standards, schema and open ecosystem integrations in mind, Snowflake telemetry and notification capabilities integrate with some of the most favored developer tools, including Datadog, Grafana, Metaplane, Monte Carlo, PagerDuty and Slack. Or simply use Snowsight, where developers can monitor and trace their pipelines, apps and runtime usage directly within Snowflake.\nSection Title: Snowflake Trail for Observability > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\nstart for free\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n[Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n[Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n[Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n[Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n[Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n[Technology](https://www.snowflake.com/en/solutions/industries/technology/)\nSection Title: Snowflake Trail for Observability > Where Data Does More\nContent:\nLearn * [Resource Library](https://snowflake.com/en/resources/)\nLive Demos\n[Fundamentals](https://www.snowflake.com/en/fundamentals/)\n[Training](https://www.snowflake.com/en/resources/learn/training/)\n[Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n[Snowflake University](https://learn.snowflake.com/en/)\n[Developer Guides](https://www.snowflake.com/en/developers/guides)\n[Documentation](https://docs.snowflake.com/)\n[Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n[Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\nCookie Settings\n[Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Legal](https://www.snowflake.com/en/legal/)\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\nSection Title: Snowflake Trail for Observability > Where Data Does More\nContent:\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"]},{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/native-apps-about","title":"About the Snowflake Native App Framework | Snowflake Documentation","excerpts":["Developer Snowflake Native App Framework\nSection Title: About the Snowflake Native App Framework ¶\nContent:\nFeature — Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nThis topic provides general information about the Snowflake Native App Framework.\nSection Title: About the Snowflake Native App Framework ¶ > Introduction to the Snowflake Native App Framework ¶\nContent:\nThe Snowflake Native App Framework allows you to create data applications that leverage core Snowflake functionality.\nThe Snowflake Native App Framework allows you to:\nExpand the capabilities of other Snowflake features by sharing data and related\nbusiness logic with other Snowflake accounts. The business logic of an application can include a Streamlit app,\nstored procedures, and functions written using Snowpark API ,\nJavaScript, and SQL.\nShare an application with consumers through listings. A listing can be either free or paid.\nYou can distribute and monetize your apps in the Snowflake Marketplace or distribute them to\nspecific consumers using private listings.\nInclude rich visualizations in your application using Streamlit.\nThe Snowflake Native App Framework also supports an enhanced development experience that provides:\nSection Title: About the Snowflake Native App Framework ¶ > Introduction to the Snowflake Native App Framework ¶\nContent:\nA streamlined testing environment where you can test your applications from a single account.\nA robust developer workflow. While your data and related database objects remain within Snowflake,\nyou can manage supporting code files and resources within source control using your preferred\ndeveloper tools.\nThe ability to release versions and patches for your application that allows you, as a provider,\nto change and evolve the logic of your applications and release them incrementally to consumers.\nSupport for logging of structured and unstructured events so that you can troubleshoot and monitor\nyour applications.\nSection Title: About the Snowflake Native App Framework ¶ > Components of the Snowflake Native App Framework ¶\nContent:\nThe following diagram shows a high-level view of the Snowflake Native App Framework.\nThe Snowflake Native App Framework is built around the concept of provider and consumer used by other\nSnowflake features, including Snowflake Collaboration and Secure Data Sharing\nProvider\nA Snowflake user who wants to share data content and application logic with other Snowflake users.\nConsumer\nA Snowflake user who wants to access the data content and application logic shared by providers.\nSection Title: About the Snowflake Native App Framework ¶ > ... > Develop and Test an Application Package ¶\nContent:\nTo share data content and application logic with a consumer, providers create an application package.\nApplication package\nAn application package encapsulates the data content, application logic,\nmetadata, and setup script required by an application. An application package also contains\ninformation about versions and patch levels defined for the application. See Create and manage an application package for details.\nAn application package can include references to data content and external code files that a provider\nwants to include in the application. An application package requires a manifest file and a setup script.\nSection Title: About the Snowflake Native App Framework ¶ > ... > Develop and Test an Application Package ¶\nContent:\nManifest file\nDefines the configuration and setup properties required by the application, including the location of\nthe setup script, versions, etc. See Create the manifest file for an app for details.\nSetup script\nContains SQL statements that are run when the consumer installs or upgrades an application or when\na provider installs or upgrades an application for testing. The location of the setup script is\nspecified in the manifest file. See Create the setup script for details.\nSection Title: About the Snowflake Native App Framework ¶ > ... > Publish an Application Package ¶\nContent:\nAfter developing and testing an application package, a provider can share an application with consumers by\npublishing a listing containing the application package as the data product of a listing. The listing can be a Snowflake Marketplace\nlisting or a private listing.\nSnowflake Marketplace listing\nAllows providers to market applications across the Snowflake Data Cloud. Offering a listing on the Snowflake Marketplace\nlets providers share applications with many consumers simultaneously, rather than maintain\nsharing relationships with each individual consumer.\nPrivate listing\nAllows providers to take advantage of the capabilities of listings to share applications directly with another\nSnowflake account in any Snowflake region supported by the Snowflake Native App Framework.\nSee About listings for details.\nSection Title: About the Snowflake Native App Framework ¶ > ... > Install and Manage an Application ¶\nContent:\nAfter a provider publishes a listing containing an application package, consumers can discover the listing and\ninstall the application.\nSnowflake Native App\nA Snowflake Native App is the database object installed in the consumer account. When a consumer installs the Snowflake Native App,\nSnowflake creates the application and runs the setup script to create the required objects within the application.\nSee Install and test an app locally for details.\nAfter installing the application, consumers can perform additional tasks, including:\n[Enable logging and event sharing](https://other-docs.snowflake.com/en/native-apps/consumer-enable-logging) to help providers troubleshoot the application.\n[Grant privileges required by the application](https://other-docs.snowflake.com/en/native-apps/consumer-granting-privs) .\nSection Title: About the Snowflake Native App Framework ¶ > ... > Install and Manage an Application ¶\nContent:\nSee [Working with Applications as a Consumer](https://other-docs.snowflake.com/en/native-apps/consumer-about) for details on how consumers install and manage an application.\n ... \nSection Title: ... > About Snowflake Native Apps with Snowpark Container Services ¶\nContent:\nApplication package:\nTo manage containers, the application package must have access to a services specification file on a\nstage. Within this file, there are references to the container images required by the app. These images\nmust be stored in an image repository in the provider account.\nApplication object:\nWhen a consumer installs an app with containers, the application object that is created contains a\ncompute pool that stores the containers required by the app.\nCompute pool:\nA compute pool is a collection of one or more virtual machine (VM) nodes on which Snowflake runs your\nSnowpark Container Services jobs and services. When a consumer installs an app with containers, they can\ngrant the CREATE COMPUTE POOL privilege to the app or they can create the compute pools manually.\nSection Title: ... > Protect provider intellectual property in an app with containers ¶\nContent:\nWhen an app with containers is installed in the consumer account, the query history of the services\nis available in the consumer account. To protect a provider’s confidential information, the Snowflake Native App Framework redacts\nthe following information:\nThe query text is hidden from the QUERY_HISTORY view .\nAll information in the ACCESS_HISTORY view is hidden.\nThe Query Profile graph for the service’s query is collapsed\ninto a single empty node instead of displaying the full query profile tree.\nSection Title: ... > Multi-factor requirements for users in a provider account ¶\nContent:\nDepending on the type of user, Snowflake requires different types of authentication for\nusers in the provider account.\n ... \nSection Title: About the Snowflake Native App Framework ¶ > ... > Service users ¶\nContent:\nUsers who have the TYPE parameter set to SERVICE must use key-pair authentication or OAuth .\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nIntroduction to the Snowflake Native App Framework\nComponents of the Snowflake Native App Framework\nAbout Snowflake Native Apps with Snowpark Container Services\nProtect provider intellectual property in an app with containers\nMulti-factor requirements for users in a provider account\nRelated content\nSnowflake Native App Framework workflow\nTutorial 1: Create a basic Snowflake Native App\nSnowflake Native Apps commands\n ... \nSection Title: About the Snowflake Native App Framework ¶ > ... > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details‎"]}],"usage":[{"name":"sku_search","count":1}]}
{
  "search_id": "search_5f81521b8c1747fdb4399c72155a3e5e",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/user-guide/warehouses-multicluster",
      "title": "Multi-cluster warehouses | Snowflake Documentation",
      "excerpts": [
        "Section Title: Multi-cluster warehouses \u00b6\nContent:\nEnterprise Edition Feature\nTo inquire about upgrading to Enterprise Edition (or higher), please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\nMulti-cluster warehouses enable you to scale compute resources to manage your user and query concurrency needs as they change, such as during\npeak and off hours.\n ... \nSection Title: Multi-cluster warehouses \u00b6 > What is a multi-cluster warehouse? \u00b6\nContent:\nMaximum number of clusters, greater than 1. The highest value you can specify depends on the warehouse size.\nFor the upper limit on the number of clusters for each warehouse size,\nsee Upper limit on number of clusters for a multi-cluster warehouse (in this topic).\nMinimum number of clusters, equal to or less than the maximum.\nAdditionally, multi-cluster warehouses support all the same properties and actions as single-cluster warehouses, including:\nSpecifying a warehouse size.\nResizing a warehouse at any time.\nAuto-suspending a running warehouse due to inactivity; note that this does not apply to individual clusters, but rather the entire\nmulti-cluster warehouse.\nAuto-resuming a suspended warehouse when new queries are submitted.\n ... \nSection Title: Multi-cluster warehouses \u00b6 > What is a multi-cluster warehouse? \u00b6 > Maximized vs. auto-scale \u00b6\nContent:\nYou can choose to run a multi-cluster warehouse in either of the following modes:\nMaximized :\nThis mode is enabled by specifying the same value for both maximum and minimum number of clusters (note that the\nspecified value must be larger than 1). In this mode, when the warehouse is started, Snowflake starts all the clusters so\nthat maximum resources are available while the warehouse is running.\nThis mode is effective for statically controlling the available compute resources, particularly if you have large numbers of concurrent\nuser sessions and/or queries and the numbers do not fluctuate significantly.\nAuto-scale :\nThis mode is enabled by specifying different values for maximum and minimum number of clusters. In this mode,\nSnowflake starts and stops clusters as needed to dynamically manage the load on the warehouse:\nSection Title: Multi-cluster warehouses \u00b6 > What is a multi-cluster warehouse? \u00b6 > Maximized vs. auto-scale \u00b6\nContent:\nAs the number of concurrent user sessions and/or queries for the warehouse increases, and queries start to queue due to\ninsufficient resources, Snowflake automatically starts additional clusters, up to the maximum number defined for the warehouse.\nSimilarly, as the load on the warehouse decreases, Snowflake automatically shuts down clusters to reduce the number of\nrunning clusters and, correspondingly, the number of credits used by the warehouse.\nTo help control the usage of credits in Auto-scale mode, Snowflake provides a property, SCALING_POLICY, that determines the scaling policy\nto use when automatically starting or shutting down additional clusters. For more information, see Setting the scaling policy for a multi-cluster warehouse (in\nthis topic).\nTo create a multi-cluster warehouse, see Creating a multi-cluster warehouse (in this topic).\nSection Title: Multi-cluster warehouses \u00b6 > What is a multi-cluster warehouse? \u00b6 > Maximized vs. auto-scale \u00b6\nContent:\nFor auto-scale mode, the maximum number of clusters must be *greater* than the minimum number of clusters.\nFor maximized mode, the maximum number of clusters must be *equal* to the minimum number of clusters.\nTip\nWhen determining the maximum and minimum number of clusters to use for a multi-cluster warehouse, start with Auto-scale mode and start\nsmall (for example, maximum = 2 or 3, minimum = 1). As you track how your warehouse load fluctuates over time, you can increase the maximum and\nminimum number of clusters until you determine the numbers that best support the upper and lower boundaries of your user/query concurrency.\n ... \nSection Title: Multi-cluster warehouses \u00b6 > ... > Multi-cluster size and credit usage \u00b6\nContent:\nThe actual number of credits consumed per hour depends on the number of clusters running during each hour that the warehouse\nis running. For more details, see Examples of multi-cluster credit usage (in this topic).\nTip\nIf you use Query Acceleration Service (QAS) for a multi-cluster warehouse, consider adjusting the QAS scale\nfactor higher than for a single-cluster warehouse. That helps to apply the QAS optimizations across all the\nclusters of the warehouse.\nFor more information, see Adjusting the scale factor .\n ... \nSection Title: Multi-cluster warehouses \u00b6 > ... > Benefits of multi-cluster warehouses \u00b6\nContent:\nIn Auto-scale mode, a multi-cluster warehouse eliminates the need for resizing the warehouse or starting and stopping additional\nwarehouses to handle fluctuating workloads. Snowflake automatically starts and stops additional clusters as needed.\nIn Maximized mode, you can control the capacity of the multi-cluster warehouse by increasing or decreasing the number of clusters as\nneeded.\nTip\nMulti-cluster warehouses are best utilized for scaling resources to improve concurrency for users/queries. They are not as beneficial for\nimproving the performance of slow-running queries or data loading. For these types of operations, resizing the warehouse provides\nmore benefits.\n ... \nSection Title: Multi-cluster warehouses \u00b6 > ... > Example 2: Auto-scale (2 Hours) \u00b6\nContent:\nIn this example, a Medium-size Standard warehouse with 3 clusters runs in Auto-scale mode for 2 hours:\nCluster 1 runs continuously.\nCluster 2 runs continuously for the 2nd hour only.\nCluster 3 runs for 30 minutes during the 2nd hour.\nSection Title: Multi-cluster warehouses \u00b6 > ... > Example 2: Auto-scale (2 Hours) \u00b6\nContent:\n|  | Cluster 1 | Cluster 2 | Cluster 3 | **Total Credits** |\n| 1st Hour | 4 | 0 | 0 | **4** |\n| 2nd Hour | 4 | 4 | 2 | **10** |\n| **Total Credits** | **8** | **4** | **2** | **14** |\nSection Title: Multi-cluster warehouses \u00b6 > ... > Example 3: Auto-scale (3 Hours) \u00b6\nContent:\nIn this example, a Medium-size Standard warehouse with 3 clusters runs in Auto-scale mode for 3 hours:\nCluster 1 runs continuously.\nCluster 2 runs continuously for the entire 2nd hour and 30 minutes in the 3rd hour.\nCluster 3 runs for 30 minutes in the 3rd hour.\nSection Title: Multi-cluster warehouses \u00b6 > ... > Example 3: Auto-scale (3 Hours) \u00b6\nContent:\n|  | Cluster 1 | Cluster 2 | Cluster 3 | **Total Credits** |\n| 1st Hour | 4 | 0 | 0 | **4** |\n| 2nd Hour | 4 | 4 | 0 | **8** |\n| 3rd Hour | 4 | 2 | 2 | **8** |\n| **Total Credits** | **12** | **6** | **2** | **20** |\nSection Title: Multi-cluster warehouses \u00b6 > ... > Example 4: Auto-scale (3 Hours) with resize \u00b6\nContent:\nIn this example, the same warehouse from example 3 runs in Auto-scale mode for 3 hours with a resize from Medium to Large:\nCluster 1 runs continuously.\nCluster 2 runs continuously for the 2nd and 3rd hours.\nWarehouse is resized from Medium to Large at 1:30 hours.\nCluster 3 runs for 15 minutes in the 3rd hour.\nSection Title: Multi-cluster warehouses \u00b6 > ... > Example 4: Auto-scale (3 Hours) with resize \u00b6\nContent:\n|  | Cluster 1 | Cluster 2 | Cluster 3 | **Total Credits** |\n| 1st Hour | 4 | 0 | 0 | **4** |\n| 2nd Hour | 4+2 | 4+2 | 0 | **12** |\n| 3rd Hour | 8 | 8 | 2 | **18** |\n| **Total Credits** | **18** | **14** | **2** | **34** |\n ... \nSection Title: Multi-cluster warehouses \u00b6 > Creating a multi-cluster warehouse \u00b6\nContent:\nAll other tasks for multi-cluster warehouses (except for the remaining tasks described in this topic) are identical to single-cluster warehouse tasks .\nSection Title: Multi-cluster warehouses \u00b6 > Setting the scaling policy for a multi-cluster warehouse \u00b6\nContent:\nTo help control the credits consumed by a multi-cluster warehouse running in Auto-scale mode, Snowflake provides scaling policies.\nSnowflake uses the scaling policies to determine how to adjust the capacity of your multi-cluster warehouse\nby starting or shutting down individual clusters while the warehouse is running. You can specify a scaling policy\nto make Snowflake prioritize responsiveness and throughput for the queries in that warehouse, or to minimize costs\nfor that warehouse.\nThe scaling policy for a multi-cluster warehouse only applies if it is running in Auto-scale mode.\nIn Maximized mode, all clusters run concurrently, so there is no need to start or shut down individual clusters.\nSnowflake supports the following scaling policies:\nSection Title: Multi-cluster warehouses \u00b6 > Setting the scaling policy for a multi-cluster warehouse \u00b6\nContent:\n| Policy | Description | A new cluster starts\u2026 | An idle or lightly loaded cluster shuts down\u2026 |\n| Standard (default) | Prevents/minimizes queuing by favoring starting additional clusters over conserving credits. | When a query is queued, or if Snowflake estimates the currently running clusters don\u2019t have |  |\n| enough resources to handle any additional queries, Snowflake increases the number of clusters |  |  |  |\n| in the warehouse. |  |  |  |\n ... \nSection Title: Multi-cluster warehouses \u00b6 > Increasing or decreasing clusters for a multi-cluster warehouse \u00b6\nContent:\nThe effect of changing the maximum and minimum clusters for a running warehouse depends on whether it is running in\nMaximized or Auto-scale mode:\nMaximized:\u2191 max & min :\nSpecified number of clusters start immediately.\n\u2193 max & min :\nSpecified number of clusters shut down when they finish executing statements and the auto-suspend period elapses.\nAuto-scale:\u2191 max :\nIf `new_max_clusters > running_clusters` , no changes until additional clusters are needed.\n\u2193 max :\nIf `new_max_clusters < running_clusters` , excess clusters shut down when they finish executing statements and the scaling policy conditions are met.\n\u2191 min :\nIf `new_min_clusters > running_clusters` , additional clusters immediately started to meet the minimum.\n\u2193 min :\nIf `new_min_clusters < running_clusters` , excess clusters shut down when they finish executing statements and the scaling policy conditions are met."
      ]
    },
    {
      "url": "https://www.snowflake.com/en/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/",
      "title": "Automatic Concurrency Scaling in Snowflake | Snowflake Auto Scaling",
      "publish_date": "2024-08-05",
      "excerpts": [
        "Product\nSolutions\nWhy Snowflake\nResources\nDevelopers\nPricing\nFeatured Capabilities\nFeatured Open Source Technologies\nINDUSTRIES\nDEPARTMENTS\nEnablement Solutions\nPARTNER SOLUTIONS\nConnect\nLearn\nBuild\nLearn\nConnect\nblog\nSection Title: Category\nContent:\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\nApplications\nApr 28, 2016 | 5 min read\nSection Title: ... > The challenge with concurrency\nContent:\nToday we take a major step forward by extending our [elastic architecture](https://www.snowflake.com/cloud-data-platform/) to solve another major pain point in existing on-premises and cloud data warehousing solutions: how to run massively concurrent workloads at scale in a single system. Have you had the following experiences when building mission-critical applications that incorporate data analytics: * My application can only support a certain level of user concurrency due to the underlying data warehouse, which only allows 32-50 concurrent user queries.\nSection Title: ... > The challenge with concurrency\nContent:\nTo build my application, I need to acquire multiple data warehouse instances in order to isolate numerous workloads and users from each other. This adds to costs and complexity.\nWe have built our own scheduling policies around the data warehouse. We use query queues to control and prioritize incoming queries issued by our numerous users.\nDuring peak times, users are getting frustrated because their requests are getting queued or fail entirely.\nAt Snowflake, we separate compute from storage by introducing the unique concept of virtual data warehouses. That concept makes it possible to instantly resize virtual warehouses or pause them entirely. In addition, because of that concept Snowflake is the only cloud data warehousing solution that allows concurrent workloads to run without impacting each other.\n ... \nSection Title: ... > The challenge with concurrency\nContent:\nImagine you didn\u2019t need users to adjust their workloads to accommodate data warehouse bottlenecks.\nImagine the data warehouse itself could detect increasing workloads and add additional compute resources as needed or shut-down/pause compute resources when workload activities subside again.\nImagine your application could scale out-of-the-box with one single (virtual) data warehouse without the need to provision additional data warehouses.\nImagine a world without any scheduling scripts and queued queries - a world in which you can leverage a smart data warehousing service that ensures all your users get their questions answered within the application\u2019s SLA.\nWith Snowflake, we allow you to do that all of this for real, not just in your imagination, with our new multi-cluster data warehouse feature.\nSection Title: ... > Multi-cluster data warehouses\nContent:\nA virtual warehouse represents a number of physical nodes a user can provision to perform data warehousing tasks, e.g. running analytical queries. While a user can instantly resize a warehouse by choosing a different size (e.g. from small to 3X large), until now a virtual data warehouse in Snowflake always consisted of one physical cluster. With the recent introduction of multi-cluster warehouses, Snowflake supports allocating, either statically or dynamically, more resources for a warehouse by specifying additional clusters for the warehouse.\nThe figure above shows a multi-cluster DW that consists of three compute clusters. All compute clusters in the warehouse are of the same size. The user can choose from two different modes for the warehouse: * **Maximized:** When the warehouse is started, Snowflake always starts all the clusters to ensure maximum resources are available while the warehouse is running.\nSection Title: ... > Multi-cluster data warehouses\nContent:\n**Auto Scaling:** Snowflake starts and stops clusters as needed to dynamically manage the workload on the warehouse.\nAs always, in Snowflake a user can either leverage the user interface or use SQL to specify the minimum/maximum number of clusters per multi-cluster DW:\nSection Title: ... > Create Warehouse UI wizard\nContent:\nCreate Warehouse SQL script\nSection Title: ... > Create Warehouse UI wizard\nContent:\nSimilar to regular virtual warehouses, a user can resize all additional clusters of a multi-cluster warehouse instantly by choosing a different size (e.g. XS, S, M, L, \u2026) either through the UI or programmatically via corresponding SQL DDL statements. In auto-scale mode, Snowflake automatically adds or resumes additional clusters (up to the maximum number defined by user) as soon as the workload increases. If the load subsides again, Snowflake shuts down or pauses the additional clusters. No user interaction is required - this all takes place transparently to the end user. For these decisions, internally, the query scheduler takes into account multiple factors. There are two main factors considered in this context: 1. The memory capacity of the cluster, i.e. whether clusters have reached their maximum memory capacity\n2. The degree of concurrency in a particular cluster, i.e.\nSection Title: ... > Create Warehouse UI wizard\nContent:\nwhether there are many queries executing concurrently on the cluster\nAs we learn more from our customers\u2019 use cases, we will extend this feature further and share interesting use cases where multi-cluster data warehouses make a difference. Please stay tuned as we continue reinventing modern data warehousing and analytics by leveraging the core principles of cloud computing. *We would like to thank our co-authors, Florian Funke and Benoit Dageville, for their significant contributions as main engineer and architect, respectively, to making multi-cluster warehouses a reality. As always, keep an eye on the [blog](https://www.snowflake.com/blog) and our Snowflake Twitter feed [(@SnowflakeDB](https://twitter.com/SnowflakeDB) ) f or updates on Snowflake Computing. *\nSection Title: ... > Additional Links\nContent:\n**Concurrent Load and Query**\n[**Snowflake Architecture**](https://www.snowflake.com/product/architecture/)\nSection Title: Automatic Concurrency Scaling in Snowflake - Another Way the Cloud Changes the Game > ... > Author\nContent:\nArtin Avanes\nSection Title: ... > Share Article\nContent:\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fauto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications&title=Automatic+Concurrency+Scaling+in+Snowflake+-+Another+Way+the+Cloud+Changes+the+Game)\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fauto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications&text=Automatic+Concurrency+Scaling+in+Snowflake+-+Another+Way+the+Cloud+Changes+the+Game)\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fauto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications)\nSubscribe to our blog newsletter\nGet the best, coolest and latest delivered to your inbox each week\nSection Title: ... > Share Article\nContent:\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\nSection Title: ... > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\n[start for free](https://signup.snowflake.com/)\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n[Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n[Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n[Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n[Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n[Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n[Technology](https://www.snowflake.com/en/solutions/industries/technology/)\n ... \nSection Title: ... > Where Data Does More\nContent:\n[](https://www.facebook.com/Snowflake-Computing-709171695819345/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/metering_history",
      "title": "METERING_HISTORY view - Snowflake Documentation",
      "excerpts": [
        "Reference General reference SNOWFLAKE database Account Usage METERING_HISTORY\nSchemas:\nACCOUNT_USAGE , READER_ACCOUNT_USAGE\nSection Title: METERING_HISTORY view \u00b6\nContent:\nThe METERING_HISTORY view in the ACCOUNT_USAGE schema can be used to return the hourly credit usage for an account within the last 365 days (1 year).\nSection Title: METERING_HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description |\n| SERVICE_TYPE | VARCHAR | Type of service that is consuming credits. The following list includes many, **but not all** , of the possible service types: |\nSection Title: METERING_HISTORY view \u00b6 > Columns \u00b6\nContent:\n`AI_SERVICES` : See Snowflake Cortex AI Functions (including LLM functions) , Cortex Analyst , and Document AI . `ARCHIVE_STORAGE_RETRIEVAL_FILE_PROCESSING` : See Billing for storage lifecycle policies . `ARCHIVE_STORAGE_WRITE` : See Billing for storage lifecycle policies . `AUTO_CLUSTERING` : See Automatic Clustering . `BACKUP` : See Backups for disaster recovery and immutable storage . `COPY_FILES` : See COPY FILES . `DATA_QUALITY_MONITORING` : See Introduction to data quality checks . `FAILSAFE_RECOVERY` : See Understanding and viewing Fail-safe . `HYBRID_TABLE_REQUESTS` : See Hybrid tables . `MATERIALIZED_VIEW` : See Working with Materialized Views . `OPENFLOW_COMPUTE_BYOC` : See Openflow BYOC cost and scaling considerations . `OPENFLOW_COMPUTE_SNOWFLAKE` : See Openflow Snowflake Deployment cost and scaling considerations . `PIPE` : See Snowpipe . `POSTGRES_COMPUTE` : See Snowflake Postgres . `POSTGRES_COMPUTE_HA` : See Snowflake Postgres .\nSection Title: METERING_HISTORY view \u00b6 > Columns \u00b6\nContent:\n`QUERY_ACCELERATION` : See Using the Query Acceleration Service (QAS) . `REPLICATION` : See Introduction to replication and failover across multiple accounts . `SEARCH_OPTIMIZATION` : See Search optimization service . `SENSITIVE_DATA_CLASSIFICATION` : See Introduction to sensitive data classification . `SERVERLESS_ALERTS` : See Setting up alerts based on data in Snowflake . `SERVERLESS_TASK` : See Introduction to tasks . `SNOWPARK_CONTAINER_SERVICES` : See Snowpark Container Services . `SNOWPIPE_STREAMING` : See Snowpipe Streaming . `STORAGE_LIFECYCLE_POLICY_EXECUTION` : Compute cost to apply a policy on a target table and expire or archive rows (policy execution). See Storage lifecycle policies . `TELEMETRY_DATA_INGEST` : See Event table overview . `TRUST_CENTER` : See Trust Center . `WAREHOUSE_METERING` : See Overview of warehouses . `WAREHOUSE_METERING_READER` : See Manage reader accounts .\nSection Title: METERING_HISTORY view \u00b6 > Columns \u00b6\nContent:\n|\n|START_TIME |TIMESTAMP_LTZ |The date and beginning of the hour (in the local time zone) in which the usage took place. |\n|END_TIME |TIMESTAMP_LTZ |The date and end of the hour (in the local time zone) in which the usage took place. |\n|ENTITY_ID |NUMBER |A system-generated identifier for the entity associated with the service.\nSection Title: METERING_HISTORY view \u00b6 > Columns \u00b6\nContent:\nIn most cases, this is the internal ID of the monitored entity; for example, a pipe, task, or replication group.\nWhen the SERVICE_TYPE is COPY_FILES, this column shows the ID of the database, schema, or stage from which files are copied.\nIf the SERVICE_TYPE is an Openflow type, the value is NULL.\nIf the SERVICE_TYPE is Snowpipe Streaming, this shows the ID of the relevant pipe; which is the default pipe ID for the default pipe. |\n|ENTITY_TYPE |VARCHAR |Type of Snowflake resource that consumed credits, such as WAREHOUSE, TASK, or TABLE. Note that TABLE is used for all table-like objects. |\n|NAME |VARCHAR |The name of the service or object associated with the cost entry, which varies significantly based on the SERVICE_TYPE.\nStandard (General): This column shows the name of the service type itself; for example, REPLICATION, TASK.\nSNOWPIPE_STREAMING: This service type generates two distinct cost entries, and the NAME column varies for each:\nSection Title: METERING_HISTORY view \u00b6 > Columns \u00b6\nContent:\nCost entry 1 (table name): The value is the name of the Snowflake target table. For the high-performance default pipe, the name is derived from the target table name and appended with -STREAMING; for example, MY_TABLE-STREAMING.\nCost entry 2 (client string): The value is a colon-separated string in the format: SNOWPIPE_STREAMING:CLIENT_NAME:SNOWFLAKE_PROVIDED_ID. This is used for tracking client-side costs.\nCOPY_FILES: The value is the name of the database from which the files are copied.\nSection Title: METERING_HISTORY view \u00b6 > Columns \u00b6\nContent:\nOpenflow Types: The value is NULL. |\n|DATABASE_ID |NUMBER |Internal/system-generated identifier of the database associated with the resource of type `ENTITY_TYPE` . Contains a NULL value when the resource isn\u2019t associated with a specific database; for example, a warehouse or compute pool. |\n|DATABASE_NAME |VARCHAR |Name of the database associated with the resource of type `ENTITY_TYPE` . Contains a NULL value when the resource isn\u2019t associated with a specific database. |\n|SCHEMA_ID |NUMBER |Internal or system-generated identifier of the schema associated with the resource of type `ENTITY_TYPE` . Contains a NULL value when the resource isn\u2019t associated with a specific schema. |\n|SCHEMA_NAME |VARCHAR |Name of the schema associated with the resource of type `ENTITY_TYPE` . Contains a NULL value when the resource isn\u2019t associated with a specific schema.\nSection Title: METERING_HISTORY view \u00b6 > Columns \u00b6\nContent:\n|\n|CREDITS_USED_COMPUTE |NUMBER |Number of credits used by warehouses, serverless compute, and Openflow resources in the hour. |\n|CREDITS_USED_CLOUD_\nSERVICES |NUMBER |Number of credits used for cloud services in the hour. Always `0` when the SERVICE_TYPE is one of the Openflow types. |\n|CREDITS_USED |NUMBER |Total number of credits used for the account in the hour. This is a sum of CREDITS_USED_COMPUTE and CREDITS_USED_CLOUD_SERVICES. This value does not take into account the adjustment for cloud services, and may therefore be greater than your actual credit consumption. |\n|BYTES |NUMBER |When the service type is `auto_clustering` , indicates the number of bytes reclustered during the START_TIME and END_TIME window. When the service type is `pipe` , indicates the number of bytes inserted during the START_TIME and END_TIME window.\n ... \nSection Title: METERING_HISTORY view \u00b6 > Usage notes \u00b6\nContent:\nLatency for the view may be up to 180 minutes (3 hours), except for the CREDITS_USED_CLOUD_SERVICES column. Latency for\nCREDITS_USED_CLOUD_SERVICES may be up to 6 hours.\nLatency for showing the credit consumption of `SNOWPIPE_STREAMING` may be up to 12 hours.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nColumns\nUsage notes\nSection Title: METERING_HISTORY view \u00b6 > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\n ... \nSection Title: METERING_HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Details\u200e\n ... \nSection Title: METERING_HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details\u200e\nSection Title: METERING_HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Targeting Cookies\nContent:\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nCookies Details\u200e\nSection Title: METERING_HISTORY view \u00b6 > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://www.dataexpert.io/blog/optimize-query-concurrency-snowflake",
      "title": "How to Optimize Query Concurrency in Snowflake - DataExpert.io",
      "excerpts": [
        "Section Title: How to Optimize Query Concurrency in Snowflake\nContent:\n**Adjust concurrency settings** ( `MAX_CONCURRENCY_LEVEL` ) to control how many queries run simultaneously.\nUse **warehouse auto-scaling** to handle spikes in query load without wasting credits.\nMonitor for issues like **memory spillage** or **query queuing** using Snowflake tools like [Snowsight](https://docs.snowflake.com/en/user-guide/ui-snowsight) and [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) .\nApply **clustering keys** and **materialized views** to speed up query execution.\nSnowflake Query Concurrency Optimization Decision Framework\nSection Title: How to Optimize Query Concurrency in Snowflake > ... > Query Concurrency Defined\nContent:\nQuery concurrency in Snowflake refers to **multiple queries running simultaneously on a single virtual warehouse** , all sharing the same compute resources. Think of it like a highway with a fixed number of lanes: as more cars (queries) enter, traffic slows down.\nEach virtual warehouse in Snowflake is equipped with a specific allocation of **CPU and memory** . For instance, if two queries are running, each gets about half of the compute resources; with ten queries, each query receives roughly one-tenth. While this setup works well under light workloads, performance can drop significantly as the number of concurrent queries increases.\nSection Title: How to Optimize Query Concurrency in Snowflake > ... > Query Concurrency Defined\nContent:\nBy default, Snowflake limits **concurrent queries to 8 per warehouse** , controlled by the `MAX_CONCURRENCY_LEVEL` parameter. The workload on a warehouse is measured by dividing the total execution time of all queries by the monitoring interval. This helps assess how heavily the warehouse is being utilized. These dynamics often lead to specific challenges when concurrency levels rise.\n ... \nSection Title: How to Optimize Query Concurrency in Snowflake > How to Use Warehouse Auto-Scaling and Sizing\nContent:\nAuto-scaling and proper sizing are essential for managing fluctuating query loads in Snowflake. These features ensure your warehouses adapt to demand automatically, balancing performance and cost without manual adjustments.\nSection Title: How to Optimize Query Concurrency in Snowflake > ... > How Auto-Scaling Works\nContent:\nSnowflake's auto-scaling operates through multi-cluster warehouses, which dynamically add or remove clusters based on query load. This flexibility is controlled by setting `MIN_CLUSTER_COUNT` and `MAX_CLUSTER_COUNT` . When queries start queuing or existing clusters hit capacity, additional clusters are spun up. As the load decreases, the extra clusters are shut down.\n\"In auto-scale mode, Snowflake automatically adds or resumes additional clusters... as soon as the workload increases. If the load subsides again, Snowflake shuts down or pauses the additional clusters.\" \u2013 Artin Avanes, Snowflake\nTwo scaling policies determine how quickly clusters are added:\nSection Title: How to Optimize Query Concurrency in Snowflake > ... > How Auto-Scaling Works\nContent:\n**Standard Policy** : Starts new clusters immediately when queries queue or resources are insufficient. This is ideal for performance-critical workloads.\n**Economy Policy** : Adds clusters only if the workload can sustain them for at least six minutes. This option is better for reducing credit usage.\nAs a starting point, set `MIN_CLUSTER_COUNT` to 1 and `MAX_CLUSTER_COUNT` to 2 or 3, and monitor your workload. If queuing frequently occurs during peak times, consider increasing the maximum cluster count. Additionally, align your warehouse size with the complexity of your queries and the volume of data being processed for optimal results.\nSection Title: How to Optimize Query Concurrency in Snowflake > ... > How to Choose the Right Warehouse Size\nContent:\nWhile auto-scaling adjusts the number of clusters, the warehouse size dictates the compute power of each cluster. Compute power is measured in credits per hour, with an X-Small warehouse consuming 1 credit per hour and a 4X-Large warehouse using 128 credits per hour.\n**Scaling Up** : Increasing warehouse size boosts performance for complex queries by providing more compute power.\n**Scaling Out** : Adding clusters is better for handling high concurrency.\nFor complex queries, larger warehouses are recommended to avoid memory spillage, which can slow performance significantly. For simpler queries, a Medium or smaller warehouse usually suffices. If you notice queries spilling data to local or remote storage, increasing the warehouse size can help by providing additional memory.\nSection Title: How to Optimize Query Concurrency in Snowflake > ... > How to Choose the Right Warehouse Size\nContent:\nFor production environments, a Large or X-Large warehouse is often a good choice. Pair this with per-second billing and auto-suspend settings to control costs. In testing environments or when using the Snowsight UI, smaller warehouses (X-Small, Small, or Medium) are typically enough. To identify resource bottlenecks, monitor the Query Overload % metric in Performance Explorer. If queuing is due to high query volume rather than complexity, a multi-cluster warehouse setup might be more effective than simply increasing warehouse size.\n ... \nSection Title: How to Optimize Query Concurrency in Snowflake > ... > How Query Queuing Works in Snowflake\nContent:\nWhen a warehouse runs out of compute resources or hits its concurrency limit, Snowflake places additional queries in a queue. The number of simultaneous queries a cluster can handle is controlled by the `MAX_CONCURRENCY_LEVEL` setting, which defaults to 8. Once this cap is reached, any new queries must wait in line.\nThere are two main types of queuing in Snowflake:\n**\"Queued (Provisioning)\"** happens when a query waits for a warehouse to start or scale up.\n**\"Queued\"** occurs when the warehouse is already running but overloaded, requiring the query to wait for other queries to complete.\nFor multi-cluster warehouses, Snowflake can automatically add clusters in Auto-scale mode to handle queued queries. However, if the Economy scaling policy is enabled, new clusters will only be added if there\u2019s sufficient workload to keep them active for at least six minutes.\n ... \nSection Title: How to Optimize Query Concurrency in Snowflake > ... > How to Set Query Priorities\nContent:\nWhile Snowflake\u2019s auto-scaling feature adjusts compute resources dynamically, managing query priorities can further enhance performance during high-demand periods. One effective way to minimize delays is through **workload isolation** , which ensures critical tasks have the resources they need.\nWorkload isolation involves creating separate warehouses for different types of queries. For example, high-priority workloads should run on dedicated warehouses to avoid competing with ad-hoc or low-priority queries. This method is more effective than trying to prioritize within a single shared warehouse.\nSection Title: How to Optimize Query Concurrency in Snowflake > ... > How to Set Query Priorities\nContent:\nFor critical workloads, configure multi-cluster warehouses with a higher `MAX_CLUSTERS` value and use the **Standard** scaling policy. This policy starts new clusters immediately when queries begin to queue, ensuring faster processing. Avoid the Economy scaling policy for essential tasks, as it prioritizes saving credits over performance, which can result in delays. Additionally, lowering the `MAX_CONCURRENCY_LEVEL` for warehouses handling complex queries can allocate more resources to each query, though this may increase queuing for other tasks.\nTo maintain availability during peak usage, assign critical warehouses to Resource Monitors with sufficient credit quotas. Set the monitors to \"Notify\" instead of \"Suspend\" to avoid unexpected disruptions when nearing credit limits. This ensures that high-priority workloads continue running without interruption.\nSection Title: How to Optimize Query Concurrency in Snowflake > ... > How to Set Query Priorities\nContent:\n| Feature | Impact on Priority | Recommended Setting |\n| **Warehouse Isolation** | Prevents resource competition between tasks | Use dedicated warehouses for key workloads |\n| **Scaling Policy** | Determines when extra clusters are added | Use **Standard** for immediate scaling |\n| **MAX_CONCURRENCY_LEVEL** | Allocates resources to fewer queries at a time | Lower for complex, resource-intensive queries |\n| **Resource Monitors** | Prevents warehouse suspension | Use \"Notify\" for critical warehouses |\n ... \nSection Title: How to Optimize Query Concurrency in Snowflake > Conclusion\nContent:\nBalancing query concurrency in Snowflake requires thoughtful management of compute resources, workload types, and cost considerations. Start by monitoring warehouse activity in Snowsight to identify queuing patterns. From there, decide whether to **scale out** with multi-cluster warehouses for handling high concurrency or **scale up** to improve performance for complex queries.\nQueuing patterns often point to misaligned workload distribution. To address this, assign similar workloads to dedicated warehouses. Mixing queries with vastly different complexities can make it challenging to configure the right warehouse size and concurrency settings. For larger accounts, dedicating an X\u2011Small warehouse for Snowsight UI tasks can help. This approach minimizes interference from internal metadata queries, ensuring they don\u2019t compete with production workloads.\nSection Title: How to Optimize Query Concurrency in Snowflake > Conclusion\nContent:\nFine-tune auto-suspend settings based on workload behavior. For BI and SELECT-heavy use cases, set auto-suspend to at least 10 minutes to retain the warehouse cache. For ad-hoc tasks like DevOps or Data Science, a 5-minute setting works better since caching offers fewer benefits in these scenarios. Keep in mind that Snowflake charges for a minimum of 60 seconds each time a warehouse starts, so frequent suspensions can lead to wasted credits.\nThe default `MAX_CONCURRENCY_LEVEL` of 8 is suitable for many cases, but reducing it can improve performance for complex queries by allocating more resources per query. Test changes by re-running queries and analyzing performance metrics. For large scans involving selective filters, consider using Query Acceleration to offload processing without needing to increase warehouse size.\nSection Title: How to Optimize Query Concurrency in Snowflake > Conclusion\nContent:\nFor multi-cluster setups, the Economy scaling policy is a cost-efficient option. It prioritizes fully utilizing existing clusters before spinning up new ones, making it ideal when cost management takes precedence over immediate query execution. Regularly track metrics like \"Query Overload %\" and \"Query Blocked %.\" Keeping these near zero ensures minimal wait times for users and smooth query execution.\n ... \nSection Title: ... > What are the advantages of using auto-scaling for warehouses in Snowflake?\nContent:\nSnowflake's auto-scaling feature is designed to handle workloads efficiently by dynamically adjusting compute resources based on demand. When query traffic spikes, auto-scaling ensures your system can handle the increased load without slowing down. This means even during peak usage, your queries run smoothly.\nAnother major advantage is cost optimization. Snowflake automatically pauses warehouses when they're idle and resizes them as needed. This way, you're only charged for the resources you actively use, helping you manage expenses more effectively.\nWhat sets Snowflake apart is its separation of compute and storage. This design keeps your system agile and responsive, even when workloads are heavy. It's a great fit for businesses that need to scale up or down without sacrificing performance or overspending."
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/warehouses-overview",
      "title": "Overview of warehouses | Snowflake Documentation",
      "excerpts": [
        "Section Title: Overview of warehouses \u00b6 > Warehouse size \u00b6\nContent:\nSize specifies the amount of compute resources available per cluster in a warehouse. Snowflake supports the following warehouse sizes:\nSection Title: Overview of warehouses \u00b6 > Warehouse size \u00b6\nContent:\n| Warehouse size | Credits / hour (Gen1 warehouses) | Credits / second (Gen1 warehouses) | Notes |\n| X-Small | 1 | 0.0003 | Default size for warehouses created in Snowsight and using CREATE WAREHOUSE . |\n| Small | 2 | 0.0006 |  |\n| Medium | 4 | 0.0011 |  |\n| Large | 8 | 0.0022 |  |\n| X-Large | 16 | 0.0044 | Default size for warehouses created using Snowsight. |\n| 2X-Large | 32 | 0.0089 |  |\n| 3X-Large | 64 | 0.0178 |  |\n| 4X-Large | 128 | 0.0356 |  |\n| 5X-Large | 256 | 0.0711 | Generally available in Amazon Web Services (AWS) and Microsoft Azure regions, and in preview in US Government regions. |\n| 6X-Large | 512 | 0.1422 | Generally available in Amazon Web Services (AWS) and Microsoft Azure regions, and in preview in US Government regions. |\n ... \nSection Title: Overview of warehouses \u00b6 > Warehouse size \u00b6\nContent:\nAnother way that you can scale the capacity of Snowflake warehouses without changing the warehouse size is by using\nmulti-cluster warehouses. For more information about that feature, see Multi-cluster warehouses .\n ... \nSection Title: Overview of warehouses \u00b6 > Warehouse size \u00b6 > Impact on credit usage and billing \u00b6\nContent:\nAs shown in the above table, there is a doubling of credit usage as you increase in size to the next larger warehouse size for each full\nhour that the warehouse runs; however, note that Snowflake utilizes per-second billing (with a 60-second minimum each time the warehouse\nstarts) so warehouses are billed only for the credits they actually consume.\nThe total number of credits billed depends on how long the warehouse runs continuously. For comparison purposes, the following table shows\nthe billing totals for three different size Gen1 standard warehouses based on their running time (totals rounded to the nearest 1000th of a credit):\nSection Title: Overview of warehouses \u00b6 > Warehouse size \u00b6 > Impact on credit usage and billing \u00b6\nContent:\n| Running Time | Credits . (X-Small) | Credits . (X-Large) | Credits . (5X-Large) |\n| 0-60 seconds | 0.017 | 0.267 | 4.268 |\n| 61 seconds | 0.017 | 0.271 | 4.336 |\n| 2 minutes | 0.033 | 0.533 | 8.532 |\n| 10 minutes | 0.167 | 2.667 | 42.668 |\n| 1 hour | 1.000 | 16.000 | 256.000 |\nSection Title: Overview of warehouses \u00b6 > Warehouse size \u00b6 > Impact on credit usage and billing \u00b6\nContent:\nNote\nFor a multi-cluster warehouse , the number of credits billed is calculated based on the\nwarehouse size and the number of clusters that run within the time period.For example, if a 3X-Large multi-cluster warehouse runs 1 cluster for one full hour and then runs 2 clusters for the next full hour, the total number of credits billed would be 192 (i.e. 64 + 128).Multi-cluster warehouses are an Enterprise Edition feature.\nSection Title: Overview of warehouses \u00b6 > Warehouse size \u00b6 > Impact on data loading \u00b6\nContent:\nIncreasing the size of a warehouse does not always improve data loading performance. Data loading performance is influenced more by\nthe number of files being loaded (and the size of each file) than the size of the warehouse.\nTip\nUnless you are bulk loading a large number of files concurrently (i.e. hundreds or thousands of files), a smaller warehouse\n(Small, Medium, Large) is generally sufficient. Using a larger warehouse (X-Large, 2X-Large, etc.) will consume more credits and may not\nresult in any performance increase.\nFor more data loading tips and guidelines, see Data loading considerations .\nSection Title: Overview of warehouses \u00b6 > Warehouse size \u00b6 > Impact on query processing \u00b6\nContent:\nThe size of a warehouse can impact the amount of time required to execute queries submitted to the warehouse, particularly for larger, more\ncomplex queries. In general, query performance scales with warehouse size because larger warehouses have more compute resources available to\nprocess queries.\nIf queries processed by a warehouse are running slowly, you can always resize the warehouse to provision more compute resources. The\nadditional resources do not impact any queries that are already running, but once they are fully provisioned they become available for use\nby any queries that are queued or newly submitted.\nTip\nLarger is not necessarily faster for small, basic queries.\nFor more warehouse tips and guidelines, see Warehouse considerations .\nSection Title: Overview of warehouses \u00b6 > Auto-suspension and auto-resumption \u00b6\nContent:\nYou can set a warehouse to automatically resume or suspend, based on activity:\nBy default, auto-suspend is enabled. Snowflake automatically suspends the warehouse if it is inactive for the specified period of time.\nBy default, auto-resume is enabled. Snowflake automatically resumes the warehouse when any statement that requires a warehouse is submitted and the warehouse is the current warehouse for the session.\nThese properties can be used to simplify and automate your monitoring and usage of warehouses to match your workload. Auto-suspend ensures\nthat you don\u2019t leave a warehouse running (and consuming credits) when there are no incoming queries. Similarly, auto-resume ensures that\nthe warehouse starts up again as soon as it is needed.\nNote\nAuto-suspend and auto-resume apply only to the entire warehouse and not to the individual clusters in the warehouse.\nFor a multi-cluster warehouse :\nSection Title: Overview of warehouses \u00b6 > Auto-suspension and auto-resumption \u00b6\nContent:\nAuto-suspend only occurs when the minimum number of clusters is running and there is no activity for the specified period of time. The\nminimum is typically 1 (cluster), but could be more than 1.\nAuto-resume only applies when the entire warehouse is suspended (i.e. no clusters are running).\nSection Title: Overview of warehouses \u00b6 > Query processing and concurrency \u00b6\nContent:\nThe number of queries that a warehouse can concurrently process is determined by the size and complexity of each query. As queries are\nsubmitted, the warehouse calculates and reserves the compute resources needed to process each query. If the warehouse does not have enough\nremaining resources to process a query, the query is queued, pending resources that become available as other running queries complete.\nSnowflake provides some object-level parameters that can be set to help control query processing and concurrency:\nSTATEMENT_QUEUED_TIMEOUT_IN_SECONDS\nSTATEMENT_TIMEOUT_IN_SECONDS\nNote\nIf queries are queuing more than desired, another warehouse can be created and queries can be manually redirected to the new warehouse.\nIn addition, resizing a warehouse can enable limited scaling for query concurrency and queuing; however, warehouse resizing is primarily\nintended for improving query performance.\nSection Title: Overview of warehouses \u00b6 > Query processing and concurrency \u00b6\nContent:\nTo enable fully automated scaling for concurrency, Snowflake recommends multi-cluster warehouses ,\nwhich provide essentially the same benefits as creating additional warehouses and redirecting queries, but without requiring manual\nintervention.\nMulti-cluster warehouses are an Enterprise Edition feature.\nSection Title: Overview of warehouses \u00b6 > Warehouse usage in sessions \u00b6\nContent:\nWhen a session is initiated in Snowflake, the session does not, by default, have a warehouse associated with it. Until a session has a\nwarehouse associated with it, queries cannot be submitted within the session.\n ... \nSection Title: Overview of warehouses \u00b6 > Warehouse usage in sessions \u00b6 > Default warehouse for notebooks \u00b6\nContent:\nPreview Feature \u2014 Open\nAvailable to all accounts.\nTo enhance cost efficiency for notebook workloads, a multi-cluster X-Small warehouse, SYSTEM$STREAMLIT_NOTEBOOK_WH, is automatically\nprovisioned within each account. This warehouse, featuring a maximum of 10 clusters and a 60-second default timeout, uses improved bin\npacking. The ACCOUNTADMIN role has OWNERSHIP privileges.\nSection Title: Overview of warehouses \u00b6 > ... > Recommendations for cost management \u00b6\nContent:\nSnowflake recommends using the SYSTEM$STREAMLIT_NOTEBOOK_WH warehouse exclusively for notebook workloads.\nTo improve bin-packing efficiency and reduce cluster fragmentation, direct SQL queries from Notebook apps to a separate customer-managed query warehouse. Using a single shared warehouse for all Notebook apps in an account further enhances bin-packing efficiency.\nSeparating notebook Python workloads from SQL queries minimizes cluster fragmentation. This approach optimizes overall costs by ensuring that notebook Python workloads are not co-located with larger warehouses, which are typically used for query execution.\n ... \nSection Title: Overview of warehouses \u00b6 > Warehouse usage in sessions \u00b6 > Precedence for warehouse defaults \u00b6\nContent:\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nWarehouse size\nAuto-suspension and auto-resumption\nQuery processing and concurrency\nWarehouse usage in sessions\nRelated content\nUnderstanding compute cost\nWorking with resource monitors"
      ]
    },
    {
      "url": "https://stellans.io/automate-snowflake-warehouse-scaling-with-resource-monitors/",
      "title": "Automate Snowflake Scaling with Monitors - Stellans",
      "publish_date": "2025-11-02",
      "excerpts": [
        "Section Title: Automate Snowflake Warehouse Scaling with Resource Monitors\nContent:\n11 minutes to read\nJuly 29, 2025\nGet free consultation\nManaging Snowflake warehouse costs while maintaining optimal query performance is one of the biggest challenges facing data teams today. Manual warehouse management leads to 75% cost overruns in enterprise environments, while poorly configured auto-scaling can throttle critical workloads during peak demand periods.\nThis comprehensive guide demonstrates how to implement automated Snowflake warehouse scaling using resource monitors and Infrastructure as Code (IaC) with Terraform. You\u2019ll learn to build enterprise-grade automation that balances performance requirements with cost optimization, eliminating the guesswork from warehouse management.\nSection Title: Automate Snowflake Warehouse Scaling with Resource Monitors > Key Takeaways\nContent:\nResource monitors provide automated cost controls and scaling triggers for Snowflake warehouses\nTerraform automation enables consistent, repeatable warehouse configurations across environments\nMulti-cluster warehouses automatically scale horizontally to handle concurrent workload spikes\nProper sizing methodology can reduce warehouse costs by 40-60% while improving performance\nDataOps integration creates self-healing infrastructure that adapts to changing business needs\nSection Title: ... > Snowflake Warehouse Scaling Fundamentals\nContent:\nUnderstanding Snowflake\u2019s warehouse scaling mechanisms is essential before implementing automation. Snowflake offers both vertical and horizontal scaling options, each suited to different workload patterns and performance requirements.\nWarehouse scaling directly impacts your credit consumption and query performance. A poorly sized warehouse can either waste credits on unused compute power or create performance bottlenecks that affect user experience. The key is implementing intelligent automation that responds to actual workload demands rather than static configurations.\nSection Title: ... > Vertical vs Horizontal Scaling Strategies\nContent:\nVertical scaling involves changing warehouse sizes (X-Small to 4X-Large and beyond), while horizontal scaling uses multi-cluster warehouses to handle concurrent queries. Vertical scaling affects individual query performance , whereas horizontal scaling addresses concurrency challenges.\nFor analytical workloads with complex queries, vertical scaling typically provides better performance per credit. Data transformation jobs often benefit from larger warehouses that can process more data in parallel. However, interactive dashboards and user-facing applications usually perform better with horizontal scaling to handle multiple simultaneous users.\nThe optimal strategy depends on your workload characteristics. Mixed workloads often require a combination of both approaches, with different warehouse configurations for different use cases.\nSection Title: ... > Auto-Scaling Configuration Best Practices\nContent:\nSnowflake\u2019s auto-scaling features include auto-suspend, auto-resume, and multi-cluster scaling policies. Auto-suspend should be set to 1-5 minutes for interactive workloads and 10-60 seconds for batch processing to minimize idle credit consumption.\nAuto-resume ensures warehouses start automatically when queries are submitted, eliminating manual intervention. However, cold start times can impact user experience , so consider keeping frequently-used warehouses running during business hours with scheduled suspension during off-peak periods.\nMulti-cluster scaling policies should align with your concurrency requirements. Set minimum clusters to handle baseline load and maximum clusters to prevent runaway scaling costs during unexpected demand spikes.\nSection Title: Automate Snowflake Warehouse Scaling with Resource Monitors > Multi-Cluster Warehouse Implementation\nContent:\nMulti-cluster warehouses automatically add or remove clusters based on query queue length and concurrency demands. Each cluster operates independently , allowing Snowflake to distribute queries across available compute resources efficiently.\nConfigure scaling policies based on your specific concurrency patterns. For example, a data science team might need 1-8 clusters during business hours but only 1 cluster overnight. Queue length thresholds of 6-10 queries typically provide good responsiveness without excessive scaling.\nMonitor cluster utilization metrics to optimize your scaling policies over time. Underutilized clusters indicate over-provisioning, while consistently maxed-out clusters suggest the need for higher maximum limits or larger warehouse sizes.\nSection Title: ... > Resource Monitor Setup and Configuration\nContent:\nResource monitors are Snowflake\u2019s primary cost control mechanism, providing automated alerts and actions when credit consumption exceeds defined thresholds. Properly configured resource monitors can prevent budget overruns while maintaining service availability for critical workloads.\nResource monitors operate at the account, warehouse, or user level, offering granular control over credit consumption. They support multiple alert thresholds and can automatically suspend warehouses or prevent new queries when limits are reached.\nSection Title: **Creating Resource Monitors via Web UI**\nContent:\nThe Snowflake web interface provides an intuitive way to create basic resource monitors. Navigate to Admin > Resource Monitors and click \u201cCreate Resource Monitor\u201d to access the configuration wizard.\nSet meaningful names that reflect the monitor\u2019s purpose , such as \u201cETL_WAREHOUSE_DAILY_LIMIT\u201d or \u201cANALYTICS_TEAM_MONTHLY_BUDGET\u201d. Define the credit quota based on your budget allocation and expected usage patterns.\nConfigure alert thresholds at 50%, 75%, and 90% of your credit limit to provide early warning before reaching suspension thresholds. Email notifications should go to both technical teams and budget owners to ensure appropriate stakeholders are informed of potential overruns.\n ... \nSection Title: **Infrastructure as Code Configuration Examples**\nContent:\nStart with a basic warehouse configuration that includes auto-scaling and resource monitor assignment:\n```\nresource \"snowflake_warehouse\" \"analytics_warehouse\" {\n  name           = \"ANALYTICS_WH\"\n  warehouse_size = \"MEDIUM\"\n  \n  auto_suspend = 300  # 5 minutes\n  auto_resume  = true\n  \n  max_cluster_count = 4\n  min_cluster_count = 1\n  scaling_policy    = \"STANDARD\"\n  \n  resource_monitor = snowflake_resource_monitor.analytics_monitor.name\n  \n  comment = \"Managed by Terraform - Analytics workloads\"\n}\n\nresource \"snowflake_resource_monitor\" \"analytics_monitor\" {\n  name         = \"ANALYTICS_DAILY_MONITOR\"\n  credit_quota = 200\n  frequency    = \"DAILY\"\n  \n  notify_triggers    = [50, 75]\n  suspend_triggers   = [90]\n  suspend_immediate_triggers = [100]\n  \n  notify_users = [\"data-team@company.com\"]\n}\n```\nUse variables and locals to maintain consistency across multiple warehouse configurations:\n ... \nSection Title: **Multi-Cluster Warehouse Provisioning**\nContent:\nMulti-cluster warehouses require careful configuration to balance performance and cost. Define scaling policies that match your workload patterns :\n```\nresource \"snowflake_warehouse\" \"user_facing_warehouse\" {\n  name = \"USER_FACING_WH\"\n  warehouse_size = \"SMALL\"\n  \n  # Multi-cluster configuration\n  max_cluster_count = 8\n  min_cluster_count = 2\n  scaling_policy    = \"STANDARD\"\n  \n  # Aggressive auto-suspend for cost optimization\n  auto_suspend = 60  # 1 minute\n  auto_resume  = true\n  \n  # Resource monitoring\n  resource_monitor = snowflake_resource_monitor.user_facing_monitor.name\n  \n  comment = \"User-facing analytics with auto-scaling\"\n}\n```\nEconomy scaling policy provides cost optimization for less time-sensitive workloads:\nSection Title: **Multi-Cluster Warehouse Provisioning**\nContent:\n```\nresource \"snowflake_warehouse\" \"batch_processing_warehouse\" {\n  name = \"BATCH_PROCESSING_WH\"\n  warehouse_size = \"X-LARGE\"\n  \n  max_cluster_count = 3\n  min_cluster_count = 1\n  scaling_policy    = \"ECONOMY\"  # Cost-optimized scaling\n  \n  auto_suspend = 300\n  auto_resume  = true\n}\n```\n ... \nSection Title: **CI/CD Pipeline Integration**\nContent:\n```\n# .github/workflows/snowflake-infrastructure.yml\nname: Snowflake Infrastructure\n\non:\n  push:\n    branches: [main]\n    paths: ['terraform/snowflake/**']\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Setup Terraform\n        uses: hashicorp/setup-terraform@v2\n        with:\n          terraform_version: 1.5.0\n          \n      - name: Terraform Plan\n        run: |\n          terraform init\n          terraform workspace select production\n          terraform plan -var-file=\"production.tfvars\"\n        env:\n          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}\n          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}\n          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}\n          \n      - name: Terraform Apply\n        if: github.ref == 'refs/heads/main'\n        run: terraform apply -auto-approve -var-file=\"production.tfvars\"\n```\nSection Title: **CI/CD Pipeline Integration**\nContent:\nImplement proper state management with remote backends and state locking to prevent concurrent modifications:\n```\nterraform {\n  backend \"s3\" {\n    bucket = \"company-terraform-state\"\n    key    = \"snowflake/warehouses/terraform.tfstate\"\n    region = \"us-west-2\"\n    \n    dynamodb_table = \"terraform-state-lock\"\n    encrypt        = true\n  }\n}\n```\nSection Title: **CI/CD Pipeline Integration** > Warehouse Sizing Optimization Framework\nContent:\nEffective warehouse sizing requires a data-driven approach that considers query complexity, data volume, concurrency requirements, and cost constraints. Right-sizing can reduce warehouse costs by 40-60% while improving query performance through optimal resource allocation.\nThe optimization process involves analyzing query performance metrics, identifying bottlenecks, and testing different warehouse configurations under realistic workload conditions. This iterative approach ensures your warehouse sizes match actual requirements rather than theoretical estimates.\nSection Title: **Right-Sizing Decision Methodology**\nContent:\nStart with baseline performance measurements using Snowflake\u2019s query history and warehouse utilization metrics. Analyze query execution times, queue wait times, and resource utilization patterns to identify optimization opportunities.\nUse this decision framework for warehouse sizing:\nAnalyze query complexity : Simple queries (< 1 minute) often perform well on smaller warehouses, while complex analytical queries benefit from larger sizes\nEvaluate concurrency requirements : High concurrent user loads require multi-cluster configurations regardless of individual query complexity\nConsider data volume : Large data scans and transformations typically need bigger warehouses for optimal performance\nFactor in SLA requirements : Time-sensitive workloads may justify larger warehouses for faster execution\nMonitor key performance indicators including average query time, queue depth, and credit consumption per query to validate sizing decisions.\n ... \nSection Title: **Automated Scaling Policies**\nContent:\nImplement time-based scaling policies that align with business operations and user activity patterns. Schedule warehouse size changes based on predictable workload variations :\n ... \nSection Title: **Enterprise Governance Frameworks** > Conclusion\nContent:\nStart with conservative configurations and iterate based on actual usage patterns rather than theoretical requirements. Monitor key performance indicators including query execution times, credit consumption, and user satisfaction metrics to validate your automation effectiveness.\nThe investment in proper warehouse automation typically pays for itself within 2-3 months through reduced manual overhead and optimized resource utilization. Enterprise organizations often see 40-60% cost reductions while improving query performance and system reliability.\n[Transform your Snowflake infrastructure with expert automation strategies. Schedule a consultation with Stellans.io\u2019s data engineering team.](http://stellans.io)"
      ]
    },
    {
      "url": "https://medium.com/@somen.swain/6-techniques-to-run-concurrency-testing-on-snowflakes-virtual-warehouse-e1cc0af696ff",
      "title": "6 techniques to run \u201cconcurrency testing\u201d on Snowflake\u2019s virtual warehouse | by Somen Swain | Medium",
      "publish_date": "2024-10-23",
      "excerpts": [
        "Section Title: 6 techniques to run \u201cconcurrency testing\u201d on Snowflake\u2019s virtual warehouse\nContent:\nSomen Swain\n11 min read\n\u00b7\nOct 23, 2024\n--\n1\nListen\nShare\nSnowflake\u2019s virtual warehouse one of the most critical pillar when it comes to supporting all the workloads by allocating the compute. Now, while doing the workload assessment and designing for the \u201ccompute allocation\u201d very often there is a challenge w.r.t what can be the optimal size of the warehouse ? how do we address the complexity and concurrency workloads ? etc.\nIn this blog I would deep dive into various aspects of concurrency measurements & testing which can be done by performing necessary tests. Now, concurrency testing of a workload(engineering pipelines, data transformation pipelines, support of in-house developers, testers, etc.) remains of paramount importance. Some of the ***key benefits*** that we can get by ***doing the concurrency testing*** of a warehouse are:\nPress enter or click to view image in full size\nConcurrency Testing Benefits\nSection Title: ... > **1. Comprehending workload behavior.**\nContent:\nThe ability of a Snowflake virtual warehouse to manage several queries or processes operating at once is determined through concurrency testing. You may watch how the system behaves under pressure and identify any performance bottlenecks by simulating different workloads.\nSection Title: ... > 2. Query queuing management.\nContent:\nInsufficient concurrency testing may result in query queuing when multiple users or queries attempt to access the warehouse at once. By measuring the system\u2019s limits and confirming that you have allotted enough resources to avoid query queues or delays, concurrency testing helps.\nSection Title: ... > 3. Auto scaling management.\nContent:\nThe multi-cluster warehouse in Snowflake enables automatic scalability by adding or deleting clusters in accordance with concurrent requirements. Concurrency testing simulates real-world scenarios and determines when more clusters are needed, which aids in fine-tuning this auto-scaling capability.\n ... \nSection Title: ... > Test Scenario 3: Single cluster, Running >8 queries in X-Small WH with max_concurrency_level=8\nContent:\nPress enter or click to view image in full size\nSnapshot of output.\nAs you can see over here 8 queries at max got executed and rest 2 are there in queue state. Hence at max in X-Small warehouse 8 queries can get executed in parallel.\n**Inferences of this testing** \u2192 Single cluster warehouse with \u201cX-Small\u201d in size and having Max_Concurrency_Level level set as 8, can at max run 8 queries in parallel. Rest of the queries would go in queueing state.\nSection Title: ... > Test Scenario 4: Multi-Cluster with 2 clusters, Scaling policy as Standard, run 12 queries ...\nContent:\nNow, we would keep the default value of Max_Concurrency_Level as 8 but we would execute 12 queries and see the pattern. Everything else would remain the same.\nMulti-cluster-Warehouse configurations\nI would be executing 12 queries all in parallel to see if the additional clusters are getting activated up and what is the status of the queueing. Below is the screenshot post executing the queries through SnowSQL CLI.\n ... \nSection Title: ... > Test Scenario 4: Multi-Cluster with 2 clusters, Scaling policy as Standard, run 12 queries ...\nContent:\nREPLACE  TABLE  demo_db.demo_schema.orders_012  AS SELECT * FROM  SNOWFLAKE_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >\n```\nSection Title: ... > Test Scenario 4: Multi-Cluster with 2 clusters, Scaling policy as Standard, run 12 queries ...\nContent:\nPress enter or click to view image in full size\nObservations\n**Inferences of this testing** \u2192 Multi-cluster warehouse of size X-Small and 2 clusters. When we run 12 queries that is when additional cluster got activated to ensure there is no queuing.\nSection Title: ... > Test Scenario 5: Multi-Cluster with 2 clusters, Scaling policy as Standard, run 8 queries i...\nContent:\nNow, we would keep the default value of Max_Concurrency_Level as 8 and I would be executing the 8 queries in parallel.\nSection Title: ... > Test Scenario 5: Multi-Cluster with 2 clusters, Scaling policy as Standard, run 8 queries i...\nContent:\n```\nSOMEN202410 @DEMO _DB.DEMO_SCHEMA > CREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_001  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_002  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_003  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_004  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >\nSection Title: ... > Test Scenario 5: Multi-Cluster with 2 clusters, Scaling policy as Standard, run 8 queries i...\nContent:\nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_005  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_006  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_007  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_008  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nSOMEN202410 @DEMO _DB.DEMO_SCHEMA >\n```\nSection Title: ... > Test Scenario 5: Multi-Cluster with 2 clusters, Scaling policy as Standard, run 8 queries i...\nContent:\nPress enter or click to view image in full size\nAs we can see all the 8 queries are running fine without spinning up of additional clusters although this warehouse is set as multi-cluster warehouse.\n**Inferences of this testing** \u2192 Multi-cluster warehouse of size X-Small and 2 clusters. When we run 8 queries no additional cluster got activated and all of the queries ran successfully without queuing.Hence no additional cost even though it is configured as multi-cluster warehouse.\nSection Title: ... > Test Scenario 6: Multi-Cluster with 2 clusters, Scaling policy as Economy, run 12 queries i...\nContent:\nOver here we would be running 12 queries in parallel but scaling policy would be changed to \u2018 ***economy*** \u2019. Below is the command to do that:\n```\nALTER  WAREHOUSE DEMO_MULTI_CLSTR_WH  SET  SCALING_POLICY = ECONOMY;\n```\nOnce the queries are executed through the Snowsql, we would see the below pattern in the query profile:\nSection Title: ... > Test Scenario 6: Multi-Cluster with 2 clusters, Scaling policy as Economy, run 12 queries i...\nContent:\n```\nSOMEN202410 @DEMO _DB.DEMO_SCHEMA >  \nSOMEN202410 @DEMO _DB.DEMO_SCHEMA > CREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_001  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_002  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_003  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_004  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS\n ... \nSection Title: ... > Test Scenario 6: Multi-Cluster with 2 clusters, Scaling policy as Economy, run 12 queries i...\nContent:\nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_009  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_010  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_011  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nCREATE OR  REPLACE  TABLE  demo_db.demo_schema.orders_012  AS SELECT * FROM  SNOWFLAK  \n                                                    E_SAMPLE_DATA.TPCH_SF1000.ORDERS  where  o_orderdate <= '1992-01-10' ; >  \nSOMEN202410 @DEMO _DB.DEMO_SCHEMA >\n```\nSection Title: ... > Test Scenario 6: Multi-Cluster with 2 clusters, Scaling policy as Economy, run 12 queries i...\nContent:\nPress enter or click to view image in full size\n**Inferences of this testing** \u2192 With scaling policy being set as \u201ceconomy\u201d, we do not immediately see additional clusters getting activated hence 4 queries are seen with status as queued. After sometime(15~20 secs), all of them got activated and queries went through fine."
      ]
    },
    {
      "url": "https://www.reddit.com/r/snowflake/comments/1gdea63/understanding_snowflake_multicluster_warehouse/",
      "title": "Understanding Snowflake Multi-cluster Warehouse Cluster ... - Reddit",
      "publish_date": "2024-10-27",
      "excerpts": [
        "Skip to main content Open menu Open navigation  Go to Reddit Home\nr/snowflake\nExpand user menu Open settings menu\nGo to snowflake r/snowflake \u2022\nStriking-Nebula-385 [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](https://www.reddit.com/r/snowflake/comments/1gdea63/understanding_snowflake_multicluster_warehouse/?tl=ru) [Magyar](https://www.reddit.com/r/snowflake/comments/1gdea63/understanding_snowflake_multicluster_warehouse/?tl=hu) [T\u00fcrk\u00e7e](https://www.reddit.com/r/snowflake/comments/1gdea63/understanding_snowflake_multicluster_warehouse/?tl=tr) [Bahasa Melayu](https://www.reddit.com/r/snowflake/comments/1gdea63/understanding_snowflake_multicluster_warehouse/?tl=ms)\nSection Title: Understanding Snowflake Multi-cluster Warehouse Cluster Suspension Behavior\nContent:\nI'm trying to understand the cluster suspension behavior in Snowflake multi-cluster warehouses. Here's my setup and observation:\n**Current Configuration:**\nMulti-cluster warehouse\nMaximum cluster count: 10\nAuto-scaling is enabled\nScaling Policy: STANDARD\n**What I'm observing:**\nAs queries queue up, clusters scale up one by one until reaching the max of 10\nAfter queries complete, clusters seem to take a very long time to scale down (Some more than 5 mins). Some clusters suspend quick (10, 9) but subsequent ones are longer.\nThis happens even when most queries have already finished executing\nQuestion:\nWhat triggers cluster suspension in a multi-cluster setup? Is there a minimum active time for a cluster?\nRead more Share\nSection Title: Related Answers Section\nContent:\nRelated Answers\nBest practices for Snowflake scaling\nSnowflake cluster management tips\nSnowflake warehouse sizing strategies\nSnowflake auto clustering advantages\nUnderstanding clustering keys in Snowflake\nPublic\nAnyone can view, post, and comment to this community\n0 0\nSection Title: Related Answers Section > Top Posts\nContent:\n[Reddit reReddit: Top posts of October 27, 2024 * * *](https://www.reddit.com/posts/2024/october-27-1/global/)\n[Reddit reReddit: Top posts of October 2024 * * *](https://www.reddit.com/posts/2024/october/global/)\n[Reddit reReddit: Top posts of 2024 * * *](https://www.reddit.com/posts/2024/global/)\nExpand Navigation Collapse Navigation"
      ]
    },
    {
      "url": "https://medium.com/@sriramarun/monitoring-snowflake-usage-access-credit-optimization-854cf8bc8674",
      "title": "Monitoring Snowflake \u2014 Usage, Access & Credit Optimization | by Sriram Krishnan | Medium",
      "publish_date": "2025-07-22",
      "excerpts": [
        "Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nSearch\nSection Title: Monitoring Snowflake \u2014 Usage, Access & Credit Optimization\nContent:\nSriram Krishnan\nFollow\n3 min read\n\u00b7\nJul 22, 2025\n8\nListen\nShare\nIn our last blog, we learned how to *define* governance with tags and policies. But a strategy is only effective if you can verify and enforce it. This is where monitoring comes in. In a platform like Snowflake, where usage and cost scale with demand, observability is essential for control.\nEffective monitoring follows a diagnostic workflow: **Monitor** high-level trends, **Identify** specific anomalies, **Diagnose** the root cause, and **Optimize** . This guide explores how to use Snowflake\u2019s native tools for each step, so you can run a high-performance, governed, and cost-efficient platform.\nSection Title: ... > Usage Visibility: Monitor What\u2019s Running and Why\nContent:\nSnowflake offers two key sources for monitoring:\n**INFORMATION_SCHEMA views:** Real-time metadata scoped to a database.\n**ACCOUNT_USAGE views:** Historical metadata scoped to the entire account (with some latency).\nUse them together to get a high-level view of your environment. A great starting point is tracking warehouse credit usage to understand where your budget is going.\n**Example: Track Warehouse Credit Usage**\n```\nSELECT  \n  warehouse_name,  \nSUM (credits_used)  AS  total_credits  \nFROM  SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY  \nWHERE  start_time  >=  DATEADD( 'day' ,  -7 ,  CURRENT_TIMESTAMP ())  \nGROUP BY  warehouse_name  \nORDER BY  total_credits  DESC ;\n```\nThis helps you monitor trends and identify which warehouses are the most expensive, beginning the \u201cIdentify\u201d phase of our workflow.\nSection Title: ... > Query Monitoring: Identify and Diagnose Bottlenecks\nContent:\nThe QUERY_HISTORY views are your primary tool for investigating performance issues.\n**Example: Identify Top Long-Running Queries**\n```\nSELECT  \n  query_text,  \n  user_name,  \n  warehouse_name,  \n  execution_status,  \n  total_elapsed_time / 1000 AS  duration_sec  \nFROM  SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY  \nWHERE  start_time  >=  DATEADD( 'day' ,  -1 ,  CURRENT_TIMESTAMP ())  \nORDER BY  duration_sec  DESC  \nLIMIT  10 ;\n```\n**Diagnosing with the Query Profile**\nOnce you identify a problematic query, the next step is to diagnose its execution plan. The **Query Profile** , available in Snowsight for every query, is the most powerful tool for this. It provides a visual breakdown of every operator in the query, allowing you to spot common bottlenecks like:\nSection Title: ... > Query Monitoring: Identify and Diagnose Bottlenecks\nContent:\n**Spilling:** When data overflows from memory to local or remote storage, dramatically slowing down the query.\n**\u201cExploding\u201d Joins:** Incorrect join logic that creates a massive number of intermediate rows.\n**Full Table Scans:** Queries that fail to prune data effectively.\nSection Title: ... > Access Monitoring: Verifying Your Governance Strategy\nContent:\nTracking object-level access is how you verify that the tags and policies from our previous chapter are working. The ACCESS_HISTORY view shows who accessed what, when, and which columns were involved.\nSection Title: ... > Get Sriram Krishnan\u2019s stories in your inbox\nContent:\nJoin Medium for free to get updates from this writer.\nSubscribe\nSubscribe\n**Example: Verify Who Accessed Tagged PII Columns**\n```\nSELECT  \n  user_name,  \n  direct_objects_accessed[ 0 ].objectName  as  table_name,  \n  direct_objects_accessed[ 0 ].columnNames  as  columns_accessed,  \n  query_start_time  \nFROM  SNOWFLAKE.ACCOUNT_USAGE.ACCESS_HISTORY  \nWHERE  columns_accessed ILIKE  '%EMAIL%' -- Or use tags to find sensitive columns  \nAND  query_start_time  >=  DATEADD( 'day' ,  -7 ,  CURRENT_TIMESTAMP ());\n```\nThis is how you close the loop on governance \u2014 by actively monitoring access to sensitive data and ensuring your policies are being enforced as expected.\nSection Title: ... > Cost Optimization: The \u201cOptimize\u201d Phase in Practice\nContent:\nThe WAREHOUSE_METERING_HISTORY view is critical for optimization. A key technique is to distinguish between compute and cloud services credits.\nSection Title: ... > Diagnosing Credit Burn: Compute vs. Cloud Services\nContent:\n```\nSELECT  \n  warehouse_name,  \nSUM (credits_used_compute)  AS  compute_credits,  \nSUM (credits_used_cloud_services)  AS  cloud_services_credits  \nFROM  SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY  \nWHERE  start_time  >=  DATEADD( 'day' ,  -30 ,  CURRENT_TIMESTAMP ())  \nGROUP BY  warehouse_name  \nORDER BY SUM (credits_used)  DESC ;\n```\n**High Compute Credits:** Often solved by resizing warehouses, tuning queries, or using materialized views.\n**High Cloud Services Credits:** Indicates inefficient operations that don\u2019t use warehouse compute. Common culprits include queries that are too complex to compile, frequent file listings in stages, or excessive use of SHOW commands. This requires fixing the underlying query or process.\nSection Title: ... > Diagnosing Credit Burn: Compute vs. Cloud Services\nContent:\n**Proactive Controls: Resource Monitors**\nDon\u2019t just track costs \u2014 prevent them. **Resource Monitors** are your primary tool for avoiding budget overruns. You can set credit quotas for warehouses or your entire account and configure them to automatically suspend a warehouse or send alerts when a threshold is reached. This is a non-negotiable best practice for any production environment.\nSection Title: ... > Monitoring Best Practices: The Optimization Loop\nContent:\nThink of monitoring not as a one-time task, but as a continuous **Optimization Loop** : **Monitor** dashboards -> **Identify** anomalies -> **Diagnose** with query/access history -> **Optimize** queries and controls -> **Repeat** .\n**Build Snowsight Dashboards:** Visualize warehouse activity, query performance, and user access.\n**Tag PII and Cost Centers:** Use tags to track and report on sensitive or high-cost assets.\n**Alert on Anomalies:** Use Resource Monitors or third-party tools to flag runaway queries or suspicious access.\n**Review Regularly:** Schedule monthly reviews of usage, credit burn, and access activity.\nSection Title: Monitoring Snowflake \u2014 Usage, Access & Credit Optimization > Final Thoughts\nContent:\nMonitoring in Snowflake is powerful \u2014 but only if you use it. Every query, warehouse, and access event is already being logged. The views are there. The insights are yours to extract. The best Snowflake environments aren\u2019t just fast and governed \u2014 they\u2019re visible, predictable, and cost-efficient.\nSnowflake\nMonitoring\n8\n8\nFollow\nSection Title: Monitoring Snowflake \u2014 Usage, Access & Credit Optimization > Written by Sriram Krishnan\nContent:\n92 followers\n\u00b7 45 following\nSharing lessons, tools & patterns to build scalable, modern data platforms \u2014one post at a time. By Sriram Krishnan, data architect & builder.\nFollow\nSection Title: Monitoring Snowflake \u2014 Usage, Access & Credit Optimization > No responses yet\nContent:\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--854cf8bc8674---------------------------------------)\nWrite a response\nWhat are your thoughts?\nCancel\nRespond"
      ]
    },
    {
      "url": "https://digiqt.com/blog/snowflake-query-queues/",
      "title": "Snowflake Query Queues and the Illusion of Scalability | Digiqt Blog",
      "excerpts": [
        "Feb 17, 2026 \u00b7 Snowflake query queues are a capacity signal more than a configuration flaw, revealing concurrency limits and workload contention across\u00a0..."
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}
{"search_id":"search_a27b68dedb654c379870d2671810e140","results":[{"url":"https://www.snowflake.com/en/blog/storage-lifecycle-policies-ga/","title":"Optimize Storage Costs and Simplify Compliance with Storage Lifecycle Policies, Now Generally Available","publish_date":"2025-11-15","excerpts":["Section Title: Optimize Storage Costs and Simplify Compliance with Storage Lifecycle Policies, Now Generally Ava...\nContent:\nEvery organization today faces the same fundamental challenge: how to balance the need to retain vast amounts of data with the growing pressure to control costs. A financial services firm may need to keep years of model outputs for regulatory audits. A media company might accumulate terabytes of log data that’s rarely touched but must be preserved. Security teams across industries generate endless logs that are essential to keep but seldom accessed. Whether it’s compliance, analytics or operational traceability, the story is the same — data that was once critical for daily operations eventually becomes cold, yet remains too valuable or mandated to delete outright. At Snowflake, we believe in making the complex simple. To address these challenges, we are excited to announce the general availability of Storage Lifecycle Policies.\nSection Title: Optimize Storage Costs and Simplify Compliance with Storage Lifecycle Policies, Now Generally Ava...\nContent:\nThis feature provides a simple, automated way to manage your data lifecycle, helping you dramatically reduce storage costs (by 55%-90%) for dormant data and streamline compliance with minimal operational overhead. In this blog post, we will show how you can use Storage Lifecycle Policies to automatically manage your data lifecycle — from archiving cold data to deleting expired records — so you can save costs, maintain compliance and focus on innovation instead of infrastructure. ## What is a Storage Lifecycle Policy? A Storage Lifecycle Policy is a schema-level object that lets you automatically archive or delete data from standard Snowflake tables. These policies are applied at a row level, providing fine-grained control over which specific data is archived or deleted based on a defined timeline.\nSection Title: Optimize Storage Costs and Simplify Compliance with Storage Lifecycle Policies, Now Generally Ava...\nContent:\nYou define a simple policy expression, and Snowflake takes care of the rest, automatically running the policy every day on shared compute resources. Getting started involves just two steps, with a new set of privileges enabling you to control who manages and applies these policies: 1. **Create a policy** that specifies which rows to archive or delete. 2. **Apply the policy** to one or more tables. Here’s a quick example: ### Step 1: Create the policy Create a storage lifecycle policy that archives data older than 360 days into the COLD tier for five years before deletion.\n ... \nSection Title: ... > FinOps for Snowflake\nContent:\nThe FinOps for Snowflake on-demand course provides participants with a high-level overview of the FinOps framework within Snowflake.\n[enroll now](https://learn.snowflake.com/en/courses/OD-FINOPS/)\n ... \nSection Title: ... > Share Article\nContent:\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fstorage-lifecycle-policies-ga&title=Optimize+Storage+Costs+and+Simplify+Compliance+with+Storage+Lifecycle+Policies%2C+Now+Generally+Available)\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fstorage-lifecycle-policies-ga&text=Optimize+Storage+Costs+and+Simplify+Compliance+with+Storage+Lifecycle+Policies%2C+Now+Generally+Available)\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fstorage-lifecycle-policies-ga)\nSubscribe to our blog newsletter\nGet the best, coolest and latest delivered to your inbox each week\n*\nSubscribe Now\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice."]},{"url":"https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs","title":"Storage costs for Time Travel and Fail-safe | Snowflake Documentation","excerpts":["Section Title: Storage costs for Time Travel and Fail-safe ¶\nContent:\nStorage fees are incurred for maintaining historical data during both the Time Travel and Fail-safe periods.\nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Storage usage and fees ¶\nContent:\nThe fees are calculated for each 24-hour period (that is, 1 day) from the time that the data changed. The number of days that Snowflake maintains\nhistorical data is based on the table type and the Time Travel retention period for the table.\nAlso, Snowflake minimizes the amount of storage required for historical data by maintaining only the information required to restore the individual table rows that were updated or deleted. As a result,\nstorage usage is calculated as a percentage of the table that changed. Snowflake only maintains full copies of tables when tables are dropped or truncated.\nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Temporary and transient tables ¶\nContent:\nTo help manage the storage costs associated with Time Travel and Fail-safe, Snowflake provides two table types, temporary and transient, which do not incur the same fees as standard (that is, permanent) tables:\nTransient tables can have a Time Travel retention period of either 0 or 1 day.\nTemporary tables can also have a Time Travel retention period of 0 or 1 day; however, this retention period ends as soon as the table is dropped or the session in which the table was created ends.\nTransient and temporary tables have no Fail-safe period.\nAs a result, the maximum additional fees incurred for Time Travel and Fail-safe by these types of tables is limited to 1 day. The following table illustrates the different scenarios, based on\ntable type:\nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Temporary and transient tables ¶\nContent:\n| Table Type | Time Travel Retention Period (Days) | Fail-safe Period (Days) | Min , Max Historical Data Maintained (Days) |\n| Permanent | 0 or 1 (for Snowflake Standard Edition) | 7 | **7 , 8** |\n| 0 to 90 (for Snowflake Enterprise Edition) | 7 | **7 , 97** |  |\n| Transient | 0 or 1 | 0 | **0 , 1** |\n| Temporary | 0 or 1 | 0 | **0 , 1** |\n ... \nSection Title: ... > Considerations for using temporary and transient tables to manage storage costs ¶\nContent:\nTemporary tables are dropped when the session in which they were created ends. Data stored in temporary tables is not recoverable after the table is dropped.\nHistorical data in transient tables can’t be recovered by Snowflake after the Time Travel retention period ends. Use transient tables only for data you can replicate or reproduce\nindependently from Snowflake.\nLong-lived tables, such as fact tables, should always be defined as permanent to ensure they are fully protected by Fail-safe.\nYou can define short-lived tables as transient to eliminate Fail-safe costs. For example, you might use transient tables for data with a lifetime of less than 1 day, such as ETL work tables.\nIf downtime and the time required to reload lost data are factors, permanent tables, even with their added Fail-safe costs, might offer a better overall solution than transient tables.\nNote\n ... \nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Cost for backups ¶\nContent:\nThe following table describes charges for backups.\nFor information about credit consumption, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\n ... \nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies"]},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization - Snowflake","excerpts":["Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n| Metric Category | Description | Key Metrics | Primary Data Sources |\n| Compute & query metrics | Understand the cost of query execution, warehouse consumption, and overall compute health. These are often the most dynamic and largest portion of your spend. | - Credits used: total credits by warehouse |  |\n| - Query performance: execution time, bytes scanned, compilation time, parameterized query hash |  |  |  |\n| - Warehouse health: % idle time, queueing, spilling, concurrency | - `ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY` (hourly warehouse credit usage) |  |  |\n| - `ACCOUNT_USAGE.QUERY_HISTORY` (detailed query metrics and associated warehouses) |  |  |  |\n| Storage metrics | Costs for compressed data, including active data, Time Travel, and Fail‑safe. | - Storage volume (avg monthly compressed GB/TB) |  |\n| - Inactive storage (Time Travel, Fail‑safe) |  |  |  |\n| - Storage growth rates |  |  |  |\n ... \nSection Title: Cost Optimization > Optimize > Overview > Data storage types & lifecycle management\nContent:\n**Review policy-driven data lifecycle management:**\n**Time Travel & Fail-safe:** Set the [DATA_RETENTION_TIME_IN_DAYS](https://docs.snowflake.com/en/sql-reference/parameters) parameter on a per-table or per-schema basis to the minimum required\nfor your business needs. For transient data, use [TRANSIENT](https://docs.snowflake.com/en/user-guide/tables-temp-transient) tables to eliminate Fail-safe costs.\n**Retained for clone:** Be mindful of cloning operations. While\nzero-copy cloning is cost-effective initially, any subsequent DML\n(Data Manipulation Language) operations on the clone will create new\nmicro-partitions, increasing storage costs. It is recommended to\ndrop clones when they are no longer needed.\n**Be aware of high-churn tables:**\nSection Title: Cost Optimization > Optimize > Overview > Data storage types & lifecycle management\nContent:\nIf a table is updated consistently, inactive storage (Time Travel &\nFail-safe data) can grow at a much faster rate than active storage.\nA high churn table is generally characterized as one that has 40% or\nmore of its storage inactive. Therefore, aligning both the retention\ntime and the use of an appropriate table type with business and\nrecovery requirements is paramount to keeping costs under control.\nReview High Churn tables on a consistent basis to ensure their\nconfiguration is as desired."]},{"url":"https://docs.snowflake.com/en/user-guide/data-failsafe","title":"Understanding and viewing Fail-safe | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg™\nApache Iceberg™ Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Business continuity & data recovery Fail-safe\nSection Title: Understanding and viewing Fail-safe ¶\nContent:\nSeparate and distinct from Time Travel, Fail-safe ensures historical data is protected in the event of a system failure or other event (e.g. a\nsecurity breach).\nSection Title: Understanding and viewing Fail-safe ¶ > What is Fail-safe? ¶\nContent:\nFail-safe provides a (non-configurable) 7-day period during which historical data may be recoverable by Snowflake. This period starts\nimmediately after the Time Travel retention period ends. Note, however, that a long-running Time Travel query will delay moving any data and\nobjects (tables, schemas, and databases) in the account into Fail-safe, until the query completes.\nAttention\nFail-safe is a data recovery service that is provided on a best effort basis and is intended only for use when all other recovery options have been attempted.\nFail-safe is not provided as a means for accessing historical data after the Time Travel retention period has ended. It is for use only by\nSnowflake to recover data that may have been lost or damaged due to extreme operational failures.\nData recovery through Fail-safe may take from several hours to several days to complete.\nSection Title: Understanding and viewing Fail-safe ¶ > View Fail-safe storage for your account ¶\nContent:\nWhen you review the total data storage usage for your account in Snowsight, you can view the\nhistorical data storage in Fail-safe.\nYou must use the ACCOUNTADMIN role to view the amount of data that is stored in Snowflake.\nIn Snowsight, follow these steps:\nIn the navigation menu, select Admin » Cost management , and then select Consumption .\nUse the Usage Type filter to select Storage .\nReview the graph and table for Fail-safe storage. The Storage Breakdown column in the table uses color-coded bars\nto represent the different kinds of storage, including Fail-safe storage. Hover the mouse pointer over\neach bar to see the size for each kind of storage.\n ... \nSection Title: Understanding and viewing Fail-safe ¶ > Considerations ¶\nContent:\nFor fail-safe and Snowpipe Streaming Classic, be aware of the following limitations:\nFail-safe doesn’t support tables that contain data ingested by Snowpipe Streaming Classic. For such tables, you can’t use fail-safe for recovery because fail-safe operations on that table will fail completely. For more information, see Snowpipe Streaming limitations .\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nWhat is Fail-safe?\nView Fail-safe storage for your account\nBilling for Fail-safe\nConsiderations\nRelated content\nUnderstanding & using Time Travel\nData storage considerations\nLanguage: **English**\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês"]},{"url":"https://articles.analytics.today/snowflake-best-practices-time-travel-fail-safe-and-data-retention","title":"Snowflake Time Travel & Fail-safe Best Practices (2025 Guide)","publish_date":"2025-08-11","excerpts":["Section Title: Maximize Protection and Control: Time Travel & Fail‑Safe Best Practices in Snowflake\nContent:\nUpdated\n•\n8 min read\n[](https://hashnode.com/@JohnRyan) [### John Ryan](https://hashnode.com/@JohnRyan)\nAfter 30 years of experience building multi-terabyte data warehouse systems, I spent five years at Snowflake as a Senior Solution Architect, helping customers across Europe and the Middle East deliver lightning-fast insights from their data.\nIn 2023, he joined [**Altimate.AI**](http://altimate.ai/) , which uses generative artificial intelligence to provide Snowflake performance and cost optimization insights and maximize customer return on investment.\nCertifications include Snowflake Data Superhero, Snowflake Subject Matter Expert, SnowPro Core, and SnowPro Advanced Architect.\nTags\n\\ \\\nViews\n1.8K views\nOn this page\nSection Title: Maximize Protection and Control: Time Travel & Fail‑Safe Best Practices in Snowflake\nContent:\nAvailable Options for Snowflake Data Recovery Do we need to Backup Snowflake Data? Snowflake Data Retention and Data Storage Tracking Snowflake Storage Cost Reducing Snowflake Stage Cost Reporting Snowflake Data Storage Usage Reducing Fail-Safe Storage Cost Reducing Time Travel Storage Cost Time Travel, Fail-safe, and Data Retention: Best Practices Conclusion\n**First Published:** August 5th 2024\nIn my previous article on [Snowflake Time Travel](https://articles.analytics.today/mastering-time-travel-in-snowflake-tips-and-techniques) , I explained the basics of how Snowflake deploys data versioning at the micro-partition level to support data recovery. In this article, I'll discuss some of the best practices for time travel, fail-safe, and data retention. In particular, I'll explain some common pitfalls and show how I helped customers save over $50,000 annually with simple changes.\n ... \nSection Title: ... > Reducing Fail-Safe Storage Cost\nContent:\nIn the case of [Automatic Clustering](https://articles.analytics.today/best-practices-to-maximize-query-performance-using-snowflake-clustering-keys) , frequent updates to clustered data often lead to data re-clustering, which on `permanent` tables results in large volumes of fail-safe storage. Likewise, frequent updates against tables that source Materialized Views can lead to high fail-safe.\nSection Title: ... > Reducing Time Travel Storage Cost\nContent:\nWhile it's relatively easy to identify Fail-Safe storage savings, time travel savings can be challenging.\nThe most common cause is simply setting a high value for the `DATA_RETENTION_TIME_IN_DAYS` at the account or database level. This can be adjusted by a user with ACCOUNT_ADMIN privilege using the following SQL:\n```\n-- Setting at the ACCOUNT level alter  account \n    set  data_retention_time_in_days  = 1 ;\n\n -- Setting at the DATABASE level alter  database edw\n    set  data_retention_time_in_days  = 1 ;\n```\nSection Title: ... > Reducing Time Travel Storage Cost\nContent:\nWhile the above SQL will solve the data retention problem, it may not be obvious that creating database or schema clones can lead to unexpectedly high storage costs. While working for another Snowflake customer, I [reduced storage costs](https://articles.analytics.today/how-to-cut-snowflake-data-storage-costs-with-zero-copy-clones) by 50% by dropping redundant clones, as explained in the [article here](https://articles.analytics.today/how-to-cut-snowflake-data-storage-costs-with-zero-copy-clones) .\n ... \nSection Title: ... > Conclusion\nContent:\nIn my experience with over 50 Snowflake customers across Europe and the Middle East, administrators are often unaware of the impact of data storage on the overall cost of operating Snowflake. However, knowing how Snowflake handles data recovery and the pitfalls of Time-Travel and Fail-Safe can pay dividends.\n[](https://analytics.today/Optimizing-Snowflake-Cost)"]},{"url":"https://docs.snowflake.com/en/user-guide/performance-query-storage","title":"Optimizing storage for performance - Snowflake Documentation","excerpts":["Section Title: Optimizing storage for performance ¶\nContent:\nThis topic discusses storage optimizations that can improve query performance, such as storing similar data together, creating optimized\ndata structures, and defining specialized data sets. Snowflake provides three of these storage strategies: automatic clustering, search\noptimization, and materialized views.\nIn general, these storage strategies do not substantially improve the performance of queries that already execute in a second or faster.\nThe strategies discussed in this topic are just one way to boost the performance of queries. For strategies related to the computing\nresources used to execute a query, refer to Optimizing warehouses for performance .\n ... \nSection Title: Optimizing storage for performance ¶ > Implementation and cost considerations ¶ > Ongoing cost ¶\nContent:\nHowever, reclustering can incur additional storage costs if it increases the size of Fail-safe storage. For details, refer to Credit and Storage Impact of Reclustering . Search Optimization / Materialized Views\nMaterialized views and the Search Optimization Service incur the cost of additional storage, which is billed at the standard rate."]},{"url":"https://www.montecarlodata.com/blog-snowflake-cost-optimization/","title":"5 Snowflake Cost Optimization Techniques You Should Know","publish_date":"2025-07-06","excerpts":["Section Title: ... > Step 4: Snowflake data lifecycle and retention optimization\nContent:\nHere’s where many teams miss significant cost savings opportunities. By default, Snowflake retains 1-day time-travel and 7-day fail-safe for all tables, which can dramatically increase storage costs, sometimes up to 90× more than the base table size for historical copies.\nUnderstanding the cost implications of Snowflake’s data retention features is crucial for cost optimization without getting too crazy about data recovery capabilities. Standard tables in Snowflake include both [Time Travel](https://docs.snowflake.com/en/user-guide/data-time-travel) , which can range from 1-90 days, and [Fail-safe](https://docs.snowflake.com/en/user-guide/data-failsafe) at 7 days, which means you’re paying for multiple copies of your data. But not every table needs this level of protection."]},{"url":"https://docs.snowflake.com/en/user-guide/data-time-travel","title":"Understanding & using Time Travel | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nGuides Business continuity & data recovery Time Travel\nSection Title: Understanding & using Time Travel ¶\nContent:\nSnowflake Time Travel enables accessing historical data (that is, data that has been changed or deleted) at any point within a defined period.\nIt serves as a powerful tool for performing the following tasks:\nRestoring objects that might have been accidentally or intentionally deleted. You can restore individual objects,\nsuch as tables, or restore all the objects inside a container object by restoring an entire schema or database.\nDuplicating and backing up data from key points in the past.\nAnalyzing data usage/manipulation over specified periods of time."]},{"url":"https://www.flexera.com/blog/finops/snowflake-query-tuning-part1/","title":"Snowflake query optimization: A comprehensive guide 2026 (part 1)","publish_date":"2026-01-27","excerpts":["Section Title: ... > 1) Choosing the Right Virtual Warehouse size\nContent:\nChoosing the “right-sized” Snowflake warehouse is critical for optimizing queries as it reduces Snowflake costs and maximizes Snowflake query performance.\nTo choose the ideal Snowflake warehouse size, it’s important to consider the specific needs/requirements of each individual query, like for complex queries that require extensive calculation, a larger warehouse is preferable; for simple queries, a smaller warehouse should suffice. Small warehouse sizes may also be sufficient for queries that don’t require much compute resources or for scenarios where cost optimization is a MAIN concern. Therefore, when scaling up or down the Snowflake warehouse size, it’s crucial to strike a perfect balance between the specific requirements of each query and select the appropriate or “right-sized” warehouse size to optimize query performance and reduce Snowflake costs."]},{"url":"https://yukidata.com/snowflake-finops-guide/","title":"Snowflake FinOps: Complete Guide to Automated Cost Optimization | Yuki","publish_date":"2025-09-19","excerpts":["SolutionsClose Solutions Open Solutions\nResourcesClose Resources Open Resources[Blog](https://yukidata.com/blog/) [Customers](https://yukidata.com/customers/) [News](https://yukidata.com/news/) [Documentation](https://yukidata.com/customers/)\n[About Us](https://yukidata.com/about/)\n[Contact Us](https://yukidata.com/contact/)\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization\nContent:\nBy Perry Tapiero\nSeptember 19, 2025 | 5 min read\nYour Snowflake bill increased 40% last quarter, but query performance actually got worse.\nSound familiar?\nAll FinOps organizations eventually run into this wall when they outgrow Snowflake’s basic auto-suspend and resource monitors. While Snowflake’s per-second billing offers flexibility, it also means a single efficient query can eat through hundreds of dollars – which is why manual warehouse management can’t keep pace with enterprise-sized workloads.\nThe bright side: modern Snowflake FinOps fixes this with automated systems that allow you to optimize spend and performance in real time.\nHow do you get to that point? Read on. We’ll share all of our FinOps best practices,e automation strategies, and real-world tactics that have helped enterprises cut monthly Snowflake costs by 30% or more."]}],"usage":[{"name":"sku_search","count":1}]}

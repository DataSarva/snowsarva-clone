{
  "extract_id": "extract_dbbb83ccee3b48df93f1c04636645c30",
  "results": [
    {
      "url": "https://www.snowflake.com/en/developers/guides/getting-started-cost-performance-optimization/",
      "title": "Getting Started with Cost and Performance Optimization",
      "publish_date": null,
      "excerpts": [
        "Data for Breakfast Around the World\n\nDrive impact across your organization with data and agentic intelligence.\n\nregister now\n\nSnowflake for Developers Guides Getting Started with Cost and Performance Optimization\n\n## Getting Started with Cost and Performance Optimization\n\nPraveen Purushothaman\n\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/getting-started-cost-performance-optimization)\n\n## **Overview**\n\nBy completing this guide, you will be able to understand and implement various optimization features on Snowflake.\n\n* **Setup Environment** : Use correct roles and sample datasets to use the optimization features\n* **Account Usage** : Understand purpose of Account Usage schema and use it to uncover savings opportunities\n* **Warehouse Controls** : Leverage settings on a virtual warehouse to optimize usage\n* **Storage** : Determine cost savings with high-churn and short-lived tables\n* **Optimization Features** : Utilize Snowflake optimization features to achieve cost or performance savings\n\n### **Prerequisites**\n\n* Familiarity with [Snowflake platform](https://docs.snowflake.com/en/user-guide/intro-key-concepts)\n* Basic understanding of [micro-partitions](https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions)\n* [Accountadmin](https://docs.snowflake.com/en/user-guide/security-access-control-considerations) access on a Snowflake account\n* If you do not have access to a Snowflake account, you can sign up for a [free trial](https://signup.snowflake.com/?utm_source=snowflake-devrel&utm_medium=developer-guides&utm_cta=developer-guides)\n\n### **What You\u2019ll Learn**\n\n* How to identify optimization patterns in your Snowflake account\n* How to implement performance improvements in your Snowflake environment\n\n## **Setup**\n\nThis section contains the code that needs to be executed in your Snowflake account to enable understanding of content in this guide.\n\n```\n```\n-- WAREHOUSE CREATION --\nUSE ROLE ACCOUNTADMIN;\n\ncreate warehouse if not exists hol_compute_wh\nwith warehouse_size='SMALL'\n     warehouse_type='STANDARD'\n     initially_suspended=TRUE\n     auto_resume=FALSE   \n;\n\nuse warehouse hol_compute_wh;\n\n-- DATABASE SCHEMA CREATION --\ncreate database if not exists OPT_HOL;\nuse database OPT_HOL;\ncreate schema if not exists DEMO;\n\nuse schema OPT_HOL.DEMO;\n\ncreate or replace table lineitem as select * from snowflake_sample_data.tpch_sf100.lineitem order by L_PARTKEY;\ncreate or replace table orders as select * from snowflake_sample_data.tpch_sf100.orders;\ncreate or replace table part as select * from snowflake_sample_data.tpch_sf100.part;\n\ncreate or replace table lineitem_cl as select * from lineitem;\nalter table lineitem_cl cluster by linear(l_shipdate);\n\ncreate or replace materialized view lineitem_mv as\nselect  \n        to_char(l_shipdate,'YYYYMM') as ship_month\n        ,l_orderkey\n        ,sum(l_quantity*l_extendedprice) as order_price\n        ,sum(l_quantity*l_discount) as order_discount\n        ,order_price-order_discount as net_selling_price\nfrom    \n        lineitem_cl\ngroup by\n        to_char(l_shipdate,'YYYYMM')\n        ,l_orderkey\n;\n\ncreate or replace table DATE_DIM\nas\nselect * from SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.DATE_DIM\n;\n\ncreate or replace table CATALOG_RETURNS\nas\nselect \n        cr.*\nfrom \n        SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.catalog_RETURNS cr\n        JOIN SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.DATE_DIM d\n          ON cr.cr_returned_date_sk=d.d_date_sk\nwhere   \n        d.d_year in (2001,2002)\n;\n```\n\nCopy\n```\n\n## **Warehouse Controls**\n\nThis section covers the code for controls that can be enforced on virtual warehouses.\n\n#### SQL\n\n```\n```\n-- SETTING CONTEXT FOR THE SESSION --\nUSE ROLE ACCOUNTADMIN;\n\n-- Check what parameters or settings are being used in a warehouse \nshow parameters for warehouse hol_compute_wh;\n\n-- Setting Auto suspend for a warehouse, value in seconds\nalter warehouse hol_compute_wh set auto_suspend=60;\n\n-- Setting Auto Resume for a warehouse\nalter warehouse hol_compute_wh set auto_resume=TRUE;\n\n-- Changing Statement Timeout at account level\nalter account set statement_timeout_in_seconds=7200;\n\n-- Changing Statement Timeout at warehouse level\nalter warehouse hol_compute_wh set statement_timeout_in_seconds=3600;\n\n-- Create a resource monitor\nCREATE OR REPLACE RESOURCE MONITOR Credits_Quota_Monitoring\n  WITH CREDIT_QUOTA = 5000\n       NOTIFY_USERS = (JDOE, \"Jane Smith\", \"John Doe\")\n  TRIGGERS ON 75 PERCENT DO NOTIFY\n           ON 100 PERCENT DO SUSPEND\n           ON 110 PERCENT DO SUSPEND_IMMEDIATE;\n\n-- Activating a resource monitor\nalter warehouse hol_compute_wh set resource_monitor=Credits_Quota_Monitoring;\n```\n\nCopy\n```\n\n## **Account Usage Queries**\n\n* [Account Usage](https://docs.snowflake.com/en/sql-reference/account-usage) is a powerful tool in an administrator's toolbox to identify optimization opportunites. Apart from metadata about objects in the Snowflake account, it contains usage metrics related to all services consumed in the account - Credits, Storage, Data Transfer.\n* In addition, [Query History](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) contains all information related to a query - metrics for each operation in a query, rows scanned, warehouse used, query text etc. The below queries are examples for viewing results of different views in Account Usage schema.\n\n#### SQL\n\n```\n```\n-- SETTING CONTEXT FOR THE SESSION --\nUSE ROLE ACCOUNTADMIN;\nUSE SCHEMA SNOWFLAKE.ACCOUNT_USAGE;\n\nuse warehouse hol_compute_wh;\n\n-- SAMPLE ACCOUNT USAGE QUERIES\n-- Warehouse Usage Metrics\nselect * from snowflake.account_usage.warehouse_metering_history limit 10;\n\n-- Access History for objects used in queries\nselect * from snowflake.account_usage.access_history limit 10;\n\n-- Snowpipe Usage Metrics\nselect * from snowflake.account_usage.pipe_usage_history limit 10;\n\n-- Storage Metrics\nselect * from snowflake.account_usage.storage_usage limit 10;\n\n-- Table Storage Detailed Metrics\nselect * from snowflake.account_usage.table_storage_metrics limit 10;\n```\n\nCopy\n```\n\n## **Storage Usage Monitoring**\n\nThis section covers the code to identify high churn tables - significant DML, short lived tables - tables truncated and reloaded everyday and tables not active in the past 90 days.\n\n#### SQL\n\n```\n```\n-- SETTING CONTEXT FOR THE SESSION ----\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE hol_compute_wh;\n\n-- Identify high churn tables or short lived tables\nSELECT\n        t.table_catalog||'.'||t.table_schema||'.'||t.table_name as fq_table_name\n        ,t.active_bytes/power(1024,3) as active_size_gb\n        ,t.time_travel_bytes/power(1024,3) as time_travel_gb\n        ,t.failsafe_bytes/power(1024,3) as failsafe_gb\n        ,t.retained_for_clone_bytes/power(1024,3) as clone_retain_gb\n        ,active_size_gb+time_travel_gb+failsafe_gb+clone_retain_gb as total_size_gb\n        ,(t.time_travel_bytes + t.failsafe_bytes + t.retained_for_clone_bytes)/power(1024,3) as non_active_size_gb\n       ,div0(non_active_size_gb,active_size_gb)*100 as churn_pct\n        ,t.deleted\n        ,timediff('hour',t.table_created,t.table_dropped) as table_life_duration_hours\n        ,t1.is_transient\n        ,t1.table_type\n        ,t1.retention_time\n        ,t1.auto_clustering_on\n        ,t1.clustering_key\n        ,t1.last_altered\n        ,t1.last_ddl\nFROM\n        snowflake.account_usage.table_storage_metrics t\n        JOIN snowflake.account_usage.tables t1\n          ON t.id=t1.table_id\nWHERE\n        1=1\n        --AND t1.table_catalog in ('','') -- use this to filter on specific databases\n        AND \n            (\n             churn_pct>=40\n\n             table_life_duration_hours<=24  -- short lived tables\n            )\nORDER BY total_size_gb desc;\n\n-- Unused tables\n-- Identify Table sizes and Last DDL/DML Timestamps\nSELECT TABLE_CATALOG || '.' || TABLE_SCHEMA || '.' || TABLE_NAME AS TABLE_PATH\n       ,TABLE_NAME\n       ,TABLE_SCHEMA AS SCHEMA\n       ,TABLE_CATALOG AS DATABASE\n       ,BYTES\n       ,TO_NUMBER(BYTES / POWER(1024,3),10,2) AS GB\n       ,LAST_ALTERED AS LAST_USE\n       ,DATEDIFF('Day',LAST_USE,CURRENT_DATE) AS DAYS_SINCE_LAST_USE\nFROM INFORMATION_SCHEMA.TABLES\nWHERE DAYS_SINCE_LAST_USE > 90 --Use your Days Threshold\nORDER BY BYTES DESC;\n \n-- Tables not used in any query in the last 90 days\nWITH access_history as\n(   \nSELECT  \n        distinct\n        split(base.value:objectName, '.')[0]::string as DATABASE_NAME\n        ,split(base.value:objectName, '.')[1]::string as SCHEMA_NAME\n        ,split(base.value:objectName, '.')[2]::string as TABLE_NAME\nFROM snowflake.account_usage.access_history \n     ,lateral flatten (base_objects_accessed) base\nwhere query_start_time between current_date()-90 and current_date()\n)\nSELECT  tbl.table_catalog||'.'||tbl.table_schema||'.'||tbl.table_name as FQ_table_name\nFROM    snowflake.account_usage.tables tbl\n        LEFT JOIN access_history ah\n          ON tbl.table_name=ah.table_name\n         AND tbl.table_schema=ah.schema_name\n         AND tbl.table_catalog=ah.database_name\nWHERE   ah.table_name is NULL\n        AND tbl.deleted is null\n;\n```\n\nCopy\n```\n\n#### Actions from query results\n\n* Decide on time travel setting for high churn tables\n* Consider using transient table type for high churn tables and short lived tables\n* Investigate the business value of tables that haven't been used in the last 90 days\n\n## **Automatic Clustering**\n\nThis section covers [Automatic Clustering](https://docs.snowflake.com/en/user-guide/tables-auto-reclustering) . Automatic Clustering is a Snowflake managed service that manages reclustering (as needed) of clustered tables. Reclustering is the process of reordering data in tables to colocate rows that have same cluster key values, which reduces the number of micro-partitions that need to be scanned during execution of a query thereby reducing execution times and help with efficient query execution on smaller sized warehouses.\n\n#### SQL\n\n```\n```\n---- SETTING CONTEXT FOR THE SESSION ----\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE hol_compute_wh;\nUSE SCHEMA OPT_HOL.DEMO;\n\n-- Query to show Clustering information on a non-clustered table\nSELECT SYSTEM$CLUSTERING_INFORMATION('LINEITEM','LINEAR(L_SHIPDATE)');\n\n-- Executing a query on an non-clustered table\n-- Ensuring that we are not using cached results\nalter session set USE_CACHED_RESULT=false;\n\nSELECT  *\nFROM    lineitem\nWHERE   l_shipdate BETWEEN '1995-01-01' AND '1995-03-01'\n;\n\n-- uery to show Clustering information on a clustered table\nSELECT SYSTEM$CLUSTERING_INFORMATION('LINEITEM_CL');\n\n-- Executing a query on an clustered table\nSELECT  *\nFROM    lineitem_cl \nWHERE   l_shipdate BETWEEN '1995-01-01' AND '1995-03-01'\n;\n```\n\nCopy\n```\n\n#### Results Screenshot\n\n* Unclustered table\n* Clustered table\n\n#### Outcome\n\n* Automatic clustering improves query performance by scanning less data (less micropartitions). Refer to [Clustering Considerations](https://docs.snowflake.com/en/user-guide/tables-clustering-keys) for clustering considerations and choosing the right clustering key for a table.\n\n## **Materialized Views**\n\nThis section covers use of [Materialized Views](https://docs.snowflake.com/en/user-guide/views-materialized) as an option to optimize Snowflake workloads. A materialized view is a pre-computed data set derived from a query specification and stored for later use. Because the data is pre-computed, querying a materialized view is faster than executing a query against the base table of the view. This performance difference can be significant when a query is run frequently or is sufficiently complex. As a result, materialized views can speed up expensive aggregation, projection, and selection operations, especially those that run frequently and that run on large data sets.\n\n#### SQL\n\n```\n```\n---- SETTING CONTEXT FOR THE SESSION ----\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE hol_compute_wh;\nUSE SCHEMA OPT_HOL.DEMO;\n\n-- Let's say a user ran this query without knowing a Materialized View exists\n-- After execution of the query, check the query profile \nSELECT  \n        to_char(l_shipdate,'YYYYMM') as ship_month\n        ,l_orderkey\n        ,sum(l_quantity*l_extendedprice) as order_price\n        ,sum(l_quantity*l_discount) as order_discount\nFROM    \n        lineitem_cl\nWHERE\n        l_orderkey between 1000000 and 2000000\nGROUP BY \n        ALL;\n```\n\nCopy\n```\n\n#### Screenshot\n\n* The image below shows the use of a materialized view even if a user query does not contain the materialized view.\n\n#### Outcome\n\n* Refer to [Materialized Views Best Practices](https://docs.snowflake.com/en/user-guide/views-materialized) for considerations on choosing materialized views.\n\n## **Query Acceleration**\n\nThis section covers use of [Query Acceleration Service](https://docs.snowflake.com/en/user-guide/query-acceleration-service) which can accelerate parts of a query workload in a warehouse. When it is enabled for a warehouse, it can improve overall warehouse performance by reducing the impact of outlier queries, which are queries that use more resources than the typical query. The query acceleration service does this by offloading portions of the query processing work to shared compute resources that are provided by the service.\n\nExamples of the types of workloads that might benefit from the query acceleration service include:\n\n* Ad hoc analytics.\n* Workloads with unpredictable data volume per query.\n* Queries with large scans and selective filters.\n\nThe query acceleration service can handle these types of workloads more efficiently by performing more work in parallel and reducing the wall-clock time spent in scanning and filtering.\n\n#### SQL\n\n```\n```\n-- SETTING CONTEXT FOR THE SESSION --\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE hol_compute_wh;\nUSE SCHEMA OPT_HOL.DEMO;\n\n-- After execution of the query, check the query profile \n-- before and after enabling Query Acceleration on the warehouse\nSELECT \n        i_brand\n        ,sum(ss_quantity)\n        ,sum(ss_wholesale_cost)\n        ,sum(ss_sales_price)\n        ,sum(ss_list_price) \nFROM \n        snowflake_sample_data.tpcds_sf10tcl.store_sales ss \n        JOIN  snowflake_sample_data.tpcds_sf10tcl.Item i\n          ON  i.i_item_sk=ss.ss_item_sk\nWHERE \n        ss_store_sk=946\nGROUP BY\n        i_brand\n;\n\nalter warehouse hol_compute_wh \nset enable_query_acceleration=true \n    query_acceleration_max_scale_factor = 4;\n\n-- Find Queries that could be accelerated (for cost consistency, best to find an application workload with consistent query \"templates\").\n-- \"Trusted\" user warehouses are also excellent use cases for QAS (but will accelerate poorly written queries)\nSELECT \n        LEFT(qh.QUERY_TEXT,25) as QueryCat\n        ,qh.USER_NAME\n        ,qae.WAREHOUSE_NAME\n        ,COUNT(*) as QueryCount\n        ,AVG(qae.UPPER_LIMIT_SCALE_FACTOR) as AvgScaleFactor\n        ,AVG(ELIGIBLE_QUERY_ACCELERATION_TIME) as AvgTimeSavings\n        ,MAX(UPPER_LIMIT_SCALE_FACTOR) as MaxScaleFactor\n        ,MIN(UPPER_LIMIT_SCALE_FACTOR) as MinScaleFactor\n        ,SUM(ELIGIBLE_QUERY_ACCELERATION_TIME) as TotalAccelerationTime\nFROM    \n        SNOWFLAKE.ACCOUNT_USAGE.QUERY_ACCELERATION_ELIGIBLE qae\n        JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY qh \n          ON qh.query_id = qae.query_id\nWHERE \n        qae.WAREHOUSE_NAME IN ('')\n        AND USER_NAME = ''\n        AND ELIGIBLE_QUERY_ACCELERATION_TIME > 120\n        AND qae.START_TIME >= CURRENT_DATE() - 7\nGROUP BY \n        ALL\nORDER BY \n        TotalAccelerationTime DESC\nLIMIT 1000;\n\n-- Isolate the application queries that can be pulled together into a single warehouse\nSELECT  \n        qae.*\n        ,qh.USER_NAME\n        ,qh.ROLE_NAME\nFROM \n        SNOWFLAKE.ACCOUNT_USAGE.QUERY_ACCELERATION_ELIGIBLE qae\n        JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY qh \n          ON qh.query_id = qae.query_id\nWHERE \n        qae.WAREHOUSE_NAME IN ('')\n        AND USER_NAME = ''\n        AND ELIGIBLE_QUERY_ACCELERATION_TIME > 120\n        AND qae.START_TIME >= CURRENT_DATE() - 7\nLIMIT 1000;\n\nSELECT SYSTEM$ESTIMATE_QUERY_ACCELERATION('');\n```\n\nCopy\n```\n\n#### Results Screenshot\n\n#### Outcome\n\n* Refer to [Evaluating Cost and Performance](https://docs.snowflake.com/en/user-guide/query-acceleration-service) to understand impact of Query Acceleration on workloads in your account.\n\n## **Search Optimization**\n\nThis section covers use of [Search Optimization](https://docs.snowflake.com/en/user-guide/search-optimization-service) which can significantly improve the performance of certain types of lookup and analytical queries. The search optimization service aims to significantly improve the performance of certain types of queries on tables, such as:\n\n* Selective point lookup queries on tables\n* Substring and regular expression searches\n* Queries on fields in VARIANT, OBJECT, and ARRAY (semi-structured) columns that use the following types of predicates: EQUALITY, IN, ARRAY\\_CONTAINS, ARRAYS\\_OVERLAP etc.\n* Queries that use selected geospatial functions with GEOGRAPHY values\n\nThe search optimization service is generally transparent to users. Queries work the same as they do without search optimization; some are just faster. To improve performance of search queries, the search optimization service creates and maintains a persistent data structure called a search access path. The search access path keeps track of which values of the table\u2019s columns might be found in each of its micro-partitions, allowing some micro-partitions to be skipped when scanning the table.\n\nA maintenance service is responsible for creating and maintaining the search access path.\n\n#### SQL\n\n```\n```\n-- SETTING CONTEXT FOR THE SESSION --\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE hol_compute_wh;\nUSE SCHEMA OPT_HOL.DEMO;\n\nSELECT SYSTEM$ESTIMATE_SEARCH_OPTIMIZATION_COSTS('OPT_HOL.DEMO.CATALOG_RETURNS')\n  AS estimate_for_table_without_search_optimization;\n\nSELECT SYSTEM$ESTIMATE_SEARCH_OPTIMIZATION_COSTS('OPT_HOL.DEMO.CATALOG_RETURNS', 'EQUALITY(CR_ITEM_SK)')\n  AS estimate_for_columns_without_search_optimization;\n\nSELECT SYSTEM$ESTIMATE_SEARCH_OPTIMIZATION_COSTS('OPT_HOL.DEMO.CATALOG_RETURNS', 'EQUALITY(CR_ITEM_SK,CR_RETURNED_DATE_SK)')\n  AS estimate_for_columns_without_search_optimization;\n\nSELECT SYSTEM$ESTIMATE_SEARCH_OPTIMIZATION_COSTS('OPT_HOL.DEMO.LINEITEM', 'SUBSTRING(L_SHIPMODE)')\n  AS estimate_for_columns_without_search_optimization;\n```\n\nCopy\n```\n\n#### Results Screenshot\n\n#### Outcome\n\n* Refer to [Search Optimization Cost Estimation and Management](https://docs.snowflake.com/en/user-guide/search-optimization/cost-estimation) for Search Optimization cost management considerations.\n\n## Conclusion And Resources\n\nCongratulations! You have learned about optimization features and tools to assist in your quest to optimize workloads on your Snowflake account. Apart from the features and options discussed in this guide, the below mentioned resources are worth taking a look to get guidance to optimize workloads on Snowflake.\n\n### What You Learned\n\n* How to set up warehouse controls to optimize warehouse usage\n* How to identify savings opportunities for table storage\n* How to implement automatic clustering, materialized views, query acceleration or search optimization service to improve performance of Snowflake workloads\n\n#### Call to Action\n\n* Definitive Guide to managing spend in Snowflake .\n* [Snowflake Education](https://learn.snowflake.com/en/)\n* Professional Services\n\nUpdated Dec 20, 2025\n\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\n\n\\*\n\n\\*\n\n\\* Country United States Canada United Kingdom Germany France Australia Japan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo The Democratic Republic of The Cook Islands Costa Rica Cote D'Ivoire (Ivory Coast) Croatia (Hrvatska) Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Heard and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jordan Kazakhstan Kenya Kiribati Korea Republic of (South) Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia Federated States of Moldova Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Occupied Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Reunion Romania Russian Federation Saint Helena Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and The South Sandwich Island Spain Sri Lanka Suriname Svalbard and Jan Mayen Islands Swaziland Sweden Switzerland Taiwan Tajikistan Tanzania United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Venezuela Viet Nam Virgin Islands (British) Virgin Islands (U.S.) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe\n\n\\*\n\nAdd me to the list to receive dedicated product updates and general availability emails.\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\n\nSubscribe Now\n\nIndustries * Advertising, Media & Entertainment\n* Financial Services\n* Healthcare & Life Sciences\n* Manufacturing\n* Public Sector\n* Retail & Consumer Goods\n* Technology\n\nLearn * Resource Library\n* Live Demos\n* Fundamentals\n* Training\n* Certifications\n* Snowflake University\n* Developer Guides\n* Documentation\n\n* Privacy Policy\n* Site Terms\n* Communication Preferences\n* Cookie Settings\n* Do Not Share My Personal Information\n* Legal\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 1
    }
  ]
}

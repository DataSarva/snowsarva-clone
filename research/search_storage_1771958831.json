{"search_id":"search_4016f152b3454ee5bea7d29c75e894d6","results":[{"url":"https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs","title":"Storage costs for Time Travel and Fail-safe | Snowflake Documentation","excerpts":["Section Title: Storage costs for Time Travel and Fail-safe ¶\nContent:\nStorage fees are incurred for maintaining historical data during both the Time Travel and Fail-safe periods.\nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Storage usage and fees ¶\nContent:\nThe fees are calculated for each 24-hour period (that is, 1 day) from the time that the data changed. The number of days that Snowflake maintains\nhistorical data is based on the table type and the Time Travel retention period for the table.\nAlso, Snowflake minimizes the amount of storage required for historical data by maintaining only the information required to restore the individual table rows that were updated or deleted. As a result,\nstorage usage is calculated as a percentage of the table that changed. Snowflake only maintains full copies of tables when tables are dropped or truncated.\nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Temporary and transient tables ¶\nContent:\nTo help manage the storage costs associated with Time Travel and Fail-safe, Snowflake provides two table types, temporary and transient, which do not incur the same fees as standard (that is, permanent) tables:\nTransient tables can have a Time Travel retention period of either 0 or 1 day.\nTemporary tables can also have a Time Travel retention period of 0 or 1 day; however, this retention period ends as soon as the table is dropped or the session in which the table was created ends.\nTransient and temporary tables have no Fail-safe period.\nAs a result, the maximum additional fees incurred for Time Travel and Fail-safe by these types of tables is limited to 1 day. The following table illustrates the different scenarios, based on\ntable type:\nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Temporary and transient tables ¶\nContent:\n| Table Type | Time Travel Retention Period (Days) | Fail-safe Period (Days) | Min , Max Historical Data Maintained (Days) |\n| Permanent | 0 or 1 (for Snowflake Standard Edition) | 7 | **7 , 8** |\n| 0 to 90 (for Snowflake Enterprise Edition) | 7 | **7 , 97** |  |\n| Transient | 0 or 1 | 0 | **0 , 1** |\n| Temporary | 0 or 1 | 0 | **0 , 1** |\n ... \nSection Title: ... > Considerations for using temporary and transient tables to manage storage costs ¶\nContent:\nTemporary tables are dropped when the session in which they were created ends. Data stored in temporary tables is not recoverable after the table is dropped.\nHistorical data in transient tables can’t be recovered by Snowflake after the Time Travel retention period ends. Use transient tables only for data you can replicate or reproduce\nindependently from Snowflake.\nLong-lived tables, such as fact tables, should always be defined as permanent to ensure they are fully protected by Fail-safe.\nYou can define short-lived tables as transient to eliminate Fail-safe costs. For example, you might use transient tables for data with a lifetime of less than 1 day, such as ETL work tables.\nIf downtime and the time required to reload lost data are factors, permanent tables, even with their added Fail-safe costs, might offer a better overall solution than transient tables.\nNote\n ... \nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Cost for backups ¶\nContent:\nThe following table describes charges for backups.\nFor information about credit consumption, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\n ... \nSection Title: Storage costs for Time Travel and Fail-safe ¶ > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies"]},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization - Snowflake","excerpts":["Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n| Metric Category | Description | Key Metrics | Primary Data Sources |\n| Compute & query metrics | Understand the cost of query execution, warehouse consumption, and overall compute health. These are often the most dynamic and largest portion of your spend. | - Credits used: total credits by warehouse |  |\n| - Query performance: execution time, bytes scanned, compilation time, parameterized query hash |  |  |  |\n| - Warehouse health: % idle time, queueing, spilling, concurrency | - `ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY` (hourly warehouse credit usage) |  |  |\n| - `ACCOUNT_USAGE.QUERY_HISTORY` (detailed query metrics and associated warehouses) |  |  |  |\n| Storage metrics | Costs for compressed data, including active data, Time Travel, and Fail‑safe. | - Storage volume (avg monthly compressed GB/TB) |  |\n| - Inactive storage (Time Travel, Fail‑safe) |  |  |  |\n| - Storage growth rates |  |  |  |\n ... \nSection Title: Cost Optimization > Optimize > Overview > Data storage types & lifecycle management\nContent:\n**Review policy-driven data lifecycle management:**\n**Time Travel & Fail-safe:** Set the [DATA_RETENTION_TIME_IN_DAYS](https://docs.snowflake.com/en/sql-reference/parameters) parameter on a per-table or per-schema basis to the minimum required\nfor your business needs. For transient data, use [TRANSIENT](https://docs.snowflake.com/en/user-guide/tables-temp-transient) tables to eliminate Fail-safe costs.\n**Retained for clone:** Be mindful of cloning operations. While\nzero-copy cloning is cost-effective initially, any subsequent DML\n(Data Manipulation Language) operations on the clone will create new\nmicro-partitions, increasing storage costs. It is recommended to\ndrop clones when they are no longer needed.\n**Be aware of high-churn tables:**\nSection Title: Cost Optimization > Optimize > Overview > Data storage types & lifecycle management\nContent:\nIf a table is updated consistently, inactive storage (Time Travel &\nFail-safe data) can grow at a much faster rate than active storage.\nA high churn table is generally characterized as one that has 40% or\nmore of its storage inactive. Therefore, aligning both the retention\ntime and the use of an appropriate table type with business and\nrecovery requirements is paramount to keeping costs under control.\nReview High Churn tables on a consistent basis to ensure their\nconfiguration is as desired."]},{"url":"https://docs.snowflake.com/en/user-guide/data-failsafe","title":"Understanding and viewing Fail-safe | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg™\nApache Iceberg™ Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Business continuity & data recovery Fail-safe\nSection Title: Understanding and viewing Fail-safe ¶\nContent:\nSeparate and distinct from Time Travel, Fail-safe ensures historical data is protected in the event of a system failure or other event (e.g. a\nsecurity breach).\nSection Title: Understanding and viewing Fail-safe ¶ > What is Fail-safe? ¶\nContent:\nFail-safe provides a (non-configurable) 7-day period during which historical data may be recoverable by Snowflake. This period starts\nimmediately after the Time Travel retention period ends. Note, however, that a long-running Time Travel query will delay moving any data and\nobjects (tables, schemas, and databases) in the account into Fail-safe, until the query completes.\nAttention\nFail-safe is a data recovery service that is provided on a best effort basis and is intended only for use when all other recovery options have been attempted.\nFail-safe is not provided as a means for accessing historical data after the Time Travel retention period has ended. It is for use only by\nSnowflake to recover data that may have been lost or damaged due to extreme operational failures.\nData recovery through Fail-safe may take from several hours to several days to complete.\nSection Title: Understanding and viewing Fail-safe ¶ > View Fail-safe storage for your account ¶\nContent:\nWhen you review the total data storage usage for your account in Snowsight, you can view the\nhistorical data storage in Fail-safe.\nYou must use the ACCOUNTADMIN role to view the amount of data that is stored in Snowflake.\nIn Snowsight, follow these steps:\nIn the navigation menu, select Admin » Cost management , and then select Consumption .\nUse the Usage Type filter to select Storage .\nReview the graph and table for Fail-safe storage. The Storage Breakdown column in the table uses color-coded bars\nto represent the different kinds of storage, including Fail-safe storage. Hover the mouse pointer over\neach bar to see the size for each kind of storage.\n ... \nSection Title: Understanding and viewing Fail-safe ¶ > Considerations ¶\nContent:\nFor fail-safe and Snowpipe Streaming Classic, be aware of the following limitations:\nFail-safe doesn’t support tables that contain data ingested by Snowpipe Streaming Classic. For such tables, you can’t use fail-safe for recovery because fail-safe operations on that table will fail completely. For more information, see Snowpipe Streaming limitations .\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nWhat is Fail-safe?\nView Fail-safe storage for your account\nBilling for Fail-safe\nConsiderations\nRelated content\nUnderstanding & using Time Travel\nData storage considerations\nLanguage: **English**\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês"]},{"url":"https://articles.analytics.today/snowflake-best-practices-time-travel-fail-safe-and-data-retention","title":"Snowflake Time Travel & Fail-safe Best Practices (2025 Guide)","publish_date":"2025-08-11","excerpts":["Section Title: Maximize Protection and Control: Time Travel & Fail‑Safe Best Practices in Snowflake\nContent:\nUpdated\n•\n8 min read\n[](https://hashnode.com/@JohnRyan) [### John Ryan](https://hashnode.com/@JohnRyan)\nAfter 30 years of experience building multi-terabyte data warehouse systems, I spent five years at Snowflake as a Senior Solution Architect, helping customers across Europe and the Middle East deliver lightning-fast insights from their data.\nIn 2023, he joined [**Altimate.AI**](http://altimate.ai/) , which uses generative artificial intelligence to provide Snowflake performance and cost optimization insights and maximize customer return on investment.\nCertifications include Snowflake Data Superhero, Snowflake Subject Matter Expert, SnowPro Core, and SnowPro Advanced Architect.\nTags\n\\ \\\nViews\n1.8K views\nOn this page\nSection Title: Maximize Protection and Control: Time Travel & Fail‑Safe Best Practices in Snowflake\nContent:\nAvailable Options for Snowflake Data Recovery Do we need to Backup Snowflake Data? Snowflake Data Retention and Data Storage Tracking Snowflake Storage Cost Reducing Snowflake Stage Cost Reporting Snowflake Data Storage Usage Reducing Fail-Safe Storage Cost Reducing Time Travel Storage Cost Time Travel, Fail-safe, and Data Retention: Best Practices Conclusion\n**First Published:** August 5th 2024\nIn my previous article on [Snowflake Time Travel](https://articles.analytics.today/mastering-time-travel-in-snowflake-tips-and-techniques) , I explained the basics of how Snowflake deploys data versioning at the micro-partition level to support data recovery. In this article, I'll discuss some of the best practices for time travel, fail-safe, and data retention. In particular, I'll explain some common pitfalls and show how I helped customers save over $50,000 annually with simple changes.\n ... \nSection Title: ... > Reducing Fail-Safe Storage Cost\nContent:\nIn the case of [Automatic Clustering](https://articles.analytics.today/best-practices-to-maximize-query-performance-using-snowflake-clustering-keys) , frequent updates to clustered data often lead to data re-clustering, which on `permanent` tables results in large volumes of fail-safe storage. Likewise, frequent updates against tables that source Materialized Views can lead to high fail-safe.\nSection Title: ... > Reducing Time Travel Storage Cost\nContent:\nWhile it's relatively easy to identify Fail-Safe storage savings, time travel savings can be challenging.\nThe most common cause is simply setting a high value for the `DATA_RETENTION_TIME_IN_DAYS` at the account or database level. This can be adjusted by a user with ACCOUNT_ADMIN privilege using the following SQL:\n```\n-- Setting at the ACCOUNT level alter  account \n    set  data_retention_time_in_days  = 1 ;\n\n -- Setting at the DATABASE level alter  database edw\n    set  data_retention_time_in_days  = 1 ;\n```\nSection Title: ... > Reducing Time Travel Storage Cost\nContent:\nWhile the above SQL will solve the data retention problem, it may not be obvious that creating database or schema clones can lead to unexpectedly high storage costs. While working for another Snowflake customer, I [reduced storage costs](https://articles.analytics.today/how-to-cut-snowflake-data-storage-costs-with-zero-copy-clones) by 50% by dropping redundant clones, as explained in the [article here](https://articles.analytics.today/how-to-cut-snowflake-data-storage-costs-with-zero-copy-clones) .\n ... \nSection Title: ... > Conclusion\nContent:\nIn my experience with over 50 Snowflake customers across Europe and the Middle East, administrators are often unaware of the impact of data storage on the overall cost of operating Snowflake. However, knowing how Snowflake handles data recovery and the pitfalls of Time-Travel and Fail-Safe can pay dividends.\n[](https://analytics.today/Optimizing-Snowflake-Cost)"]},{"url":"https://www.montecarlodata.com/blog-snowflake-cost-optimization/","title":"5 Snowflake Cost Optimization Techniques You Should Know","publish_date":"2025-07-06","excerpts":["Section Title: ... > Is there a Snowflake object that does not consume any storage costs?\nContent:\nYou can get started with Snowflake for free with a [trial account](https://docs.snowflake.com/en/user-guide/admin-trial-account) , and the balance of your free usage decreases as you consume credits to use compute and accrue storage costs.\nIn addition, temporary and transient tables do not incur the same fees as standard permanent tables. This helps to manage the storage costs associated with time travel and fail-safe.\nCourtesy of [Snowflake.](https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs)\nAll of this is to say that Snowflake pricing is mainly driven by credit consumption, which is mainly driven by storage and compute; **which is mainly driven by the amount of tables in your environment, the SQL queries being run on those tables, and the sizes of your data warehouses** .\nSection Title: ... > Is there a Snowflake object that does not consume any storage costs?\nContent:\nThat is why our Snowflake cost optimization strategy will focus on optimizing those three areas by leveraging native features, best practices, and other solutions.\n ... \nSection Title: 5 Simple Steps For Snowflake Cost Optimization Without Getting Too Crazy > ... > Unused tables\nContent:\n“That’s the beauty of Monte Carlo because it allows us to see who is using data and where it is being consumed. This has allowed us to actually free up some of our processing time from unused data elements which no one was using anymore and were no longer relevant,” [said Valerie Rogoff](https://www.techtarget.com/searchdatamanagement/news/252509032/Monte-Carlo-boosts-data-pipeline-observability-insights) , director of analytics data architecture at ShopRunner.\nSection Title: ... > Step 4: Snowflake data lifecycle and retention optimization\nContent:\nHere’s where many teams miss significant cost savings opportunities. By default, Snowflake retains 1-day time-travel and 7-day fail-safe for all tables, which can dramatically increase storage costs, sometimes up to 90× more than the base table size for historical copies.\nUnderstanding the cost implications of Snowflake’s data retention features is crucial for cost optimization without getting too crazy about data recovery capabilities. Standard tables in Snowflake include both [Time Travel](https://docs.snowflake.com/en/user-guide/data-time-travel) , which can range from 1-90 days, and [Fail-safe](https://docs.snowflake.com/en/user-guide/data-failsafe) at 7 days, which means you’re paying for multiple copies of your data. But not every table needs this level of protection.\nSection Title: ... > Time-travel vs transient vs temporary tables\nContent:\n[Transient tables](https://docs.snowflake.com/en/user-guide/tables-temp-transient) include Time Travel but no Fail-safe, cutting storage costs significantly. These are perfect for staging tables, temporary analytics, or frequently refreshed data where you don’t need extensive recovery options. You can create new tables as transient or convert existing tables to transient status.\n[Temporary tables](https://docs.snowflake.com/en/user-guide/tables-temp-transient) also include Time Travel but no Fail-safe, and are automatically dropped at session end. These are ideal for intermediate processing steps within ETL pipelines where the data is only needed temporarily.\n ... \nSection Title: ... > Zero-copy cloning for development\nContent:\nOne of Snowflake’s most cost-effective features is [zero-copy cloning](https://docs.snowflake.com/en/user-guide/object-clone.html) . Clones leverage Time Travel mechanics without consuming additional storage until the data diverges. This creates an instant copy of your production data for development or testing without doubling storage costs. As the development environment makes changes, only the delta consumes additional storage."]},{"url":"https://docs.snowflake.com/en/user-guide/data-time-travel","title":"Understanding & using Time Travel | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nGuides Business continuity & data recovery Time Travel\nSection Title: Understanding & using Time Travel ¶\nContent:\nSnowflake Time Travel enables accessing historical data (that is, data that has been changed or deleted) at any point within a defined period.\nIt serves as a powerful tool for performing the following tasks:\nRestoring objects that might have been accidentally or intentionally deleted. You can restore individual objects,\nsuch as tables, or restore all the objects inside a container object by restoring an entire schema or database.\nDuplicating and backing up data from key points in the past.\nAnalyzing data usage/manipulation over specified periods of time."]},{"url":"https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/","title":"FinOps on Snowflake: Built-In Cost and Performance Control","excerpts":["Section Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\nRead the case study Financial Services “The speed at which we’ve delivered wouldn’t have been possible with other providers.” Kaushik Ghosh Dastidar\nHead of ESG Cloud Solutions, NatWest Read the story * **6x** reduction in onboarding time from 3 months to 2 weeks\n**$750K** saved in salaries & staff training costs\nRead the case study\n[Resource #### Snowflake Joins the FinOps Foundation Snowflake joins The FinOps Foundation as a Premier Enterprise Member to provide thought leadership and set industry financial best practices. Read more](https://www.finops.org/members/snowflake/)\nResource #### Snowflake Pricing Calculator Curious about Snowflake pricing? Our Snowflake pricing calculator shows credit usage, warehouse costs, and total expenses. Access calculator"]},{"url":"https://docs.snowflake.com/en/user-guide/performance-query-storage","title":"Optimizing storage for performance - Snowflake Documentation","excerpts":["Section Title: Optimizing storage for performance ¶\nContent:\nThis topic discusses storage optimizations that can improve query performance, such as storing similar data together, creating optimized\ndata structures, and defining specialized data sets. Snowflake provides three of these storage strategies: automatic clustering, search\noptimization, and materialized views.\nIn general, these storage strategies do not substantially improve the performance of queries that already execute in a second or faster.\nThe strategies discussed in this topic are just one way to boost the performance of queries. For strategies related to the computing\nresources used to execute a query, refer to Optimizing warehouses for performance .\n ... \nSection Title: Optimizing storage for performance ¶ > Implementation and cost considerations ¶ > Ongoing cost ¶\nContent:\nHowever, reclustering can incur additional storage costs if it increases the size of Fail-safe storage. For details, refer to Credit and Storage Impact of Reclustering . Search Optimization / Materialized Views\nMaterialized views and the Search Optimization Service incur the cost of additional storage, which is billed at the standard rate."]},{"url":"https://www.flexera.com/blog/finops/snowflake-native-apps/","title":"Snowflake Native Apps 101: Build and monetize data apps (2026)","publish_date":"2026-01-27","excerpts":["Section Title: ... > What Are Snowflake Native Applications?\nContent:\n**TL;DR:\n** **What They Are** — Snowflake Native Apps run entirely within Snowflake, eliminating the need to move data out for processing.\n**How They Work** — Snowflake Native Apps operate in your Snowflake account, using your existing data without external connections. Developers create and distribute them via the Snowflake Marketplace.\n**Why Use Them** :\n**Fast** — Snowflake Native Apps process data directly in Snowflake, reducing latency.\n**Secure** — Snowflake Native Apps don’t access or store data outside your account, relying on Snowflake’s encryption and access controls.\n**Scalable** — Snowflake Native Apps can grow alongside your Snowflake resources, handling larger datasets or more users.\nSnowflake Native Apps (Source: [Snowflake](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/) )\nSection Title: ... > 1) **Native Integration with Snowflake Services**\nContent:\nSnowflake Native Apps work directly with Snowflake’s core services. They use stored procedures , user-defined functions (Snowflake UDFs UDFs) and the Snowpark API, making them efficient and seamless.\n ... \nSection Title: ... > [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Req...\nContent:\nFebruary 22, 2023\nFinOps"]},{"url":"https://www.snowflake.com/en/blog/storage-lifecycle-policies-ga/","title":"Optimize Storage Costs and Simplify Compliance with Storage Lifecycle Policies, Now Generally Available","publish_date":"2025-11-15","excerpts":["Section Title: ... > Key benefits *\"At Block's scale, managing petabytes of security logs for compliance isn't j...\nContent:\nFigure 1: Example customer economics\nSection Title: ... > FinOps for Snowflake\nContent:\nThe FinOps for Snowflake on-demand course provides participants with a high-level overview of the FinOps framework within Snowflake.\n[enroll now](https://learn.snowflake.com/en/courses/OD-FINOPS/)\nSection Title: ... > Author\nContent:\nKaushal Jain"]}],"usage":[{"name":"sku_search","count":1}]}

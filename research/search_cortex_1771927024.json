{"search_id":"search_b2dcdfd501c84444a0e7642c9b5365a5","results":[{"url":"https://medium.com/@angelamarieharney/cortex-ai-cost-queries-in-snowflake-07331811d42d","title":"Cortex AI Cost Queries in Snowflake | by Angela Harney | Jan, 2026 | Medium","publish_date":"2026-02-10","excerpts":["Section Title: Cortex AI Cost Queries in Snowflake > Cortex Usage History Views > Cortex Analyst\nContent:\n*Usage is metered at a secondary level in the* `CORTEX_ANALYST_USAGE_HISTORY` *view and is aggregated hourly, showing credits consumed and number of messages per user. It does not include* `_query_id_` *, but offers per-user visibility into AI interaction costs. Costs are incurred per successful response (message) not by token count.*\nSection Title: Cortex AI Cost Queries in Snowflake > Cortex Usage History Views > Cortex Search\nContent:\n*Usage is metered at a secondary level in the* `CORTEX_SEARCH_DAILY_USAGE_HISTORY` *view and breaks down costs daily by consumption type (* `_serving_` *,* `_embed_text_tokens_` *) and includes costs for storage, embedding, and serving compute as follows:*\n`CORTEX_SEARCH_DAILY_USAGE_HISTORY` (serving vs. embedding)\n`CORTEX_SEARCH_SERVING_USAGE_HISTORY` (hourly serving credits)\nSection Title: Cortex AI Cost Queries in Snowflake > Cortex Usage History Views > Metering History\nContent:\n*A broader view that can be filtered by* `_SERVICE_TYPE = 'AI_SERVICES'_` *to track overall AI credit usage across all Cortex services, including AI functions, Analyst, and Search.*\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > ... > Query Patterns and Resource Allocation\nContent:\nEffective cost control still hinges on optimizing query patterns and resource allocation like all other data querying does.\nA common mistake is relying on simple duration-based cost estimates, which ignore concurrency and auto-suspend delays. Instead, use a normalized, warehouse-uptime-based method to allocate costs. Treat each warehouse runtime as a single event, then distribute costs proportionally based on query duration within that window.\nThis method accounts for idle time and concurrency, revealing true cost per query. For AI workloads, this approach helps identify inefficient models or redundant invocations that inflate spend.\nInference costs are especially sensitive to query complexity and model selection.\nSection Title: Cortex AI Cost Queries in Snowflake > ... > Query Patterns and Resource Allocation\nContent:\nReduce data scanned to answer AI prompts that have frequently used data chunks that are flattened and stored in separate columns. This takes advantage of Snowflake’s natural columnar storage optimizations.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > Other Cost Considerations and Monitoring > Resource Monitors\nContent:\nProactive cost optimization involves continuous monitoring and automation.\nResource monitors in Snowflake are designed to track and control credit consumption primarily for virtual warehouses and cloud services and are limited in their ability to monitor and limit AI Services costs.\nThey can only indirectly control the compute costs associated with executing AI functions by setting credit quotas and triggering alerts or suspensions when usage thresholds are exceeded on virtual warehouses where AI queries run.\nYou can assign a warehouse used for AI workloads to a resource monitor to prevent runaway compute spend, even though the AI token processing cost (serverless) is unaffected. For direct AI cost control, use budgets with monitoring on `AI_SERVICES` usage.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > Conclusion\nContent:\nIn conclusion, mastering AI cost querying in Snowflake requires a blend of technical precision and strategic governance.\nFocus on accurate cost attribution, optimized query patterns, smart resource sizing, and vigilance on egress.\nUse native tools like `ACCOUNT_USAGE` , `QUERY_ATTRIBUTION_HISTORY` , and `WAREHOUSE_METERING_HISTORY` to build a transparent cost model.\nCombine this with tagging, budgets, and anomaly detection to turn AI from a cost center into a value-driven capability.\nWith the right approach, Snowflake’s AI Data Cloud can deliver powerful insights without breaking the bank.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > APPENDIX I — Invoice Cost Details\nContent:\n'CORTEX_FINE_TUNING' AS SERVICE_NAME,  \n        SUM(TOKEN_CREDITS) AS CREDITS_USED,  \n        SUM(TOKEN_CREDITS) * << your credit price here >> AS COST,  \n        SUM(TOKENS) as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_FINE_TUNING_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n    -- CORTEX FUNCTIONS USAGE  \n    SELECT  \n        TO_DATE(START_TIME) AS USAGE_DATE,  \n        'AI SERVICES' as SERVICE_GROUP,  \n        'CORTEX_FUNCTIONS_USAGE' AS SERVICE_NAME,  \n        SUM(TOKEN_CREDITS) AS CREDITS_USED,  \n        SUM(TOKEN_CREDITS) * << your credit price here >> AS COST,  \n        SUM(TOKENS) as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_FUNCTIONS_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME),"]},{"url":"https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-costs","title":"Understanding cost for Cortex Search Services | Snowflake Documentation","excerpts":["Guides Snowflake AI & ML Cortex Search Understanding cost\n ... \nSection Title: Understanding cost for Cortex Search Services ¶ > Cost categories ¶\nContent:\n| Category | Description |\n| Virtual warehouse compute | A Cortex Search Service requires a virtual warehouse to refresh the service: to |\n| run queries against base objects when they are initialized and refreshed, including orchestrating text embedding jobs |  |\n| and building the search index. These operations use compute resources, which consume credits . |  |\n| If no changes are identified during a refresh, virtual warehouse credits aren’t consumed since there’s no new data to refresh. |  |\n| You incur these costs while the service is available to respond to queries, even if |  |\n| no queries are served during a given period. For the Cortex Search Serving credit rate per GB/mo of indexed data, |  |\n| see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) . |  |\n ... \nSection Title: Understanding cost for Cortex Search Services ¶ > Observing costs ¶\nContent:\nTo learn more about the costs of your Cortex Search services, use the following Account Usage views.\nCORTEX_SEARCH_DAILY_USAGE_HISTORY view contains daily totals for EMBED_TEXT tokens compute and serving credit compute usage per service. Snowflake\nintends to also provide virtual warehouse usage in this view in the future.\nCORTEX_SEARCH_SERVING_USAGE_HISTORY view includes hourly serving credits per service.\nSnowflake intends to make this information available in the Cortex Search administration interface in the future.\nSection Title: Understanding cost for Cortex Search Services ¶ > Estimating costs ¶ > EMBED_TEXT tokens compute ¶\nContent:\nEMBED_TEXT tokens compute is charged per token of text in the search column, per document, charged in to\non the cost of the credit rate of the selected embedding model. This compute cost\nis incurred for each row that is inserted or updated, including for each row in the ON column during the initialization\nof the service and every insert or update thereafter. For information on the per-token\ncost of each embedding model, see Cortex Search Embedding Models :\nFor example, if you create a service on a source query with 10 million rows, each with 500 tokens, and the selected embedding model incurs\n0.05 credits per 1 million tokens, you would expect to pay the following for the initial refresh:\n(0.05 credits per 1 million tokens) * (10,000,000 rows) * (500 tokens per row) / (1,000,000 tokens)= **250 credits**\nSection Title: Understanding cost for Cortex Search Services ¶ > Estimating costs ¶ > EMBED_TEXT tokens compute ¶\nContent:\nFor each row inserted or updated thereafter, you’d incur a cost of 0.05 credits per 1 million tokens.\nTip\nAs an approximation, one token is equivalent to about 3/4 of an English word, or around 4 characters.\nTo get an accurate estimate of tokens per row, use the COUNT_TOKENS function with a representative sample of your actual data.\nSection Title: Understanding cost for Cortex Search Services ¶ > Estimating costs ¶ > Serving compute ¶\nContent:\nServing compute is charged per gigabyte-month of indexed data, where indexed data is the user-provided data\nin the Cortex Search source query, plus vector embeddings computed on the user’s behalf. This is an ongoing cost\nthat is incurred as long as the service’s serving status is resumed. This cost is based on the number of rows indexed,\nthe size of the total indexed data, and the dimensionality of the selected vector embedding model. For information on the dimensionality\nof each embedding model, see Cortex Search Embedding Models :\nFor example, if you have a service with 10 million rows, the selected embedding model has dimension of 768, each row\nin the source query is around 1,000 bytes (including the search column), and the credit cost per GB/mo of indexed data is 6.3,\nyou would expect to pay the following cost per month:\n ... \nSection Title: Understanding cost for Cortex Search Services ¶ > Estimating costs ¶ > Warehouse compute ¶\nContent:\nThe virtual warehouse compute cost for Cortex Search Services can vary based on the change rate of your data, target lag, and warehouse size.\nIn general, Cortex Search Services with lower target lag values and higher change rates on underlying data will incur higher Warehouse-related\ncompute costs.\nTipTo get a clear understanding of Warehouse costs related to your Cortex Search pipelines, test\nCortex Search using dedicated warehouses so that the virtual warehouse consumption attributed to Cortex Search refreshes\ncan be isolated. You can move your Cortex Search Service to a shared warehouse after you establish a cost\nbaseline."]},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization - Snowflake","excerpts":["Section Title: Cost Optimization > Business Impact > Overview > Measure business value KPIs baseline\nContent:\nBest practices include measuring technical unit economic metrics (e.g.\ncredits per 1K queries, credits per 1 TB scanned), warehouse efficiency\nand utilization by workload type, and business unit economics (e.g.\ncredits per customer acquired, credits per partner onboarded, or credits\nper data product-specific KPIs). This provides a more comprehensive\npicture of consumption in relation to cost and value. Outliers should be\nhighlighted in executive communications as either success stories or\ncautionary examples. Benchmarking should be embedded in a continuous\nimprovement loop, where insights drive action, action improves\nefficiency, and those improvements are effectively measured.\nSection Title: Cost Optimization > Visibility > Overview\nContent:\nThe Snowflake Visibility principle is designed to transform opaque cloud\nspending into actionable insights, fostering financial accountability\nand maximizing business value within your Snowflake environment. It is\nfoundational to the FinOps framework, as you cannot control, optimize,\nor attribute business value to what you cannot see. To effectively\nmanage and optimize cloud costs in Snowflake, it's crucial to align\norganizationally to an accountability structure of spend, gain deep and\ngranular insight into all aspects of your cloud spending, and\ntransparently display it to the appropriate stakeholders to take action.\nSection Title: Cost Optimization > Visibility > Overview > Recommendations\nContent:\nImplementing a robust FinOps visibility framework in Snowflake,\nsupported by cross-functional collaboration, enables each business\nfunction to access timely and relevant usage and cost data. This\nempowers them to understand the business impact of their consumption and\ntake prompt action when anomalies arise. To meet this vision, consider\nthe following recommendations based on industry best practices and\nSnowflake's capabilities:\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\nUsage\nfor cloud services is only charged if the daily consumption of cloud\nservices exceeds 10% of the daily usage of virtual warehouses. **AI features:** Snowflake additionally offers artificial intelligence\nfeatures that run on Snowflake-managed compute resources, including\nCortex AISQL functions (e.g. COMPLETE, CLASSIFY, etc. ), Cortex\nAnalyst, Cortex Search, Fine Tuning, and Document AI. The usage of\nthese features, often with tokens, are converted to credits to unify\nwith the rest of Snowflake’s billing model. Details are listed in the\nCredit Consumption Document. **Data transfer:** Data transfer is the process of moving data into\n(ingress) and out of (egress) Snowflake.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Platform cost tracking:** Pinpoint specific Snowflake credit\nconsumption (compute, storage, serverless, AI, and data transfer),\nusage patterns, and efficiency opportunities to deconstruct credit\nusage, understand drivers, identify anomalies, and (eventually) drive\nforecasting operations.\n**Normalization of consumption:** Once consumption has been attributed\nand aggregated to meaningful levels, normalizing it against relevant\nbusiness and technical metrics contextualizes it in relation to\norganizational goals. It allows for the natural growth and seasonality\nof platform usage to be put into context with business and technical\ndemand drivers.\n**Clear reporting:** Presenting Snowflake cost data in an\nunderstandable format for various stakeholders is vital. This enables\nbudgeting, forecasting, KPIs, and business value metrics directly tied\nto Snowflake credit consumption.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Integrating with third-party BI tools for advanced analytics:** Connecting to Snowflake from tools like Tableau, Power BI, Looker, or\ncustom applications offers highly customizable and extensive control\nover cost data visualization. Cloud-specific third-party data programs\n(FinOps platforms) offer easier setup and more out-of-the-box\nSnowflake cost optimization insights. **Leverage Cortex Code (In Preview):** This AI Assistant capability\nallows users to query cost and usage data in ACCOUNT_USAGE views using\nnatural language natively in the Snowsight UI."]},{"url":"https://docs.snowflake.com/en/user-guide/snowflake-cortex/aisql","title":"Snowflake Cortex AI Functions (including LLM functions) | Snowflake Documentation","excerpts":["Section Title: ... > Cortex AI Functions storage best practices ¶\nContent:\nYou may find the following best practices helpful when working with media files in stages with Cortex AI Functions:\nEstablish a scheme for organizing media files in stages. For example, create a separate stage for each team or\nproject, and store the different types of media files in subdirectories.\nEnable directory listings on stages to allow querying and programmatic access to its files.TipTo automatically refresh the directory table for the external stage when new or updated files are available, set\nAUTO_REFRESH = TRUE when creating the stage.\nFor external stages, use fine-grained policies on the cloud provider side (for example, AWS IAM policies)\nto restrict the storage integration’s access to only what is necessary.\nAlways use encryption, such as AWS_SSE or SNOWFLAKE_SSE, to protect your data at rest.\nSection Title: Snowflake Cortex AI Functions (including LLM functions) ¶ > Cost considerations ¶\nContent:\nSnowflake Cortex AI functions incur compute cost based on the number of tokens processed. Refer to the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) for each function’s cost in credits per million tokens.\nA token is the smallest unit of text processed by Snowflake Cortex AI functions. An industry convention for text is that a token is approximately equal to four\ncharacters, although this can vary by model, as can token equivalence for media files.\nSection Title: Snowflake Cortex AI Functions (including LLM functions) ¶ > Cost considerations ¶\nContent:\nFor functions that generate new text using provided text (AI_COMPLETE, AI_CLASSIFY, AI_FILTER, AI_AGG, AI_SUMMARIZE, and\nAI_TRANSLATE, and their previous versions in the SNOWFLAKE.CORTEX schema), both input and output tokens are billable. For Cortex Guard, only input tokens are counted. The number of input tokens is based on the number of tokens output from AI_COMPLETE (or COMPLETE). Cortex Guard usage is billed in addition to the cost of the AI_COMPLETE (or COMPLETE) function. For AI_SIMILARITY, AI_EMBED, and the SNOWFLAKE.CORTEX.EMBED_* functions, only input tokens are counted. For EXTRACT_ANSWER, the number of billable tokens is the sum of the number of tokens in the `from_text` and `question` fields.\n ... \nSection Title: Snowflake Cortex AI Functions (including LLM functions) ¶ > Cost considerations ¶\nContent:\nFor document formats consisting of pages, the number of pages processed is counted as input tokens. Each page in a document is counted as 970 tokens. AI_COUNT_TOKENS incurs only compute cost to run the function. No additional token-based costs are incurred.\nSection Title: Snowflake Cortex AI Functions (including LLM functions) ¶ > Cost considerations ¶\nContent:\nFor models that support media files such as images or audio:\nAudio files are billed at 50 tokens per second of audio.\nThe token equivalence of images is determined by the model used. For more information, see AI Image cost considerations .\nSnowflake recommends executing queries that call a Snowflake Cortex AI Function with a smaller\nwarehouse (no larger than MEDIUM). Larger warehouses do not increase performance. The cost associated with keeping a warehouse active\ncontinues to apply when executing a query that calls a Snowflake Cortex LLM Function. For general information on\ncompute costs, see Understanding compute cost .\n ... \nSection Title: ... > Track credit consumption for Cortex AI Functions ¶\nContent:\nTo view the credit and token consumption for each AI Function call, use the CORTEX_FUNCTIONS_USAGE_HISTORY view :\n```\nSELECT * \n  FROM SNOWFLAKE . ACCOUNT_USAGE . CORTEX_FUNCTIONS_USAGE_HISTORY ;\n```\nCopy\nYou can also view the credit and token consumption for each query within your Snowflake account. Viewing the credit and token consumption for each query helps you identify queries that are consuming the most credits and tokens.\nThe following example query uses the CORTEX_FUNCTIONS_QUERY_USAGE_HISTORY view to show the credit and token consumption for all of your queries within your account.\n```\nSELECT * FROM SNOWFLAKE . ACCOUNT_USAGE . CORTEX_FUNCTIONS_QUERY_USAGE_HISTORY ;\n```\nCopy\nYou can also use the same view to see the credit and token consumption for a specific query.\n```\nSELECT * FROM SNOWFLAKE . ACCOUNT_USAGE . CORTEX_FUNCTIONS_QUERY_USAGE_HISTORY \n WHERE query_id = '<query-id>' ;\n```\nCopy\nNote\nSection Title: ... > Track credit consumption for Cortex AI Functions ¶\nContent:\nYou can’t get granular usage information for requests made with the REST API.\nThe query usage history is grouped by the models used in the query. For example, if you ran:\n```\nSELECT AI_COMPLETE ( 'mistral-7b' , 'Is a hot dog a sandwich' ), AI_COMPLETE ( 'mistral-large' , 'Is a hot dog a sandwich' );\n```\nCopy\nThe query usage history would show two rows, one for `mistral-7b` and one for `mistral-large` ."]},{"url":"https://community.snowflake.com/s/article/how-to-track-and-understand-cortex-ai-related-charges","title":"How to track and understand Cortex AI-related charges in Snowflake","excerpts":["Loading\n\n× Sorry to interrupt\n\nCSS Error\n\nRefresh\n\nCREATE ACCOUNT SIGN IN\n\nKNOWLEDGE BASE ARTICLES\n\nCan't find what you're looking for? **Ask The Community**\n\n**Sign Up for snowflake** **communications**\n\n[Privacy Notice](https://www.snowflake.com/privacy-policy/) | [Site Terms](https://www.snowflake.com/legal/snowflake-community-terms-of-service/) | Cookie Settings | [Do not Share My personal Information](https://www.snowflake.com/privacy-policy/)\n\n[unsubscribe here](https://info.snowflake.com/2024-Preference-center.html) or customize your communication preferences\n\n* [](https://twitter.com/SnowflakeDB \"Snowflake Twitter\")\n* [](https://www.linkedin.com/uas/login?session_redirect=%2Fcompany%2F3653845 \"Snowflake LinkedIn\")\n* [](https://www.youtube.com/user/snowflakecomputing \"Snowflake YouTube\")\n* [](https://www.facebook.com/snowflakedb/ \"Snowflake Facebook\")\n\nLoading\n\nHow to track and understand Cortex AI-related charges in Snowflake"]},{"url":"https://docs.snowflake.com/en/release-notes/2026/other/2026-01-27-ai-count-tokens-function-ga","title":"Jan 27, 2026: Estimate token usage with AI_COUNT_TOKENS (General availability) | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nRelease notes Snowflake server release notes and feature updates Earlier 2026 server release notes and feature updates Feature updates Jan 27, 2026 - Estimate token usage with AI_COUNT_TOKENS (General availability)\nSection Title: Jan 27, 2026: Estimate token usage with AI_ COUNT_ TOKENS ( *General availability* ) ¶\nContent:\nAI_COUNT_TOKENS, a Cortex AI helper function that helps users estimate token consumption and understand how prompt\ncontext impacts cost, is now generally available. AI_COUNT_TOKENS takes into account the function, the LLM model (if\napplicable), and any additional inputs that affect token count, such as categories/labels for classification tasks.\nIn general, token usage increases as prompts become more descriptive and complex. Minimal prompts with limited context\nconsume fewer tokens, while deeper context, task descriptions, and examples increase token counts. With AI_COUNT_TOKENS,\nusers can evaluate how these tradeoffs affect token usage and therefore cost while developing their AI workloads.\nThis capability is especially useful for establishing best practices around:"]},{"url":"https://docs.snowflake.com/en/sql-reference/account-usage/cortex_functions_usage_history","title":"CORTEX_FUNCTIONS_USAGE_HISTORY view | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nReference General reference SNOWFLAKE database Account Usage CORTEX_FUNCTIONS_USAGE_HISTORY\nSchema:\nACCOUNT_USAGE\nSection Title: CORTEX_FUNCTIONS_USAGE_HISTORY view ¶\nContent:\nImportant\nThis view is no longer updated. Use the CORTEX_AISQL_USAGE_HISTORY view instead.\nThis Account Usage view can be used to query the usage history of Cortex Functions such\nas COMPLETE and TRANSLATE. The information in the view includes the number of tokens and credits consumed each time a Cortex Function is\ncalled, aggregated in one hour increments based on function and model. The view also includes relevant metadata, such as the warehouse ID,\nstart and end times of the function execution, and the name of the function and the model, if specified.\nNote\nThe view might not include usage information on functions called with recently added models. A new model can take up to 2 weeks to\nbe included in this view."]},{"url":"https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/","title":"FinOps on Snowflake: Built-In Cost and Performance Control","excerpts":["Section Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\nRead the case study Financial Services “The speed at which we’ve delivered wouldn’t have been possible with other providers.” Kaushik Ghosh Dastidar\nHead of ESG Cloud Solutions, NatWest Read the story * **6x** reduction in onboarding time from 3 months to 2 weeks\n**$750K** saved in salaries & staff training costs\nRead the case study\n[Resource #### Snowflake Joins the FinOps Foundation Snowflake joins The FinOps Foundation as a Premier Enterprise Member to provide thought leadership and set industry financial best practices. Read more](https://www.finops.org/members/snowflake/)\nResource #### Snowflake Pricing Calculator Curious about Snowflake pricing? Our Snowflake pricing calculator shows credit usage, warehouse costs, and total expenses. Access calculator\nSection Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\nGuide #### Definitive Guide to Managing Spend in Snowflake Learn about considerations for consumption models such as Snowflake's, frameworks for better managing spend, and more. Get the guide\n ... \nSection Title: FinOps on Snowflake > ... > Snowflake Documentation > On-Demand Cost Governance Training\nContent:\nLearn how to successfully examine, control, and optimize Snowflake costs.\nRegister Now\n ... \nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation > Priority Support\nContent:\nLearn more about how our Priority Support team can help you reduce consumption spend through performance monitoring, observability, and management.\nLearn about Priority Support"]}],"usage":[{"name":"sku_search","count":1}]}

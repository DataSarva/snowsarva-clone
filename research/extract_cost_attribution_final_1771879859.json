{
  "extract_id": "extract_357853abf4b7414185a07d545d017468",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-attributing",
      "title": "Attributing cost | Snowflake Documentation",
      "publish_date": null,
      "excerpts": [
        "Guides Cost & Billing Visibility Attributing cost\n\n# Attributing cost \u00b6\n\nAn organization can apportion the cost of using Snowflake to logical units within the organization (for example, to different\ndepartments, environments, or projects). This chargeback or showback model is useful for accounting purposes and pinpoints\nareas of the organization that could benefit from controls and optimizations that can reduce costs.\n\nTo attribute costs to different groups like departments or projects, use the following recommended approach:\n\n* Use object tags to associate resources and users with departments or projects.\n* Use query tags to associate individual queries with departments or projects when the queries are\n  made by the same application on behalf of users belonging to multiple departments.\n\n## Types of cost attribution scenarios \u00b6\n\nThe following cost attribution scenarios are the most commonly encountered. In these scenarios, warehouses are used as an\nexample of a resource that incurs costs.\n\n* **Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate\n  warehouses with a department. You can use these object tags to attribute the costs incurred by those warehouses to that\n  department entirely.\n  \n  \n* **Resources that are shared by users from multiple departments:** An example of this is a warehouse shared by users from\n  different departments. In this case, you use object tags to associate each user with a department. The costs of queries are\n  attributed to the users. Using the object tags assigned to users, you can break down the costs by department.\n  \n  \n* **Applications or workflows shared by users from different departments:** An example of this is an application that issues\n  queries on behalf of its users. In this case, each query executed by the application is assigned a query tag that identifies\n  the team or cost center of the user on whose behalf the query is being made.\n  \n  \n\nThe next sections explain how to set up object tags in your accounts and provide the details for each of these cost attribution\nscenarios.\n\n## Setting up object tags for cost attribution \u00b6\n\nWhen you set up tags to represent the groupings that you want to use for cost attribution, you should determine if the\ngroupings apply to a single account or multiple accounts. This determines how you set up your tags.\n\nFor example, suppose that you want to attribute costs based on department.\n\n* If the resources used by the department are located in a single account, you create the tags in a database in that account.\n* If the resources used by the department span multiple accounts, you create the tags in a key account in your organization (for example, in your organization account ),\n  and you make those tags available in other accounts through replication .\n\nThe next sections explain how to create the tags, replicate the tags, and apply the tags to resources.\n\n* Creating the tags\n* Replicating the tag database\n* Tagging the resources and users\n\nNote\n\nThe examples in these sections use the custom role `tag_admin` , which is assumed to have been granted the privileges to\ncreate and manage tags. Within your organization, you can use more granular privileges for object tagging to develop a secure tagging strategy.\n\n### Creating the tags \u00b6\n\nAs part of designing the strategy, decide on the database and schema where you plan to create the tags.\n\n* You can create a dedicated database and schema for the tags.\n* If you want to tag resources in different accounts across your organization, you can create the tags in a key account in your\n  organization (for example, in your organization account ).\n\nThe following example creates a database named `cost_management` and a schema named `tags` for the tags that you plan to use:\n\n```\nUSE ROLE tag_admin ; \n\n CREATE DATABASE cost_management ; \n CREATE SCHEMA tags ;\n```\n\nCopy\n\nWith `cost_management` and `tags` selected as the current database and schema, create a tag named `cost_center` and set\nthe values allowed for the tag to the names of cost centers:\n\n```\nCREATE TAG cost_center \n  ALLOWED_VALUES 'finance' , 'marketing' , 'engineering' , 'product' ;\n```\n\nCopy\n\n### Replicating the tag database \u00b6\n\nIf you have an organization with multiple accounts and you want to make the tags available in these other accounts, set up your accounts for replication , and create a replication group in a main account (for example, in the organization account ). Set up this replication group to replicate the database\ncontaining the tags.\n\nFor example, to replicate the tags to the accounts named `my_org.my_account` and `my_org.my_account_2` , execute this\nstatement in your organization account:\n\n```\nCREATE REPLICATION GROUP cost_management_repl_group \n  OBJECT_TYPES = DATABASES \n  ALLOWED_DATABASES = cost_management \n  ALLOWED_ACCOUNTS = my_org . my_account_1 , my_org . my_account_2 \n  REPLICATION_SCHEDULE = '10 MINUTE' ;\n```\n\nCopy\n\nThen, in each account in which you want to make the tags available, create a secondary replication group, and refresh this\ngroup from the primary group:\n\n```\nCREATE REPLICATION GROUP cost_management_repl_group \n  AS REPLICA OF my_org . my_org_account . cost_management_repl_group ; \n\n ALTER REPLICATION GROUP cost_management_repl_group REFRESH ;\n```\n\nCopy\n\n### Tagging the resources and users \u00b6\n\nAfter creating and replicating the tags, you can use these tags to identify the warehouses and users belonging to each\ndepartment. For example, because the sales department uses both `warehouse1` and `warehouse2` , you can set the `cost_center` tag to `'SALES'` for both warehouses.\n\nTip\n\nIdeally, you should have workflows that automate the process of applying these tags when you create resources and users.\n\n```\nUSE ROLE tag_admin ; \n\n ALTER WAREHOUSE warehouse1 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse2 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse3 SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n\n ALTER USER finance_user SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n ALTER USER sales_user SET TAG cost_management . tags . cost_center = 'SALES' ;\n```\n\nCopy\n\n## Viewing cost by tag in SQL \u00b6\n\nYou can attribute costs within an account or across accounts in an organization:\n\n* **Attributing costs within an account**\n  \n  You can attribute costs within an account by querying the following views in the ACCOUNT\\_USAGE schema:\n  \n    + TAG\\_REFERENCES view : Identifies objects (for example, warehouses and users) that have tags.\n    + WAREHOUSE\\_METERING\\_HISTORY view : Provides credit usage for warehouses.\n    + QUERY\\_ATTRIBUTION\\_HISTORY view : Provides the compute costs for queries. The cost per query is\n        the warehouse credit usage for executing the query.\n        \n        For more information on using this view, see About the QUERY\\_ATTRIBUTION\\_HISTORY view .\n* **Attributing costs across accounts in an organization**\n  \n  Within an organization, you can also attribute costs for resources that are used **exclusively by a single department** by\n  querying views in the ORGANIZATION\\_USAGE schema from the organization account .\n  \n  Note\n  \n    + In the ORGANIZATION\\_USAGE schema, the TAG\\_REFERENCES view is only available in the organization account.\n    + The QUERY\\_ATTRIBUTION\\_HISTORY view is only available in the ACCOUNT\\_USAGE schema for an account. There is no\n        organization-wide equivalent of the view.\n\nThe next sections explain how to attribute costs for some of the common cost-attribution scenarios :\n\n* Resources not shared by departments\n* Resources shared by users from different departments\n* Resources used by applications that need to attribute costs to different departments\n\n### Resources not shared by departments \u00b6\n\nSuppose that you want to attribute costs by department and that each department uses a set of dedicated warehouses.\n\nIf you tag warehouses with a `cost_center` tag to identify the department that owns the warehouse, you can join the\nACCOUNT\\_USAGE TAG\\_REFERENCES view with the WAREHOUSE\\_METERING\\_HISTORY view on the `object_id` and `warehouse_id` columns to get usage\ninformation by warehouse, and you can use the `tag_value` column to identify the departments that own those warehouses.\n\nThe following SQL statement performs this join:\n\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\n\nCopy\n\n```\n+-------------+-------------+-----------------+ \n | TAG_NAME    | TAG_VALUE   |   TOTAL_CREDITS | \n |-------------+-------------+-----------------| \n | NULL        | untagged    |    20.360277159 | \n | COST_CENTER | Sales       |    17.173333333 | \n | COST_CENTER | Finance     |      8.14444444 | \n +-------------+-------------+-----------------+\n```\n\nYou can run a similar query to perform the same attribution for all the accounts in your organization using views in the\nORGANIZATION\\_USAGE schema from the organization account . The rest of the query\ndoes not change.\n\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ORGANIZATION_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ORGANIZATION_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n          AND tag_database = 'COST_MANAGEMENT' AND tag_schema = 'TAGS' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\n\nCopy\n\n### Resources shared by users from different departments \u00b6\n\nSuppose that users in different departments share the same warehouses and you want to break down the credits used by each\ndepartment. You can tag the users with a `cost_center` tag to identify the department that they belong to, and you can join\nthe TAG\\_REFERENCES view with the QUERY\\_ATTRIBUTION\\_HISTORY view .\n\nNote\n\nYou can only get this data for a single account at a time. You cannot execute a query that retrieves this data across\naccounts in an organization.\n\nThe next sections provide examples of SQL statements for attributing costs for shared resources.\n\n* Calculating the cost of user queries for the last month\n* Calculating the cost of user queries by department without idle time\n* Calculating the cost of queries by users without idle time\n* Calculating the cost of queries by users without tags\n\n#### Calculating the cost of user queries for the last month \u00b6\n\nThis following SQL statement calculates the costs for the last month.\n\nIn this example, idle time is distributed among the users in proportion to their usage.\n\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n  ), \n  user_credits AS ( \n    SELECT user_name , SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n      GROUP BY user_name \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n    FROM user_credits \n  ) \n SELECT \n    u . user_name , \n    u . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM user_credits u , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\n\nCopy\n\n```\n+-----------+--------------------+ \n | USER_NAME | ATTRIBUTED_CREDITS | \n |-----------+--------------------+ \n | FINUSER   | 6.603575468        | \n | SALESUSER | 4.321378049        | \n | ENGUSER   | 0.6217131392       | \n |-----------+--------------------+\n```\n\n#### Calculating the cost of user queries by department without idle time \u00b6\n\nThe following example attributes the compute cost to each department through the queries executed by users in that department.\nThis query depends on the user objects having a tag that identifies their department.\n\n```\nWITH joined_data AS ( \n  SELECT \n      tr . tag_name , \n      tr . tag_value , \n      qah . credits_attributed_compute , \n      qah . start_time \n    FROM SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES tr \n      JOIN SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n        ON tr . domain = 'USER' AND tr . object_name = qah . user_name \n ) \n SELECT \n    tag_name , \n    tag_value , \n    SUM ( credits_attributed_compute ) AS total_credits \n  FROM joined_data \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY tag_name , tag_value \n  ORDER BY tag_name , tag_value ;\n```\n\nCopy\n\n```\n+-------------+-------------+-----------------+ \n | TAG_NAME    | TAG_VALUE   |   TOTAL_CREDITS | \n |-------------+-------------+-----------------| \n | COST_CENTER | engineering |   0.02493688426 | \n | COST_CENTER | finance     |    0.2281084988 | \n | COST_CENTER | marketing   |    0.3686840545 | \n |-------------+-------------+-----------------|\n```\n\n#### Calculating the cost of queries by users without idle time \u00b6\n\nThis following SQL statement calculates the costs per user for the past month (excluding idle time).\n\n```\nSELECT user_name , SUM ( credits_attributed_compute ) AS credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE \n    start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY user_name ;\n```\n\nCopy\n\n```\n+-----------+--------------------+ \n | USER_NAME | ATTRIBUTED_CREDITS | \n |-----------+--------------------| \n | JSMITH    |       17.173333333 | \n | MJONES    |         8.14444444 | \n | SYSTEM    |         5.33985393 | \n +-----------+--------------------+\n```\n\n#### Calculating the cost of queries by users without tags \u00b6\n\nThe following example calculates the cost of queries by users who are not tagged. You can use this to verify that tags are\nbeing applied consistently to users.\n\n```\nSELECT qah . user_name , SUM ( qah . credits_attributed_compute ) as total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n    LEFT JOIN snowflake . account_usage . tag_references tr \n    ON qah . user_name = tr . object_name AND tr . DOMAIN = 'USER' \n  WHERE \n    start_time >= dateadd ( month , - 1 , current_date ) \n    AND qah . user_name IS NULL OR tr . object_name IS NULL \n  GROUP BY qah . user_name \n  ORDER BY total_credits DESC ;\n```\n\nCopy\n\n```\n+------------+---------------+ \n | USER_NAME  | TOTAL_CREDITS | \n |------------+---------------| \n | RSMITH     |  0.1830555556 | \n +------------+---------------+\n```\n\n### Resources used by applications that need to attribute costs to different departments \u00b6\n\nThe examples in this section calculate the costs for one or more applications that are powered by Snowflake.\n\nThe examples assume that these applications set query tags that identify the application for all queries executed. To set the\nquery tag for queries in a session, execute the ALTER SESSION command. For example:\n\n```\nALTER SESSION SET QUERY_TAG = 'COST_CENTER=finance' ;\n```\n\nCopy\n\nThis associates the `COST_CENTER=finance` tag with all subsequent queries executed during the session.\n\nYou can then use the query tag to trace back the cost incurred by these queries to the appropriate departments.\n\nThe next sections provide examples of using this approach.\n\n* Calculating the cost of queries by department\n* Calculating the cost of queries (excluding idle time) by query tag\n* Calculating the cost of queries (including idle time) by query tag\n\n#### Calculating the cost of queries by department \u00b6\n\nThe following example calculates the compute credits and the credits used for the query acceleration service for the finance department. This depends on the `COST_CENTER=finance` query tag being applied to the original queries that were executed.\n\nNote that the costs exclude idle time.\n\n```\nSELECT \n    query_tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_tag = 'COST_CENTER=finance' \n  GROUP BY query_tag ;\n```\n\nCopy\n\n```\n+---------------------+-----------------+------+ \n | QUERY_TAG           | COMPUTE_CREDITS | QAS  | \n |---------------------+-----------------|------| \n | COST_CENTER=finance |      0.00576115 | null | \n +---------------------+-----------------+------+\n```\n\n#### Calculating the cost of queries (excluding idle time) by query tag \u00b6\n\nThe following example calculates the cost of queries by query tag and includes queries without tags (identified as \u201cuntagged\u201d).\n\n```\nSELECT \n    COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n  GROUP BY tag \n  ORDER BY compute_credits DESC ;\n```\n\nCopy\n\n```\n+-------------------------+-----------------+------+ \n | TAG                     | COMPUTE_CREDITS | QAS  | \n |-------------------------+-----------------+------+ \n | untagged                | 3.623173449     | null | \n | COST_CENTER=engineering | 0.531431948     | null | \n |-------------------------+-----------------+------+\n```\n\n#### Calculating the cost of queries (including idle time) by query tag \u00b6\n\nThe following example distributes the idle time that is not captured in the per-query cost across departments in proportion\nto their usage of the warehouse.\n\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n      AND start_time < CURRENT_DATE \n  ), \n  tag_credits AS ( \n    SELECT \n        COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n        SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n      GROUP BY tag \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n      FROM tag_credits \n  ) \n SELECT \n    tc . tag , \n    tc . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM tag_credits tc , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\n\nCopy\n\n```\n+-------------------------+--------------------+ \n | TAG                     | ATTRIBUTED_CREDITS | \n +-------------------------+--------------------| \n | untagged                |        9.020031304 | \n | COST_CENTER=finance     |        1.027742521 | \n | COST_CENTER=engineering |        1.018755812 | \n | COST_CENTER=marketing   |       0.4801370376 | \n +-------------------------+--------------------+\n```\n\n## Viewing cost by tag in Snowsight \u00b6\n\nYou can attribute costs by reporting on the use of resources that have the `cost_center` tag. You can access this data in Snowsight .\n\n1. Switch to a role that has access to the ACCOUNT\\_USAGE schema .\n2. In the navigation menu, select Admin \u00bb Cost management .\n3. Select Consumption .\n4. From the Tags drop-down, select the `cost_center` tag.\n5. To focus on a specific cost center, select a value from the list of the tag\u2019s values.\n6. Select Apply .\n\nFor more details about filtering in Snowsight, see Filter by tag .\n\n## About the QUERY\\_ ATTRIBUTION\\_ HISTORY view \u00b6\n\nYou can use the QUERY\\_ATTRIBUTION\\_HISTORY view to attribute cost based on queries. The cost per\nquery is the warehouse credit usage for executing the query. This cost does not include any other credit usage that is incurred\nas a result of query execution. For example, the following are not included in the query cost:\n\n* Data transfer costs\n* Storage costs\n* Cloud services costs\n* Costs for serverless features\n* Costs for tokens processed by AI services\n\nFor queries that are executed concurrently, the cost of the warehouse is attributed to individual queries based on the weighted\naverage of their resource consumption during a given time interval.\n\nThe cost per query does not include warehouse _idle time_ . Idle time is a period of time in which no queries are running in the\nwarehouse and can be measured at the warehouse level.\n\n## Additional examples of queries \u00b6\n\nThe next sections provide additional queries that you can use for cost attribution:\n\n* Grouping similar queries\n* Attributing costs of hierarchical queries\n\n### Grouping similar queries \u00b6\n\nFor recurrent or similar queries, use the `query_hash` or `query_parameterized_hash` to group costs\nby query.\n\nTo find the most expensive recurrent queries for the current month, execute the following statement:\n\n```\nSELECT query_parameterized_hash , \n       COUNT (*) AS query_count , \n       SUM ( credits_attributed_compute ) AS total_credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n  AND start_time < CURRENT_DATE \n  GROUP BY query_parameterized_hash \n  ORDER BY total_credits DESC \n  LIMIT 20 ;\n```\n\nCopy\n\nFor an additional query based on query ID, see Examples .\n\n### Attributing costs of hierarchical queries \u00b6\n\nFor stored procedures that issue multiple hierarchical queries, you can compute the attributed query costs for the\nprocedure by using the root query ID for the procedure.\n\n1. To find the root query ID for a stored procedure, use the ACCESS\\_HISTORY view . For example,\n   to find the root query ID for a stored procedure, set the `query_id` and execute the following statements:\n   \n   ```\n   SET query_id = '<query_id>' ; \n   \n    SELECT query_id , \n          parent_query_id , \n          root_query_id , \n          direct_objects_accessed \n     FROM SNOWFLAKE . ACCOUNT_USAGE . ACCESS_HISTORY \n     WHERE query_id = $ query_id ;\n   ```\n   \n   Copy\n   \n   For more information, see Ancestor queries with stored procedures .\n2. To sum the query cost for the entire procedure, replace `<root_query_id>` and execute the following statements:\n   \n   ```\n   SET query_id = '<root_query_id>' ; \n   \n    SELECT SUM ( credits_attributed_compute ) AS total_attributed_credits \n     FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n     WHERE ( root_query_id = $ query_id OR query_id = $ query_id );\n   ```\n   \n   Copy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)"
      ],
      "full_content": null
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/application_daily_usage_history",
      "title": "APPLICATION_DAILY_USAGE_HISTORY view | Snowflake Documentation",
      "publish_date": "2026-01-01",
      "excerpts": [
        "Reference General reference SNOWFLAKE database Account Usage APPLICATION\\_DAILY\\_USAGE\\_HISTORY\n\nSchema:\n    ACCOUNT\\_USAGE\n\n# APPLICATION\\_ DAILY\\_ USAGE\\_ HISTORY view \u00b6\n\nUse this view to return the daily credit and storage usage for Snowflake Native Apps in an account within the last 365 days\n(1 year).\n\n## Columns \u00b6\n\nThe following table provides definitions for the APPLICATION\\_DAILY\\_USAGE\\_HISTORY view columns.\n\n|Field |Data type |Description |\n| --- | --- | --- |\n|APPLICATION\\_NAME |VARCHAR |The application name. |\n|APPLICATION\\_ID |NUMBER |An internal, system-generated identifier for the application. |\n|LISTING\\_GLOBAL\\_NAME |VARCHAR |The listing global name that appears in Snowflake Marketplace or in the data exchange hosting the application. |\n|USAGE\\_DATE |DATE |The date the Snowflake Native App usage occurred. |\n|CREDITS\\_USED |NUMBER |The number of credits consumed by the Snowflake Native App in a day. |\n|CREDITS\\_USED\\_BREAKDOWN |ARRAY |An array of data objects that identify the Snowflake service that consumed daily credits. See CREDITS\\_USED\\_BREAKDOWN array for formatting. |\n|STORAGE\\_BYTES |NUMBER |The daily average of storage bytes used by the Snowflake Native App. |\n|STORAGE\\_BYTES\\_BREAKDOWN |ARRAY |An array of data objects that identify the type and number of storage bytes used. See STORAGE\\_BYTES\\_BREAKDOWN array for formatting. |\n\n## Usage notes \u00b6\n\n* The maximum latency for this view is one day.\n* Usage is attributed to the start day when usage events span multiple days.\n* The APPLICATION\\_DAILY\\_USAGE\\_HISTORY view and the Snowsight cost management tools can return different daily credit and storage usage values. This discrepancy is caused by the methods used to determine daily credit and storage usage. To determine these values, the APPLICATION\\_DAILY\\_USAGE\\_HISTORY view uses the current session\u2019s TIMEZONE parameter and the Snowsight cost management tools use Coordinated Universal Time (UTC). To resolve any discrepancies, Snowflake recommends setting the TIMEZONE parameter to UTC.\n\n### CREDITS\\_USED\\_BREAKDOWN array \u00b6\n\nThe CREDITS\\_USED\\_BREAKDOWN array provides details about the services that consumed daily credits.\n\nExample:\n\n```\n[ \n  { \n    \"credits\": 0.005840921,\n    \"serviceType\": \"AUTO_CLUSTERING\" \n  } ,\n  { \n    \"credits\": 0.115940725,\n    \"serviceType\": \"SERVERLESS_TASK\" \n  } ,\n  { \n    \"credits\": 6.033448041,\n    \"serviceType\": \"SNOWPARK_CONTAINER_SERVICES\" \n  } \n ]\n```\n\nCopy\n\nThe following table provides descriptions for the key-value pairs in the objects in the array.\n\n|Field |Data type |Description |\n| --- | --- | --- |\n|`credits` |DECIMAL |Number of credits consumed by the service type specified by `serviceType` on the usage date. |\n|`serviceType` |VARCHAR |The service type, which can be one of the following values:\n\n* `AUTO_CLUSTERING` \u2014 See Automatic Clustering .\n* `DATA_QUALITY_MONITORING` \u2014 See Introduction to data quality checks .\n* `MATERIALIZED_VIEW` \u2014 See Working with Materialized Views .\n* `PIPE` \u2014 See Snowpipe .\n* `SEARCH_OPTIMIZATION` \u2014 See Search optimization service .\n* `SERVERLESS_TASK` \u2014 See Introduction to tasks .\n* `SNOWPARK_CONTAINER_SERVICES` \u2014 See Snowpark Container Services .\n* `WAREHOUSE_METERING` \u2014 See Overview of warehouses . |\n\nThe following are used in the determination of credit consumption:\n\n* The credits used by objects in the Snowflake Native App. For example, auto-clustering on tables in the Snowflake Native App.\n* The credits used by the warehouses owned by the Snowflake Native App.\n* The credits used by the compute pools dedicated to the Snowflake Native App.\n\n### STORAGE\\_ BYTES\\_ BREAKDOWN array \u00b6\n\nThe STORAGE\\_BYTES\\_BREAKDOWN array provides details about the services that consumed storage.\n\nExample:\n\n```\n[ \n  { \n    \"bytes\": 34043221,\n    \"storageType\": \"DATABASE\" \n  } ,\n  { \n    \"bytes\": 109779541,\n    \"storageType\": \"FAILSAFE\" \n  } \n ]\n```\n\nCopy\n\nThe following table provides descriptions for the key-value pairs in the objects in the array.\n\n|Field |Data type |Description |\n| --- | --- | --- |\n|`bytes` |INTEGER |Number of storage bytes used. |\n|`storageType` |VARCHAR |The storage type, which can be one of the following values:\n\n* `DATABASE` : Database storage.\n* `FAILSAFE` : Fail-safe storage .\n* `HYBRID_TABLE` : Storage for hybrid tables . |\n\nOnly data stored in the Snowflake Native App is used to determine storage byte consumption. External databases created by the Snowflake Native App are not included in the determination of this value.\n\n## Examples \u00b6\n\nRetrieve the daily credit and storage usage for a Snowflake Native App in an account and order the results by usage date:\n\n```\nSELECT * \n  FROM SNOWFLAKE . ACCOUNT_USAGE . APPLICATION_DAILY_USAGE_HISTORY \n  ORDER BY usage_date DESC ;\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Columns\n2. Usage notes\n3. Examples\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    },
    {
      "url": "https://medium.com/@angelamarieharney/cortex-ai-cost-queries-in-snowflake-07331811d42d",
      "title": "Cortex AI Cost Queries in Snowflake | by Angela Harney | Jan, 2026 | Medium",
      "publish_date": "2026-02-10",
      "excerpts": [
        "Sitemap\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nWrite\n\nSearch\n\n# Cortex AI Cost Queries in Snowflake\n\n## Know how to track your AI Costs and increase Trust\n\nAngela Harney\n\n14 min read\n\n\u00b7\n\nJan 19, 2026\n\n\\--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\n**Contents**  \n\\- Ingest Costs  \n\\- Ingress Costs  \n\\- Egress Costs  \n\\- Compute Costs  \n\\- Counting Tokens  \n\\- Monthly Invoice Amount  \n\\- Cortex AI Usage History Views  \n\\- My Cost Queries  \n\\- Other Cost Considerations and Monitoring  \n\\-Cost Summary  \n\\-Cortex Code Costs  \n\\-Appendix I: Invoice Cost Details  \n\\- Appendix II: Individual AI Query Details  \n\\- Appendix III: Second-Level Cost Details\n\n## Introduction\n\nI recently had to provide a breakdown of the AI services costs that are listed on the monthly Snowflake invoice so that it was clearly understood where AI costs were being attributed to help make planning decisions.\n\nSnowflake\u2019s integration of AI services like Cortex AI introduces new dimensions to cost management and understanding AI costs.\n\n> AI costs fall into four main categories: ingest, inference, standard compute, and egress.\n> \n>\n\n## Ingest Costs\n\nIngest costs arise when data is processed by AI models, such as during document parsing or text analysis via Cortex Document AI.\n\nThese costs are tracked in the `CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY` view, where `CREDITS_USED` reflects the compute consumed per operation.\n\n## Inference Costs\n\nInference costs occur when AI models are invoked to generate insights, such as through Cortex Search or AI-driven query suggestions or when calling functions like `AI_COMPLETE` or `AI_EMBED` and are billed per token.\n\nThese can be monitored using the `CORTEX_INFERENCE_USAGE_HISTORY` view. To control costs, teams should add query tags to AI queries, limit high-volume operations, and use serverless compute wisely, especially for one-off AI tasks.\n\nGenerative functions charge for input and output tokens, whereas Embedding and Similarity functions only charge for input tokens.\n\nThese costs are tracked in service-specific usage views and are rolled up into the `AI_INFERENCE` line item on the Snowflake monthly invoice.\n\n## Compute Costs\n\nStandard compute costs for AI workloads are tracked separately in `WAREHOUSE_METERING_HISTORY` .\n\nWhile AI processing uses serverless AI Services compute, the warehouse used to orchestrate the query, such as when using `AI_COMPLETE,` incurs standard compute charges.\n\nThis cost is independent of AI token usage and must be monitored alongside AI Services credits.\n\n## Egress Costs\n\nEgress costs are a critical but often underestimated component of AI workloads. Monitoring `CLOUD_SERVICES_USAGE_HISTORY` can help identify high egress patterns.\n\nWhile Snowflake offers zero egress cost for cross-region and cross-cloud data sharing via the Egress Cost Optimizer (ECO) which eliminates cross-region and cross-cloud data transfer fees for shared data helps minimize egress costs. Manual replication or data exported to external systems using `COPY INTO` can incur significant fees.\n\nAI models that generate large outputs (e.g., summaries, embeddings) and transfer them outside Snowflake will trigger egress charges.\n\nTo mitigate this, use internal data sharing, limit output size, and ensure AI outputs are consumed within the same cloud region.\n\n## Counting Tokens\n\n> The `_AI_COUNT_TOKENS_` function plays a critical role in cost estimation and optimization.\n> \n> \n\nIt calculates the number of input tokens a given text will consume when processed by a specific AI model, allowing teams to estimate costs before execution, similar to using Explain on a SQL query.\n\nThis is especially valuable for prompt engineering and avoiding model limits.\n\nNo AI token charges apply when using `AI_COUNT_TOKENS.` It only incurs standard compute cost, so it is great to use in pre-validating high-volume AI workflows.\n\nIn this example, it is used in a select query over a table that stores document chunks in columns. Putting document chunks in separate columns takes advantage of Snowflake\u2019s columnar optimization and limits the amount of data scanned for the AI Function being used. This approach is setup to run data through a standard data pipeline.\n\n```\nselect  \n    SNOWFLAKE.CORTEX.COUNT_TOKENS('llama3-8b',doc_contents) AS token_count, token_count * (.0003/1000) as token_cost;  \n    count_tokens(  \n          , ai_complete  \n          , 'llama3-8b',  \n          , << column_name that stores the document or chunk to assess >>  \n          , 'What are the revenue projections for the next 5 years based on the last two years profit.'  \n        )  \nFROM my_table_name  \n;\n```\n\n## Monthly Invoice Amount\n\nTo get the view of AI costs that will be reflected on your Snowflake invoice at the end of the month, throughout the month you can query the `METERING_DAILY_HISTORY` view filtered by service type.\n\nThis view contains an aggregation of all AI-related credit consumption and aligns with billing statements. Multiply `CREDITS_USED` by your negotiated credit rate to estimate actual spend.\n\n```\nSELECT   \n  USAGE_DATE,  \n  SERVICE_TYPE,  \n  CREDITS_USED,  \n  CREDITS_USED * <your_credit_rate> AS estimated_cost_usd  \nFROM SNOWFLAKE.ORGANIZATION_USAGE.METERING_DAILY_HISTORY  \nWHERE SERVICE_TYPE IN ('AI_SERVICES', 'AI_INFERENCE')  \n  AND USAGE_DATE >= DATEADD('month', -3, CURRENT_DATE())  \nORDER BY USAGE_DATE DESC;\n```\n\n## Cortex Usage History Views\n\nCortex AI costs are tracked across different levels in dedicated `ACCOUNT_USAGE` views, each providing granular insights into credit consumption. These views enable monitoring at multiple levels, from high-level trends to individual query costs. Understanding the basics of how usage costs are grouped will help you execute cost queries to review the breakouts.\n\n_This is not a detailed explanation of all Cortex Usage History views. See cost queries in the Appendixes for alphabetical list of Cortex Usage History views broken out by first-level and second-level views._\n\n### AISQL **Functions**\n\n_The_ `CORTEX_FUNCTIONS_QUERY_USAGE_HISTORY` _view provides individual query-level details, including_ `_query_id_` _,_ `_user_name_` _,_ `_warehouse_name_` _,_ `_function_name_` _,_ `_model_name_` _,_ `_tokens_` _, and_ `_token_credits_` _. This view is essential for pinpointing expensive queries and optimizing performance and is a first-level consolidation cost on invoices._\n\n### Cortex Analyst\n\n_Usage is metered at a secondary level in the_ `CORTEX_ANALYST_USAGE_HISTORY` _view and is aggregated hourly, showing credits consumed and number of messages per user. It does not include_ `_query_id_` _, but offers per-user visibility into AI interaction costs. Costs are incurred per successful response (message) not by token count._\n\n### Cortex Search\n\n_Usage is metered at a secondary level in the_ `CORTEX_SEARCH_DAILY_USAGE_HISTORY` _view and breaks down costs daily by consumption type (_ `_serving_` _,_ `_embed_text_tokens_` _) and includes costs for storage, embedding, and serving compute as follows:_\n\n* `CORTEX_SEARCH_DAILY_USAGE_HISTORY` (serving vs. embedding)\n* `CORTEX_SEARCH_SERVING_USAGE_HISTORY` (hourly serving credits)\n\n### Metering History\n\n_A broader view that can be filtered by_ `_SERVICE_TYPE = 'AI_SERVICES'_` _to track overall AI credit usage across all Cortex services, including AI functions, Analyst, and Search._\n\n## My Cost Queries\n\nMy Snowflake cost queries are broken out by the following practical uses and are filtered to the beginning of the month for 60 days ago including the current month-to-date. So, mid-way through January, the costs for Nov / Dec / Jan will be returned.\n\n> **Invoice Amounts**\n> \n> \n\n* **Invoice Summary** :  \n  _Invoice line-item alignment_\n* **Invoice (First-Level) Detail** :  \n  _First-level Cortex usage history views that are consolidated within the invoice line items, and also include a standard Compute aggregation from virtual warehouse costs_\n\n> **Activity Details**\n> \n> \n\n* **Individual AI Query Detail** :  \n  _Cost and performance stats for individual Cortex AI queries_\n* **Other Tracking (Second-Level) Detail** :  \n  _Second-level Cortex usage history views not consolidated for invoicing, but are used for tracking specific activities_\n\n### Invoice Summary\n\nUse the following query to isolate warehouse costs at an Organizational billing level to tie-out to the invoice cost for AI INFERENCE and AI SERVICES that are shown in the Snowflake Monthly Invoice Statement.\n\n```\nSELECT  \n      TO_DATE(USAGE_DATE) AS USAGE_DATE,  \n      'INVOICE SUMMARY' AS SERVICE_GROUP,  \n      SERVICE_TYPE,  \n      SUM(CREDITS_USED) as CREDITS_USED,  \n      SUM(CREDITS_USED) * << your credit price >> AS INVOICE_COST,  \n      0 as TOKENS  \nFROM snowflake.organization_usage.metering_daily_history h  \nWHERE service_type IN ('AI_SERVICES','AI_INFERENCE')   \nAND TO_DATE(USAGE_DATE) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \nGROUP BY ALL  \nORDER BY 1  \n;\n```\n\n### Invoice and First-Level Details\n\nIn my invoice detail query, AI Inference breaks out to document processing, AI Services breaks out to function usage and there is an aggregation section for Compute as standard compute for virtual warehouse costs. See `APPENDIX I \u2014 Invoice Cost Details.`\n\n_Note: Additional Cortex Usage History views may be added as first-level charges that aggregate to invoice line-items over time as Snowflake enhances their offerings._\n\n### Individual AI Query Details\n\nIn addition to invoice costs, I also wanted to know how individual AI queries performed, and if they scanned more than the amount of data that I expected.\n\nI created the query listed in `APPENDIX II \u2014 Individual AI Query Details` to return a user-friendly selection and display of columns that focus on assessing individual query executions, along with basic execution stats, and includes columns such as invocations and row counts for assessing how much data was actually scanned.\n\nThe table listed in the JOIN clause can be swapped out for any of the Cortex usage history views that contain a query ID.\n\n### Second-Level Details\n\nSecond-level charges provided by Snowflake for tracking of specific activities can be found in my query in Appendix II- `APPENDIX III \u2014 Second-Level Cost Details.`\n\nFor example, although the `cortex_search_history_usage` view is consolidated to an invoice line item, the secondary-level `cortex_search_serving_usage_history` view is next-level details for Cortex Search activities.\n\n_Note: Additional second-level Cortex Usage History views may be added over time as Snowflake enhances their offerings._\n\n## Other Cost Considerations and Monitoring\n\nQuerying AI costs using Snowflake\u2019s native monitoring tools covers several features and areas of functionality.\n\n### Notebooks\n\nThe `ACCOUNT_USAGE` schema provides granular visibility into AI-related credit consumption. For example, running a query against `SNOWFLAKE.ACCOUNT_USAGE.CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY` allows you to identify which Notebooks or users trigger the highest AI processing costs.\n\nSimilarly, `NOTEBOOKS_CONTAINER_RUNTIME_HISTORY` helps track AI workload duration and credit usage for Snowflake Notebooks with AI components.\n\nBy combining these views with object and query tags, teams can attribute AI costs to specific projects, departments, or users that enable accurate budgeting and chargeback models.\n\n### Containers\n\nContainers in Snowflake, powered by Snowpark Container Services (SPCS), directly impact AI costs by providing dedicated compute pools (ranging from CPU to GPU), that can be selected for running containerized AI/ML workloads such as fine-tuning LLMs, distributed embedding generation, or deploying custom AI models.\n\nUnlike standard virtual warehouses, container runtime costs are based on the uptime of the compute pool (small, medium, or GPU-enabled) and are billed per credit hour, with pricing varying by instance type and region.\n\nRunning AI notebooks or Streamlit apps using container runtime (instead of warehouse runtime) can be more cost-effective, especially for GPU-intensive tasks, since Snowflake manages the underlying infrastructure with no additional fees for registry, networking, or logs.\n\nHowever, long-running or oversized containers can quickly increase costs, so it\u2019s essential to monitor `CONTAINER_SERVICES_MONITORING_HISTORY` and use auto-suspend settings.\n\n### Query Patterns and Resource Allocation\n\nEffective cost control still hinges on optimizing query patterns and resource allocation like all other data querying does.\n\nA common mistake is relying on simple duration-based cost estimates, which ignore concurrency and auto-suspend delays. Instead, use a normalized, warehouse-uptime-based method to allocate costs. Treat each warehouse runtime as a single event, then distribute costs proportionally based on query duration within that window.\n\nThis method accounts for idle time and concurrency, revealing true cost per query. For AI workloads, this approach helps identify inefficient models or redundant invocations that inflate spend.\n\nInference costs are especially sensitive to query complexity and model selection.\n\nReduce data scanned to answer AI prompts that have frequently used data chunks that are flattened and stored in separate columns. This takes advantage of Snowflake\u2019s natural columnar storage optimizations.\n\n### Complexity\n\nComplex AI models with high token counts or large context windows consume more credits per inference. Teams should evaluate model efficiency, use caching where possible, and avoid overusing AI for simple tasks.\n\n### Cost Anomalies Feature\n\nSnowflake\u2019s Cost Anomalies feature (released May 2025) can flag unexpected spikes in AI usage, enabling proactive intervention.\n\n### Query Tags\n\nAdditionally, leveraging query tags on AI queries allows for real-time cost tracking and automated alerts when thresholds are breached.\n\n### Resource Monitors\n\nProactive cost optimization involves continuous monitoring and automation.\n\nResource monitors in Snowflake are designed to track and control credit consumption primarily for virtual warehouses and cloud services and are limited in their ability to monitor and limit AI Services costs.\n\nThey can only indirectly control the compute costs associated with executing AI functions by setting credit quotas and triggering alerts or suspensions when usage thresholds are exceeded on virtual warehouses where AI queries run.\n\nYou can assign a warehouse used for AI workloads to a resource monitor to prevent runaway compute spend, even though the AI token processing cost (serverless) is unaffected. For direct AI cost control, use budgets with monitoring on `AI_SERVICES` usage.\n\n### Cost Insights and Cloud Services Optimizations Features\n\nSnowflake\u2019s Cost Insights and Cloud Services Optimizations features provide recommendations to reduce inefficiencies, such as unused AI assets or poorly optimized queries.\n\n### External Tools utilizing Account Usage Views\n\nExternal tools like Atlan can enhance visibility by leveraging metadata from `ACCOUNT_USAGE` to detect unused tables, materialized views, or transient data used in AI pipelines. Regular cleanup of unused AI artifacts and temporary data reduces both storage and compute costs.\n\n## Cost Summary\n\nTo query standard compute costs for AI workloads in Snowflake, focus on the `WAREHOUSE_METERING_HISTORY` view in the `SNOWFLAKE.ACCOUNT_USAGE` schema, which tracks credit consumption for virtual warehouses used during AI operations such as running Cortex Analyst queries, refreshing Cortex Search indexes, or executing embedding jobs.\n\nSince AI functions like `AI_EXTRACT` or `AI_EMBED` rely on standard warehouses for orchestration, even though the AI processing itself uses AI Services compute, their associated compute costs appear under `CREDITS_USED_COMPUTE` .\n\nSo, filter by `WAREHOUSE_NAME` and `START_TIME` to isolate usage during AI-related workloads and join with `QUERY_HISTORY` to attribute costs to specific AI queries, such as those using `SNOWFLAKE.CORTEX` functions.\n\nUse resource monitors or tagging to allocate these costs by team or project, ensuring visibility into AI-driven compute spend alongside general usage.\n\n## Cortex Code Costs\n\nCortex Code charges standard Cloud Services compute costs for accessing metadata in Snowflake such as using `DESCRIBE` or querying `SNOWFLAKE.ACCOUNT_USAGE` tables and these costs can be found in the regular `QUERY_HISTORY` view.\n\nAt the time of this writing ( _2026\u201302\u201309),_ Snowflake is not charging for the use of Cortex Code. There are charges being incurred as AI Services for the LLMs that are executed. They will be billed once Snowflake starts charging for the use of Cortex Code with token pricing.\n\nThese costs show under AI\\_SERVICES in `WAREHOUSE_METERING_HISTORY` in the following query:\n\n```\nSELECT  \n  USAGE_DATE,  \n  SERVICE_TYPE,  \n  CREDITS_USED,  \n  CREDITS_USED * <your_credit_rate> AS estimated_cost_usd  \nFROM SNOWFLAKE.ORGANIZATION_USAGE.METERING_DAILY_HISTORY  \nWHERE SERVICE_TYPE IN ('AI_SERVICES', 'AI_INFERENCE')  \n  AND USAGE_DATE >= DATEADD('month', -3, CURRENT_DATE())  \nORDER BY USAGE_DATE DESC;\n```\n\n## Cortex Search Services Cost\n\nThe Cortex Search Service incurs \u201cServing\u201d compute costs as daily charges even when not actively used.\n\nYou can run this command to see which Cortex Search Services are running:\n\n> SHOW CORTEX SEARCH SERVICES IN ACCOUNT;\n> \n> \n\nUnlike virtual warehouses that can auto-suspend, the serving layer of a Cortex Search service runs continuously to enable low-latency queries. You pay a fee based on the uncompressed indexed data size (in GB/month), which includes both your source data and vector embeddings, regardless of whether any queries are executed.\n\nThis is a \u201crunning cost\u201d that persists as long as the service\u2019s SERVING status is RUNNING. For example:\n\nA very small index < 1 GB ~ less than 1/1000th of a credit/month.  \nA 50GB index incurs ~315 credits/month (at $3/credit).  \nA 100GB index costs ~630 credits/month (~$1,890), even with zero queries.\n\nKey Insight: The cost is tied to availability, not usage. To avoid these charges, suspend the service when not in use (e.g., during development). Resuming takes minutes, and it\u2019s a best practice to only keep production services running.\n\n> ALTER CORTEX SEARCH SERVICE my\\_search\\_service SUSPEND SERVING;\n> \n> \n\nReplace my\\_search\\_service in this statement with your service\u2019s name. This halts billing for the serving layer while preserving your index. To resume later, use RESUME SERVING. You need the OPERATE privilege on the service to perform this action.\n\n## Conclusion\n\n> In conclusion, mastering AI cost querying in Snowflake requires a blend of technical precision and strategic governance.\n> \n> \n\nFocus on accurate cost attribution, optimized query patterns, smart resource sizing, and vigilance on egress.\n\nUse native tools like `ACCOUNT_USAGE` , `QUERY_ATTRIBUTION_HISTORY` , and `WAREHOUSE_METERING_HISTORY` to build a transparent cost model.\n\nCombine this with tagging, budgets, and anomaly detection to turn AI from a cost center into a value-driven capability.\n\nWith the right approach, Snowflake\u2019s AI Data Cloud can deliver powerful insights without breaking the bank.\n\n## APPENDIX I \u2014 Invoice Cost Details\n\nThe COST column in this query should equal the amount of AI INFERENCE and AI SERVICES line-items on the Snowflake Monthly Statement.\n\nA COMPUTE aggregation is also provided as a total for virtual warehouse cost.\n\n```\nWITH invoice_cost_details as (  \n  \n    -- COMPUTE - BY WAREHOUSE OR USER NAME  \n    SELECT start_date as USAGE_DATE,  \n        'COMPUTE' as SERVICE_GROUP,  \n        'COMPUTE_WAREHOUSES' AS SERVICE_NAME,  \n        SUM(CREDITS_USED_CLOUD_SERVICES) AS CREDITS_USED,  \n        SUM(CREDITS_USED_CLOUD_SERVICES) * << your credit price here >> AS COST,  \n        0 as TOKENS  \n    FROM snowflake.account_usage.query_history  \n    WHERE TO_DATE(START_DATE) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    AND WAREHOUSE_NAME = '<< your warehouse name here >>'  \n    GROUP BY ALL  \n    UNION ALL    \n  \n  \n    -- COREX AISQL  \n    SELECT  \n        TO_DATE(USAGE_TIME) AS USAGE_DATE,  \n        'AI SERVICES' as SERVICE_GROUP,  \n        'CORTEX_AISQL' AS SERVICE_NAME,  \n        SUM(TOKEN_CREDITS) AS CREDITS_USED,  \n        SUM(TOKEN_CREDITS) * << your credit price here >> AS COST,  \n        SUM(TOKENS) as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_AISQL_USAGE_HISTORY  \n    WHERE TO_DATE(USAGE_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(USAGE_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n  \n    -- DOCUMENT AI (sequence out of alpha order but placed near other doc ai charges)  \n    SELECT  \n        TO_DATE(START_TIME) AS USAGE_DATE,  \n        'INFERENCE DETAIL' as SERVICE_GROUP,  \n        'DOCUMENT_AI' AS SERVICE_NAME,  \n        SUM(CREDITS_USED) AS CREDITS_USED,  \n        SUM(CREDITS_USED) * << your credit price here >> AS COST,  \n        0 as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.DOCUMENT_AI_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n    -- CORTEX FINE_TUNING  \n    SELECT  \n        TO_DATE(START_TIME) AS USAGE_DATE,  \n        'AI SERVICES' as SERVICE_GROUP,  \n        'CORTEX_FINE_TUNING' AS SERVICE_NAME,  \n        SUM(TOKEN_CREDITS) AS CREDITS_USED,  \n        SUM(TOKEN_CREDITS) * << your credit price here >> AS COST,  \n        SUM(TOKENS) as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_FINE_TUNING_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n    -- CORTEX FUNCTIONS USAGE  \n    SELECT  \n        TO_DATE(START_TIME) AS USAGE_DATE,  \n        'AI SERVICES' as SERVICE_GROUP,  \n        'CORTEX_FUNCTIONS_USAGE' AS SERVICE_NAME,  \n        SUM(TOKEN_CREDITS) AS CREDITS_USED,  \n        SUM(TOKEN_CREDITS) * << your credit price here >> AS COST,  \n        SUM(TOKENS) as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_FUNCTIONS_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n    -- CORTEX PROVISIONED  \n    SELECT  \n        TO_DATE(INTERVAL_START_TIME) AS USAGE_DATE,  \n        'AI SERVICES' as SERVICE_GROUP,  \n        'CORTEX_AISQL' AS SERVICE_NAME,  \n        SUM(PTU_CREDITS) AS CREDITS_USED,  \n        SUM(PTU_CREDITS) * << your credit price here >> AS COST,  \n        0 as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_PROVISIONED_THROUGHPUT_USAGE_HISTORY  \n    WHERE TO_DATE(INTERVAL_START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(INTERVAL_START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n    -- REST API ACTIVITY  \n    SELECT  \n        TO_DATE(START_TIME) AS USAGE_DATE,  \n        'AI SERVICES' as SERVICE_GROUP,  \n        'CORTEX_REST_API' AS SERVICE_NAME,  \n        0 AS CREDITS_USED,  \n        SUM(0) * << your credit price here >> AS COST,  \n        SUM(TOKENS) as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_REST_API_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n    -- CORTEX SEARCH (used for serving/text embeddings)  \n    SELECT  \n        TO_DATE(USAGE_DATE) AS USAGE_DATE,  \n        'AI SERVICES' as SERVICE_GROUP,  \n        'CORTEX_SEARCH_DAILY' AS SERVICE_NAME,  \n        SUM(CREDITS) AS CREDITS_USED,  \n        SUM(CREDITS) * << your credit price here >> AS COST,  \n        SUM(TOKENS) as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_SEARCH_DAILY_USAGE_HISTORY  \n    WHERE TO_DATE(USAGE_DATE) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(USAGE_DATE), DATABASE_NAME  \n  \n)  \nSELECT  \n    USAGE_DATE,  \n    SERVICE_GROUP,  -- AI INFERENCE, AI SERVICES or COMPUTE  \n    SERVICE_NAME,  \n    CREDITS_USED,  \n    COST, -- Adjust multiplier as needed  \n    TOKENS  \nFROM invoice_cost_details  \nORDER BY   \n;\n```\n\n## APPENDIX II\u2014 Individual AI Query Details\n\nAny of the Cortex Usage History views that return a Query ID can be applied to the join and query fields in this query.\n\n```\nSELECT   \n    qh.start_time::date as start_date  \n    , 'aisql_usage' as cost_category  \n    , c.query_id  \n    , qh.warehouse_name  \n    , qh.user_name  \n    , c.model_name  \n    , c.function_name  \n    , c.token_credits  \n    , c.token_credits * << your credit price here >> as cost  \n    , c.tokens  \n    , qh.query_text  \n    , qh.database_name  \n    , qh.schema_name  \n    , qh.query_type  \n    , qh.role_name  \n    , qh.query_tag  \n    , qh.start_time  \n    , qh.end_time  \n    , round(qh.total_elapsed_time / (1000*60),2) duration_mins  \n    , qh.total_elapsed_time as duration_as_ms  \n    , round(div0null(qh.bytes_scanned,(pow(1024,3))),1) as bytes_scan_gb  \n    , round(div0null(qh.bytes_spilled_to_local_storage,pow(1024,3)),2) as local_spill_gb  \n    , round(div0null(qh.bytes_spilled_to_remote_storage,pow(1024,3)),2) as remote_spill_gb  \n    , round(qh.percentage_scanned_from_cache,2) as cache_pct  \n    , round(div0null(qh.bytes_sent_over_the_network,pow(1024,3)),2) as network_gb  \n    , qh.queued_overload_time  \n    , qh.queued_provisioning_time  \n    , qh.transaction_blocked_time  \n    , qh.rows_produced  \n    , qh.rows_inserted  \n    , qh.rows_updated  \n    , qh.rows_deleted  \n    , qh.rows_unloaded  \n    , qh.external_function_total_invocations  \n    , qh.external_function_total_sent_bytes  \n    , qh.external_function_total_sent_rows  \n    , qh.external_function_total_received_bytes  \n    , qh.external_function_total_received_rows  \n    , qh.rows_written_to_result  \nFROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY qh  \nJOIN SNOWFLAKE.ACCOUNT_USAGE.CORTEX_AISQL_USAGE_HISTORY c  \nON c.QUERY_ID = qh.QUERY_ID  \nWHERE TO_DATE(qh.START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n;\n```\n\n## `APPENDIX III \u2014 Second-Level Cost Details`\n\nThese second-level costs encompass all of the other Cortex Usage History views at the time of the writing of this article and were collected to be able to see the lower layers of activities in Snowflake where Cortex costs are being attributed for tracking purposes.\n\n```\nWITH second_level_details as (  \n  \n    -- CORTEX ANALYST  \n    SELECT  \n        TO_DATE(START_TIME) AS USAGE_DATE,  \n        'SECOND_LEVEL_DETAIL' as SERVICE_GROUP,  \n        'CORTEX_ANALYST' AS SERVICE_NAME,  \n        'N/A' as DATABASE_NAME,  \n        SUM(CREDITS) AS CREDITS_USED,  \n        SUM(CREDITS) * << your credit price here >> AS COST,  \n        0 as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_ANALYST_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n    --CORTEX DOCUMENT PROCESSING  \n    SELECT  \n        TO_DATE(START_TIME) AS USAGE_DATE,  \n        'SECOND_LEVEL_DETAIL' as SERVICE_GROUP,  \n        'DOCUMENT_PROCESSING' AS SERVICE_NAME,  \n        'N/A' as DATABASE_NAME,  \n        SUM(CREDITS_USED) AS CREDITS_USED,  \n        SUM(CREDITS_USED) * << your credit price here >> AS COST,  \n        0 as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n    -- CORTEX FUNCTIONS USAGE  \n    SELECT  \n        TO_DATE(START_TIME) AS USAGE_DATE,  \n        'SECOND_LEVEL_DETAIL' as SERVICE_GROUP,  \n        'CORTEX_FUNCTIONS_USAGE' AS SERVICE_NAME,  \n        'N/A' as DATABASE_NAME,  \n        SUM(TOKEN_CREDITS) AS CREDITS_USED,  \n        SUM(TOKEN_CREDITS) * << your credit price here >> AS COST,  \n        SUM(TOKENS) as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_FUNCTIONS_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n  \n    -- CORTEX PROVISIONED  \n    SELECT  \n        TO_DATE(INTERVAL_START_TIME) AS USAGE_DATE,  \n        'SECOND_LEVEL_DETAIL' as SERVICE_GROUP,  \n        'CORTEX_AISQL' AS SERVICE_NAME,  \n        'N/A' as DATABASE_NAME,  \n        SUM(PTU_CREDITS) AS CREDITS_USED,  \n        SUM(PTU_CREDITS) * << your credit price here >> AS COST,  \n        0 as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_PROVISIONED_THROUGHPUT_USAGE_HISTORY  \n    WHERE TO_DATE(INTERVAL_START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(INTERVAL_START_TIME), DATABASE_NAME  \n    UNION ALL  \n  \n    -- CORTEX SEARCH SERVICE  \n    SELECT  \n        TO_DATE(START_TIME) AS USAGE_DATE,  \n        'SECOND_LEVEL_DETAIL' as SERVICE_GROUP,  \n        'CORTEX_SEARCH_SERVING' AS SERVICE_NAME,  \n        DATABASE_NAME,  \n        SUM(CREDITS) AS CREDITS_USED,  \n        SUM(CREDITS) * << your credit price here >> AS COST,  \n        0 as TOKENS  \n    FROM SNOWFLAKE.ACCOUNT_USAGE.CORTEX_SEARCH_SERVING_USAGE_HISTORY  \n    WHERE TO_DATE(START_TIME) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \n    GROUP BY TO_DATE(START_TIME), DATABASE_NAME  \n  \n)  \nSELECT  \n    USAGE_DATE,  \n    SERVICE_GROUP,  -- SECOND_LEVEL_DETAIL  \n    SERVICE_NAME,  \n    CREDITS_USED,  \n    COST, -- Adjust multiplier as needed  \n    TOKENS  \nFROM second_level_details  \nORDER BY   \n;\n```\n\n\\-- \n\n\\--\n\n## Written by Angela Harney\n\n41 followers\n\n\u00b7 2 following\n\n## No responses yet\n\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--07331811d42d---------------------------------------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----07331811d42d---------------------------------------)\n\n[Status](https://status.medium.com/?source=post_page-----07331811d42d---------------------------------------)\n\nAbout\n\nCareers\n\nPress\n\n[Blog](https://blog.medium.com/?source=post_page-----07331811d42d---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----07331811d42d---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----07331811d42d---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----07331811d42d---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----07331811d42d---------------------------------------)"
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 3
    }
  ]
}

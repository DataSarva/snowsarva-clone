{"search_id":"search_199e31520ba7462180d4a3395951488c","results":[{"url":"https://docs.snowflake.com/en/user-guide/performance-query-warehouse","title":"Optimizing warehouses for performance | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\n[Get started](/en/user-guide-getting-started)\n\n[Guides](/en/guides)\n\n[Developer](/en/developer)\n\n[Reference](/en/reference)\n\n[Release notes](/en/release-notes/overview)\n\n[Tutorials](/en/tutorials)\n\n[Status](https://status.snowflake.com)\n\n[Guides](/en/guides) [Performance optimization](/en/guides-overview-performance) Optimizing warehouses for performance\n\n# Optimizing warehouses for performance [¶]( \"Link to this heading\")\n\nIn the Snowflake architecture, virtual warehouses provide the computing power that is required to execute queries. Fine-tuning the compute\nresources provided by a warehouse can improve the performance of a query or set of queries.\n\nA warehouse owner or administrator can try the following warehouse-related strategies as they attempt to improve the performance of one or\nmore queries. As they adjust a warehouse based on one of these strategies, they can test the change by re-running the query and [checking its execution time](performance-query-exploring.html) .\n\nWarehouse-related strategies are just one way to boost the performance of queries. For performance strategies involving how data\nis stored, refer to [Optimizing storage for performance](performance-query-storage) .\n\n|Strategy |Description |\n| --- | --- |\n|[Reduce queues](performance-query-warehouse-queue) |Minimizing queuing can improve performance because the time between submitting a query and getting its results is longer when the\nquery must wait in a queue before starting. |\n|[Resolve memory spillage](performance-query-warehouse-memory) |Adjusting the available memory of a warehouse can improve performance because a query runs substantially slower when a warehouse runs\nout of memory, which results in bytes “spilling” onto storage. |\n|[Increase warehouse size](performance-query-warehouse-size) |The larger a warehouse, the more compute resources are available to execute a query or set of queries. |\n|[Try query acceleration](performance-query-warehouse-qas) |The query acceleration service offloads portions of query processing to serverless compute resources, which speeds up the processing\nof a query while reducing its demand on the warehouse’s compute resources. |\n|[Optimize the warehouse cache](performance-query-warehouse-cache) |Query performance improves if a query can read from the warehouse’s cache instead of from tables. |\n|[Limit concurrently running queries](performance-query-warehouse-max-concurrency) |Limiting the number of queries that are running concurrently in a warehouse can improve performance because there are fewer queries\nputting demands on the warehouse’s resources. |\n\n\nTip\n\nOptimizing a warehouse for query performance is more straightforward when the warehouse runs similar workloads. For example, if a\nwarehouse runs significantly different queries, the cost of a performance enhancement might be wasted on a query that does not benefit\nfrom the optimization.\n\nFor general guidelines about distributing workloads to your organization’s warehouses, see the Analyzing Your Workloads section of\nthe [Managing Snowflake’s Compute Resources](https://www.snowflake.com/blog/managing-snowflakes-compute-resources/) (Snowflake blog).\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\n[Share your feedback](/feedback)\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\n[Privacy Notice](https://www.snowflake.com/privacy-policy/) [Site Terms](https://www.snowflake.com/legal/snowflake-site-terms/) Cookies Settings © 2026 Snowflake, Inc. All Rights Reserved.\n\nRelated content\n\n1. [Overview of warehouses](/user-guide/warehouses-overview)\n2. [Exploring execution times](/user-guide/performance-query-exploring)\n3. [Optimizing storage for performance](/user-guide/performance-query-storage)\n\nLanguage: **English**\n\n* [English](/en/user-guide/performance-query-warehouse)\n* [Français](/fr/user-guide/performance-query-warehouse)\n* [Deutsch](/de/user-guide/performance-query-warehouse)\n* [日本語](/ja/user-guide/performance-query-warehouse)\n* [한국어](/ko/user-guide/performance-query-warehouse)\n* [Português](/pt/user-guide/performance-query-warehouse)\n"]},{"url":"https://docs.snowflake.com/en/user-guide/warehouses-overview","title":"Overview of warehouses | Snowflake Documentation","excerpts":["Section Title: Overview of warehouses [¶]( \"Link to this heading\")\nContent:\nWarehouses are required for queries, as well as all DML operations, including loading data into tables. In addition to being defined by its\ntype as either Standard or Snowpark-optimized, a warehouse is defined by its size, as well as the other properties that can be set to help\ncontrol and automate warehouse activity.\nWarehouses can be started and stopped at any time. They can also be resized at any time, even while running, to accommodate the need for more\nor less compute resources, based on the type of operations being performed by the warehouse.\nSection Title: Overview of warehouses [¶]( \"Link to this heading\") > Warehouse size [¶]( \"Link to this heading\")\nContent:\nSize specifies the amount of compute resources available per cluster in a warehouse. Snowflake supports the following warehouse sizes:\nSection Title: Overview of warehouses [¶]( \"Link to this heading\") > Warehouse size [¶]( \"Link to this heading\")\nContent:\n| Warehouse size | Credits / hour (Gen1 warehouses) | Credits / second (Gen1 warehouses) | Notes |\n| X-Small | 1 | 0.0003 | Default size for warehouses created in Snowsight and using [CREATE WAREHOUSE](../sql-reference/sql/create-warehouse) . |\n| Small | 2 | 0.0006 |  |\n| Medium | 4 | 0.0011 |  |\n| Large | 8 | 0.0022 |  |\n| X-Large | 16 | 0.0044 | Default size for warehouses created using Snowsight. |\n| 2X-Large | 32 | 0.0089 |  |\n| 3X-Large | 64 | 0.0178 |  |\n| 4X-Large | 128 | 0.0356 |  |\n| 5X-Large | 256 | 0.0711 | Generally available in Amazon Web Services (AWS) and Microsoft Azure regions, and in preview in US Government regions. |\n| 6X-Large | 512 | 0.1422 | Generally available in Amazon Web Services (AWS) and Microsoft Azure regions, and in preview in US Government regions. |\nSection Title: Overview of warehouses [¶]( \"Link to this heading\") > Warehouse size [¶]( \"Link to this heading\")\nContent:\nThe numbers in the preceding table refer to the first generation (Gen1) of Snowflake standard warehouses.\nFor usage information about the newer Gen2 warehouses, see [Snowflake generation 2 standard warehouses](warehouses-gen2) .\nFor information about credit consumption for generation 2 standard warehouses,\nsee the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nGen2 warehouses aren’t yet available for all cloud service providers or for all regions, and currently are not the default\nwhen you create a standard warehouse.\nTip\nFor information about cost implications of changing the RESOURCE_CONSTRAINT property, see [considerations for changing RESOURCE_CONSTRAINT while a warehouse is running or suspended](warehouses-gen2.html) .\nSection Title: Overview of warehouses [¶]( \"Link to this heading\") > Warehouse size [¶]( \"Link to this heading\")\nContent:\nAnother way that you can scale the capacity of Snowflake warehouses without changing the warehouse size is by using\nmulti-cluster warehouses. For more information about that feature, see [Multi-cluster warehouses](warehouses-multicluster) .\nSection Title: ... > Larger warehouse sizes [¶]( \"Link to this heading\")\nContent:\nLarger warehouse sizes 5X-Large and 6X-Large are generally available in all Amazon Web Services (AWS) and Microsoft Azure regions.\nLarger warehouse sizes are in preview in US Government regions (requires FIPS support on ARM).\nSection Title: ... > Impact on credit usage and billing [¶]( \"Link to this heading\")\nContent:\nAs shown in the above table, there is a doubling of credit usage as you increase in size to the next larger warehouse size for each full\nhour that the warehouse runs; however, note that Snowflake utilizes per-second billing (with a 60-second minimum each time the warehouse\nstarts) so warehouses are billed only for the credits they actually consume.\nThe total number of credits billed depends on how long the warehouse runs continuously. For comparison purposes, the following table shows\nthe billing totals for three different size Gen1 standard warehouses based on their running time (totals rounded to the nearest 1000th of a credit):\n ... \nSection Title: ... > Impact on data loading [¶]( \"Link to this heading\")\nContent:\nIncreasing the size of a warehouse does not always improve data loading performance. Data loading performance is influenced more by\nthe number of files being loaded (and the size of each file) than the size of the warehouse.\nTip\nUnless you are bulk loading a large number of files concurrently (i.e. hundreds or thousands of files), a smaller warehouse\n(Small, Medium, Large) is generally sufficient. Using a larger warehouse (X-Large, 2X-Large, etc.) will consume more credits and may not\nresult in any performance increase.\nFor more data loading tips and guidelines, see [Data loading considerations](data-load-considerations) .\nSection Title: ... > Impact on query processing [¶]( \"Link to this heading\")\nContent:\nThe size of a warehouse can impact the amount of time required to execute queries submitted to the warehouse, particularly for larger, more\ncomplex queries. In general, query performance scales with warehouse size because larger warehouses have more compute resources available to\nprocess queries.\nIf queries processed by a warehouse are running slowly, you can always resize the warehouse to provision more compute resources. The\nadditional resources do not impact any queries that are already running, but once they are fully provisioned they become available for use\nby any queries that are queued or newly submitted.\nTip\nLarger is not necessarily faster for small, basic queries.\nFor more warehouse tips and guidelines, see [Warehouse considerations](warehouses-considerations) .\n ... \nSection Title: ... > Default warehouse for notebooks [¶]( \"Link to this heading\")\nContent:\n[](../_images/logo-snowflake-black.png) [Preview Feature](../release-notes/preview-features) — Open\nAvailable to all accounts.\nTo enhance cost efficiency for notebook workloads, a multi-cluster X-Small warehouse, SYSTEM$STREAMLIT_NOTEBOOK_WH, is automatically\nprovisioned within each account. This warehouse, featuring a maximum of 10 clusters and a 60-second default timeout, uses improved bin\npacking. The ACCOUNTADMIN role has OWNERSHIP privileges.\nSection Title: ... > Recommendations for cost management [¶]( \"Link to this heading\")\nContent:\nSnowflake recommends using the SYSTEM$STREAMLIT_NOTEBOOK_WH warehouse exclusively for notebook workloads.\nTo improve bin-packing efficiency and reduce cluster fragmentation, direct SQL queries from Notebook apps to a separate customer-managed query warehouse. Using a single shared warehouse for all Notebook apps in an account further enhances bin-packing efficiency.\nSeparating notebook Python workloads from SQL queries minimizes cluster fragmentation. This approach optimizes overall costs by ensuring that notebook Python workloads are not co-located with larger warehouses, which are typically used for query execution.\n ... \nSection Title: ... > Precedence for warehouse defaults [¶]( \"Link to this heading\")\nContent:\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\n[Privacy Notice](https://www.snowflake.com/privacy-policy/) [Site Terms](https://www.snowflake.com/legal/snowflake-site-terms/) Cookies Settings © 2025 Snowflake, Inc. All Rights Reserved.\nOn this page\n[Warehouse size]()\n[Auto-suspension and auto-resumption]()\n[Query processing and concurrency]()\n[Warehouse usage in sessions]()\nRelated content\n[Understanding compute cost](/user-guide/cost-understanding-compute)\n[Working with resource monitors](/user-guide/resource-monitors)\nLanguage: **English**\n[English](/en/user-guide/warehouses-overview)\n[Français](/fr/user-guide/warehouses-overview)\n[Deutsch](/de/user-guide/warehouses-overview)\n[日本語](/ja/user-guide/warehouses-overview)\n[한국어](/ko/user-guide/warehouses-overview)\n[Português](/pt/user-guide/warehouses-overview)"]},{"url":"https://docs.snowflake.com/en/user-guide/warehouses-considerations","title":"Warehouse considerations | Snowflake Documentation","excerpts":["Section Title: Warehouse considerations [¶]( \"Link to this heading\")\nContent:\nThis topic provides general guidelines and best practices for using virtual warehouses in Snowflake to process queries. It does not provide specific or absolute numbers, values, or recommendations because every query scenario is different and is affected by\nnumerous factors, including number of concurrent users/queries, number of tables being queried, and data size and composition, as well as\nyour specific requirements for warehouse availability, latency, and cost.\nIt also does not cover warehouse considerations for data loading, which are covered in another topic (see the sidebar).\nThe keys to using warehouses effectively and efficiently are:\nSection Title: Warehouse considerations [¶]( \"Link to this heading\")\nContent:\nExperiment with different types of queries and different warehouse sizes to determine the combinations that best meet your\nspecific query needs and workload.\nDon’t focus on warehouse size. Snowflake utilizes per-second billing, so you can run larger warehouses (Large, X-Large,\n2X-Large, etc.) and simply suspend them when not in use.\nNote\nThese guidelines and best practices apply to both single-cluster warehouses, which are standard for all accounts, and [multi-cluster warehouses](warehouses-multicluster) , which are available in [Snowflake Enterprise Edition](intro-editions) (and higher).\n ... \nSection Title: ... > How does query composition impact warehouse processing? [¶]( \"Link to this heading\")\nContent:\nThe compute resources required to process a query depend on the size and complexity of the query. For the most part, queries scale\nlinearly with respect to warehouse size, particularly for larger, more complex queries. When considering factors that impact query\nprocessing, consider the following:\nThe overall size of the tables being queried has more impact than the number of rows.\nQuery filtering using predicates has an impact on processing, as does the number of joins/tables in the query.\nTip\nTo achieve the best results, try to execute relatively homogeneous queries (complexity, data sets, etc.) on the same warehouse;\nexecuting queries of widely varying complexity on the same warehouse makes it more difficult to analyze warehouse load,\nwhich can make it more difficult to select the best warehouse size to match the complexity, composition, and number of queries in your\nworkload.\n ... \nSection Title: ... > Creating a warehouse [¶]( \"Link to this heading\")\nContent:\nWhen creating a warehouse, the two most critical factors to consider, from a cost and performance perspective, are:\nWarehouse size (that is, available compute resources)\nManual vs automated management (for starting/resuming and suspending warehouses).\nThe number of clusters in a warehouse is also important if you are using [Snowflake Enterprise Edition](intro-editions) (or higher) and [multi-cluster warehouses](warehouses-multicluster) . For more details, see [Scaling Up vs Scaling Out]() (in this topic).\nSection Title: ... > Selecting an initial warehouse size [¶]( \"Link to this heading\")\nContent:\nThe initial size you select for a warehouse depends on the task the warehouse is performing and the workload it processes. For example:\nFor data loading, the warehouse size should match the number of files being loaded and the amount of data in each file. For more information,\nsee [Planning a data load](data-load-considerations-plan) .\nFor queries in small-scale testing environments, smaller warehouses sizes (X-Small, Small, Medium) may be sufficient.\nFor queries in large-scale production environments, larger warehouse sizes (Large, X-Large, 2X-Large, etc.) may be more cost effective.\nHowever, note that per-second credit billing and auto-suspend give you the flexibility to start with larger sizes and then adjust the size\nto match your workloads. You can decrease the size of a warehouse at any time.\nSection Title: ... > Selecting an initial warehouse size [¶]( \"Link to this heading\")\nContent:\nAlso, larger is not necessarily faster for smaller, more basic queries. Small/simple queries typically do not need an X-Large (or larger)\nwarehouse because they do not necessarily benefit from the additional resources, regardless of the number of queries being processed\nconcurrently. In general, you should try to match the size of the warehouse to the expected size and complexity of the queries to be\nprocessed by the warehouse.\nTip\nExperiment by running the same queries against warehouses of multiple sizes (for example, X-Large, Large, Medium). The queries you experiment\nwith should be of a size and complexity that you know will typically complete within 5 to 10 minutes (or less).\n ... \nSection Title: ... > Selecting a warehouse for Snowsight [¶]( \"Link to this heading\")\nContent:\nSnowsight performance can be affected if the warehouse is temporarily overloaded and UI queries are queued behind other active\nworkloads. If you notice inconsistent Snowsight performance, Snowflake recommends that you review the selected warehouse for\noverload and consider using one with lower utilization. Large accounts with many active users might benefit from a dedicated X-Small\nwarehouse for UI-related tasks.\nYou can view which Snowsight queries have been running on the currently selected warehouse and when they ran. To monitor these\nqueries, follow these steps:\nIn the navigation menu, select Monitoring » Query History .\nSelect the Filters drop-down list.\nSelect the Client-generated statements checkbox to view internal queries run by a client, driver, or library, including the web interface.\nSelect Apply Filters .\nFor information about cost governance, see [Exploring compute cost](cost-exploring-compute) .\nSection Title: ... > Using the default warehouse for Notebook apps [¶]( \"Link to this heading\")\nContent:\n[](../_images/logo-snowflake-black.png) [Preview Feature](../release-notes/preview-features) — Open\nAvailable to all accounts.\nEach account is provisioned with the SYSTEM$STREAMLIT_NOTEBOOK_WH warehouse that is specifically designed to run Notebook Python code. This multi-cluster X-Small warehouse helps reduce cluster fragmentation, optimize costs, and improve bin-packing efficiency. For more\ndetails, see [Default warehouse for notebooks](warehouses-overview.html) .\n ... \nSection Title: ... > Automating warehouse suspension [¶]( \"Link to this heading\")\nContent:\nImportant\nIf you choose to disable auto-suspend, carefully consider the costs associated with running a warehouse continually, even when the\nwarehouse is not processing queries. The costs can be significant, especially for larger warehouses (X-Large, 2X-Large, etc.).\nTo disable auto-suspend, you must explicitly select Never in the web interface, or specify `0` or `NULL` in SQL.\n ... \nSection Title: ... > Multi-cluster warehouses improve concurrency [¶]( \"Link to this heading\")\nContent:\nIf you are using Snowflake Enterprise Edition (or a higher edition), all your warehouses should be configured as multi-cluster\nwarehouses. Unless you have a specific requirement for running in Maximized mode, multi-cluster warehouses should be configured to run in Auto-scale\nmode, which enables Snowflake to automatically start and stop clusters as needed. When choosing the minimum and maximum number of clusters for a multi-cluster warehouse:Minimum :\nKeep the default value of `1` ; this ensures that additional clusters are only started as needed. However, if\nhigh-availability of the warehouse is a concern, set the value higher than `1` . This helps ensure multi-cluster warehouse availability\nand continuity in the unlikely event that a cluster fails. Maximum :\nSet this value as large as possible, while being mindful of the warehouse size and corresponding credit costs."]},{"url":"https://select.dev/posts/snowflake-cost-optimization","title":"Snowflake Cost Optimization: 15 proven strategies for reducing costs","excerpts":["Section Title: Snowflake Cost Optimization: 15 proven strategies for reducing costs > Cost Optimization Techniques\nContent:\nThe cost reduction techniques in this post fall into six broad categories:\n**1. Virtual warehouse configuration**\nReducing auto-suspend\nReducing the warehouse size\nEnsure minimum clusters are set to 1\nConsolidate warehouses\n**2. Workload configuration**\nReducing query frequency\nOnly process new or updated data\n**3. Table configuration**\nEnsure your tables are clustered correctly\nDrop unused tables\nLower data retention\nUse transient tables\n**5. Data loading patterns**\nAvoid frequent DML operations\nEnsure your files are optimally sized before loading\n**6. Leverage built-in Snowflake controls**\nLeverage access control to restrict warehouse usage & modifications\nEnable query timeouts\nConfigure Snowflake resource monitors\nLet's get straight into it.\n ... \nSection Title: ... > 2. Reduce virtual warehouse size\nContent:\nVirtual warehouse computation resources and cost scale exponentially. Here’s a quick reminder, with compute costs displayed per hour as credits (dollars) assuming a typical rate of $2.5 per credit.\nSection Title: ... > 2. Reduce virtual warehouse size\nContent:\n| Warehouse Size | Hourly virtual warehouse pricing |\n| X-Small | 1 ($2.50) |\n| Small | 2 ($5) |\n| Medium | 4 ($10) |\n| Large | 8 ($20) |\n| X-Large | 16 ($40) |\n| 2X-Large | 32 ($80) |\n| 3X-Large | 64 ($160) |\n| 4X-Large | 128 ($320) |\n| 5X-Large | 256 ($640) |\n| 6X-Large | 512 ($1280) |\nSection Title: ... > 2. Reduce virtual warehouse size\nContent:\nHere’s the monthly pricing for each warehouse assuming running continuously (though not typically how warehouses run due to auto-suspend, it gives a better sense of cost than hourly):\nSection Title: ... > 2. Reduce virtual warehouse size\nContent:\n| Warehouse Size | Hourly virtual warehouse pricing |\n| X-Small | 720 ($1,800) |\n| Small | 1,440 ($3,600) |\n| Medium | 2,880 ($7,200) |\n| Large | 5,760 ($14,400) |\n| X-Large | 11,520 ($28,800) |\n| 2X-Large | 23,040 ($57,600) |\n| 3X-Large | 46,080 ($115,200) |\n| 4X-Large | 92,160 ($230,400) |\n| 5X-Large | 184,320 ($460,800) |\n| 6X-Large | 368,640 ($921,600) |\nSection Title: ... > 2. Reduce virtual warehouse size\nContent:\nOver-sized warehouses can sometimes make up the majority of Snowflake usage. Reduce warehouse sizes and observe the impact on workloads. If performance is still acceptable, try reducing size again. Check out our full guide to [choosing the right warehouse size in Snowflake](https://select.dev/posts/snowflake-warehouse-sizing) , which includes practical heuristics you can use to identify oversized warehouses.\n**Example of reducing the warehouse size:**\nAs a quick practical example, consider a data loading job that loads ten files every hour on a Small size warehouse. A small size warehouse has 2 nodes and a total of 16 cores available for processing. This job can at most saturate 10 out of the 16 cores (1 file per core), meaning this warehouse will not be fully utilized. It would be significantly more cost effective to run this job on an X-Small warehouse.\n ... \nSection Title: Snowflake Cost Optimization: 15 proven strategies for reducing costs > 4. Consolidate warehouses\nContent:\nThe best way to ensure virtual warehouses are being utilized efficiently is to use as few as possible. Where needed, create separate warehouses based on performance requirements versus domains of workload.\nFor example, creating one warehouse for all [data loading](https://select.dev/posts/snowflake-data-loading-overview) , one for transformations, and one for live BI querying will lead to better cost efficiency than one warehouse for marketing data and one for finance data. All data-loading workloads typically have the same performance requirements (tolerate some queueing) and can often share a multi-cluster X-Small warehouse. In contrast, all live, user-facing queries may benefit from a larger warehouse to reduce latency.\nSection Title: Snowflake Cost Optimization: 15 proven strategies for reducing costs > 4. Consolidate warehouses\nContent:\nWhere workloads within each category (loading, transformation, live querying, etc.) need a larger warehouse size for acceptable query speeds, create a new larger warehouse just for those. For best cost efficiency, queries should always run on the smallest warehouse they perform sufficiently quickly on.\n ... \nSection Title: ... > 12. Ensure files are optimally sized\nContent:\nTo ensure cost effective data loading, a best practice is to keep your files around 100-250MB. To demonstrate these effects, consider the image below. If we only have one 1GB file, we will only saturate 1/16 threads on a Small warehouse used for loading.\nIf you instead split this file into ten files that are 100 MB each, you will utilize 10 threads out of 16. This level parallelization is much better as it leads to better utilisation of the given compute resources (although it's worth noting that an X-Small would still be the better choice in this scenario).\nHave too many small files can also lead to excessive costs if you are using [Snowpipe](https://select.dev/posts/snowflake-snowpipe) for data loading, since Snowflake charges an overhead fee of 0.06 credits per 1000 files loaded.\nSection Title: Snowflake Cost Optimization: 15 proven strategies for reducing costs > 13. Leverage access control\nContent:\nAccess control is a powerful technique for controlling costs that many Snowflake customers don't think of. By restricting who can make changes to virtual warehouses, you will minimize the chances of someone accidentally making an unintended resource modification that leads to unexpected costs. We've seen many scenarios where someone increases a virtual warehouse size and then forgets to change it back. By implementing stricter access control, companies can ensure that resource modifications go through a controlled process and minimize the chance of unintended changes being made.\nSection Title: Snowflake Cost Optimization: 15 proven strategies for reducing costs > 13. Leverage access control\nContent:\nYou can also use access control to limit which users can run queries on certain warehouses. By only allowing users to use smaller warehouses, you will force them to write more efficient queries rather than defaulting to running on a larger warehouse size. When required, there can be policies or processes in place to allow certain queries/users to run on a larger warehouse wehn absolutely necessary.\n ... \nSection Title: ... > The Missing Manual: Everything You Need to Know about Snowflake Cost Optimization (April 2023)\nContent:\nIf you're looking for a presentation which covers many of the topics discussed in the post, we recommend watching the talk we gave at Data Council in April 2023.\nIn this talk, we cover everything you need to know about cost and performance optimization in Snowflake. We start with a deep dive into Snowflake’s architecture & billing model, covering key concepts like virtual warehouses, micro-partitioning, the lifecycle of a query and Snowflake’s two-tiered cache. We go in depth on the most important optimization strategies, like virtual warehouse configuration, table clustering and query writing best practices. Throughout the talk, we share code snippets and other resources you can leverage to get the most out of Snowflake.\n**Recording**\nA recording of the presentation is [available on YouTube](https://youtu.be/z6sbY-c6gAQ) ."]},{"url":"https://www.montecarlodata.com/blog-snowflake-cost-optimization/","title":"5 Snowflake Cost Optimization Techniques You Should Know","publish_date":"2025-07-06","excerpts":["Section Title: ... > Snowflake pricing: how the model works\nContent:\nTo understand Snowflake cost optimization strategies and best practices, you first need to know how the consumption based pricing model works.\n[Actual pricing](https://www.snowflake.com/pricing/) depends on a few different variables such as the cloud provider, region, plan type, services, and more. Since we’re not getting too crazy, we will oversimplify a bit.\nEssentially, your Snowflake cost is based on the actual monthly usage of three separate items: storage, compute, and cloud services. You will be charged for any [Snowflake serverless features](https://docs.snowflake.com/en/user-guide/admin-serverless-billing.html) you use as well.\nExample Snowflake pricing in the AWS – US East region. Image from Snowflake.com.\n ... \nSection Title: ... > Is there a Snowflake object that does not consume any storage costs?\nContent:\nYou can get started with Snowflake for free with a [trial account](https://docs.snowflake.com/en/user-guide/admin-trial-account) , and the balance of your free usage decreases as you consume credits to use compute and accrue storage costs.\nIn addition, temporary and transient tables do not incur the same fees as standard permanent tables. This helps to manage the storage costs associated with time travel and fail-safe.\nCourtesy of [Snowflake.](https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs)\nAll of this is to say that Snowflake pricing is mainly driven by credit consumption, which is mainly driven by storage and compute; **which is mainly driven by the amount of tables in your environment, the SQL queries being run on those tables, and the sizes of your data warehouses** .\nSection Title: ... > Is there a Snowflake object that does not consume any storage costs?\nContent:\nThat is why our Snowflake cost optimization strategy will focus on optimizing those three areas by leveraging native features, best practices, and other solutions.\nSection Title: ... > Step 1: Snowflake warehouse size optimization\nContent:\nSnowflake virtual warehouses come in 10 sizes. The larger the warehouse the more credits it consumes per hour of actively running queries.\nImage from Snowflake [pricing guide](https://www.snowflake.com/pricing-page-registration-page/) .\nYou might assume the best Snowflake cost optimization strategy would be to keep your warehouse as small as possible, but that’s not necessarily true. That’s because the larger data warehouses also run queries faster.\nThere isn’t a magical formula for how to optimize warehouse size for your typical workloads– **it’s a process of trial and error** . That being said, there are some Snowflake cost optimization best practices related to rightsizing your warehouse.\nSection Title: ... > Group similar workloads in the same virtual warehouse\nContent:\nGrouping similar workloads is how [Snowflake runs](https://www.snowflake.com/blog/managing-snowflakes-compute-resources/) its own internal instance. It’s an effective strategy because you can tailor configuration settings that impact efficiency **without worrying about cutting too deep** . These settings include [auto-suspend, auto-resume](https://docs.snowflake.com/en/user-guide/warehouses-overview.html) , [scaling policy, clusters](https://docs.snowflake.com/en/user-guide/warehouses-multicluster.html) , and [statement timeout](https://docs.snowflake.com/en/sql-reference/parameters.html) .\nFor example, the [default Snowflake auto-suspend policy is 600 seconds](https://docs.snowflake.com/en/sql-reference/sql/alter-warehouse.html) , but that can be adjusted as low as [60 seconds by running a SQL statement](https://www.nagarro.com/en/blog/reduce-snowflake-cost) .\nSection Title: ... > Group similar workloads in the same virtual warehouse\nContent:\nThis is a great example of how you can “get too crazy” with Snowflake cost optimization because if you set the auto-suspend more tightly than any gaps in your query workload, the **warehouse could end up in a continual state of auto-suspend and auto-resume** .\nIn fact, [Snowflake recommends](https://docs.snowflake.com/en/user-guide/warehouses-considerations.html) considering disabling auto-suspend for a warehouse if there are heavy, steady workloads on the warehouse or you want availability with no lag time.\nEssentially, this strategy comes down to making sure you have the right configuration for the right workload.\nHow Snowflake groups their workloads by virtual warehouse, which can help with Snowflake cost optimization efforts. [Source](https://www.snowflake.com/blog/managing-snowflakes-compute-resources/) .\nSection Title: ... > Leverage data SLAs to define workloads and value to business\nContent:\nYou’ve decided to group similar workloads in the same virtual warehouse. Great! But first you need to do the hard part–talking to business stakeholders to properly scope workloads and set SLAs.\nWhile you can look at your logs to determine workload type, frequency, typical scan size, etc., what you can’t determine without talking to your business stakeholders is the expectations for [data freshness](https://www.montecarlodata.com/blog-data-freshness-explained/) and its true value to the business.\nIf you’re feeling fancy, this can also help you **implement a chargeback model** to assign spending to different data products or domains.\n ... \nSection Title: ... > Start small and right size utilization\nContent:\nOnce you have your workloads grouped together, err on the side of selecting a smaller warehouse and begin the trial and error process.\n[Snowflake recommends](https://docs.snowflake.com/en/user-guide/warehouses-considerations.html) , “The queries you experiment with should be of a size and complexity that you know will typically complete within 5 to 10 minutes (or less).”\n**You’ll also want to [monitor your queue](https://community.snowflake.com/s/article/Understanding-Queuing)** to make sure jobs aren’t spending an inordinate amount of time waiting their turn as the overall workload will take longer to execute and consume more credits.\n ... \nSection Title: ... > Step 2: Snowflake query optimization\nContent:\nBest practices for optimizing Snowflake queries could be an [entirely separate blog](https://www.snowflake.com/blog/how-cisco-optimized-performance-on-snowflake-to-reduce-costs-15-part-2/) and were not getting too crazy, so we will focus our attention here on how to leverage helpful features to identify costly outliers.\nWe will also assume you have optimized your session timeout, queue, and other warehouse settings discussed in the previous section.\n ... \nSection Title: ... > Zero-copy cloning for development\nContent:\nOne of Snowflake’s most cost-effective features is [zero-copy cloning](https://docs.snowflake.com/en/user-guide/object-clone.html) . Clones leverage Time Travel mechanics without consuming additional storage until the data diverges. This creates an instant copy of your production data for development or testing without doubling storage costs. As the development environment makes changes, only the delta consumes additional storage.\nSection Title: ... > Zero-copy cloning for development\nContent:\nZero-copy cloning is perfect for creating development and test environments, sharing data across teams without duplication, taking snapshots before major data transformations, and implementing blue-green deployment strategies. This makes [data warehouse testing](https://www.montecarlodata.com/blog-data-warehouse-testing-7-steps) both cost-effective and risk-free since you can test schema changes, new ETL processes, or optimization strategies against production-scale data without impacting costs or performance. You can clone individual tables, entire schemas, or even databases instantaneously."]},{"url":"https://keebo.ai/2024/01/12/choosing-the-best-snowflake-warehouse-size/","title":"Choosing the Optimal Snowflake Warehouse Size for Your Workloads","excerpts":["[Solutions](#)\n+ [By Use Case](#)\n- [Cost Optimization](/cost-optimization/)\n- [Visibility and FinOps](/visibility-finops/)\n- [Performance and Team Efficiency](/performance-and-team-efficiency/)\n+ [By Industry](#)\n- [Financial Services](/snowflake-databricks-financial-services/)\n- [Healthcare](/snowflake-databricks-healthcare/)\n- [Technology](/snowflake-databricks-technology/)\n- [Retail and CPG](/snowflake-databricks-retail/)\n[Platform](#)\n+ [Warehouse Optimization](https://keebo.ai/warehouse-optimization/)\n+ [Workload Intelligence](https://keebo.ai/workload-intelligence/)\n+ [Query Routing](https://keebo.ai/query-routing/)\n+ [Architecture & Security](https://keebo.ai/architecture-security/)\n[Pricing](/pricing/)\n[Resources](/resources/)\n+ [Case Studies](/resources/case-studies/)\n+ [Guides and Whitepapers](/resources/documentation/)\n+ [Solution Briefs](/resources/solution-briefs/)\n+ [Security](/architecture-security/)\n+ [Videos](/resources/videos/)\n+ [Blog](/blogs/)\n[Company](#)\n+\n ... \nSection Title: Choosing the Best Snowflake Warehouse Size\nContent:\n[](https://keebo.ai/author/carl/) [Carl Dubler](https://keebo.ai/author/carl/ \"Posts by Carl Dubler\")\nJanuary 12, 2024\n[Automation](https://keebo.ai/category/automation/) , [Data Engineering](https://keebo.ai/category/data-engineering/) , [Reducing Snowflake Costs](https://keebo.ai/category/reducing-snowflake-costs/) , [Warehouse Optimization](https://keebo.ai/category/warehouse-optimization/)\n*Because Snowflake is a pay-as-you-go data cloud, and the main thing you are paying for is warehouse size, it pays to get the warehouse size right. You can try to do this on your own or enlist a fully-automated optimizer like Keebo to do it for you. Let’s discuss.*\n ... \nSection Title: Choosing the Best Snowflake Warehouse Size > Author > Carl Dubler > Recent News\nContent:\n[Independent FinOps or Vendor‑Native FinOps: A Practical Guide for Snowflake & Databricks](https://keebo.ai/2025/09/11/independent-finops-vs-vendor/) September 11, 2025\n[Automating FinOps without Losing Accountability: A Data Engineering Guide to Snowflake Cost Optimization](https://keebo.ai/2025/09/08/automation-vs-accountability-snowflake-and-databricks-cost-optimization/) September 8, 2025\n[How Snowflake Query Timeout Keeps Costs from Snowballing Out of Control](https://keebo.ai/2025/09/05/snowflake-query-timeout/) September 5, 2025\n[Why Execution Is the Next Frontier in FinOps | Keebo](https://keebo.ai/2025/07/28/finops-execution-gap/) July 28, 2025\n[Storage Cost Optimization for Snowflake — How to Reduce Your TB Spend](https://keebo.ai/2025/06/26/snowflake-storage-unit-rates-cost-optimization/) June 26, 2025"]},{"url":"https://medium.com/@vuppala.venkat/snowflake-warehouse-sizing-best-practices-27b3063b6261","title":"Snowflake warehouse sizing best practices | by Vuppala Venkat | Medium","publish_date":"2024-06-30","excerpts":["Section Title: Snowflake warehouse sizing best practices > Understanding Snowflake Costs > Optimize Warehouse Size\nContent:\nThe first and most effective step in reducing Snowflake costs is optimizing the size of your warehouses. Here’s why this matters:\n**Cost Impact** : Larger warehouses consume more credits per hour, which can quickly add up, especially in environments with high query loads.\n**Performance Balance** : While larger warehouses can offer better performance for complex queries, they may not always be necessary for smaller or less demanding workloads.\nSection Title: Snowflake warehouse sizing best practices > ... > Best Practices for Optimization\nContent:\n**Right-Sizing** : Assess your workload requirements and choose a warehouse size that balances cost and performance. This may involve experimenting with different sizes to find the most efficient configuration.\n**Scaling** : Take advantage of Snowflake’s ability to scale up or down based on demand. Use auto-suspend and auto-resume features to minimize costs during low-usage periods.\n**Monitoring** : Regularly review your usage patterns and adjust warehouse sizes as necessary. Snowflake provides detailed usage reports that can help you identify optimization opportunities.\nBy focusing on the size and configuration of your Snowflake warehouses, you can significantly reduce costs while maintaining the performance levels needed for your business operations. Remember, the key is to balance cost efficiency with the performance demands of your data environment.\n ... \nSection Title: Snowflake warehouse sizing best practices > **Scaling policy**\nContent:\nFor complex queries, consider using scale-up to increase compute power. For handling multiple concurrent queries, scale-out is the best approach. You can also enable ENABLE_QUERY_ACCELERATION =TRUE and set the correct QUERY_ACCELERATION_MAX_SCALE_FACTOR if you only need bigger warehouse size for a very small set of queries\n**Balancing Performance and Cost:**\nIf you’re comfortable with some additional queuing time, setting the `SCALING_POLICY` to `ECONOMY` can further optimize costs. This setting allows for more efficient resource allocation while keeping expenses in check.\nBy implementing these strategies, you can maintain a balance between performance needs and cost optimization, ensuring your Snowflake environment runs smoothly and economically."]},{"url":"https://docs.snowflake.com/en/user-guide/cost-optimize","title":"Optimizing cost - Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\n[Get started](/en/user-guide-getting-started)\n[Guides](/en/guides)\n[Developer](/en/developer)\n[Reference](/en/reference)\n[Release notes](/en/release-notes/overview)\n[Tutorials](/en/tutorials)\n[Status](https://status.snowflake.com)\n[Guides](/en/guides) [Cost & Billing](/en/guides-overview-cost) Optimization\nSection Title: Optimizing cost [¶]( \"Link to this heading\")\nContent:\nThis topic summarizes the features and strategies you can use to optimize Snowflake to reduce costs and maximize your spend.\n[Using cost insights to save](cost-insights)\nLearn how to use cost insights to optimize Snowflake for cost within a particular account.\n[Optimizing cloud services for cost](cost-optimize-cloud-services)\nLearn how to adjust your cloud services usage to reduce costs.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\n[Share your feedback](/feedback)\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\n[Privacy Notice](https://www.snowflake.com/privacy-policy/) [Site Terms](https://www.snowflake.com/legal/snowflake-site-terms/) Cookies Settings © 2026 Snowflake, Inc. All Rights Reserved.\nRelated content\nSection Title: Optimizing cost [¶]( \"Link to this heading\")\nContent:\n[Managing cost in Snowflake](/user-guide/cost-management-overview)\n[Understanding overall cost](/user-guide/cost-understanding-overall)\nLanguage: **English**\n[English](/en/user-guide/cost-optimize)\n[Français](/fr/user-guide/cost-optimize)\n[Deutsch](/de/user-guide/cost-optimize)\n[日本語](/ja/user-guide/cost-optimize)\n[한국어](/ko/user-guide/cost-optimize)\n[Português](/pt/user-guide/cost-optimize)\nSection Title: Optimizing cost [¶]( \"Link to this heading\") > Snowflake's Use of Cookies\nContent:\nWe use cookies to enhance your experience and to analyze site traffic as described in our Cookie Statement. By accepting, you consent to our use of cookies. [Cookie Statement.](https://www.snowflake.com/privacy-policy/cookie-statement/)\nCookies Settings Reject All Accept All Cookies\nSection Title: Optimizing cost [¶]( \"Link to this heading\") > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies"]},{"url":"https://medium.com/@riyukhandelwal/snowflake-warehouse-tuning-guide-sizing-scaling-cost-optimization-1f943be9d0b4","title":"Snowflake Warehouse Tuning Guide: Sizing, Scaling & Cost Optimization | by Riya Khandelwal | Medium","publish_date":"2025-11-17","excerpts":["Section Title: Snowflake Warehouse Tuning Guide: Sizing, Scaling & Cost Optimization\nContent:\n✔ How sizing impacts execution plans\n✔ How Snowflake handles memory, CPU & spills\n✔ How multi-cluster scaling works internally\n✔ Managing concurrency without overprovisioning\n✔ Query profile analysis for warehouse optimization\n✔ Cost optimization strategies backed by best practices\nIf you’ve used XS, S, M without understanding what happens under the hood, this guide is for you.\n ... \nSection Title: ... > 2. Warehouse Sizing: How Size Impacts Performance\nContent:\nSnowflake’s Virtual Warehouses scale **linearly** as you move from XS → S → M → L → XL → 2XL → 3XL → 4XL → 5XL → 6XL.\nPress enter or click to view image in full size\nThis scaling directly increases three resources that determine query performance:\nCPU cores — More CPU means Snowflake can break your query into more micro-tasks and process them concurrently.\nMemory — Memory determines whether Snowflake can execute operations **in-memory** or whether it must spill to disk.\nTemp disk — Even if memory is insufficient, Snowflake continues processing by spilling to temporary disk.\n ... \nSection Title: Snowflake Warehouse Tuning Guide: Sizing, Scaling & Cost Optimization > ... > 5.3 Spill Behavior\nContent:\nSpilling is one of the most important and least understood performance factors. Spilling happens when snowflake **runs out of memory** for joins, sorts, aggregates, or window functions. There are 3 levels:\n1️⃣ No Spill — best performance, all operations fit in RAM.\n2️⃣ Spill to Local Disk — moderate slowdown, if memory is insufficient, Snowflake writes temporary data to the warehouse’s local SSD.\n3️⃣ Spill to Remote — severe slowdown, if local disk isn’t enough, Snowflake spills to cloud storage (S3/Azure/GCS). This is the *worst-case scenario* .\nIf you see spill-to-remote → scale up immediately."]},{"url":"https://ternary.app/blog/snowflake-cost-optimization/","title":"Snowflake cost optimization: 8 proven strategies for reducing costs","publish_date":"2025-09-22","excerpts":["Section Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Compute layer\nContent:\nVirtual warehouses are basically compute clusters that run your queries and handle your data loads. They scale independently, and you can have more than one running at a time. The key thing to know here: compute is paid using Snowflake credits. You spin up a warehouse, and you start spending credits. You pause it, it stops costing you. Sounds fair, but it also means you’ve got to stay on top of usage.\n ... \nSection Title: ... > An example of how Snowflake calculates cost\nContent:\n**Note:** This example is courtesy of Snowflake.\nSuppose we have a customer using Snowflake Capacity Standard Service with Premier Support in the U.S.\nThey do 3 main things:\nLoad data nightly using a small virtual warehouse.\nSupport 8 users working 10 hours a day, 5 days a week, using a medium virtual warehouse.\nStore 4 TB of compressed data on Snowflake.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Data loading costs\nContent:\n| **Warehouse used** | Small Standard Virtual Warehouse |\n| **Rate** | 2 credits per hour |\n| **Usage** | 2.5 hours daily for 31 days/month |\n| **Monthly Credits** | 2 credits/hour × 2.5 hours/day × 31 days = 155 credits/month |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > User activity costs\nContent:\n| **Users** | 8 users |\n| **Warehouse used** | Medium Standard Virtual Warehouse |\n| **Rate** | 4 credits per hour |\n| **Usage** | 10 hours/day, 20 workdays/month |\n| **Monthly credits for users** | 4 credits/hour × 10 hours/day × 20 days = 800 credits/month |\n| **Total monthly credits (users + loading)** | 800 + 155 = 955 credits/month |\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Total annual cost\nContent:\n| **Storage** | $1,104 |\n| **Virtual warehouse** | $21,774 |\n| **Grand Total** | $22,878 per year |\n ... \nSection Title: ... > 6. Allow multi-cluster warehouses to scale down\nContent:\n| ALTER WAREHOUSE your_warehouse_name **SET** MIN_CLUSTER_COUNT = 1; |\n ... \nSection Title: ... > Check for query performance tuning support\nContent:\nLook for tools that not only track slow or costly queries but also help you understand why they’re inefficient, whether it’s due to joins, filters, or warehouse sizing.\n ... \nSection Title: ... > What is the biggest contributor to Snowflake costs?\nContent:\nCompute is usually the biggest cost driver in Snowflake. Virtual warehouses charge based on per-second usage, and costs vary with warehouse size and workload."]}],"usage":[{"name":"sku_search","count":1}]}

{
  "search_id": "search_5b81b2ff14d04c91a54e8ffafa9fc462",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/user-guide/search-optimization/cost-estimation",
      "title": "Search optimization cost estimation and management | Snowflake Documentation",
      "excerpts": [
        "Guides Databases, Tables, & Views Search optimization service Cost estimation and management\n\n# Search optimization cost estimation and management \u00b6\n\n Enterprise Edition Feature\n\nThis feature requires Enterprise Edition (or higher). To inquire about upgrading, please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\n\nThe search optimization service impacts costs for both storage and compute resources:\n\n* Storage resources: The search optimization service creates a search access path data structure that requires space\n  for each table on which search optimization is enabled. The storage cost of the search access path depends upon\n  multiple factors, including:\n  \n    + The number of distinct values in the table. In the extreme case where all columns have data types that use\n        the search access path, and all data values in each column are unique, the required storage can be as much as\n        the original table\u2019s size.\n        \n        Typically, however, the size is approximately 1/4 of the original table\u2019s size.\n* Compute resources:\n  \n    + Adding search optimization to a table consumes resources during the initial build phase.\n    + Maintaining the search optimization service also requires resources. Resource consumption is higher when there is\n        high churn (i.e. when large volumes of data in the table change). These costs are roughly proportional to the\n        amount of data ingested (added or changed). Deletes also have some cost.\n        \n        Automatic clustering , while improving the latency of queries in tables with\n        search optimization, can further increase the maintenance costs of search optimization. If a table has a high churn rate,\n        enabling automatic clustering and configuring search optimization for the table can result in higher maintenance costs than\n        if the table is just configured for search optimization.\n        \n        Snowflake ensures efficient credit usage by billing your account only for the actual resources used. Billing is\n        calculated in 1-second increments.\n        \n        See the \u201cServerless Feature Credit Table\u201d in the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) for the costs per compute hour.\n        \n        Once you enable the search optimization service, you can view the costs for your use of the service .\n\nTip\n\nSnowflake recommends starting slowly with this feature (i.e. adding search optimization to only a few tables at\nfirst) and closely monitoring the costs and benefits.\n\n## Estimating the costs of search optimization \u00b6\n\nTo estimate the cost of adding search optimization to a table and configuring specific columns for search optimization, use the SYSTEM$ESTIMATE\\_SEARCH\\_OPTIMIZATION\\_COSTS function.\n\nIn general, the costs are proportional to:\n\n* The number of columns on which the feature is enabled and the number of distinct values in those columns.\n* The amount of data that changes in these tables.\n\nImportant\n\nCost estimates returned by the SYSTEM$ESTIMATE\\_SEARCH\\_OPTIMIZATION\\_COSTS function are best efforts. The actual realized\ncosts can vary by up to 50% (or, in rare cases, by several times) from the estimated costs.\n\n* Build and storage cost estimates are based on sampling a subset of the rows in the table\n* Maintenance cost estimates are based on recent create, delete, and update activity in the table\n\n## Viewing the costs of search optimization \u00b6\n\nYou can view the actual billed costs for the search optimization service by using either the web interface or SQL.\nSee Exploring compute cost .\n\n## Reducing the costs of search optimization \u00b6\n\nYou can control the cost of the search optimization service by carefully choosing the tables and columns for which to enable search optimization .\n\nIn addition, to reduce the cost of the search optimization service:\n\n* Snowflake recommends batching DML operations on the table:\n  \n    + `DELETE` : If tables store data for the most recent time period (e.g. the most recent day or week or month),\n        then when you trim your table by deleting old data, the search optimization service must take into account the\n        updates. In some cases, you might be able to reduce costs by deleting less frequently (e.g. daily rather than\n        hourly).\n    + `INSERT` , `UPDATE` , and `MERGE` : Batching these types of DML statements on the\n        table can reduce the cost of maintenance by the search optimization service.\n* If you recluster the entire table, consider dropping the SEARCH OPTIMIZATION property for that table before\n  reclustering, and then add the SEARCH OPTIMIZATION property back to the table\n  after reclustering.\n* Before enabling search optimization for substring searches ( `ON SUBSTRING( _col_ )` ) or VARIANTs ( `ON EQUALITY( _variant_col_ )` ),\n  call SYSTEM$ESTIMATE\\_SEARCH\\_OPTIMIZATION\\_COSTS to estimate the costs. The initial build and maintenance\n  for these search methods can be computationally intensive, so you should assess the trade-off between performance and cost.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Estimating the costs of search optimization\n2. Viewing the costs of search optimization\n3. Reducing the costs of search optimization\n\nRelated content\n\n1. SYSTEM$ESTIMATE\\_SEARCH\\_OPTIMIZATION\\_COSTS"
      ]
    },
    {
      "url": "https://www.flexera.com/blog/finops/snowflake-search-optimization-part1/",
      "title": "HOW TO: Implement Snowflake search optimization service (2026)",
      "excerpts": [
        "Snowflake Search Optimization Service (SOS) is a feature\u00a0... The example below shows Snowflake search optimization ..."
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-optimize",
      "title": "Optimizing cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nGuides Cost & Billing Optimization\n\n# Optimizing cost \u00b6\n\nThis topic summarizes the features and strategies you can use to optimize Snowflake to reduce costs and maximize your spend.\n\nUsing cost insights to save\n    Learn how to use cost insights to optimize Snowflake for cost within a particular account.\nOptimizing cloud services for cost\n    Learn how to adjust your cloud services usage to reduce costs.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nRelated content\n\n1. Managing cost in Snowflake\n2. Understanding overall cost\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/search-optimization-service",
      "title": "Search optimization service | Snowflake Documentation",
      "excerpts": [
        "Guides Databases, Tables, & Views Search optimization service\n\n# Search optimization service \u00b6\n\n Enterprise Edition Feature\n\nThis feature requires Enterprise Edition (or higher). To inquire about upgrading, please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\n\nThe search optimization service can significantly improve the performance of certain types of lookup and analytical\nqueries. An extensive set of filtering predicates are supported (see Identifying queries that can benefit from search optimization ).\n\nNote\n\nTo start with a tutorial that compares execution time with and without search optimization, see [Getting Started with Search Optimization](https://quickstarts.snowflake.com/guide/getting_started_with_search_optimization/index.html) .\n\nThe search optimization service aims to significantly improve the performance of certain types of queries on tables, including:\n\n* Selective point lookup queries on tables. A point lookup query returns only one or a small number of distinct rows. Use case\n  examples include:\n  \n    + Business users who need fast response times for critical dashboards with highly selective filters.\n    + Data scientists who are exploring large data volumes and looking for specific subsets of data.\n    + Data applications retrieving a small set of results based on an extensive set of filtering predicates.\n  \n  For more information, see Speeding up point lookup queries with search optimization .\n* Character data (text) and IP address searches executed with the SEARCH and SEARCH\\_IP functions. For more information, see Speeding up text queries with search optimization .\n* Substring and regular expression searches (for example, LIKE , ILIKE , RLIKE , and so on). For more information, see Speeding up substring and regular expression queries with search optimization .\n* Queries on elements in VARIANT, OBJECT, and ARRAY (semi-structured)\n  columns that use the following types of predicates:\n  \n    + Equality predicates.\n    + IN predicates.\n    + Predicates that use ARRAY\\_CONTAINS .\n    + Predicates that use ARRAYS\\_OVERLAP .\n    + Predicates that use full-text search with SEARCH .\n    + Substring and regular expression predicates.\n    + Predicates that check for NULL values.\n  \n  For more information, see Speeding up queries of semi-structured data with search optimization .\n* Queries on elements in structured ARRAY, OBJECT, and MAP (structured)\n  columns that use the following types of predicates:\n  \n    + Equality predicates.\n    + IN predicates.\n    + Substring predicates (on STRING fields).\n  \n  For more information, see Speeding up queries of structured data with search optimization .\n* Queries that use selected geospatial functions with GEOGRAPHY values.\n  For more information, see Speeding up geospatial queries with search optimization .\n\nOnce you identify the queries that can benefit from the search optimization service, you can enable search optimization for the columns and tables used in those queries.\n\nThe search optimization service is generally transparent to users. Queries work the same as they do without search\noptimization; some are just faster. However, search optimization does have effects on certain other table operations. For\nmore information, see Working with search-optimized tables .\n\n## How the search optimization service works \u00b6\n\nTo improve performance of search queries, the search optimization service creates and maintains a persistent data\nstructure called a _search access path_ . The search access path keeps track of which values of the table\u2019s columns might\nbe found in each of its micro-partitions , allowing some micro-partitions to be\nskipped when scanning the table.\n\nA maintenance service is responsible for creating and maintaining the search access path:\n\n* When you enable search optimization, the maintenance service creates and populates the search access path with the\n  data needed to perform the lookups.\n  \n  Building the search access path can take significant time, depending on the size of the table. The maintenance service\n  works in the background and does not block any operations on the table. Queries are not accelerated until the search\n  access path has been fully built.\n* When data in the table is updated (for example, by loading new data sets or through DML operations), the maintenance service\n  automatically updates the search access path to reflect the changes to the data.\n  \n  If queries are run while the search access path is still being updated, queries might run more slowly, but will still\n  return correct results.\n\nThe progress of each table\u2019s maintenance service appears in the `search_optimization_progress` column in the\noutput of SHOW TABLES . Before you measure the performance improvement of search\noptimization on a newly-optimized table, make sure this column shows that the table has been fully optimized.\n\nSearch access path maintenance is transparent. You don\u2019t need to create a virtual warehouse for running the\nmaintenance service. However, there is a cost for the storage and compute resources of maintenance. For more details\non costs, see Search optimization cost estimation and management .\n\n## Other options for optimizing query performance \u00b6\n\nThe search optimization service is one of several ways to optimize query performance. The following list shows\nother techniques:\n\n* Query acceleration\n* Creating one or more materialized views (clustered or unclustered)\n* Clustering a table\n\nFor more information, see Optimizing query performance .\n\n## Examples \u00b6\n\nStart by creating a table with data:\n\n```\nCREATE OR REPLACE TABLE test_table ( id INT , c1 INT , c2 STRING , c3 DATE ) AS \n  SELECT * FROM VALUES \n    ( 1 , 3 , '4' ,  '1985-05-11' ), \n    ( 2 , 4 , '3' ,  '1996-12-20' ), \n    ( 3 , 2 , '1' ,  '1974-02-03' ), \n    ( 4 , 1 , '2' ,  '2004-03-09' ), \n    ( 5 , NULL , NULL , NULL );\n```\n\nCopy\n\nAdd the SEARCH OPTIMIZATION property to the table using ALTER TABLE :\n\n```\nALTER TABLE test_table ADD SEARCH OPTIMIZATION ;\n```\n\nCopy\n\nThe following queries can use the search optimization service:\n\n```\nSELECT * FROM test_table WHERE id = 2 ;\n```\n\nCopy\n\n```\nSELECT * FROM test_table WHERE c2 = '1' ;\n```\n\nCopy\n\n```\nSELECT * FROM test_table WHERE c3 = '1985-05-11' ;\n```\n\nCopy\n\n```\nSELECT * FROM test_table WHERE c1 IS NULL ;\n```\n\nCopy\n\n```\nSELECT * FROM test_table WHERE c1 = 4 AND c3 = '1996-12-20' ;\n```\n\nCopy\n\nThe following query can use the search optimization service because the implicit cast is on the constant, not the column:\n\n```\nSELECT * FROM test_table WHERE c2 = 2 ;\n```\n\nCopy\n\nThe following can\u2019t use the search optimization service because the cast is on the table\u2019s column:\n\n```\nSELECT * FROM test_table WHERE CAST ( c2 AS NUMBER ) = 2 ;\n```\n\nCopy\n\nAn IN clause is supported by the search optimization service:\n\n```\nSELECT id , c1 , c2 , c3 \n  FROM test_table \n  WHERE id IN ( 2 , 3 ) \n  ORDER BY id ;\n```\n\nCopy\n\nIf predicates are individually supported by the search optimization service, then they can be joined by the conjunction `AND` and still be supported by the search optimization service:\n\n```\nSELECT id , c1 , c2 , c3 \n  FROM test_table \n  WHERE c1 = 1 \n    AND c3 = TO_DATE ( '2004-03-09' ) \n  ORDER BY id ;\n```\n\nCopy\n\nDELETE and UPDATE (and MERGE) can also use the search optimization service:\n\n```\nDELETE FROM test_table WHERE id = 3 ;\n```\n\nCopy\n\n```\nUPDATE test_table SET c1 = 99 WHERE id = 4 ;\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. How the search optimization service works\n2. Other options for optimizing query performance\n3. Examples\n\nRelated content\n\n1. [Getting Started with Search Optimization](https://quickstarts.snowflake.com/guide/getting_started_with_search_optimization/index.html)"
      ]
    },
    {
      "url": "https://ternary.app/blog/snowflake-cost-optimization/",
      "title": "Top 8 Snowflake Cost Optimization Strategies to Reduce Cost",
      "publish_date": "2025-09-22",
      "excerpts": [
        "Ternary named a Leader in the 2025 ISG Provider Lens\u00ae for FinOps Platforms . [Download the report](https://ternary.app/isg-names-ternary-a-leader/) .\n\u2715\n[](https://ternary.app/)\nPlatform\nKey Features\n[earthquake Anomaly detection](https://ternary.app/platform/anomaly-detection/)\n[pie_chart Cost allocation](https://ternary.app/platform/cloud-cost-allocation/)\n[finance_mode Cost optimization](https://ternary.app/platform/cloud-cost-optimization/)\n[area_chart Forecasting](https://ternary.app/platform/cloud-cost-forecasting/)\n[empty_dashboard Reporting engine](https://ternary.app/platform/finops-reporting-engine/)\nIntegrations\nSolutions\nBy Industry\n[credit_card Financial Services](https://ternary.app/solutions/financial-services/)\n[earthquake Healthcare & Life Sciences](https://ternary.app/solutions/healthcare-life-sciences/)\n[deployed_code Manufacturing](https://ternary.app/solutions/manufacturing/)\n[mic Media & Entertainment](https://ternary.app/solutions/media-entertainment/)\n[account_balance Public Sector (SLED)](https://ternary.app/solutions/finops-for-public-sector/)\n[shopping_bag Retail & Consumer Goods](https://ternary.app/solutions/retail/)\nBy Use\n ... \nSection Title: ... > Snowflake cost components\nContent:\nSnowflake\u2019s architecture has 3 layers:\nStorage\nCompute (which Snowflake calls virtual warehouses)\nCloud services\nThese layers are billed separately, and Snowflake pricing is usage-based, meaning you get charged for what you actually use.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Storage\nContent:\nEvery file, every table, every backup, all adds up in Snowflake. Snowflake charges a monthly fee based on the average amount of storage used over the month. The data is stored in compressed format. Depending on what kind of data you\u2019re working with, like if you\u2019re pulling in a bunch of raw CSVs versus more compact file types, the compression can significantly lower Snowflake storage costs.\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Cloud services layer\nContent:\nThis layer handles all the coordination across the platform, such as authentication, metadata management, and query optimization. It also runs on Snowflake credits, but the cost here is usually a smaller percentage compared to compute. Still, it adds up if you\u2019ve got a lot going on, especially with serverless features in play.\nSpeaking of credits, let\u2019s clarify this: a Snowflake credit is a unit that measures usage.\nOne credit = one unit of usage. Simple. You\u2019re charged credits whenever you\u2019re running a virtual warehouse, leveraging cloud services, or tapping into Snowflake\u2019s serverless features.\nOne more component that Snowflake charges is data transfer between cloud regions or providers. This applies if you\u2019re using features like external tables or exporting data from Snowflake to a data lake.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Cloud services layer\nContent:\nThese Snowflake cost components vary depending on whether you\u2019re on Amazon Web Services (AWS), Microsoft Azure, or Google Cloud (GCP), and the pricing structure for that is a bit more granular.\nLook at the tables below for Snowflake data transfer charges for AWS, Azure, and GCP:\n[AWS pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ] [Azure pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ] [GCP pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ]\nSection Title: ... > An example of how Snowflake calculates cost\nContent:\n**Note:** This example is courtesy of Snowflake.\nSuppose we have a customer using Snowflake Capacity Standard Service with Premier Support in the U.S.\nThey do 3 main things:\nLoad data nightly using a small virtual warehouse.\nSupport 8 users working 10 hours a day, 5 days a week, using a medium virtual warehouse.\nStore 4 TB of compressed data on Snowflake.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Data loading costs\nContent:\n| **Warehouse used** | Small Standard Virtual Warehouse |\n| **Rate** | 2 credits per hour |\n| **Usage** | 2.5 hours daily for 31 days/month |\n| **Monthly Credits** | 2 credits/hour \u00d7 2.5 hours/day \u00d7 31 days = 155 credits/month |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > User activity costs\nContent:\n| **Users** | 8 users |\n| **Warehouse used** | Medium Standard Virtual Warehouse |\n| **Rate** | 4 credits per hour |\n| **Usage** | 10 hours/day, 20 workdays/month |\n| **Monthly credits for users** | 4 credits/hour \u00d7 10 hours/day \u00d7 20 days = 800 credits/month |\n| **Total monthly credits (users + loading)** | 800 + 155 = 955 credits/month |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Storage costs\nContent:\n| **Data stored** | 4TB compressed |\n| **Rate** | $23 per TB/month |\n| **Annual storage cost** | 4 TB \u00d7 $23 \u00d7 12 months = $1,104/year |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Virtual warehouse cost\nContent:\n| **Credits used per year** | 955 credits/month \u00d7 12 = 11,460 credits/year |\n| **Rate per credit** | $2 (with 5% discount: \u00d7 0.95) |\n| **Annual compute cost** | 11,460 \u00d7 $2 \u00d7 0.95 = $21,774/year |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Total annual cost\nContent:\n| **Storage** | $1,104 |\n| **Virtual warehouse** | $21,774 |\n| **Grand Total** | $22,878 per year |\n ... \nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\nThis one step alone can help reduce Snowflake costs tied to unnecessary background compute.\n ... \nSection Title: ... > 3. Remove unused search optimization paths\nContent:\nSearch optimization can speed up point lookups and analytical queries, but just like everything else in Snowflake, that speed boost doesn\u2019t come free.\nThese access paths require extra storage and compute resources to stay in sync with your data.\nIf Snowflake tells you that a particular search optimization path is being used fewer than ten times a week, it might be time to rethink things. Especially if you\u2019re trying to reduce Snowflake costs without compromising your actual workloads.\nYou can remove search optimization with a simple command:\nSection Title: ... > 3. Remove unused search optimization paths\nContent:\n| **ALTER** **TABLE** your_table_name **DROP** **SEARCH** OPTIMIZATION; |\n ... \nSection Title: ... > 7. Reduce transaction lock wait times with batch updates\nContent:\nA sneaky Snowflake cost drain is when queries get blocked by transaction locks.\nThis happens when multiple users run updates or merges on the same table at the same time. Each command locks the table, and while other queries are waiting, they\u2019re still racking up cloud services credits. So even though nothing\u2019s happening, you\u2019re paying for the wait.\nTo avoid this, change how your updates work. Use batch inserts into temporary tables instead of single-row updates. Then run periodic merges from the temp table to the main one. This cuts down on locks and lets Snowflake handle things more efficiently.\nFor workflows that receive a steady stream of new data, consider using a scheduled task to handle updates at intervals, say, every 15 minutes, instead of processing every change as it comes in.\nIt\u2019s a small shift, but it adds up fast. And it\u2019s one of those Snowflake optimization techniques that improves both performance and billing.\nSection Title: ... > 8. Reduce the frequency and scope of cloning operations\nContent:\nCloning in Snowflake [saves a ton of resources](https://ternary.app/blog/cloud-cost-savings/) compared to full copies.\nBut if you\u2019re cloning entire databases or schemas over and over again, that metadata usage starts to pile up.\nAnd since cloning relies on cloud services, doing it frequently means your costs quietly creep up.\nSo instead of cloning full environments, clone only what you actually need, maybe just a single table instead of an entire schema.\nAlso, take a hard look at how often your teams are running these clones. If it\u2019s part of an automated process, make sure it\u2019s not firing more often than it needs to.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > What is a KPI in Snowflake?\nContent:\nFor Snowflake cost optimization, KPIs are your best friend.\nSnowflake offers a bunch of performance metrics that, when tracked together, paint a full picture. These include basically anything that has a noticeable impact on credit usage, query speed, or system efficiency.\n ... \nSection Title: ... > How to pick a Snowflake FinOps and cost optimization tool\nContent:\nThere are a lot of moving parts in Snowflake, and the right tool should help you control the chaos, not add to it.\nHere\u2019s how to make the right call:\n ... \nSection Title: ... > How much do Snowflake credits cost?\nContent:\nThe cost of Snowflake credits depends on your chosen cloud provider, region, and pricing tier. Credits are consumed when using compute, cloud services, or serverless features.\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > Related articles\nContent:\n[](https://ternary.app/blog/manage-your-snowflake-costs/) Blog [Harness Snowflake\u2019s power\u2014without the pain](https://ternary.app/blog/manage-your-snowflake-costs/)\n[](https://ternary.app/blog/hidden-costs/) Blog [Hidden costs that can supersize your cloud bill\u2014and how to manage them](https://ternary.app/blog/hidden-costs/)\n[](https://ternary.app/)\nAvailable as a SaaS platform and a self-hosted solution, Ternary manages more than $7.5B in multi-cloud spend across leading enterprises and managed service providers.\n[](https://www.linkedin.com/company/ternaryinc/) [](https://twitter.com/ternaryinc) [](https://www.youtube.com/@ternaryinc)\nPlatform\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > Related articles\nContent:\n[Overview](https://ternary.app/platform/overview/)\n[Why Ternary](https://ternary.app/platform/why-ternary/)\n[Anomaly detection](https://ternary.app/platform/anomaly-detection/)\n[Cost allocation](https://ternary.app/platform/cloud-cost-allocation/)\n[Cloud cost optimization](https://ternary.app/platform/cloud-cost-optimization/)\n[Forecasting](https://ternary.app/platform/cloud-cost-forecasting/)\n[Reporting Engine](https://ternary.app/platform/finops-reporting-engine/)\nIntegrations\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > Related articles\nContent:\n[For FinOps teams](https://ternary.app/solutions/finops/)\n[For Engineers](https://ternary.app/solutions/engineers/)\n[For Finance](https://ternary.app/solutions/finance/)\n[For MSPs](https://ternary.app/partners/msps/)\n[For Financial Services](https://ternary.app/solutions/financial-services/)\n[For Healthcare & Life Sciences](https://ternary.app/solutions/healthcare-life-sciences/)\n[For Manufacturing](https://ternary.app/solutions/manufacturing/)\n[For Media & Entertainment](https://ternary.app/solutions/media-entertainment/)\n[For Public Sector](https://ternary.app/solutions/finops-for-public-sector/)\n[For Retail & Consumer Goods](https://ternary.app/solutions/retail/)\n[For AI cost management](https://ternary.app/solutions/ai-cost-management/)\nResources\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > Related articles\nContent:\n[About](https://ternary.app/about-us/)\n[Careers](https://ternary.app/careers/)\n[News](https://ternary.app/news/)\n[Contact us](https://ternary.app/contact/)\n[End User License Agreement](https://ternary.app/eula/)\n[Privacy policy](https://ternary.app/privacy-policy/)\n[LLM info](https://ternary.app/llm-info/)"
      ]
    },
    {
      "url": "https://www.reddit.com/r/snowflake/comments/1di6o0d/could_you_give_me_some_directions_on_how_to/",
      "title": "Could you give me some directions on how to reduce Snowflake costs?",
      "excerpts": [
        "Skip to main content Open menu Open navigation  Go to Reddit Home\n\nr/snowflake\n\nExpand user menu Open settings menu\n\nGo to snowflake r/snowflake\n\nr/snowflake\n\nUnofficial subreddit for discussion relating to the Snowflake Data Cloud\n\n* * *\n\nWeekly visitors Weekly contributions \u2022\n\nProud-Walk9238\n\n# Could you give me some directions on how to reduce Snowflake costs?\n\nHi, we have approximately 157 scheduled tasks for cleaning and integrating raw data into our data warehouse. Most tasks are triggered by a cron expression, such as every hour, minute, or every 30 minutes. Since we rely on streams to capture changes, tasks are not always triggered.\n\nConsidering the current distribution and duration of these tasks:\n\n1. Should we focus on improving the performance of longer tasks first?\n2. Does task distribution affect the costs? If so, how can we rearrange them to save money?\n\nHere's how task executions are distributed over time (within 1 hour). The slowest task takes 5 minutes to complete.\n\nAny feedback or guidance would be appreciated.\n\n[](https://preview.redd.it/could-you-give-me-some-directions-on-how-to-reduce-v0-0al8sgblm67d1.png?width=2654&format=png&auto=webp&s=8ca9db38356e65d14e49887a4d446acf819fd368 \"Image from r/snowflake - Could you give me some directions on how to reduce Snowflake costs?\") Share\n\n# Related Answers Section\n\nRelated Answers\n\n[Directions to reduce Snowflake costs](https://www.reddit.com/answers/dc12e246-672a-491e-9ab7-295643fcc147/?q=Directions+to+reduce+Snowflake+costs&source=PDP)\n\n[Best Snowflake cost optimization tools](https://www.reddit.com/answers/19e9a7ae-7937-4523-a2a5-41f22c99a196/?q=Best+Snowflake+cost+optimization+tools&source=PDP)\n\n[Top features of Snowflake for data analysts](https://www.reddit.com/answers/26a59f27-c4c5-4d51-9914-5f3d3820791b/?q=Top+features+of+Snowflake+for+data+analysts&source=PDP)\n\n[How to integrate Snowflake with Python](https://www.reddit.com/answers/304e42ca-04f2-4447-b941-fd90334fce58/?q=How+to+integrate+Snowflake+with+Python&source=PDP)\n\n[Common pitfalls when using Snowflake](https://www.reddit.com/answers/338b7dcb-8c19-43b5-aac7-f0a547395fc0/?q=Common+pitfalls+when+using+Snowflake&source=PDP)\n\nPublic\n\nAnyone can view, post, and comment to this community\n\n0 0\n\n## Top Posts\n\n* * *\n\n* [Reddit reReddit: Top posts of June 17, 2024 * * *](https://www.reddit.com/posts/2024/june-17-1/global/)\n* [Reddit reReddit: Top posts of June 2024 * * *](https://www.reddit.com/posts/2024/june/global/)\n* [Reddit reReddit: Top posts of 2024 * * *](https://www.reddit.com/posts/2024/global/)\n\nExpand Navigation Collapse Navigation"
      ]
    },
    {
      "url": "https://keebo.ai/2024/09/17/snowflake-cost-optimization-reduction/",
      "title": "Snowflake Cost Optimization: 6 Effective Cost Reduction Strategies",
      "publish_date": "2025-12-03",
      "excerpts": [
        "Solutions\nBy Use Case\nCost Optimization\nVisibility and FinOps\nPerformance and Team Efficiency\nBy Industry\nFinancial Services\nHealthcare\nTechnology\nRetail and CPG\nPlatform\n[Warehouse Optimization](https://keebo.ai/warehouse-optimization/)\n[Workload Intelligence](https://keebo.ai/workload-intelligence/)\n[Query Routing](https://keebo.ai/query-routing/)\n[Architecture & Security](https://keebo.ai/architecture-security/)\nPricing\nResources\nCase Studies\nGuides and Whitepapers\nSolution Briefs\nSecurity\nVideos\nBlog\nCompany\nAbout Us\nCareers\nNews\nContact Us\n[](https://keebo.ai/)\nSolutions\nBy Use Case\nCost Optimization\nVisibility and FinOps\nPerformance and Team Efficiency\nBy Industry\nFinancial Services\nHealthcare\nTechnology\nRetail and CPG\nPlatform\n[Warehouse Optimization](https://keebo.ai/warehouse-optimization/)\n[Workload Intelligence](https://keebo.ai/workload-intelligence/)\n[Query Routing](https://keebo.ai/query-routing/)\n[Architecture & Security](https://keebo.ai/architecture-security/)\nPricing\nResources\nCase Studies\nGuides and Whitepapers\nSolution Briefs\nSecurity\nVideos\nBlog\nCompany\nAbout Us\nCareers\nNews\nContact Us\n[](https://www.portal.keebo.ai/sign-in)\nRequest Demo\n[](https://keebo.ai/)\nMenu\nSection Title: Snowflake Cost Savings: How Do Enterprise Teams Optimize Snowflake Storage and Compute Spend\nContent:\n[](https://keebo.ai/author/barzan/) [Barzan Mozafari](https://keebo.ai/author/barzan/ \"Posts by Barzan Mozafari\")\nSeptember 17, 2024\n[Automation](https://keebo.ai/category/automation/) , [Data Engineering](https://keebo.ai/category/data-engineering/) , [Finops](https://keebo.ai/category/finops/) , [Reducing Snowflake Costs](https://keebo.ai/category/reducing-snowflake-costs/) , [Warehouse Optimization](https://keebo.ai/category/warehouse-optimization/)\n ... \nSection Title: ... > Serverless compute\nContent:\nOther Snowflake features like Search Optimization and Snowpipe use serverless compute resources, which Snowflake automatically scales up or down for each workload.  This can be cost-effective for irregular workloads or when you need to handle spikes in activity without maintaining a continuously running virtual warehouse.\n ... \nSection Title: ... > Reducing query frequency\nContent:\nNot all Snowflake workloads need to run hourly. Low traffic times, like weekends, can afford a higher latency. Whether or not you can afford to reduce frequency depends on a number of factors, including your desired performance and any active [service level agreements](https://keebo.ai/2024/09/17/service-level-agreement/) .\n ... \nSection Title: ... > Barzan Mozafari\nContent:\n[Articles: 0](https://keebo.ai/author/barzan/)\n[Previous Post What is an SLA and how do I get started? Service-Level Agreements Explained](https://keebo.ai/2024/09/17/service-level-agreement/) [Next Post Unlocking Advanced Snowflake Optimization: Data Insights for Peak Performance](https://keebo.ai/2024/09/26/unlocking-advanced-snowflake-optimization/)\nSearch\n ... \nSection Title: ... > Categories\nContent:\n[AI](https://keebo.ai/category/ai/)\n[Automation](https://keebo.ai/category/automation/)\n[Data Engineering](https://keebo.ai/category/data-engineering/)\n[Data Learning](https://keebo.ai/category/data-learning/)\n[Deep Learning](https://keebo.ai/category/deep-learning/)\n[Events](https://keebo.ai/category/events/)\n[Finops](https://keebo.ai/category/finops/)\n[Machine Learning](https://keebo.ai/category/machine-learning/)\n[Product Updates](https://keebo.ai/category/product-updates/)\n[Query Acceleration](https://keebo.ai/category/acceleration/)\n[Query Routing](https://keebo.ai/category/query-routing/)\n[Reducing Snowflake Costs](https://keebo.ai/category/reducing-snowflake-costs/)\n[Security](https://keebo.ai/category/security/)\n[Uncategorized](https://keebo.ai/category/uncategorized/)\n[Warehouse Optimization](https://keebo.ai/category/warehouse-optimization/)"
      ]
    },
    {
      "url": "https://www.flexera.com/blog/finops/reduce-snowflake-costs/",
      "title": "8 best practices to reduce Snowflake costs (2026)",
      "publish_date": "2026-01-27",
      "excerpts": [
        "Customers Open External Links\n[Community](https://community.flexera.com/)\n[Product login](https://app.flexera.com/login)\n[Spot login](https://console.spotinst.com/auth/signIn)\n[Partner Portal](https://partnerhub.flexera.com/)\nSearch\nBook a demo\nHome\nBlog\n[FinOps](https://www.flexera.com/blog/finops/)\n8 best practices to reduce Snowflake costs (2026)\n ... \nSection Title: ... > 1) Selecting the right Snowflake Warehouse Size\nContent:\n**Note** : \u201cCOMPUTE_WH\u201d is the name of the virtual warehouse that you want to change the size of and \u201cXSmall\u201d is the new size that you want to set it to. You can also use the [RESUME](https://docs.snowflake.com/en/sql-reference/sql/alter-task) command if you want to resume the warehouse after changing its size.\nTherefore, it\u2019s important to monitor and adjust the warehouse size as needed to avoid paying for unused resources.\n**Note:** An effective strategy to find the optimal warehouse size for your particular use case is to start with a X-small or Small-sized warehouse and experiment with different sizes.\nSection Title: ... > 2) Reduce the warehouse\u2019s auto-suspend period\nContent:\nAnother way to reduce Snowflake costs is to reduce the [auto-suspend](https://docs.snowflake.com/en/user-guide/warehouses-considerations) period of data warehouses. The auto-suspend feature automatically suspends data warehouses after a period of inactivity, reducing compute charges.\nThis feature is enabled automatically, but the default is set to 10 mins (600 seconds). This is a very large auto-suspend window and one of the main causes of Idle compute credits being billed.\nOnce you find your warehouses that are incurring low utilization and high idle credits, you should immediately investigate their auto-suspend time. Typically, it\u2019s advisable to set the limit to lowest possible ie: 60 seconds.\nTo reduce the warehouse auto-suspend period, head over to the Edit Warehouses tab and set a time limit to Auto Suspend.\n**Note** : You can also turn on the auto-resume feature to resume the warehouse when it gets queried.\nSection Title: ... > 2) Reduce the warehouse\u2019s auto-suspend period\nContent:\nEdit warehouse configuration\n ... \nSection Title: 8 best practices to reduce Snowflake costs (2026) > ... > 7) Snowflake query optimization\nContent:\nKeep in mind that querying data in Snowflake consumes a lot of credits. The little trick to achieving Snowflake cost optimization is to tweak the query code and settings for efficient operation without affecting any job performance.\nFew techniques and strategies to make queries run faster are:\n ... \nSection Title: 8 best practices to reduce Snowflake costs (2026) > FAQs\nContent:\n**Are there any tools or services available to help in estimating Snowflake costs?**\nYes, There are several tools and services available to help estimate Snowflake costs . Some notable options include Snowflake\u2019s built-in Snowflake Pricing Calculator, Cleartelligence, Ideas2it and Godatadrive.\nRelated posts:\n ... \nSection Title: 8 best practices to reduce Snowflake costs (2026) > FAQs\nContent:\n(1/4)\")\n[FinOps and ITAM: A unified approach to optimizing technology investments](https://www.flexera.com/blog/finops/finops-and-itam-a-unified-approach-to-optimizing-technology-investments/ \"FinOps and ITAM: A unified approach to optimizing technology investments\")\n[AWS cost optimization tools and tips: Ultimate guide [2025]](https://www.flexera.com/blog/finops/aws-cost-optimization-8-tools-and-tips-to-reduce-your-cloud-costs/ \"AWS cost optimization tools and tips: Ultimate guide [2025]\")\n[Navigating the SaaS security maze: Tips to protect your business](https://www.flexera.com/blog/saas-management/navigating-the-saas-security-maze-tips-to-protect-your-business/ \"Navigating the SaaS security maze: Tips to protect your business\")\n ... \nSection Title: ... > [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Req...\nContent:\nFebruary 22, 2023\nFinOps"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/",
      "title": "FinOps on Snowflake: Built-In Cost and Performance Control",
      "excerpts": [
        "Section Title: FinOps on Snowflake > Go from painstaking configurations to a proven, fully-managed service\nContent:\nSince its founding in 2012, Snowflake has provided automated cluster management, maintenance and upgrades \u2014 all without downtime \u2014 so you can spend time on valuable data projects\nGet **out-of-the-box governance and security through Snowflake Horizon Catalog** without extra configurations or protocols\n ... \nSection Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\nTravelpass CTC Natwest\nTravel and Hospitality \u201cNow, we aren\u2019t so focused on how to build things. We are focused more on what to build.\u201d Dan Shah\nManager of Data Science Read the story * **1 week** for 130 Dynamic Tables to be in production after migration\n**65%** cost savings switching from Databricks to Snowflake\nRead the case study Financial Services \u201cNow with fewer ephemeral failures and higher visibility in Snowflake, we have a platform that\u2019s much easier and cost-effective to operate than managed Spark.\u201d David Trumbell\nHead of Data Engineering, CTC Read the story * **1st** data availability deadline was hit everyday for the 1st time\n**54%** cost savings switching from managed Spark to Snowflake\nSection Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\nRead the case study Financial Services \u201cThe speed at which we\u2019ve delivered wouldn\u2019t have been possible with other providers.\u201d Kaushik Ghosh Dastidar\nHead of ESG Cloud Solutions, NatWest Read the story * **6x** reduction in onboarding time from 3 months to 2 weeks\n**$750K** saved in salaries & staff training costs\nRead the case study\n[Resource #### Snowflake Joins the FinOps Foundation Snowflake joins The FinOps Foundation as a Premier Enterprise Member to provide thought leadership and set industry financial best practices. Read more](https://www.finops.org/members/snowflake/)\nResource #### Snowflake Pricing Calculator Curious about Snowflake pricing? Our Snowflake pricing calculator shows credit usage, warehouse costs, and total expenses. Access calculator\n ... \nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation\nContent:\nAccess documentation on Managing Costs and Optimizing Performance in Snowflake.\nRead about Managing Costs\nRead about Optimizing Performance\nSection Title: FinOps on Snowflake > ... > Snowflake Documentation > On-Demand Cost Governance Training\nContent:\nLearn how to successfully examine, control, and optimize Snowflake costs.\nRegister Now\nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation > Professional Services\nContent:\nEngage Snowflake\u2019s Professional Services for expert advice on optimizing your use of Snowflake.\nDiscover Professional Services\nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation > Priority Support\nContent:\nLearn more about how our Priority Support team can help you reduce consumption spend through performance monitoring, observability, and management.\nLearn about Priority Support\n ... \nSection Title: FinOps on Snowflake > Where Data Does More\nContent:\n*\nAdd me to the list to receive dedicated product updates and general availability emails.\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\nSubscribe Now\nIndustries * Advertising, Media & Entertainment\nFinancial Services\nHealthcare & Life Sciences\nManufacturing\nPublic Sector\nRetail & Consumer Goods\nTechnology\nLearn * Resource Library\nLive Demos\nFundamentals\nTraining\nCertifications\nSnowflake University\nDeveloper Guides\nDocumentation\nPrivacy Policy\nSite Terms\nCommunication Preferences\nCookie Settings\nDo Not Share My Personal Information\nLegal\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")"
      ]
    },
    {
      "url": "https://www.flexera.com/blog/finops/search-optimization-vs-clustering-snowflake/",
      "title": "Optimizing Snowflake 101: Snowflake clustering vs SOS (2026)",
      "publish_date": "2026-01-27",
      "excerpts": [
        "Section Title: Optimizing Snowflake 101: Snowflake clustering vs SOS (2026)\nContent:\nThis post originally appeared on the chaosgenius.io blog. Chaos Genius has been [acquired by Flexera](https://www.flexera.com/more/ProsperOps-Chaos-Genius) .\nSecond part of the Snowflake search optimization series is here! This two-part series will guide you through everything you need about Snowflake search optimization. In the previous Part ( Part 1 ), we discussed what Snowflake search optimization service is, how to implement it, the associated costs, strategies for cost management, factors affecting the cost and a hands-on example of a performance comparison between Optimized vs Non-Optimized Tables.\nIn this article (Part 2), we have much to cover; we will dive into the technical benefits, the specific query profiles that benefit and the architectural differences between **Snowflake Clustering** and **Search Optimization Service** and a lot more!!\nLet\u2019s get started!\n ... \nSection Title: Optimizing Snowflake 101: Snowflake clustering vs SOS (2026) > What is Snowflake Clustering?\nContent:\nTo learn exactly when to use Snowflake clustering, check out the article below and understand more in-depth. We have broken down everything you need to know about Snowflake clustering:\n**Snowflake Clustering 101: A Comprehensive Guide**\nSection Title: ... > Difference between Snowflake Clustering vs Search Optimization Service?\nContent:\nFor users seeking lightning-fast query speeds on massive datasets, Snowflake offers these two powerful, yet distinct, performance enhancers. While both utilize Snowflake\u2019s micro-partition architecture, they solve different problems. **Clustering** optimizes physical data storage for range and group access. **Search Optimization Service (SOS)** builds auxiliary data structures (Search Access Paths) to find \u201cneedles in the haystack.\u201d\nHere of key differences between Snowflake clustering vs search optimization services:\n**Snowflake Clustering:**\nSection Title: ... > Difference between Snowflake Clustering vs Search Optimization Service?\nContent:\nAutomatically groups related rows of table data together within Snowflake micro-partitions.\nOrganizes data based on clustering key(s) defined on the table (one or more columns).\nImproves query performance by co-locating related data to minimize scanned partitions.\nWorks continuously in the background to re-organize table data as it changes.\nOnly one clustering key can be defined on a table.\nMainly optimizes equality, range predicates and sort operations.\n**Snowflake Search Optimization Service:**\n ... \nSection Title: Optimizing Snowflake 101: Snowflake clustering vs SOS (2026) > FAQs\nContent:\n**Will enabling both Auto Clustering and Search Optimization consume more credits?**\nYes, tables with both Auto Clustering and Search Optimization enabled will consume more credits.\n**Does enabling search optimization incur additional costs?**\nYes, the search optimization service requires added storage for metadata and computing for background maintenance. The costs scale based on usage, so thoughtful configuration is recommended.\n**How can search optimization costs be managed?**\nCarefully selecting tables for optimization, monitoring usage and batching DML can help manage search optimization costs.\n**Does search optimization improve join performance?**\nDirectly no, but it can accelerate filter predicates applied before joins, improving overall join query performance.\n**Can search optimization handle data with frequent changes?**\n ... \nSection Title: ... > [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Req...\nContent:\nFebruary 22, 2023\nFinOps\n ... \nSection Title: ... > [Agentic FinOps for AI: autonomous optimization for Snowflake, Databricks and AI cloud cost...\nContent:\nFebruary 12, 2026\n\u00d7\nGet updates delivered to your inbox\nSubscribe"
      ]
    },
    {
      "url": "https://www.reddit.com/r/dataengineering/comments/1m7mw87/boss_is_hyped_about_snowflake_cost_optimization/",
      "title": "Boss is hyped about Snowflake cost optimization tools..I'm ...",
      "publish_date": "2025-07-29",
      "excerpts": [
        "Skip to main content Open menu Open navigation  Go to Reddit Home\n\nr/dataengineering\n\nExpand user menu Open settings menu\n\nGo to dataengineering r/dataengineering\n\nr/dataengineering\n\nNews & discussion on Data Engineering topics, including but not limited to: data pipelines, databases, data formats, storage, data modeling, data governance, cleansing, NoSQL, distributed systems, streaming, batch, Big Data, and workflow engines.\n\n* * *\n\nWeekly visitors Weekly contributions \u2022\n\nOne-Time3079\n\n# Boss is hyped about Snowflake cost optimization tools..I'm skeptical. Anyone actually seen 30%+ savings?\n\nDiscussion Hey all,  \nMy team is being pushed to explore Snowflake cost optimization vendors, think Select, Capital One Slingshot, Espresso AI, etc. My boss is super excited, convinced these tools can cut our spend by 30% or more.\n\nI want to believe\u2026 but I\u2019m skeptical. Are these platforms actually that effective, or are they just repackaging what a savvy engineer with time and query history could already do?\n\nIf you\u2019ve used any of these tools:\n\n* Did you actually see meaningful savings?\n* What kind of optimizations did they help with (queries, warehouse sizing, schedules)?\n* Was the ROI worth it?\n* Would you recommend one over the others?\n\nTrying to separate hype from reality before we commit. Appreciate any real-world experiences or warnings!\n\nArchived post. New comments cannot be posted and votes cannot be cast.\n\nShare\n\n# Related Answers Section\n\nRelated Answers\n\n[Best Snowflake cost management tools](https://www.reddit.com/answers/e4a2e339-3eb5-4ae8-8115-566bb3629c13/?q=Best+Snowflake+cost+management+tools&source=PDP)\n\n[Snowflake optimization techniques](https://www.reddit.com/answers/c381359c-1569-40ad-a26d-30e968d6deef/?q=Snowflake+optimization+techniques&source=PDP)\n\n[Reduce data warehouse costs effectively](https://www.reddit.com/answers/71eaa680-9430-40c2-9864-4580fe2c3033/?q=Reduce+data+warehouse+costs+effectively&source=PDP)\n\n[Top tools for building data pipelines](https://www.reddit.com/answers/f656144c-8600-4773-b3d5-d3d35f1ddcb3/?q=Top+tools+for+building+data+pipelines&source=PDP)\n\n[Best practices for data governance](https://www.reddit.com/answers/8e454982-84e8-421c-82eb-3be91b3e9802/?q=Best+practices+for+data+governance&source=PDP)\n\nPublic\n\nAnyone can view, post, and comment to this community\n\n0 0\n\nExpand Navigation Collapse Navigation"
      ]
    },
    {
      "url": "https://www.getdbt.com/blog/reduce-snowflake-costs",
      "title": "How to reduce Snowflake costs with smart architecture | dbt Labs",
      "publish_date": "2025-07-30",
      "excerpts": [
        "Section Title: How to reduce Snowflake costs without sacrificing performance\nContent:\nDaniel Poppy\nlast updated on Jul 30, 2025\n[Snowflake\u2019s usage-based pricing](https://www.snowflake.com/en/pricing-options/ \"Snowflake\u2019s usage-based pricing\") gives teams flexibility\u2014but without intentional management, it\u2019s easy for costs to spiral. Most organizations overspend not because [Snowflake](https://www.snowflake.com/ \"Snowflake\") is expensive by default, but because of inefficient patterns in how compute, storage, and data movement are managed.\n**Compute** typically drives the largest share of spend. Each second a warehouse runs (even idle), it consumes credits.\n**Storage** costs build up from retained tables, Time Travel history, and unmonitored staging areas.\n**Data transfer** fees can surprise teams when data moves across clouds, regions, or external integrations.\nSection Title: How to reduce Snowflake costs without sacrificing performance\nContent:\nThese factors don\u2019t exist in silos. Poor data modeling or bloated storage can increase query complexity, which drives up compute. A lack of warehouse segmentation can lead to resource contention and slow performance\u2014pushing teams to overprovision as a quick fix.\nIn this post, we\u2019ll break down practical strategies for reducing Snowflake costs across your stack\u2014from warehouse and storage optimizations to query tuning, architecture decisions, and team practices. These aren\u2019t theoretical tips; they\u2019re based on real customer outcomes and best practices from teams using [**dbt and Snowflake together**](https://www.getdbt.com/data-platforms/snowflake \"[object Object]\") to build cost-efficient analytics workflows.\nSection Title: ... > Optimize warehouse configuration and usage\nContent:\nWarehouse optimization is often the fastest\u2014and most overlooked\u2014path to Snowflake savings. By tailoring warehouse size, schedule, and scope to actual workload needs, teams can significantly reduce compute costs without degrading performance.\n ... \nSection Title: How to reduce Snowflake costs without sacrificing performance > ... > Segment by workload type\nContent:\nSharing a single warehouse across dev, reporting, and data science creates performance bottlenecks\u2014and wastes compute. Create purpose-specific warehouses to optimize sizing and concurrency for each use case.\n ... \nSection Title: How to reduce Snowflake costs without sacrificing performance > ... > Route workloads intentionally\nContent:\nDifferent query types deserve different environments. Splitting out BI dashboards, ad hoc exploration, and data science into dedicated warehouses:\nMinimizes contention\nOptimizes performance per workload\nMakes cost attribution and forecasting easier\nSection Title: ... > Use Snowflake's built-in accelerators (selectively)\nContent:\nSnowflake offers features like [search optimization](https://docs.snowflake.com/en/user-guide/search-optimization-service \"search optimization\") and [automatic clustering](https://docs.snowflake.com/en/user-guide/tables-auto-reclustering \"automatic clustering\") to accelerate query performance. These can increase storage costs but often pay off when used for the right tables.\nConsider enabling these on:\nHigh-volume tables with frequent filters (e.g., `WHERE user_id =` )\nSlowly changing dimension tables\nModels powering operational dashboards\nSection Title: How to reduce Snowflake costs without sacrificing performance > Build a culture of cost ownership\nContent:\nYou can optimize compute and storage\u2014but sustainable Snowflake cost management comes down to people. The most successful teams treat cost awareness as a shared responsibility across engineering, analytics, and business stakeholders.\n ... \nSection Title: ... > Foster a culture of shared learning\nContent:\nDocumentation and education go further than mandates. Host recurring \u201ccost review\u201d sessions where teams share wins, inefficiencies, and lessons learned. Encourage experimentation with techniques like:\nWarehouse resizing\nQuery optimization\nStaging table cleanup"
      ]
    },
    {
      "url": "https://yukidata.com/snowflake-optimization-guide/",
      "title": "Snowflake Optimization in 2025: 7 Tips for Better Performance & Prices | Yuki",
      "publish_date": "2025-06-02",
      "excerpts": [
        "Section Title: ... > What is Snowflake Optimization\nContent:\nSimply put, Snowflake optimization is getting your data to run faster and more efficiently in your Snowflake setup.\nThe trick here is faster *and* more efficient. The two do not often come hand in hand.\nBuying a bigger warehouse to run your queries faster might lead to better performance, but it might also lead to more resources wasted \u2013 so you end up paying for something you didn\u2019t get to use.\nOur guide will walk you through the different tricks and tips you can use to keep your platform performing at an optimal level and save you up to 40% on your monthly spend.\nAnd yes, we did save the best trick for last. So if you want to skip ahead and see the easiest way to save big, scroll on.\nSection Title: ... > 7 Tactics to Improve Snowflake Performance & Costs\nContent:\nSnowflake performance and [cost optimization](https://yukidata.com/blog/snowflake-cost-optimization/) can be a manual, time-intensive process \u2013 unless you\u2019ve decided to invest in some automation tools \u2013 but the work is well-worth it. Snowflake, for all of its built-in optimization engines, can be incredibly inefficient.\nHere are the seven best tactics to keep your setup running smoothly and cut down on your bill:\nOptimize your queries\nTest your warehouses\nAdjust your storage settings\nTry data monitoring\nUse monitoring and governance tools\nTry Snowflake\u2019s [Search Optimization Service](https://yukidata.com/blog/snowflake-search-optimization-service/)\nGet automated\nSection Title: ... > Optimization Tactic #1: Snowflake Query Optimization\nContent:\nSnowflake charges you per second while compute resources are active. That means how you construct your queries has a big impact on your performance.\nThe good news is there are many tactics for you to test out with [Snowflake query optimization](https://yukidata.com/blog/snowflake-query-optimization/) . The bad news? It\u2019ll be a time investment.\nOur favorite Snowflake query optimization tactic? Adjusting the Snowflake auto-suspend function.\nBy default, Snowflake will auto-suspend a warehouse after 10 minutes of idle time.\nThere is some logic behind this: it keeps the cache warm for longer durations and means if you start running queries in that time period, the queries can act on that old cache.\nBut at the end of the day, that\u2019s 10 minutes of compute time added onto your bill.\n ... \nSection Title: ... > Optimization Tactic #6: Search Optimization Service\nContent:\nSnowflake automatically stores your data in micro-partitions. These data sets are automatically optimized for large, analytic scans, but not the best for a pinpoint query like \u201cfind all records where email = X\u201d.\nSnowflake\u2019s Search Optimization Search (SOS) helps with those kinds of lookups by creating and maintaining specialized indexes. It uses certain columns and paths like JSON fields to quickly locate and return relevant rows so it doesn\u2019t have to scan the entire dataset.\nUse this if you\u2019re filtering or [joining columns](https://yukidata.com/blog/snowflake-join-optimization/) with high cardinality (think product SKU, email, user ID). It\u2019s also useful if clustering is too expensive to maintain.\nJust know that SOS is an add-on service. Consider the pros of its pinpoint search with how much more it costs for you to use it each month.\n ... \nSection Title: ... > Optimization Tactic #7: Automated Optimization Services\nContent:\nIdo Arieli Noga is the CEO and Co-Founder of Yuki, where he helps businesses cut Snowflake spend through smart warehouse scaling and DevOps-driven optimization. He brings over 12 years of experience across data storage, BI, and FinOps, including nearly four years as Head of Data at Lightico and five years managing large-scale virtual environments in the government sector. Ido holds a degree in Computer Science and is passionate about building scalable, cost-efficient data infrastructures. Since founding Yuki in 2023, he\u2019s focused on helping teams reduce costs without changing queries or code. Find more of his insights on Medium or LinkedIn."
      ]
    },
    {
      "url": "https://www.finops.org/members/snowflake/",
      "title": "Snowflake - FinOps Foundation ",
      "excerpts": [
        "Close Search\n\nLoading...\n\n[Register Now FinOps X The conference for technology value, June 8-11, 2026](https://x.finops.org/)\n\nAbout\n\n* [Mission](https://www.finops.org/about/)\n* [Governing Board](https://www.finops.org/about/governing-board/)\n* [Technical Advisory Council](https://www.finops.org/about/technical-advisory-council/)\n* [Member Organizations](https://www.finops.org/about/members/)\n* [Ambassadors](https://www.finops.org/about/ambassadors/)\n* [Staff](https://www.finops.org/about/staff/)\n\nMake Suggestion\n\n# Snowflake\n\nMake Suggestion [Visit Partner Website](https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/)\n\n# Snowflake\n\n* Member Organizations\n* /\n* Snowflake\n\nSnowflake is the platform for the AI era, making it easy for enterprises to innovate faster and get more value from data. More than 12,000 customers around the globe, including hundreds of the world\u2019s largest companies, use Snowflake\u2019s AI Data Cloud to build, use and share data, applications and AI. With Snowflake, data and AI are transformative for everyone. Learn more at snowflake.com (NYSE: SNOW).\n\n* * *\n\n## Product or Service Overview\n\nSnowflake empowers FinOps teams with built-in cost management tools for real-time visibility, control, and optimization of cloud spend.\n\nGain visibility, control and optimization of Snowflake spend in one Cost Management interface ( a centralized UI console with tabs for Account Overview, Organization Overview, Budgets, Resource Monitors, and more) or through SQL.\n\nWith features like Performance Explorer and Query Profile, FinOps practitioners can easily analyze usage patterns, identify cost drivers, and optimize workloads\u2014all directly within the platform.\n\n## Supporting Resources\n\n* [AI Data Academy - Cost Management Bootcamp](https://www.snowflake.com/data-cloud-academy-cost-management-bootcamp/)\n* [Managing cost in Snowflake](https://docs.snowflake.com/en/user-guide/cost-management-overview?_ga=2.54939435.1242881411.1758559826-1266979266.1758559826, )\n* [SurveyMonkey Increases Snowflake Workloads By 475% With Only A 27% Increase In Credit Use](https://www.youtube.com/watch?v=4aCj0wk33kg&list=PLavJpcg8cl1Epd5ziAjdpvkO9NyzVgsy-)\n* [Optimizing performance in Snowflake](https://docs.snowflake.com/en/guides-overview-performance)\n* [How Snowflake Optimizes for Internal and Customer-Facing Value with FinOps](https://www.youtube.com/watch?v=PdbVe2FDtO8)\n\n[View Member Website](https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/)\n\n### Training Certifications\n\n13\n\n2\n\n### Tools & Services\n\nView FinOps Landscape\n\n## Key Participants\n\n[#### Douglas Gallagher Snowflake Governing Board](https://www.linkedin.com/in/douglasgallagher/) [#### Alex Landis Snowflake](https://www.linkedin.com/in/alandis/) [#### Maneesh Chhabra Snowflake](https://www.linkedin.com/in/maneesh-chhabra-1a4a84/) [#### Simarpreet Arora Snowflake](https://www.linkedin.com/in/simarpreet-arora/)\n\nFollow Us:\n\n[Linkedin](https://www.linkedin.com/company/finops-foundation) [GitHub](https://github.com/finopsfoundation) [YouTube](https://www.youtube.com/channel/UCyl26lvnoySlGWlF5oNHvYA)\n\nThis work is licensed under Attribution 4.0 International\n\n\u00a9 FinOps Foundation Project a Series of LF Projects, LLC.\n\nFor web site terms of use, trademark policy and other project policies please see <https://lfprojects.org> .\n\nFor the technical charter of FinOps Project a Series of LF Projects, LLC, please see the [Technical Charter](https://github.com/finopsfoundation/tac/blob/master/charter.md) .\n\n\u00d7\n\n### Make Suggestions\n\n\u00d7\n\n\u00d7\n\n### Suggest a Resource\n\n\u00d7"
      ]
    },
    {
      "url": "https://medium.com/snowflake/reducing-costs-by-doing-more-in-snowflake-79852dfc444b",
      "title": "Reducing Costs by Doing More in Snowflake | by Dylan Kaufman",
      "excerpts": [
        "In total, we see the following benefits of doing this entirely in Snowflake: Substantial decrease in total processing time; Elimination of\u00a0..."
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    },
    {
      "name": "sku_extract_excerpts",
      "count": 5
    }
  ]
}

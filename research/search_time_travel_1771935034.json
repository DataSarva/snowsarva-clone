{
  "search_id": "search_d2c567448343412aa1e9e2fa343ea6dd",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/user-guide/data-time-travel",
      "title": "Understanding & using Time Travel | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nGuides Business continuity & data recovery Time Travel\n ... \nSection Title: Understanding & using Time Travel \u00b6 > Introduction to Time Travel \u00b6 > Data retention period \u00b6\nContent:\nA key component of Snowflake Time Travel is the data retention period.\nWhen data in a table is modified, including deletion of data or dropping an object containing data, Snowflake preserves the state of the data\nbefore the update. The data retention period specifies the number of days for which this historical data is preserved and, therefore,\nTime Travel operations (SELECT, CREATE \u2026 CLONE, UNDROP) can be performed on the data.\nThe standard retention period is 1 day (24 hours) and is automatically enabled for all Snowflake accounts:\nSection Title: Understanding & using Time Travel \u00b6 > Introduction to Time Travel \u00b6 > Data retention period \u00b6\nContent:\nFor Snowflake Standard Edition, the retention period can be set to 0 (or unset back to the default of 1 day) at the account and object\nlevel (that is, databases, schemas, and tables).\nFor Snowflake Enterprise Edition (and higher):\nFor transient databases, schemas, and tables, the retention period can be set to 0 (or unset back to the default of 1 day). The same\nis also true for temporary tables.\nFor permanent databases, schemas, and tables, the retention period can be set to any value from 0 up to 90 days.\nNote\nA retention period of 0 days for an object effectively deactivates Time Travel for the object.\nWhen the retention period ends for an object, the historical data is moved into Snowflake Fail-safe :\nHistorical data is no longer available for querying.\nPast objects can no longer be cloned.\nPast objects that were dropped can no longer be restored.\nSection Title: Understanding & using Time Travel \u00b6 > Introduction to Time Travel \u00b6 > Data retention period \u00b6\nContent:\nTo specify the data retention period for Time Travel:\nThe DATA_RETENTION_TIME_IN_DAYS object parameter can be used by users with the ACCOUNTADMIN role to set the default\nretention period for your account.\nThe same parameter can be used to explicitly override the default when creating a database, schema, and individual table.\nThe data retention period for a database, schema, or table can be changed at any time.\nThe MIN_DATA_RETENTION_TIME_IN_DAYS account parameter can be set by users with the ACCOUNTADMIN role to set a minimum\nretention period for the account. This parameter does not alter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. However it\nmay change the effective data retention time. When this parameter is set at the account level, the effective minimum data retention\nperiod for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\n ... \nSection Title: Understanding & using Time Travel \u00b6 > Enabling and deactivating Time Travel \u00b6\nContent:\nNo tasks are required to enable Time Travel. It is automatically enabled with the standard, 1-day retention period.\nHowever, you may want to upgrade to Snowflake Enterprise Edition to enable configuring longer data retention periods of up to 90 days\nfor databases, schemas, and tables. Note that extended data retention requires additional storage which will be reflected in your monthly\nstorage charges. For more information about storage charges, see Storage costs for Time Travel and Fail-safe .\nTime Travel cannot be deactivated for an account. A user with the ACCOUNTADMIN role can set DATA_RETENTION_TIME_IN_DAYS to 0 at\nthe account level, which means that all databases (and subsequently all schemas and tables) created in the account have no retention period\nby default; however, this default can be overridden at any time for any database, schema, or table.\n ... \nSection Title: Understanding & using Time Travel \u00b6 > Enabling and deactivating Time Travel \u00b6\nContent:\nIf the Time Travel retention period is set to 0, any modified or deleted data is moved into Fail-safe (for permanent tables)\nor deleted (for transient tables) by a background process. This may take a short time to complete. During that time, the\nTIME_TRAVEL_BYTES in table storage metrics might contain a non-zero value even when the Time Travel retention period is 0 days.\nSection Title: Understanding & using Time Travel \u00b6 > Specifying the data retention period for an object \u00b6\nContent:\nEnterprise Edition Feature\nSpecifying a retention period greater than 1 day requires Enterprise Edition (or higher). To inquire about upgrading, please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\nBy default, the maximum retention period is 1 day (one 24-hour period). With Snowflake Enterprise Edition (and higher), the default\nfor your account can be set to any value up to 90 days:\nWhen creating a table, schema, or database, the account default can be overridden using the DATA_RETENTION_TIME_IN_DAYS parameter in the command.\nIf a retention period is specified for a database or schema, the period is inherited by default for all objects created in the\ndatabase/schema.\nSection Title: Understanding & using Time Travel \u00b6 > Specifying the data retention period for an object \u00b6\nContent:\nA minimum retention period can be set on the account using the MIN_DATA_RETENTION_TIME_IN_DAYS parameter. If this parameter is\nset at the account level, the data retention period for an object is determined by\nMAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).\nSection Title: Understanding & using Time Travel \u00b6 > Checking the data retention period for an object \u00b6\nContent:\nTo check the current retention period for a table, schema, or database, you can check the value of the `retention_time` column in the output of the corresponding SHOW command, such as SHOW TABLES , SHOW SCHEMAS , or SHOW DATABASES .\nFor objects that are derived from tables, schemas, or databases, such as materialized views, you can examine the\nretention periods of the parent objects.\nFor streams, you can check the value of the `stale_after` column in the output from the SHOW STREAMS command.\nTo include information about objects that have already been dropped, include the HISTORY clause with the\nSHOW command.\nThe following example shows how you might check the retention periods of certain objects by filtering the output of\nthe SHOW commands.\nThe following example checks the retention period for specific named tables:\n ... \nSection Title: Understanding & using Time Travel \u00b6 > Changing the data retention period for an object \u00b6\nContent:\nIf you change the data retention period for a table, the new retention period impacts all data that is active, as well as any data currently\nin Time Travel. The impact depends on whether you increase or decrease the period:\nIncreasing Retention :\nCauses the data currently in Time Travel to be retained for the longer time period.\nFor example, if you have a table with a 10-day retention period and increase the period to 20 days, data that would have been removed\nafter 10 days is now retained for an additional 10 days before moving into Fail-safe.\nNote that this doesn\u2019t apply to any data that is older than 10 days and has already moved into Fail-safe.\nDecreasing Retention :\nReduces the amount of time data is retained in Time Travel:\nFor active data modified after the retention period is reduced, the new shorter period applies.\nFor data that is currently in Time Travel:\n ... \nSection Title: Understanding & using Time Travel \u00b6 > Changing the data retention period for an object \u00b6\nContent:\nFor example, if you have a schema `s1` with a 90-day retention period and table `t1` is in schema `s1` ,\ntable `t1` inherits the 90-day retention period. If you drop table `s1.t1` , `t1` is retained in Time Travel\nfor 90 days. Later, if you change the schema\u2019s data retention period to 1 day, the retention\nperiod for the dropped table `t1` is unchanged. Table `t1` will still be retained in Time Travel for 90 days.\nTo alter the retention period of a dropped object, you must undrop the object, then alter its retention period.\nTo change the retention period for an object, use the appropriate ALTER  command. For example, to change the\nretention period for a table:\n```\nCREATE TABLE mytable ( col1 NUMBER , col2 DATE ) DATA_RETENTION_TIME_IN_DAYS = 90 ; \n\n ALTER TABLE mytable SET DATA_RETENTION_TIME_IN_DAYS = 30 ;\n```\nCopy\nAttention\nSection Title: Understanding & using Time Travel \u00b6 > Changing the data retention period for an object \u00b6\nContent:\nChanging the retention period for your account or individual objects changes the value for all lower-level objects that do not have a\nretention period explicitly set. For example:\nIf you change the retention period at the account level, all databases, schemas, and tables that do not have an explicit retention period\nautomatically inherit the new retention period.\nIf you change the retention period at the schema level, all tables in the schema that do not have an explicit retention period inherit the\nnew retention period.\nKeep this in mind when changing the retention period for your account or any objects in your account because the change might have\nTime Travel consequences that you did not anticipate or intend. In particular, we do not recommend changing the retention period to 0\nat the account level.\nSection Title: Understanding & using Time Travel \u00b6 > ... > Dropped containers and object retention inheritance \u00b6\nContent:\nCurrently, when a database is dropped, the data retention period for child schemas or tables, if explicitly set to be different from the\nretention of the database, is not honored. The child schemas or tables are retained for the same period of time as the database.\nSimilarly, when a schema is dropped, the data retention period for child tables, if explicitly set to be different from the retention of\nthe schema, is not honored. The child tables are retained for the same period of time as the schema.\nTo honor the data retention period for these child objects (schemas or tables), drop them explicitly before you drop the database\nor schema.\n ... \nSection Title: Understanding & using Time Travel \u00b6 > Cloning historical objects \u00b6\nContent:\nIf the specified Time Travel time is beyond the retention time of any current child (for example, a table) of the entity.As a workaround for child objects that have been purged from Time Travel, use the IGNORE TABLES WITH INSUFFICIENT DATA RETENTION parameter of the\nCREATE  \u2026 CLONE command. For more information, see Child objects and data retention time .If the specified Time Travel time is at or before the point in time when the object was created.\nThe following CREATE DATABASE statement creates a clone of a database and all its objects as they existed\nfour days ago, skipping any tables that have a data retention period of less than four days:Copy\n ... \nSection Title: Understanding & using Time Travel \u00b6 > ... > Example: Dropping and restoring a table multiple times \u00b6\nContent:\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nIntroduction to Time Travel\nEnabling and deactivating Time Travel\nSpecifying the data retention period for an object\nChecking the data retention period for an object\nChanging the data retention period for an object\nQuerying historical data\nCloning historical objects\nDropping and restoring objects\nRelated content\nOverview of the data lifecycle\nData storage considerations\nRelated info\nFor a tutorial on using Time Travel, see the following page:\n[Getting Started with Time Travel](https://quickstarts.snowflake.com/guide/getting_started_with_time_travel/index.html) (Snowflake Quickstarts)\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/performance-query-storage",
      "title": "Optimizing storage for performance - Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Performance optimization Optimizing storage for performance\nSection Title: Optimizing storage for performance \u00b6\nContent:\nThis topic discusses storage optimizations that can improve query performance, such as storing similar data together, creating optimized\ndata structures, and defining specialized data sets. Snowflake provides three of these storage strategies: automatic clustering, search\noptimization, and materialized views.\nIn general, these storage strategies do not substantially improve the performance of queries that already execute in a second or faster.\nThe strategies discussed in this topic are just one way to boost the performance of queries. For strategies related to the computing\nresources used to execute a query, refer to Optimizing warehouses for performance .\nSection Title: Optimizing storage for performance \u00b6 > Introduction to storage strategies \u00b6 > Automatic Clustering \u00b6\nContent:\nSnowflake stores a table\u2019s data in micro-partitions . Among these micro-partitions, Snowflake\norganizes (i.e. clusters) data based on dimensions of the data. If a query filters, joins, or aggregates along those dimensions, fewer\nmicro-partitions must be scanned to return results, which speeds up the query considerably.\nYou can set a cluster key to change the default organization of the micro-partitions so data is clustered\naround specific dimensions (i.e. columns). Choosing a cluster key improves the performance of queries that filter, join, or aggregate by\nthe columns defined in the cluster key.\nSnowflake enables Automatic Clustering to maintain the clustering of the table as soon as you define a cluster key. Once enabled, Automatic\nClustering updates micro-partitions as new data is added to the table. Learn More\nSection Title: Optimizing storage for performance \u00b6 > ... > Search Optimization Service \u00b6\nContent:\nThe Search Optimization Service improves the performance of point lookup queries (i.e. \u201cneedle in a haystack searches\u201d) that return a\nsmall number of rows from a table using highly selective filters. The Search Optimization Service is ideal when it is critical to have\nlow-latency point lookup queries (e.g. investigative log searches, threat or anomaly detection, and critical dashboards with selective\nfilters).\nThe Search Optimization Service reduces the latency of point lookup queries by building a persistent data structure that is optimized for\na particular type of search.\nYou can enable the Search Optimization Service for an entire table or for specific columns. As long as they are selective enough, equality searches , substring searches , and geo searches against those columns can be sped up significantly.\nSection Title: Optimizing storage for performance \u00b6 > ... > Search Optimization Service \u00b6\nContent:\nThe Search Optimization Service supports both structured and semi-structured data (see supported data types ).\nThe Search Optimization Service requires Snowflake Enterprise Edition or higher. Learn More\nSection Title: Optimizing storage for performance \u00b6 > Introduction to storage strategies \u00b6 > Materialized views \u00b6\nContent:\nA materialized view is a pre-computed data set derived from a SELECT statement that is stored for later use. Because the data is\npre-computed, querying a materialized view is faster than executing a query against the base table on which the view is defined. For\nexample, if you specify `SELECT SUM(column1)` when creating the materialized view, then a query that returns `SUM(column1)` from the\nview executes faster because `column1` has already been aggregated.\nMaterialized views are designed to improve query performance for workloads composed of common, repeated query patterns that return a small\nnumber of rows and/or columns relative to the base table.\nA materialized view cannot be based on more than one table.\nMaterialized views require Snowflake Enterprise Edition or higher. Learn More\nSection Title: Optimizing storage for performance \u00b6 > Choosing an optimization strategy \u00b6\nContent:\nDifferent types of queries benefit from different storage strategies. You can use the following sections to discover which strategy best\nfits a workload.\nAutomatic Clustering is the broadest option that can benefit a range of queries that access the same columns of a table. An administrator\noften picks the most important queries based on frequency and latency requirements, and then chooses a cluster key that maximizes the\nperformance of those queries. Automatic Clustering makes sense when many queries filter, join, or aggregate the same few columns.\nSection Title: Optimizing storage for performance \u00b6 > Choosing an optimization strategy \u00b6\nContent:\nThe Search Optimization Service and materialized views have a narrower scope. When specific queries access a well-defined subset of a\ntable\u2019s data, the administrator can use the characteristics of the query to decide whether using the Search Optimization Service or a\nmaterialized view might improve performance. For example, administrators could identify important point lookup queries and implement the\nSearch Optimization Service for a table or column. Likewise, the administrator could optimize specific query patterns by creating a\nmaterialized view.\n ... \nSection Title: Optimizing storage for performance \u00b6 > ... > Differentiating considerations \u00b6\nContent:\nSearch Optimization Service :\n* Improves point lookup queries that return *a small number of rows* . If the query returns more than a few records, consider Automatic\nClustering instead.\nIncludes support for point lookup queries that:\nMatch substrings or regular expressions using predicates such as LIKE and RLIKE.\nSearch for specific fields in VARIANT, ARRAY, or OBJECT columns.\nUse geospatial functions with GEOGRAPHY values.\nMaterialized view :\n* Improves intensive and frequent calculations such as aggregation and analyzing semi-structured data (not just filtering).\nUsually focused on a specific query/subquery calculation.\nImproves queries against external tables .\n[1] If there is an important reason to define multiple cluster keys, you could create multiple materialized views, each with its own\ncluster key.\nSection Title: Optimizing storage for performance \u00b6 > Choosing an optimization strategy \u00b6 > Prototypical queries \u00b6\nContent:\nThe following examples are intended to highlight which type of query typically runs faster with a particular storage strategy.\nPrototypical Query for Clustering\nAutomatic Clustering provides a performance boost for *range queries* with large table scans. For example, the following query will\nexecute faster if the `shipdate` column is the table\u2019s cluster key because the `WHERE` clause scans a lot of data.\n```\nSELECT \n  SUM ( quantity ) AS sum_qty , \n  SUM ( extendedprice ) AS sum_base_price , \n  AVG ( quantity ) AS avg_qty , \n  AVG ( extendedprice ) AS avg_price , \n  COUNT (*) AS count_order \n FROM lineitem \n WHERE shipdate >= DATEADD ( day , - 90 , to_date (' 2023 - 01 - 01 ));\n```\nCopy\n ... \nSection Title: Optimizing storage for performance \u00b6 > Choosing an optimization strategy \u00b6 > Prototypical queries \u00b6\nContent:\nPrototypical Query for Materialized View\nA materialized view can provide a performance boost for queries that access a small subset of data using expensive operations like\naggregation. As an example, suppose that an administrator aggregated the `totalprice` column when creating a materialized view `mv_view1` . The following query against the materialized view will execute faster than it would against the base table.\n```\nSELECT \n  orderdate , \n  SUM ( totalprice ) \n FROM mv_view1 \n GROUP BY 1 ;\n```\nCopy\nFor more use cases where materialized views can speed up queries, refer to Examples of Use Cases For Materialized Views .\n ... \nSection Title: Optimizing storage for performance \u00b6 > ... > Initial investment \u00b6\nContent:\nImplementing a storage strategy can require a bigger time commitment and upfront financial investment than other types of performance\noptimizations (e.g. re-writing SQL statements or optimizing the warehouse running the query), but the\nperformance improvements can be significant.\nSnowflake uses serverless compute resources to implement each storage strategy, which consumes\ncredits before you can test how well the optimization improves performance. In addition, it can take Snowflake a significant amount of\ntime to fully implement Automatic Clustering and the Search Optimization Service (e.g. a week for a very large table).\nThe Search Optimization Service and materialized views also require the Enterprise Edition or higher, which increases the price of a credit.\nSection Title: Optimizing storage for performance \u00b6 > Implementation and cost considerations \u00b6 > Ongoing cost \u00b6\nContent:\nStorage strategies incur both compute and storage costs.\nCompute Costs\nSnowflake uses serverless compute resources to maintain storage optimizations as new data is added to a table. The more changes to a\ntable, the higher the maintenance costs. If a table is constantly updated, the cost of maintaining a storage optimization might be\nprohibitive.\nSection Title: Optimizing storage for performance \u00b6 > Implementation and cost considerations \u00b6 > Ongoing cost \u00b6\nContent:\nThe cost of maintaining materialized views or the Search Optimization Service can be significant when Automatic Clustering is enabled\nfor the underlying table. With Automatic Clustering, Snowflake is constantly reclustering its micro-partitions around the dimensions of\nthe cluster key. Every time the base table is reclustered, Snowflake must use serverless compute resources to update the storage used by\nmaterialized views and the Search Optimization Service. As a result, Automatic Clustering activities on the base table can trigger\nmaintenance costs for materialized views and the Search Optimization Service beyond the cost of the DML commands on the base table. Storage Costs\nAutomatic Clustering\nUnlike the Search Optimization Service and materialized views, Automatic Clustering reorganizes existing data rather than creating\nadditional storage.\n ... \nSection Title: Optimizing storage for performance \u00b6 > ... > Implementation strategy \u00b6\nContent:\nBecause the compute costs and storage costs of a storage strategy can be significant, you might want to start small and carefully track the\ninitial and ongoing costs before committing to a more extensive implementation. For example, you might choose a cluster key for just one or\ntwo tables, and then assess the cost before choosing a key for other tables.\nWhen tracking the ongoing cost associated with a storage strategy, remember that virtual warehouses consume credits only during the time\nthey are running a query, so a faster query costs less to run. Snowflake recommends carefully reporting on the cost of running a query\nbefore the storage optimization and comparing it to the cost of running the same query after the storage optimization so it can be factored\ninto the cost assessment.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/replication-intro",
      "title": "Introduction to business continuity & disaster recovery | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\n1. Overview\n2. Snowflake Horizon Catalog\n3. Applications and tools for connecting to Snowflake\n4. Virtual warehouses\n5. Databases, Tables, & Views\n6. Data types\n7. Data Integration\n   \n    + Snowflake Openflow\n    + Apache Iceberg\u2122\n         \n        - Apache Iceberg\u2122 Tables\n        - Snowflake Open Catalog\n8. Data engineering\n   \n    + Data loading\n    + Dynamic Tables\n    + Streams and Tasks\n    + dbt Projects on Snowflake\n    + Data Unloading\n9. Storage Lifecycle Policies\n10. Migrations\n11. Queries\n12. Listings\n13. Collaboration\n14. Snowflake AI & ML\n15. Alerts & Notifications\n16. Security\n17. Data Governance\n18. Privacy\n19. Organizations & Accounts\n20. Business continuity & data recovery\n    \n        - Replication\n        - Introduction\n        - Considerations\n        - Configuration and usage\n        - Security integrations and network policy replication\n        - Apache Iceberg table replication\n        - Stage, pipe, and load history replication\n        - Git repository replication\n        - Understanding cost\n        - Failover\n        - Account failover\n        - Monitoring\n        - Monitoring replication and failover\n        - Error notifications for replication and failover groups\n        - Client Redirect\n        - Overview and usage\n        - Data Recovery\n        - Backups\n        - Time Travel\n        - Fail-safe\n        - Storage costs for historical data\n21. Performance optimization\n22. Cost & Billing\n\nGuides Business continuity & data recovery\n\n# Introduction to business continuity & disaster recovery \u00b6\n\n Standard & Business Critical Feature\n\n* Database and share replication are available to all accounts.\n* Replication of other account objects, failover/failback, and Client Redirect require Business Critical (or higher).\n  To inquire about upgrading, please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\n\nThis topic describes the main use cases for replication and failover across regions and cloud platforms. The Snowflake replication\nand failover/failback functionality is composed of the following features:\n\n* Replication and Failover/Failback\n* Client Redirect\n\nCollectively, these individual features are designed to support a number of different fundamental business continuity scenarios,\nincluding:\n\n* **Planned failovers** : For disaster recovery drills to test preparedness, and measure recovery point and time.\n* **Unplanned failovers** : In the case of an outage in a region or a cloud platform, promote secondary account objects and databases\n  in another region or cloud platform to serve as read-write primary objects.\n* **Migration** : Move your Snowflake account to a different region or cloud platform without disrupting your business. For example, to\n  maintain business continuity during mergers and acquisitions, or facilitate a change in cloud strategy.\n* **Multiple readable secondaries** : Account objects and databases can be replicated to multiple accounts in\n  different regions and cloud platforms, mitigating the risk of multiple region or cloud platform outages.\n\nIn addition, Snowflake Secure Data Sharing and Database Replication enable sharing data securely across regions and cloud platforms.\n\n## Account replication and failover/failback features \u00b6\n\n### Replication and failover/failback \u00b6\n\nReplication uses two Snowflake objects, replication group and failover group , to replicate a group of objects with point-in-time\nconsistency from a source account to one or more target accounts. A replication group allows customers to specify what to replicate, where\nto replicate to, and how often. This means specifying which objects to replicate, to which regions or cloud platforms, at customizable\nscheduled intervals. A failover group enables the replication and failover of the objects in the group.\n\nAccount objects can include warehouses, users, and roles, along with databases and shares (see Replicated objects for the full\nlist of objects that can be included in a replication or failover group). Account objects can be grouped in one or multiple groups.\n\nIn the case of failover, account replication enables the failover of your account to a different region or cloud platform.\nEach replication and failover group has its own replication schedule, allowing you to set the frequency for replication at different\nintervals for different groups of objects. In the case of failover groups, it also enables failover of groups individually. You can choose\nto failover all failover groups, or only select failover groups.\n\n### Client Redirect \u00b6\n\nClient Redirect provides a _connection URL_ that can be used by Snowflake clients to connect to\nSnowflake. The connection URL can redirect Snowflake clients to a different Snowflake account as needed.\n\n## Business continuity and disaster recovery \u00b6\n\nIn the event of a massive outage (due to a network issue, software bug, etc.) that disrupts the cloud services in a given region, access to\nSnowflake will be unavailable until the source of the outage is resolved and services are restored. To ensure continued availability and\ndata durability in such a scenario, replicate your critical account objects to another Snowflake account in your organization\nin a different region.\n\nWith asynchronous replication, secondary replicas typically lag behind the primary objects based on the replication schedule you\nconfigure. Secondary replica objects are at most 2x the time interval between scheduled refreshes behind the primary objects. For\nexample, if you choose to replicate a primary replication or failover group every 30 minutes, the secondary objects in the group\nare at most 60 minutes behind the primary objects during an outage.\n\nDepending on your business needs you could choose to:\n\n> * Recover reads first to let client applications read data that is 30 minutes stale.\n> * Recover writes first to reconcile the last 30 minutes of data on the new primary before\n>   opening up reads from client applications.\n> * Recover both reads and writes simultaneously, that is, open up reads from client applications on data that is 30 minutes stale as\n>   you reconcile the last 30 minutes of data on the new primary.\n> \n> \n\n### Normal status: Region is operational \u00b6\n\n**Account Object Replication:** Replicate the failover group(s) with critical account objects to one or more Snowflake accounts in\nregions different from that of the account that stores the primary (source) failover group(s). Refresh the failover group(s) frequently.\n\n### Region outage \u00b6\n\nTo prioritize reads, writes, or both at the same time, follow the steps in one of the following example scenarios.\n\n#### Reads before writes \u00b6\n\nWhen an outage in a region results in full or partial loss of Snowflake availability, this path allows you to redirect Snowflake clients to read-only replicas of account objects in critical failover group(s) first for minimal downtime. Choosing to operate in read-only mode is often desirable during short-term outages.\n\nA longer-term outage combined with the need for the latest data necessitates read-write mode.\n\n1. **Client Redirect:** Point the connection URL used by clients to a Snowflake account that stores your read-only replica (secondary) failover group(s).\n2. **Failover (When Needed):** In the event of a longer-term outage, promote the secondary failover group(s) in the Snowflake account where your connection URL is pointing to serve as read-write primary failover group(s).\n\n#### Writes before reads \u00b6\n\nWhen an outage in a region results in full or partial loss of Snowflake availability, this path allows you to recover failover group(s) with critical account objects and continue to process data first. This option is preferable for account administrators who want to fail over their databases and ETL (Extract, Transform, Load) processes first, and then choose to redirect Snowflake clients only when the data is current.\n\n1. **Failover:** Promote the secondary failover group(s) with critical account objects in a different region to serve as the primary\n   failover group(s), which allows writing to the objects included in each failover group(s). Once the databases in the group(s)\n   are writable, you can use your ETL processes to prioritize writes and reconcile data.\n   \n   If you use Snowflake data pipeline objects for ETL processes, you can replicate and fail over those objects. For more information,\n   see Stage, pipe, and load history replication .\n   \n   Otherwise, configure separate connection URLs for your data ingestion pipeline and one for your clients (for example, a BI\n   dashboard). After failing over the failover group, fail over the connection URL for data ingestion, and write data to the newly\n   promoted primary objects. After data has been reconciled, fail over the connection URL for your clients to enable reads.\n2. **Client Redirect (When Needed):** Point the connection URL used by clients to the Snowflake account that stores the new primary failover group(s).\n\n#### Prioritize both reads and writes \u00b6\n\nTo prioritize both reads and writes at the same time, fail over both the client connection and secondary failover group(s) without\nwaiting for the secondary objects to be up to date. This enables immediate access for clients to potentially stale data while the\nnewly promoted databases can start reingesting data from data pipelines.\n\n1. **Client Redirect:** Point the connection URL used by clients to a Snowflake account that stores your read-only replica (secondary)\n   failover group(s).\n2. **Failover:** Promote the secondary failover group(s) with critical account objects in a different region to serve as the primary\n   failover group(s), which enables writing to the objects included in each failover group(s).\n\n### Normal status: Outage is resolved \u00b6\n\n1. **Replication:** Refresh the failover group(s) in the Snowflake account in the region where the outage occurred.\n2. **Failback:** Promote the failover group(s) in the Snowflake account where the outage occurred to again serve as the primary failover\n   group(s).\n3. **Client Redirect:** Point the connection URL used by clients to the Snowflake account in the region where the outage occurred.\n\n## Account migration \u00b6\n\nAccount migration is the one-time process of migrating (or transferring) the Snowflake objects and your stored data to an account in\nanother region or on a different cloud platform. Typical reasons for migrating your account include a closer proximity to your user base\nor a preference for a different cloud platform based on your corporate strategy or co-location with other cloud assets (e.g. a data lake).\n\nAccount object replication supports the replication of account objects such as warehouses, users, and roles, along with databases and\nshares. See Replicated objects for the complete list of replicated objects.\n\nNote\n\nAccount object replication and failover/failback requires Business Critical (or higher). Snowflake can temporarily waive this requirement\nfor a one-time account migration.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Account replication and failover/failback features\n2. Business continuity and disaster recovery\n3. Account migration\n\nRelated content\n\n1. Share data securely across regions and cloud platforms\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/data-availability",
      "title": "Snowflake Time Travel & Fail-safe | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\n# Snowflake Time Travel & Fail-safe \u00b6\n\nSnowflake provides powerful CDP features for ensuring the maintenance and availability of your historical data (i.e. data that has been changed or deleted):\n\n> * Querying, cloning, and restoring historical data in tables, schemas, and databases for up to 90 days through Snowflake Time Travel.\n> * Disaster recovery of historical data (by Snowflake) through Snowflake Fail-safe.\n> \n> \n\nThese features are included standard for all accounts, i.e. no additional licensing is required; however, standard Time Travel is 1 day. Extended Time Travel (up to 90 days) requires Snowflake Enterprise Edition. In addition,\nboth Time Travel and Fail-safe require additional data storage, which has associated fees.\n\n**Next Topics:**\n\n* Understanding & using Time Travel\n* Understanding and viewing Fail-safe\n* Storage costs for Time Travel and Fail-safe\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nRelated content\n\n1. Securing Snowflake\n2. Continuous data protection\n3. Data storage considerations\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/guides-overview-performance",
      "title": "Optimizing performance in Snowflake | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nGuides Performance optimization\n\n# Optimizing performance in Snowflake \u00b6\n\nThe following topics help guide efforts to improve the performance of Snowflake.\n\nExploring execution times\n    Gain insights into the historical performance of queries using the web interface or by writing queries against data in the ACCOUNT\\_USAGE\nschema.\nOptimizing query performance\n    Learn about options for optimizing Snowflake query performance.\nOptimizing warehouses for performance\n    Learn about strategies to fine-tune computing power in order to improve the performance of a query or set of\nqueries running on a warehouse, including enabling the Query Acceleration Service.\nOptimizing storage for performance\n    Learn how storing similar data together, creating optimized data structures, and defining specialized data sets can improve the\nperformance of queries.\n\nHelpful when choosing between Automatic Clustering, Search Optimization Service, and materialized views.\nAnalyzing query workloads with Performance Explorer\n    Learn how to use Performance Explorer in Snowsight to monitor interactive metrics for SQL workloads.\nSnowflake Optima\n    Learn how Snowflake Optima continuously analyzes workload patterns and implements the most effective strategies automatically.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nRelated content\n\n1. Managing cost in Snowflake\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas"
      ]
    },
    {
      "url": "https://www.reddit.com/r/snowflake/comments/1e8weo4/snowflake_disaster_recovery_scenarios/",
      "title": "Snowflake Disaster recovery scenarios - Reddit",
      "publish_date": "2024-07-21",
      "excerpts": [
        "Skip to main content\nOpen menu Open navigation\nGo to Reddit Home\nr/snowflake\nA chip A close button Get App Get the Reddit app\nLog In Log in to Reddit\nExpand user menu Open settings menu\nGo to snowflake\nr/snowflake\nr/snowflake\nUnofficial subreddit for discussion relating to the Snowflake Data Cloud\nMembers Online \u2022\nBig_Length9755\nSnowflake Disaster recovery scenarios\nHello All,\nWhile going through the snowflake disaster recovery strategy , I am having some questions around it\n1)Is there flexibility to only have specific objects(say for e.g. only ~10 different tables those are used by specific application) replicated to disaster recovery side but not the full database(which includes tables, views, procedures, tasks etc.) in snowflake? Sample syntax or reference doc will help.\n2) Is the object in the DR side a logical or a physical copy(which means the clustering etc. will be exactly same as primary)? And how much time Approx. will it take for initial replication to happen to DR side. Is there a way to estimate both time and cost for it?\n3)Also I see snowflake provides option to create global connection URL(i.e. without specifying specific account) to which the application has to connect and it will automatically do the failover from primary side to DR side based on the failover configuration in case of primary side gets down. In this case I hope snowflake must keep on checking the heartbeat of the primary side and just in case it doesn't respond it must be doing the failover to the other side. Please correct me if wrong?\nHowever, in above case if the application resides near to primary region then propagating the connection to DR region will have latency added to it. I was also wondering, in case of the whole region goes down fully for that cloud provider and if the application also hosted on same region , how the failover scenario should be handled?\nShare\nShare"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/data-failsafe",
      "title": "Understanding and viewing Fail-safe | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Business continuity & data recovery Fail-safe\nSection Title: Understanding and viewing Fail-safe \u00b6\nContent:\nSeparate and distinct from Time Travel, Fail-safe ensures historical data is protected in the event of a system failure or other event (e.g. a\nsecurity breach).\nSection Title: Understanding and viewing Fail-safe \u00b6 > What is Fail-safe? \u00b6\nContent:\nFail-safe provides a (non-configurable) 7-day period during which historical data may be recoverable by Snowflake. This period starts\nimmediately after the Time Travel retention period ends. Note, however, that a long-running Time Travel query will delay moving any data and\nobjects (tables, schemas, and databases) in the account into Fail-safe, until the query completes.\nAttention\nFail-safe is a data recovery service that is provided on a best effort basis and is intended only for use when all other recovery options have been attempted.\nFail-safe is not provided as a means for accessing historical data after the Time Travel retention period has ended. It is for use only by\nSnowflake to recover data that may have been lost or damaged due to extreme operational failures.\nData recovery through Fail-safe may take from several hours to several days to complete.\nSection Title: Understanding and viewing Fail-safe \u00b6 > View Fail-safe storage for your account \u00b6\nContent:\nWhen you review the total data storage usage for your account in Snowsight, you can view the\nhistorical data storage in Fail-safe.\nYou must use the ACCOUNTADMIN role to view the amount of data that is stored in Snowflake.\nIn Snowsight, follow these steps:\nIn the navigation menu, select Admin \u00bb Cost management , and then select Consumption .\nUse the Usage Type filter to select Storage .\nReview the graph and table for Fail-safe storage. The Storage Breakdown column in the table uses color-coded bars\nto represent the different kinds of storage, including Fail-safe storage. Hover the mouse pointer over\neach bar to see the size for each kind of storage.\n ... \nSection Title: Understanding and viewing Fail-safe \u00b6 > Considerations \u00b6\nContent:\nFor fail-safe and Snowpipe Streaming Classic, be aware of the following limitations:\nFail-safe doesn\u2019t support tables that contain data ingested by Snowpipe Streaming Classic. For such tables, you can\u2019t use fail-safe for recovery because fail-safe operations on that table will fail completely. For more information, see Snowpipe Streaming limitations .\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nWhat is Fail-safe?\nView Fail-safe storage for your account\nBilling for Fail-safe\nConsiderations\nRelated content\nUnderstanding & using Time Travel\nData storage considerations\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://medium.com/snowflake/best-practices-to-optimize-snowflake-storage-spend-4d1cef6b6fb0",
      "title": "Best practices to optimize Snowflake storage cost | by Samartha Chandrashekar | Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science | Medium",
      "publish_date": "2023-10-11",
      "excerpts": [
        "Sitemap\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4d1cef6b6fb0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-4d1cef6b6fb0---------------------------------------)\n\u00b7\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-4d1cef6b6fb0---------------------------------------)\nBest practices, tips & tricks from Snowflake experts and community\nSection Title: Best practices to optimize Snowflake storage cost\nContent:\nSamartha Chandrashekar\n6 min read\n\u00b7\nOct 10, 2023\n--\n2\nListen\nShare\nSnowflake is constantly improving the efficiency of storage mechanisms used behind the scenes.\nThis post details the components of Snowflake storage costs and then covers best practices to help customers optimize storage spend on Snowflake over and above the ongoing behind-the-scenes improvements to storage efficiency delivered by Snowflake.\nSection Title: Components of Snowflake storage costs\nContent:\nFiles that are prepared for bulk data loading/unloading. These can be stored in compressed or uncompressed formats.\nDatabase tables include historical data for Time Travel (which enables accessing data that has been changed or deleted within a predefined period for restoring accidentally or intentionally deleted objects, backing up from points in the past, and analyzing past usage).\nFail-safe for database tables (which provides a 7-day period after the Time Travel retention period during which historical data may be recovered).\nClones of database tables that reference data deleted in the table that owns the clones. Snowflake minimizes the storage of historical data by maintaining only the information needed to restore updated/deleted table rows; full copies of tables are only maintained when tables are dropped or truncated.\n ... \nSection Title: ... > **2/ Use temporary and transient table types to control storage cost** implications of Time...\nContent:\nTransient tables exist until explicitly dropped, after which the historical data beyond the Time Travel retention period cannot be recovered. Temporary tables only exist for the lifetime of their associated session, after which data is purged and unrecoverable. Transient and temporary tables don\u2019t come with a Fail-safe period and can have a Time Travel retention period of either 0 or 1 day \u2014 which means they can, at most, incur one day\u2019s worth of storage cost.\nAdditionally, it is noteworthy that TIME_TRAVEL_BYTES and FAILSAFE_BYTES incur charges when data is loaded using INSERT, COPY, or SNOWPIPE. This is because the defragmentation of micro-partition deletes small micro-partitions and creates a new one with the same data. The deleted micro-partitions contribute to TIME_TRAVEL_BYTES and FAILSAFE_BYTES.\n ... \nSection Title: ... > 3/ Use appropriate data protection strategies for high-churn dimension tables and low-churn...\nContent:\nHigh-churn dimension tables can be identified if the ratio of FAILSAFE_BYTES to ACTIVE_BYTES in the TABLE_STORAGE_METRICS view is large. As a result, there is a need to consider the cost and benefits of Time Travel and Fail-safe for such dimension tables. In many cases, it can be a better strategy to use the \u2018transient\u2019 type with zero Time Travel retention and periodically copy contents into a permanent table for a full backup; old backups can be deleted when a new one is created.\nSimilarly, for short-lived tables (e.g., ETL work tables that need to stay for less than a day), we recommend using \u2018transient\u2019 or \u2018temporary\u2019 table types to minimize Fail-safe costs."
      ]
    },
    {
      "url": "https://medium.com/@nickakincilar/disaster-recovery-failover-is-so-darn-easy-with-snowflake-138563b173a6",
      "title": "Disaster Recovery & Failover is so darn easy with Snowflake! | by Nick Akincilar | Medium",
      "publish_date": "2022-10-13",
      "excerpts": [
        "Section Title: Disaster Recovery & Failover is so darn easy with Snowflake!\nContent:\nNick Akincilar\nFollow\n8 min read\n\u00b7\nJul 27, 2022\n55\nListen\nShare\nAs data volumes and complexity continue to grow, it becomes more difficult for companies to back up their data & create failover sites for business continuity. And with the rise of cloud-based storage where volumes are technically unlimited, data recovery & failover is more challenging than ever.\nBut backing up your data is simply not enough. Business continuity is an important consideration for any company, but it\u2019s especially important for companies that rely on technology to function. The cloud can help you with disaster recovery, data protection, and business continuity.\n ... \nSection Title: ... > **SO MUCH EASIER IF YOU ARE USING SNOWFLAKE !**\nContent:\nTriple redundancy is for both data & as well as compute which means you can lose 2 of the 3 data centers in your Snowflake cloud region and your account will still be operational as a read-only mode. This standard protection is great for outages are the AZ(data center) level but what happens if the region goes down (all 3 data centers) or even worse, the entire cloud provider goes down? (Don\u2019t say cloud-level outage doesn't happen, it already did a few times)\n ... \nSection Title: ... > **THIS IS HOW EASY IT IS TO SETUP DISASTER RECOVERY & FAIL-OVER USING SNOWFLAKE**\nContent:\n1. Create a new account. It can be in the same cloud using different region or be in a completely different cloud(Azure, AWS, or GCP) & region. This is because Snowflake is a true cloud agnostic data platform where users & tools will not see any difference in connectivity, usage, feature or functionality regardless of what cloud provider the account is running under. Simply give it an account name, some admin user info the chose the account edition & cloud & region info. [Current list of Snowflake Cloud & Regions](https://docs.snowflake.com/en/user-guide/intro-regions.html)\nPress enter or click to view image in full size\nSection Title: ... > **THIS IS HOW EASY IT IS TO SETUP DISASTER RECOVERY & FAIL-OVER USING SNOWFLAKE**\nContent:\n2. Use the new **Account Replication feature** which is set to go public preview soon to create a fail-over group using your primary & secondary account(s). You simply tell Snowflake **What** to replicate, **Where** **to** replicate & **How often** . (A *ccounts need to be Business Critical Edition for this feature)*\nPress enter or click to view image in full size Press enter or click to view image in full size\n3. Now let\u2019s create a single account URL alias to use for all our connection strings by various tools & users that we can repoint to any number of accounts in case of outages. Start by creating a new account URL (connection) with a unique name on your **primary account.**\nPress enter or click to view image in full size\n4. Now **Switch to your secondary account** & run the following command to enable & start the replication process. ( *using the same Failover group name* )\nSection Title: ... > **THIS IS HOW EASY IT IS TO SETUP DISASTER RECOVERY & FAIL-OVER USING SNOWFLAKE**\nContent:\nPress enter or click to view image in full size\n5. Create the exact same connection ( *using the same connection name* ) on your **Secondary** **account** .\nPress enter or click to view image in full size\nNow that the entire account is being replicated ( data, users, security, configs & all ) to other accounts running under various clouds/regions in a fully automated & incremental way AND we have a single connection URL that we can repoint to any secondary account at any time during an outage, we are pretty much done!"
      ]
    },
    {
      "url": "https://rafaelrampineli.medium.com/snowflake-overview-time-travel-tasks-fail-safe-streams-and-command-copy-796d4ffdca85",
      "title": "Snowflake Overview: Time travel, Tasks, Fail-safe, Streams, and Command Copy | by Rafael Rampineli | Medium",
      "publish_date": "2024-12-27",
      "excerpts": [
        "Section Title: Snowflake Overview: Time travel, Tasks, Fail-safe, Streams, and Command Copy\nContent:\nSnowflake is a powerful data warehouse platform known for its scalability, elasticity, and ability to process large datasets efficiently. It has become one of the most popular solutions for cloud data storage and analysis. Snowflake offers a range of advanced features to help organizations with data management, including Time Travel, Tasks, Fail-safe, and Copy. In this article, we\u2019ll provide an overview of these features and give examples of how to use them.\nSection Title: Snowflake Overview: Time travel, Tasks, Fail-safe, Streams, and Command Copy > 1. Time Travel\nContent:\n**Time Travel** is a feature in Snowflake that allows you to query historical data by viewing previous versions of tables, schemas, or databases. This is particularly useful for recovering lost data, auditing, or analyzing changes over time.\nSnowflake uses a combination of metadata and data storage to enable Time Travel. The feature allows you to perform queries on past data with a defined retention period. The default retention period for Time Travel is 1 day, but it can be extended up to 90 days, depending on the account\u2019s configuration.\nSection Title: ... > Querying Historical Data\nContent:\nSuppose you have a table named `sales_data` , and you want to retrieve the data as it existed three days ago. Here's how you can do it:\n```\nSELECT *   \nFROM sales_data   \nAT (TIMESTAMP => '2024-12-24 10:00:00');\n```\nAlternatively, you can use `TIME_SLICE` to refer to a specific point in time relative to the current system time:\n```\nSELECT *   \nFROM sales_data   \nAT (OFFSET => -3);\n```\nThis query will return the data as it appeared three days ago.\n ... \nSection Title: Snowflake Overview: Time travel, Tasks, Fail-safe, Streams, and Command Copy > 3. Fail-safe\nContent:\n**Fail-safe** is a Snowflake feature designed to provide additional data protection. When you delete or modify data in Snowflake, the data is typically still recoverable for a period through Time Travel. However, if the data is permanently deleted, it might be unrecoverable through Time Travel. This is where Fail-safe comes into play.\nFail-safe acts as a safety net for data that has been permanently deleted from the system and ensures that it can be recovered in case of critical issues. Snowflake provides a seven-day retention period for Fail-safe, during which the data can be restored by Snowflake support.\nIt is not directly accessible by users; it\u2019s a mechanism intended for extreme cases. If data needs to be restored after the Time Travel period, the Snowflake support team can assist in recovering it from the Fail-safe storage.\nSection Title: Snowflake Overview: Time travel, Tasks, Fail-safe, Streams, and Command Copy > 3. Fail-safe\nContent:\nFail-safe comes into play after the Time Travel retention period expires. If a table `sales_data` is dropped and you cannot restore it via Time Travel (because it\u2019s beyond the retention period), you would need to contact Snowflake support for fail-safe recovery.\n ... \nSection Title: Snowflake Overview: Time travel, Tasks, Fail-safe, Streams, and Command Copy > Conclusion\nContent:\nSnowflake offers a range of powerful features that streamline data management, querying, and automation, making it a top choice for cloud data processing. Key features include **Time Travel** , which allows querying historical data; **Streams** , which enable real-time tracking of inserts, updates, and deletes without scanning entire tables, ideal for Change Data Capture (CDC), real-time analytics, and incremental ETL; **Tasks** , which automate workflows and can be combined with Streams for scalable data processing; **Fail-safe** , ensuring data recovery in critical scenarios; and **Command Copy** , which simplifies loading external data. These features, combined with Snowflake\u2019s scalability and flexibility, help organizations optimize data processing, automate tasks, and ensure data consistency and security."
      ]
    },
    {
      "url": "https://www.montecarlodata.com/blog-snowflake-cost-optimization/",
      "title": "5 Snowflake Cost Optimization Techniques You Should Know",
      "publish_date": "2025-07-06",
      "excerpts": [
        "Section Title: 5 Simple Steps For Snowflake Cost Optimization Without Getting Too Crazy\nContent:\nToo light a touch, and your monthly bill will eventually catch the wandering gaze of your CTO or CFO. Cut corners too hard and you could introduce [data reliability](https://www.montecarlodata.com/blog-what-is-data-reliability/) or [data quality issues](https://www.montecarlodata.com/blog-8-data-quality-issues) that cost more than that $1.25 you were trying to save.\nWithout Snowflake cost optimization efforts, the dreaded CFO gaze may come upon your bill.\nIn that spirit, we have put together a simple three step plan for Snowflake cost optimization without getting too crazy.\nSection Title: ... > Snowflake pricing: how the model works\nContent:\nTo understand Snowflake cost optimization strategies and best practices, you first need to know how the consumption based pricing model works.\n[Actual pricing](https://www.snowflake.com/pricing/) depends on a few different variables such as the cloud provider, region, plan type, services, and more. Since we\u2019re not getting too crazy, we will oversimplify a bit.\nEssentially, your Snowflake cost is based on the actual monthly usage of three separate items: storage, compute, and cloud services. You will be charged for any [Snowflake serverless features](https://docs.snowflake.com/en/user-guide/admin-serverless-billing.html) you use as well.\nExample Snowflake pricing in the AWS \u2013 US East region. Image from Snowflake.com.\nSection Title: ... > Snowflake pricing: how the model works\nContent:\nMost customers pay on-demand with Snowflake credits, but you can pre-purchase capacity as well. For example, the on-demand pricing in the AWS-US East region as of April 2022 is $40 per terabyte per month with Snowflake credits priced at $2.00, $3.00, or $4.00 respectively depending on which plan (Standard, Enterprise, Business Critical) was selected.\nSnowflake also offers dynamic pricing. Snowflake dynamic pricing will **adjust prices in real time to help retailers maximize the profitability of each unit sold** , based on inventory levels, competitors\u2019 pricing, and current trends in consumer demand.\nThis can seem a bit abstract and intimidating at first glance. Converting your average workload into expected Snowflake credit spend takes a bit of math (and what is the conversion of Snowflake credits to Shrute bucks?).\n ... \nSection Title: ... > Is there a Snowflake object that does not consume any storage costs?\nContent:\nThat is why our Snowflake cost optimization strategy will focus on optimizing those three areas by leveraging native features, best practices, and other solutions.\n ... \nSection Title: ... > Step 4: Snowflake data lifecycle and retention optimization\nContent:\nHere\u2019s where many teams miss significant cost savings opportunities. By default, Snowflake retains 1-day time-travel and 7-day fail-safe for all tables, which can dramatically increase storage costs, sometimes up to 90\u00d7 more than the base table size for historical copies.\nUnderstanding the cost implications of Snowflake\u2019s data retention features is crucial for cost optimization without getting too crazy about data recovery capabilities. Standard tables in Snowflake include both [Time Travel](https://docs.snowflake.com/en/user-guide/data-time-travel) , which can range from 1-90 days, and [Fail-safe](https://docs.snowflake.com/en/user-guide/data-failsafe) at 7 days, which means you\u2019re paying for multiple copies of your data. But not every table needs this level of protection.\n ... \nSection Title: ... > Time-travel vs transient vs temporary tables\nContent:\nYou can also [adjust your Time Travel retention](https://docs.snowflake.com/en/user-guide/data-time-travel) based on actual recovery needs rather than accepting the default. For frequently changing staging tables, you might reduce retention to just 1 day, while critical business tables might warrant 30 days or more depending on your enterprise plan."
      ]
    },
    {
      "url": "https://www.snowflake.com/en/webinars/customer-webinar/business-continuity-disaster-recovery-2025-10-31/",
      "title": "Business Continuity & Disaster Recovery - Snowflake",
      "excerpts": [
        "Learn to use Replication Groups and Failover Groups to create point-in-time consistent copies of your objects (including databases, shares, users, and roles) to ..."
      ]
    },
    {
      "url": "https://www.reddit.com/r/snowflake/comments/1pnj0v6/snowflake_time_travel_and_failsafe/",
      "title": "Snowflake Time Travel and Fail-safe - Reddit",
      "publish_date": "2025-12-15",
      "excerpts": [
        "Skip to main content Open menu Open navigation  Go to Reddit Home\n\nr/snowflake\n\nExpand user menu Open settings menu\n\nGo to snowflake r/snowflake \u2022\n\nIdr24\n\n# Snowflake Time Travel and Fail-safe\n\n[[](https://www.idriss-benbassou.com/snowflake-time-travel-fail-safe-zero-copy-clone/)](https://www.idriss-benbassou.com/snowflake-time-travel-fail-safe-zero-copy-clone/)\n\n[idriss-benbassou.com Open](https://www.idriss-benbassou.com/snowflake-time-travel-fail-safe-zero-copy-clone/)\n\nHey,\n\nOn a lot of Snowflake projects, people feel _safe_ because \u201cwe have Time Travel\u201d\u2026 but the operational reality is different :\n\n* retention is often left at the default everywhere (or set to 0 in \u201cnon-prod\u201d without really thinking)\n* teams confuse Time Travel (self-service) and Fail-safe (support-only)\n* zero-copy CLONE gets used as a \u201cbackup\u201d pattern\u2026 until storage surprises show up because of copy-on-write\n\nI wrote a short post (in French) where I break down how I think about Time Travel, Fail-safe, and Zero-copy clone\n\n[https://www.idriss-benbassou.com/snowflake-time-travel-fail-safe-zero-copy-clone/](https://www.idriss-benbassou.com/snowflake-time-travel-fail-safe-zero-copy-clone/?utm_source=chatgpt.com)\n\nCurious how you handle this in your setup:\n\n* Do you set `DATA_RETENTION_TIME_IN_DAYS` centrally (account/db/schema), or do you let teams override per table?\n\nRead more Share\n\n# Related Answers Section\n\nRelated Answers\n\nTop features of Snowflake for data analysts\n\nHow to integrate Snowflake with Python\n\nCommon pitfalls when using Snowflake\n\nTips for managing costs in Snowflake\n\nHow to secure data in Snowflake effectively\n\nPublic\n\nAnyone can view, post, and comment to this community\n\n0 0\n\nExpand Navigation Collapse Navigation"
      ]
    },
    {
      "url": "https://ternary.app/blog/snowflake-cost-optimization/",
      "title": "Snowflake cost optimization: 8 proven strategies for reducing costs",
      "publish_date": "2025-09-22",
      "excerpts": [
        "Section Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Total annual cost\nContent:\n| **Storage** | $1,104 |\n| **Virtual warehouse** | $21,774 |\n| **Grand Total** | $22,878 per year |\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\nWhile automatic clustering can improve query performance, it runs on serverless compute. This means it racks up Snowflake credits whether anyone\u2019s actually using the table or not.\nIf the table is only getting hit a few times a week, that background compute activity is just silently chipping away at your budget. This is where smart Snowflake cost optimization begins.\nLook for tables with automatic clustering enabled that barely get queried, say, fewer than 100 times per week. Ask yourself if these tables are part of a disaster recovery setup or being shared with another account. If not, it\u2019s probably safe to hit pause.\nTo suspend automatic clustering, run:\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\n| **ALTER** **TABLE** your_table_name **SUSPEND** RECLUSTER; |\n ... \nSection Title: ... > 3. Remove unused search optimization paths\nContent:\nSearch optimization can speed up point lookups and analytical queries, but just like everything else in Snowflake, that speed boost doesn\u2019t come free.\nThese access paths require extra storage and compute resources to stay in sync with your data.\nIf Snowflake tells you that a particular search optimization path is being used fewer than ten times a week, it might be time to rethink things. Especially if you\u2019re trying to reduce Snowflake costs without compromising your actual workloads.\nYou can remove search optimization with a simple command:\n ... \nSection Title: ... > 4. Clean out large tables that haven\u2019t been touched in a week\nContent:\nThe massive tables that sit there eating up storage and haven\u2019t been queried at all in the past week not only inflate your Snowflake storage costs, but they also clutter up your environment and slow down everything from data discovery to data warehouse optimization and management.\nIf a table isn\u2019t serving any purpose (besides reminding you of a project from six months ago), drop it. But again, always check if it\u2019s being used for recovery or data sharing before swinging the axe.\nTo delete a table, run:\n ... \nSection Title: ... > 5. Use transient or temporary tables for short-lived data\nContent:\nHave you ever created a permanent table just to delete it 12 hours later?\nWhen you\u2019re dealing with short-lived data, using a permanent table doesn\u2019t make sense.\nSnowflake charges extra for things like Time Travel and Fail-safe on permanent tables even if they don\u2019t stick around long enough to need it.\nInstead, go with a transient or temporary table.\nThese are lighter-weight options that skip the fancy durability features and save you money in the process.\nIt\u2019s one of the simplest cost optimization techniques to implement, and it can have a real impact, especially in workflows where data turnover is high.\nTo create a transient table, use:\n ... \nSection Title: ... > Snowflake performance index (SPI)\nContent:\n[Snowflake Performance Index (SPI)](https://www.snowflake.com/en/pricing-options/performance-index/) is a macro-level view of how much performance has improved over time across typical customer workloads.\nIt tracks millions of jobs every month to give a reliable baseline for measuring how well Snowflake is optimizing things under the hood.\nThis tracking gradually surfaces improvements like query improvements, data ingestion speed, replication efficiency, and more.\nThe best part is that many of these performance gains happen automatically, so you benefit without needing to change your code or reconfigure anything.\n ... \nSection Title: ... > Does Snowflake charge for storing old data?\nContent:\nYes, Snowflake charges for storage based on the average compressed data stored per month. Keeping outdated or unused data increases your storage costs."
      ]
    },
    {
      "url": "https://www.snowflake.com/en/blog/enterprise-bcdr-data-platform-strategy/",
      "title": "Is Your Business Continuity and Disaster Recovery Strategy Enterprise-Ready?",
      "publish_date": "2026-01-24",
      "excerpts": [
        "Section Title: Is Your Business Continuity and Disaster Recovery Strategy Enterprise-Ready?\nContent:\nUnplanned downtime costs an average of $14,056 per minute, rising to $23,750 for large enterprises, according to [2024 research by Enterprise Management Associates (EMA)](https://www.bigpanda.io/blog/it-outage-costs-2024/) . Your business may have specific recovery objectives, such as recovery point objectives (RPOs) and recovery time objectives (RTOs). You need to ask whether the solution you're evaluating can deliver on them. Platforms built for enterprise needs should be able to walk you through setting up disaster recovery procedures. Ask for specifics: \"How can I easily re-ingest missing data in my DR account and achieve a 15-minute RTO?\" If they can't show you how this works, you're looking at a platform that may leave your team flying blind. ### 2. \"Can you demonstrate how cross-region and cross-cloud failover work?\"\nSection Title: Is Your Business Continuity and Disaster Recovery Strategy Enterprise-Ready?\nContent:\nTrue enterprise-grade BCDR means seamless failover across regions and even between cloud providers, not just local redundancy. BCDR should work whether you're dealing with a data center outage, a regional service disruption or a migration across regions or cloud providers. [Third-party analysis](https://www.linkedin.com/pulse/disaster-recovery-comparisons-snowflake-databricks-james-dinkel-hyjxc/?trackingId=Z%2FKcsDO%2BTtq2SIzwxFmBQA%3D%3D) also shows that Snowflake can handle cross-region and cross-cloud failover automatically, maintaining your data, processing capabilities and governance controls across multiple regions and clouds, while other platforms may require extensive manual intervention and custom engineering to achieve cross-region and cross-cloud failover. Be sure to ask how long it takes to get up and running.\nSection Title: Is Your Business Continuity and Disaster Recovery Strategy Enterprise-Ready?\nContent:\n[Databricks, for instance, has acknowledged publicly](https://www.youtube.com/watch?v=RxLNNNN97g4&t=67s) that its BCDR \u201ccan take months, or even a year, to get up and running the first time.\u201d Snowflake's BCDR can be set up in minutes, regardless of whether you need cross-region or cross-cloud protection.\n ... \nSection Title: ... > 3. \"How do you ensure governance policy enforcement during a disruption?\" Your data is only...\nContent:\nThis includes: * **Data and metastore:** You must handle storage replication and use dual provisioning scripts to manage databases, schemas, tables, views and other data objects. **Security and governance:** You need separate scripts to maintain redundant security models (for example, users, roles and network rules) and governance policies (for example, row-level and column-level security as well as tags). **Compute and AI services:** Compute resources, containers and even registered model artifacts for AI services must be manually provisioned in the secondary region. **Integrations and pipelines:** External integrations with identity providers, key vaults and other APIs, as well as data pipelines and code repositories, must be made redundant to ensure the system functions correctly after a failover."
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    },
    {
      "name": "sku_extract_excerpts",
      "count": 5
    }
  ]
}

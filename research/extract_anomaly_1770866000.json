{"extract_id":"extract_c923440f9ec14b069c562d1905a67ceb","results":[{"url":"https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection","title":"Anomaly Detection (Snowflake ML Functions) | Snowflake Documentation","publish_date":"2020-01-01","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nGuides Snowflake AI & ML ML Functions Anomaly Detection\n\n# Anomaly Detection (Snowflake ML Functions) ¶\n\n## Overview ¶\n\nAnomaly detection is the process of identifying outliers in data. The anomaly detection function lets you train a model\nto detect outliers in your time-series data. Outliers, which are data points that deviate from the expected range, can\nhave an outsized impact on statistics and models derived from your data. Spotting and removing outliers can therefore\nhelp improve the quality of your results.\n\nNote\n\nAnomaly Detection is part of Snowflake’s suite of business analysis tools powered by machine learning.\n\nDetecting outliers can also be useful in pinpointing the origin of problems or deviations in processes when there is no\nobvious cause. For example:\n\n* Determining when a problem started to occur with your logging pipeline.\n* Identifying the days when your Snowflake compute costs are higher than expected.\n\nAnomaly detection works with either single-series or multi-series data. Multi-series data represents multiple\nindependent threads of events. For example, if you have sales data for multiple stores, each store’s sales can be\nchecked separately by a single model based on the store identifier.\n\nThe data must include:\n\n* A timestamp column.\n* A target column representing some quantity of interest at each timestamp.\n\nNote\n\nIdeally, the training data for an Anomaly Detection model has time steps at equally spaced intervals (for example,\ndaily). However, model training can handle real-world data that has missing, duplicate, or misaligned time steps.\nFor more information, see Dealing with real-world data in Time-Series Forecasting .\n\nTo detect outliers in time-series data, use the Snowflake built-in class ANOMALY\\_DETECTION (SNOWFLAKE.ML) ,\nand follow these steps:\n\n1. Create an anomaly detection object ,\n   passing in a reference to the training data.\n   \n   This object fits a model to the training data that you provide. The model is a schema-level object.\n2. Using this anomaly detection model object, call the <model\\_name>!DETECT\\_ANOMALIES method to\n   detect anomalies, passing in a reference to the data to analyze.\n   \n   The method uses the model to identify outliers in the data.\n\nAnomaly detection is closely related to Forecasting . An anomaly detection model\nproduces a forecast for the same time period as the data you’re checking for anomalies, then compares the actual data to\nthe forecast to identify outliers.\n\nImportant\n\n**Legal notice.** This Snowflake ML function is powered by machine learning technology, which you, not Snowflake, determine when and how to use. Machine\nlearning technology and results provided may be inaccurate, inappropriate, or biased.\nSnowflake provides you with the machine learning models that you can use within your own workflows. Decisions based on machine\nlearning outputs, including those built into automatic pipelines, should have human oversight and review processes\nto ensure model-generated content is accurate.\nSnowflake provides algorithms (without any pretraining) and you’re responsible for the data that you provide the algorithm (for example, for training and inference) and the decisions you make using the resulting model’s output.\nQueries for this feature or function are treated as any\nother SQL query and may be considered metadata .\n\n**Metadata.** When you use Snowflake ML functions, Snowflake logs generic error messages returned by an ML\nfunction. These error logs help us troubleshoot issues that arise and improve these functions to serve you better.\n\nFor further information, see [Snowflake AI Trust and Safety FAQ](https://www.snowflake.com/en/legal/snowflake-ai-trust-and-safety/) .\n\n## About the Algorithm for Anomaly Detection ¶\n\nThe anomaly detection algorithm is powered by a [gradient boosting machine](https://en.wikipedia.org/wiki/Gradient_boosting) (GBM). Like an [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) model, it uses a\ndifferencing transformation to model data with a non-stationary trend and uses auto-regressive lags of the historical\ntarget data as model variables.\n\nAdditionally, the algorithm uses rolling averages of historical target data to help predict trends, and automatically\nproduces cyclic calendar variables (such as day of week and week of year) from timestamp data.\n\nYou can fit models with only historical target and timestamp data, or you may include exogenous data (variables) that\nmight have influenced the target value. Exogenous variables can be numerical or categorical and may be NULL (rows\ncontaining NULLs for exogenous variables are not dropped).\n\nThe algorithm does not rely on one-hot encoding when training on categorical variables, so you can use categorical data\nwith many dimensions (high cardinality).\n\nIf your model incorporates exogenous variables, you must provide values for those variables at timestamps in the future\nwhen detecting anomalies. Appropriate exogenous variables could include weather data (temperature, rainfall),\ncompany-specific information (historic and planned company holidays, advertisement campaigns, event schedules), or any\nother external factors you believe may help predict your target variable.\n\nOptionally, individual historical rows can be labeled as anomalous or non-anomalous by using a separate Boolean column.\n\nA _prediction interval_ is an estimated range of values within an upper bound and a lower bound in which a certain\npercentage of data is likely to fall. For example, a 0.99 value means that 99% of the data likely appears within the\ninterval. The anomaly detection model identifies any data that falls outside of the prediction interval as an anomaly. You can\nspecify a prediction interval or use the default, which is 0.99. You may want to set this value to be very close\nto 1.0; 0.9999 or even closer.\n\nImportant\n\nFrom time to time, Snowflake may refine the anomaly detection algorithm. Such improvements roll out\nthrough the regular Snowflake release process. You cannot revert to a previous version of the feature, but models you\ncreated with a previous version continue to use that version for anomaly detection.\n\n### Limitations ¶\n\n* You cannot choose or adjust the anomaly detection algorithm. In particular, the algorithm does not provide parameters\n  to override trend, seasonality, or seasonal amplitudes; these are inferred from the data.\n* The minimum number of rows for the main anomaly detection algorithm is 12 per time series. For time series with\n  between 2 and 11 observations, anomaly detection produces a “naive” result in which all predicted values are equal to\n  the last observed target value. For the labeled anomaly detection case, the number of observations used is the number\n  of rows where the label column is false.\n* The minimum acceptable granularity of data is one second. (Timestamps must not be less than one second apart.)\n* The minimum granularity of seasonal components is one minute. (The function cannot detect cyclic patterns at smaller\n  time deltas.)\n* The “season length” of autoregressive features is tied to the input frequency (24 for hourly data, 7 for daily data,\n  and so on).\n* Anomaly detection models, once trained, are immutable. You cannot update existing models with new data; you must train\n  an entirely new model. Models do not support versioning. Generally, you should retrain models on a regular cadence,\n  such as once a day, once a week, or once a month, depending on how frequently you receive new data, to help the model\n  keep up with changing trends.\n* This feature only detects anomalies in the test data; it cannot detect anomalies in the training data. Furthermore,\n  timestamps in the test data must all be greater than timestamps in the training data. Ensure that the training data\n  covers a typical period free of actual outliers, or label known outliers in a Boolean column.\n* You cannot clone models or share models across roles or accounts. When cloning a schema or database, model objects are skipped.\n* You cannot replicate an instance of the ANOMALY\\_DETECTION\n  class.\n\n## Preparing for Anomaly Detection ¶\n\nBefore you can use anomaly detection, you must:\n\n* Select a virtual warehouse in which to train and run your models.\n* Grant the privileges to create anomaly detection objects .\n\nYou might also want to modify your search path to include\nSNOWFLAKE.ML.\n\n### Selecting a Virtual Warehouse ¶\n\nA Snowflake virtual warehouse provides the compute resources for training and using your\nmachine learning models for this feature. This section provides general guidance on selecting the best size and type of\nwarehouse for this purpose, focusing on the training step (the most time-consuming and memory-intensive part of\nthe process).\n\n#### Training on Single-Series Data ¶\n\nFor models trained on single-series data, you should choose the warehouse type based on the size of your training data.\nStandard warehouses are subject to a lower Snowpark memory limit ,\nand are more appropriate for training jobs with fewer rows or exogenous features.\nIf your training data does not contain any exogenous features, you can train on a standard warehouse if the dataset has 5 million rows or less.\nIf your training data uses 5 or more exogenous features, then the maximum row count is lower.\nOtherwise, Snowflake suggests using a Snowpark-optimized warehouse for larger training jobs.\n\nIn general, for single-series data, a larger warehouse size does not result in a faster training time or higher memory limits.\nAs a rough rule of thumb, training time is proportional to the number of rows in your time series. For example, on a XS\nstandard warehouse, with evaluation turned off ( `CONFIG_OBJECT => {'evaluate': False}` ), training on a\n100,000-row dataset takes about 60 seconds, while training on a 1,000,000-row dataset takes about 125 seconds. With\nevaluation turned on, training time increases roughly linearly by the number of splits used.\n\nFor best performance, Snowflake recommends using a dedicated warehouse without other concurrent workloads to train your model.\n\n#### Training on Multi-Series Data ¶\n\nAs with single-series data, choose the warehouse type based on the number of rows in your largest time series. If your\nlargest time series contains more than 5 million rows, the training job is likely to exceed memory limits on a standard\nwarehouse.\n\nUnlike single-series data, multi-series data trains considerably faster on larger warehouse sizes.\nThe following data points can guide you in your selection. Once again, all these times are done with evaluation turned\noff.\n\n|Warehouse type and size |Number of time series |Number of rows per time series |Training time (seconds) |\n| --- | --- | --- | --- |\n|Standard XS |1 |100,000 |60 seconds |\n|Standard XS |10 |100,000 |204 seconds |\n|Standard XS |100 |100,000 |720 seconds |\n|Standard XL |10 |100,000 |104 seconds |\n|Standard XL |100 |100,000 |211 seconds |\n|Standard XL |1000 |100,000 |840 seconds |\n|Snowpark-optimized XL |10 |100,000 |65 seconds |\n|Snowpark-optimized XL |100 |100,000 |293 seconds |\n|Snowpark-optimized XL |1000 |100,000 |831 seconds |\n\n#### Detecting Anomalies ¶\n\nThe inference step takes approximately 1 second to process 100 rows in the input dataset, regardless of warehouse size.\n\n### Granting Privileges to Create Anomaly Detection Objects ¶\n\nTraining an anomaly detection model results in a schema-level object. Therefore, the role you use to create models must\nhave the CREATE SNOWFLAKE.ML.ANOMALY\\_DETECTION privilege on the schema where the model is created, which allows the\nmodel to be stored there. This privilege is similar to other schema privileges like CREATE TABLE or CREATE VIEW.\n\nSnowflake recommends that you create a role named `analyst` to be used by people who need to detect anomalies.\n\nIn the following example, the `admin` role is the owner of the schema `admin_db.admin_schema` . The `analyst` role needs to create models in this schema.\n\n```\nUSE ROLE admin ; \n GRANT USAGE ON DATABASE admin_db TO ROLE analyst ; \n GRANT USAGE ON SCHEMA admin_schema TO ROLE analyst ; \n GRANT CREATE SNOWFLAKE.ML.ANOMALY_DETECTION ON SCHEMA admin_db . admin_schema TO ROLE analyst ;\n```\n\nCopy\n\nTo use this schema, a user assumes the role `analyst` :\n\n```\nUSE ROLE analyst ; \n USE SCHEMA admin_db . admin_schema ;\n```\n\nCopy\n\nIf the `analyst` role has CREATE SCHEMA privileges in database `analyst_db` , the role can create a new schema `analyst_db.analyst_schema` and create anomaly detection models in that schema:\n\n```\nUSE ROLE analyst ; \n CREATE SCHEMA analyst_db . analyst_schema ; \n USE SCHEMA analyst_db . analyst_schema ;\n```\n\nCopy\n\nTo revoke a role’s model creation privilege on the schema, use REVOKE <privileges> … FROM ROLE :\n\n```\nREVOKE CREATE SNOWFLAKE.ML.ANOMALY_DETECTION ON SCHEMA admin_db . admin_schema FROM ROLE analyst ;\n```\n\nCopy\n\n## Setting Up the Data for the Examples ¶\n\nThe examples in the following sections use a sample dataset that contains daily sales for items in different stores along with\ndaily weather data (humidity and temperature). The dataset also contains a column that indicates whether the day is a holiday.\n\n1. Execute the following statements to create a table named `historical_sales_data` that contains the training data for the model:\n\n> ```\n> CREATE OR REPLACE TABLE historical_sales_data ( \n>   store_id NUMBER , item VARCHAR , date TIMESTAMP_NTZ , sales FLOAT , label BOOLEAN , \n>   temperature NUMBER , humidity FLOAT , holiday VARCHAR ); \n> \n>  INSERT INTO historical_sales_data VALUES \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-01' ), 2 . 0 , false , 50 , 0 . 3 , 'new year' ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-02' ), 3 . 0 , false , 52 , 0 . 3 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-03' ), 5 . 0 , false , 54 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-04' ), 30 . 0 , true , 54 , 0 . 3 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-05' ), 8 . 0 , false , 55 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-06' ), 6 . 0 , false , 55 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-07' ), 4 . 6 , false , 55 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-08' ), 2 . 7 , false , 55 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-09' ), 8 . 6 , false , 55 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-10' ), 9 . 2 , false , 55 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-11' ), 4 . 6 , false , 55 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-12' ), 7 . 0 , false , 55 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-13' ), 3 . 6 , false , 55 , 0 . 2 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-14' ), 8 . 0 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-01' ), 3 . 4 , false , 50 , 0 . 3 , 'new year' ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-02' ), 5 . 0 , false , 52 , 0 . 3 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-03' ), 4 . 0 , false , 54 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-04' ), 5 . 4 , false , 54 , 0 . 3 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-05' ), 3 . 7 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-06' ), 3 . 2 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-07' ), 3 . 2 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-08' ), 5 . 6 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-09' ), 7 . 3 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-10' ), 8 . 2 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-11' ), 3 . 7 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-12' ), 5 . 7 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-13' ), 6 . 3 , false , 55 , 0 . 2 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-14' ), 2 . 9 , false , 55 , 0 . 2 , null );\n> ```\n> \n> Copy\n> \n> \n\n1. Execute the following statements to create a table named `new_sales_data` that contains the data to analyze:\n\n> ```\n> CREATE OR REPLACE TABLE new_sales_data ( \n>   store_id NUMBER , item VARCHAR , date TIMESTAMP_NTZ , sales FLOAT , \n>   temperature NUMBER , humidity FLOAT , holiday VARCHAR ); \n> \n>  INSERT INTO new_sales_data VALUES \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-16' ), 6 . 0 , 52 , 0 . 3 , null ), \n>   ( 1 , 'jacket' , to_timestamp_ntz ( '2020-01-17' ), 20 . 0 , 53 , 0 . 3 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-16' ), 3 . 0 , 52 , 0 . 3 , null ), \n>   ( 2 , 'umbrella' , to_timestamp_ntz ( '2020-01-17' ), 70 . 0 , 53 , 0 . 3 , null );\n> ```\n> \n> Copy\n> \n>\n\n## Training, Using, Viewing, Deleting, and Updating Models ¶\n\nUse CREATE SNOWFLAKE.ML.ANOMALY\\_DETECTION to create and train a model. The model is trained on the dataset you\nprovide.\n\n```\nCREATE SNOWFLAKE.ML.ANOMALY_DETECTION mydetector (...);\n```\n\nCopy\n\nSee ANOMALY\\_DETECTION (SNOWFLAKE.ML) for complete details about the SNOWFLAKE.ML.ANOMALY\\_DETECTION\nconstructor. For examples of creating a model, see Detecting Anomalies .\n\nNote\n\nSNOWFLAKE.ML.ANOMALY\\_DETECTION runs using limited privileges, so by default it does not have access to your data. You must\ntherefore pass tables and views as references , which pass along the\ncaller’s privileges. You can also provide a query reference instead of a\nreference to a table or a view.\n\nTo create this reference, you can use the TABLE keyword with the table name, view name,\nor query, or you can call the SYSTEM$REFERENCE or SYSTEM$QUERY\\_REFERENCE function.\n\nTo detect anomalies, call the model’s <model\\_name>!DETECT\\_ANOMALIES method:\n\n```\nCALL mydetector ! DETECT_ANOMALIES (...);\n```\n\nCopy\n\nTo select columns from the tabular output of the method, you can call the method in the FROM clause :\n\n```\nSELECT ts , forecast FROM TABLE ( mydetector ! DETECT_ANOMALIES (...));\n```\n\nCopy\n\nTo view a list of your models, use the SHOW SNOWFLAKE.ML.ANOMALY\\_DETECTION command:\n\n```\nSHOW SNOWFLAKE.ML.ANOMALY_DETECTION ;\n```\n\nCopy\n\nTo remove a model, use the DROP SNOWFLAKE.ML.ANOMALY\\_DETECTION command:\n\n```\nDROP SNOWFLAKE.ML.ANOMALY_DETECTION < name >;\n```\n\nCopy\n\nTo update a model, delete it and train a new one. Models are immutable and cannot be updated in place.\n\n## Detecting Anomalies ¶\n\nThe following sections demonstrate how to use anomaly detection to detect outliers. These sections provide examples of\ndetecting anomalies for a single time series, for multiple time series, with and without exogenous variables, with a\nuser-defined prediction interval, and with a supervised (labeled) approach.\n\n* Detecting Anomalies for a Single Time Series (Unsupervised)\n* Training an Anomaly Detection Model with Labeled Data\n* Specifying the Prediction Interval For Anomaly Detection\n* Including Additional Columns for Analysis\n* Detecting Anomalies in Multiple Series\n\n### Detecting Anomalies for a Single Time Series (Unsupervised) ¶\n\nTo detect anomalies in your data:\n\n1. Train an anomaly detection model using historical data.\n2. Use the trained anomaly detection model to detect anomalies in historical or projected data. The timestamps in the test data\n   must chronologically follow the timestamps in the training data. You need at least 2 data points to train a model, at least\n   12 for non-naive results, and at least 60 for non-linear results.\n\nSee ANOMALY\\_DETECTION (SNOWFLAKE.ML) for information on the parameters used in creating and using a model.\n\n#### Training an Anomaly Detection Model ¶\n\nTo create an anomaly detection model object, execute the CREATE SNOWFLAKE.ML.ANOMALY\\_DETECTION command.\n\nFor example, suppose that you want to analyze the sales for jackets in the store with the `store_id` of 1:\n\n1. Create a view or design a query that returns the data for training the model for anomaly detection.\n   \n   For this example, execute the CREATE VIEW command to create a view named `view_with_training_data` that contains the date and sales information:\n   \n   ```\n   CREATE OR REPLACE VIEW view_with_training_data \n     AS SELECT date , sales FROM historical_sales_data \n       WHERE store_id = 1 AND item = 'jacket' ;\n   ```\n   \n   Copy\n2. Create an anomaly detection object, and train its model on the data in that view.\n   \n   For this example, execute the CREATE SNOWFLAKE.ML.ANOMALY\\_DETECTION command to create an anomaly detection object named `basic_model` . Pass in the following arguments:\n   \n   ```\n   CREATE OR REPLACE SNOWFLAKE.ML.ANOMALY_DETECTION basic_model ( \n     INPUT_DATA => TABLE ( view_with_training_data ), \n     TIMESTAMP_COLNAME => 'date' , \n     TARGET_COLNAME => 'sales' , \n     LABEL_COLNAME => '' );\n   ```\n   \n   Copy\n   \n   This example passes in a reference to a view as the INPUT\\_DATA argument. The example uses the TABLE keyword to create the reference . As an alternative, you can call SYSTEM$REFERENCE to create the reference.\n   \n   The purpose of the label column is to tell the model which rows are known anomalies. Because this example uses\n   unsupervised training, you do not need to use the label column. Pass an empty string as the name of the label column.\n   \n   Tip\n   \n   If you don’t want to create a view for the INPUT\\_DATA argument, you can pass in a reference to a query that uses a SELECT statement that serves as an inline\n   view.\n   \n   You can use the TABLE keyword to create this query reference. For example:\n   \n   ```\n   CREATE OR REPLACE SNOWFLAKE.ML.ANOMALY_DETECTION basic_model ( \n     INPUT_DATA => \n       TABLE ( SELECT date , sales FROM historical_sales_data WHERE store_id = 1 AND item = 'jacket' ), \n     TIMESTAMP_COLNAME => 'date' , \n     TARGET_COLNAME => 'sales' , \n     LABEL_COLNAME => '' );\n   ```\n   \n   Copy\n   \n   Escape any single quotes and other special characters with a backslash.\n   \n   As an alternative to using the TABLE keyword, you can call SYSTEM$QUERY\\_REFERENCE to create\n   the query reference.\n\n> If the command is executed successfully, a message indicates that your anomaly detection instance was created\n> successfully:\n> \n> ```\n> +--------------------------------------------+ \n>  |                 status                     | \n>  +--------------------------------------------+ \n>  | Instance basic_model successfully created. | \n>  +--------------------------------------------+\n> ```\n> \n>\n\n#### Using an Anomaly Detection Model to Detect Anomalies ¶\n\nCreating the anomaly detection object trains the model and stores it in the schema. To use the anomaly detection object\nto detect anomalies, call the <model\\_name>!DETECT\\_ANOMALIES method of the object. For example:\n\n1. Create a view or design a query that returns the data for analysis.\n   \n   For this example, execute the CREATE VIEW command to create a view named `view_with_data_to_analyze` that contains the date and sales information:\n   \n   ```\n   CREATE OR REPLACE VIEW view_with_data_to_analyze \n     AS SELECT date , sales FROM new_sales_data \n       WHERE store_id = 1 and item = 'jacket' ;\n   ```\n   \n   Copy\n2. Using the object for the anomaly detection model (in this example, `basic_model` , which you created earlier ),\n   call the <model\\_name>!DETECT\\_ANOMALIES method:\n   \n   ```\n   CALL basic_model ! DETECT_ANOMALIES ( \n     INPUT_DATA => TABLE ( view_with_data_to_analyze ), \n     TIMESTAMP_COLNAME => 'date' , \n     TARGET_COLNAME => 'sales' \n    );\n   ```\n   \n   Copy\n   \n   The method returns a table that includes rows for the data currently in the view `view_with_data_to_analyze` along with the\n   prediction of the detector. For a description of the columns in this table, see Returns .\n\n**Output**\n\nThe results have been rounded for readability.\n\n```\n+--------+-------------------------+----+----------+--------------+--------------+------------+--------------+--------------+ \n | SERIES | TS                      |  Y | FORECAST |  LOWER_BOUND |  UPPER_BOUND | IS_ANOMALY |   PERCENTILE |     DISTANCE | \n +--------|-------------------------+----+----------+--------------+--------------+------------+--------------+--------------| \n | NULL   | 2020-01-16 00:00:00.000 |  6 |      4.6 | -7.185885251 | 16.385885251 | False      | 0.6201873452 | 0.3059728606 | \n | NULL   | 2020-01-17 00:00:00.000 | 20 |      9   | -2.785885251 | 20.785885251 | False      | 0.9918932208 | 2.404072476  | \n +--------+-------------------------+----+----------+--------------+--------------+------------+--------------+--------------|\n```\n\nTo save your results directly to a table, use CREATE TABLE … AS SELECT … and call the DETECT\\_ANOMALIES method in the FROM clause :\n\n```\nCREATE TABLE my_anomalies AS \n  SELECT * FROM TABLE ( basic_model ! DETECT_ANOMALIES ( \n    INPUT_DATA => TABLE ( view_with_data_to_analyze ), \n    TIMESTAMP_COLNAME => 'date' , \n    TARGET_COLNAME => 'sales' \n  ));\n```\n\nCopy\n\nAs shown in the example above, when calling the method, omit the CALL command. Instead, put the call\nin parentheses, preceded by the TABLE keyword.\n\n### Training an Anomaly Detection Model with Labeled Data ¶\n\nIn the previous example, the result of the model appears to be inaccurate. This is probably because:\n\n* The anomaly detection model was trained on very little input data.\n* A larger number of jackets (30) were sold on 2020-01-03. This skewed the predictions upward and increased the size of\n  the prediction interval.\n\nTo improve the accuracy of the anomaly detection model, you can either include more training data or label the training data\n(supervised training). Labeled training data has an additional Boolean column that indicates whether each row is a known\nanomaly. Labeling can help the anomaly detection model to avoid overfitting to known anomalies in the training data.\n\nTo include labeled data in the training data, specify the column containing the label in the LABEL\\_COLNAME constructor argument\nof the CREATE SNOWFLAKE.ML.ANOMALY\\_DETECTION command. For example:\n\n1. Create a view or design a query that returns the labels with the training data.\n   \n   For this example, execute the CREATE VIEW command to create a view named `view_with_labeled_data` that contains the labels in a column named `label` :\n   \n   ```\n   CREATE OR REPLACE VIEW view_with_labeled_data_for_training \n     AS SELECT date , sales , label FROM historical_sales_data \n       WHERE store_id = 1 and item = 'jacket' ;\n   ```\n   \n   Copy\n2. Create an object for the anomaly detection model, and train the model on the data in that view.\n   \n   For this example, execute the CREATE SNOWFLAKE.ML.ANOMALY\\_DETECTION command to create an anomaly detection object named `model_trained_with_labeled_data` . The following statement creates the anomaly detection object:\n   \n   ```\n   CREATE OR REPLACE SNOWFLAKE.ML.ANOMALY_DETECTION model_trained_with_labeled_data ( \n     INPUT_DATA => TABLE ( view_with_labeled_data_for_training ), \n     TIMESTAMP_COLNAME => 'date' , \n     TARGET_COLNAME => 'sales' , \n     LABEL_COLNAME => 'label' \n    );\n   ```\n   \n   Copy\n3. Using this new anomaly detection model, call the <model\\_name>!DETECT\\_ANOMALIES method,\n   passing in the same arguments that you used in Detecting Anomalies for a Single Time Series (Unsupervised) :\n   \n   ```\n   CALL model_trained_with_labeled_data ! DETECT_ANOMALIES ( \n     INPUT_DATA => TABLE ( view_with_data_to_analyze ), \n     TIMESTAMP_COLNAME => 'date' , \n     TARGET_COLNAME => 'sales' \n    );\n   ```\n   \n   Copy\n   \n   The method returns a table that includes rows for the data currently in the view `view_with_data_to_analyze` along with the\n   prediction of the detector. For a description of the columns in this table, see Returns .\n\n**Output**\n\nThe results have been rounded for readability.\n\n> ```\n> +--------+-------------------------+----+----------+---------------+--------------+------------+--------------+------------+ \n>  | SERIES | TS                      |  Y | FORECAST |   LOWER_BOUND |  UPPER_BOUND | IS_ANOMALY |   PERCENTILE |   DISTANCE | \n>  +--------|-------------------------+----+----------+---------------+--------------+------------+--------------+------------| \n>  | NULL   | 2020-01-16 00:00:00.000 |  6 |        6 |  0.82         | 11.18        | False      | 0.5          | 0          | \n>  | NULL   | 2020-01-17 00:00:00.000 | 20 |        6 | -0.39         | 12.33        | True       | 0.99         | 5.70       | \n>  +--------+-------------------------+----+----------+---------------+--------------+------------+--------------+------------+\n> ```\n> \n>\n\n### Specifying the Prediction Interval For Anomaly Detection ¶\n\nYou can detect anomalies with varying levels of sensitivity. To specify the percentage of observations to classify as\nanomalies, create an OBJECT that contains configuration settings for <model\\_name>!DETECT\\_ANOMALIES , and set the `prediction_interval` key to the percentage of the observations that should be marked as anomalies.\n\nTo construct this object, you can use either an object constant or the OBJECT\\_CONSTRUCT function.\n\nThen, when calling the <model\\_name>!DETECT\\_ANOMALIES method, pass in this object as the CONFIG\\_OBJECT argument.\n\nBy default, the value associated with the prediction\\_interval key is set to 0.99, which means that roughly 1% of the data is\nmarked as anomalies. You can specify a value between 0 and 1:\n\n* To mark fewer observations as anomalies, specify a higher value for `prediction_interval` .\n* To mark more observations as anomalies, reduce the `prediction_interval` value.\n\nThe following example configures anomaly detection to be more strict by setting the `prediction_interval` to 0.995. The example also\nuses the model trained on labeled data (that you set up in Training an Anomaly Detection Model with Labeled Data ) with the view\nthat contains the data to analyze (that you set up in Detecting Anomalies for a Single Time Series (Unsupervised) ).\n\n```\nCALL model_trained_with_labeled_data ! DETECT_ANOMALIES ( \n  INPUT_DATA => TABLE ( view_with_data_to_analyze ), \n  TIMESTAMP_COLNAME => 'date' , \n  TARGET_COLNAME => 'sales' , \n  CONFIG_OBJECT => { 'prediction_interval' :0 . 995 } \n );\n```\n\nCopy\n\nThis statement produces a table that includes rows for the data currently in the view `view_with_data_to_analyze` . Each row\nincludes a column with the prediction of the detector. You can see that the result\nof this model is more accurate than the unlabeled example.\n\n**Output**\n\nThe results have been rounded for readability.\n\n```\n+--------+-------------------------+----+----------+---------------+--------------+------------+--------------+------------+ \n | SERIES | TS                      |  Y | FORECAST |   LOWER_BOUND |  UPPER_BOUND | IS_ANOMALY |   PERCENTILE |   DISTANCE | \n +--------|-------------------------+----+----------+---------------+--------------+------------+--------------+------------| \n | NULL   | 2020-01-16 00:00:00.000 |  6 |        6 |  0.36         | 11.64        | False      | 0.5          | 0          | \n | NULL   | 2020-01-17 00:00:00.000 | 20 |        6 | -0.90         | 12.90        | True       | 0.99         | 5.70       | \n +--------+-------------------------+----+----------+---------------+--------------+------------+--------------+------------+\n```\n\n### Including Additional Columns for Analysis ¶\n\nYou can include additional columns in the data (for example, `temperature` , `weather` , `is_black_friday` ) in the data for training\nand analysis, if these columns can help you improve the identification of true anomalies.\n\nTo include new columns for analysis:\n\n1. For the training data, create a view or design a query that includes the new columns, and create a new anomaly detection object,\n   passing in a reference to that view or query.\n2. For the data to analyze, create a view or design a query that includes the new columns, and pass a reference to that\n   view or query to the <model\\_name>!DETECT\\_ANOMALIES method.\n\nThe anomaly detection model detects and uses the additional columns automatically.\n\nNote\n\nYou must provide a view or query with the same set of additional columns when executing the CREATE SNOWFLAKE.ML.ANOMALY\\_DETECTION command and when calling the <model\\_name>!DETECT\\_ANOMALIES method. If there is a mismatch between the columns in the training data\npassed to the command and the columns in the data for analysis passed to the function, an error occurs.\n\nFor example, suppose that you want to add the columns `temperature` , `humidity` , and `holiday` :\n\n1. Create a view or design a query that returns the training data with these additional columns.\n   \n   For this example, execute the CREATE VIEW command to create a view named `view_with_training_data_extra_columns` :\n   \n   ```\n   CREATE OR REPLACE VIEW view_with_training_data_extra_columns \n     AS SELECT date , sales , label , temperature , humidity , holiday \n       FROM historical_sales_data \n       WHERE store_id = 1 AND item = 'jacket' ;\n   ```\n   \n   Copy\n2. Create an object for the anomaly detection model, and train the model on the data in that view.\n   \n   For this example, execute the CREATE SNOWFLAKE.ML.ANOMALY\\_DETECTION command to create an anomaly detection object named `model_with_additional_columns` , passing in a reference to the new view:\n   \n   ```\n   CREATE OR REPLACE SNOWFLAKE.ML.ANOMALY_DETECTION model_with_additional_columns ( \n     INPUT_DATA => TABLE ( view_with_training_data_extra_columns ), \n     TIMESTAMP_COLNAME => 'date' , \n     TARGET_COLNAME => 'sales' , \n     LABEL_COLNAME => 'label' \n    );\n   ```\n   \n   Copy\n3. Create a view or design a query that returns the data to analyze with these additional columns.\n   \n   For this example, execute the CREATE VIEW command to create a view named `view_with_data_for_analysis_extra_columns` :\n   \n   ```\n   CREATE OR REPLACE VIEW view_with_data_for_analysis_extra_columns \n     AS SELECT date , sales , temperature , humidity , holiday \n       FROM new_sales_data \n       WHERE store_id = 1 AND item = 'jacket' ;\n   ```\n   \n   Copy\n4. Using this new anomaly detection object, call the <model\\_name>!DETECT\\_ANOMALIES method, passing in the new view:\n   \n   ```\n   CALL model_with_additional_columns ! DETECT_ANOMALIES ( \n     INPUT_DATA => TABLE ( view_with_data_for_analysis_extra_columns ), \n     TIMESTAMP_COLNAME => 'date' , \n     TARGET_COLNAME => 'sales' , \n     CONFIG_OBJECT => { 'prediction_interval' :0 . 93 } \n    );\n   ```\n   \n   Copy\n   \n   This statement produces a table that includes rows for the data currently in the view `view_with_data_for_analysis_extra_columns` along with the prediction of the detector. The format of the output\n   is the same as the format of the output shown for the commands that you ran earlier.\n\n**Output**\n\nThe results have been rounded for readability.\n\n> ```\n> +--------+-------------------------+----+----------+-------------+--------------+------------+--------------+------------+ \n>  | SERIES | TS                      |  Y | FORECAST | LOWER_BOUND |  UPPER_BOUND | IS_ANOMALY |   PERCENTILE |   DISTANCE | \n>  +--------|-------------------------+----+----------+-------------+--------------+------------+--------------+------------| \n>  | NULL   | 2020-01-16 00:00:00.000 |  6 |        6 | 2.34        |  9.64        | False      | 0.5          | 0          | \n>  | NULL   | 2020-01-17 00:00:00.000 | 20 |        6 | 1.56        | 10.451       | True       | 0.99         | 5.70       | \n>  +--------+-------------------------+----+----------+-------------+--------------+------------+--------------+------------+\n> ```\n> \n>\n\n### Detecting Anomalies in Multiple Series ¶\n\nThe previous sections provided examples of detecting anomalies for a single series. These examples flagged anomalies for\nthe sale of one type of item (jackets) in one store (store ID 1). To detect anomalies for multiple time series at the\nsame time (for example, for multiple combinations of items and stores):\n\n1. For the training data, create a view or design a query that includes a column that identifies the series, and create\n   a new anomaly detection object, passing in a reference to that view or query and specifying the name of the series\n   column for the SERIES\\_COLNAME argument.\n2. For the data to analyze, create a view or design a query that includes the column that identifies the series. Call\n   the <model\\_name>!DETECT\\_ANOMALIES method, passing in a reference to that view or query and\n   specifying the name of the series column for the SERIES\\_COLNAME argument.\n\nFor example, suppose that you want to use the combination of the `store_id` and `item` columns to identify the series:\n\n1. Create a view or design a query that returns the training data with the column for the series.\n   \n   For this example, execute the CREATE VIEW command to create a view named `view_with_training_data_multiple_series` that contains a column named `store_item` that identifies the series as\n   a combination of store ID and item:\n   \n   ```\n   CREATE OR REPLACE VIEW view_with_training_data_multiple_series \n     AS SELECT \n       [ store_id , item ] AS store_item , \n       date , \n       sales , \n       label , \n       temperature , \n       humidity , \n       holiday \n     FROM historical_sales_data ;\n   ```\n   \n   Copy\n2. Create an object for the anomaly detection, and train the model on the data in that view.\n   \n   For this example, execute the CREATE SNOWFLAKE.ML.ANOMALY\\_DETECTION command to create an anomaly detection object named `model_for_multiple_series` , passing in a reference to the new view and specifying `store_item` for the SERIES\\_COLNAME\n   argument:\n   \n   ```\n   CREATE OR REPLACE SNOWFLAKE.ML.ANOMALY_DETECTION model_for_multiple_series ( \n     INPUT_DATA => TABLE ( view_with_training_data_multiple_series ), \n     SERIES_COLNAME => 'store_item' , \n     TIMESTAMP_COLNAME => 'date' , \n     TARGET_COLNAME => 'sales' , \n     LABEL_COLNAME => 'label' \n    );\n   ```\n   \n   Copy\n3. Create a view or design a query that returns the data to analyze with the series column.\n   \n   For this example, execute the CREATE VIEW command to create a view named `view_with_data_for_analysis_multiple_series` that contains a column named `store_item` for the series:\n   \n   ```\n   CREATE OR REPLACE VIEW view_with_data_for_analysis_multiple_series \n     AS SELECT \n       [ store_id , item ] AS store_item , \n       date , \n       sales , \n       temperature , \n       humidity , \n       holiday \n     FROM new_sales_data ;\n   ```\n   \n   Copy\n4. Using this new anomaly detection object, call the <model\\_name>!DETECT\\_ANOMALIES method, passing in the new view and specifying `store_item` for the SERIES\\_COLNAME argument:\n   \n   ```\n   CALL model_for_multiple_series ! DETECT_ANOMALIES ( \n     INPUT_DATA => TABLE ( view_with_data_for_analysis_multiple_series ), \n     SERIES_COLNAME => 'store_item' , \n     TIMESTAMP_COLNAME => 'date' , \n     TARGET_COLNAME => 'sales' , \n     CONFIG_OBJECT => { 'prediction_interval' :0 . 995 } \n    );\n   ```\n   \n   Copy\n   \n   This statement produces a table that includes rows for the data currently in the view `view_with_data_for_analysis_multiple_series` along with the prediction of the detector. The output includes the column that\n   identifies the series.\n\n**Output**\n\nThe results have been rounded for readability.\n\n> ```\n> +--------------+-------------------------+----+----------+---------------+--------------+------------+---------------+--------------+ \n>  | SERIES       | TS                      |  Y | FORECAST |   LOWER_BOUND |  UPPER_BOUND | IS_ANOMALY |    PERCENTILE |     DISTANCE | \n>  |--------------+-------------------------+----+----------+---------------+--------------+------------+---------------+--------------| \n>  | [            | 2020-01-16 00:00:00.000 |  3 |      6.3 |  2.07         | 10.53        | False      | 0.01          | -2.19         | \n>  |   2,         |                         |    |          |               |              |            |               |              | \n>  |   \"umbrella\" |                         |    |          |               |              |            |               |              | \n>  | ]            |                         |    |          |               |              |            |               |              | \n>  | [            | 2020-01-17 00:00:00.000 | 70 |      2.9 | -1.33         |  7.13        | True       | 1             | 44.54         | \n>  |   2,         |                         |    |          |               |              |            |               |              | \n>  |   \"umbrella\" |                         |    |          |               |              |            |               |              | \n>  | ]            |                         |    |          |               |              |            |               |              | \n>  | [            | 2020-01-16 00:00:00.000 |  6 |      6   |  0.36         | 11.64        | False      | 0.5           |  0           | \n>  |   1,         |                         |    |          |               |              |            |               |              | \n>  |   \"jacket\"   |                         |    |          |               |              |            |               |              | \n>  | ]            |                         |    |          |               |              |            |               |              | \n>  | [            | 2020-01-17 00:00:00.000 | 20 |      6   | -0.90         | 12.90        | True       | 0.99          |  5.70         | \n>  |   1,         |                         |    |          |               |              |            |               |              | \n>  |   \"jacket\"   |                         |    |          |               |              |            |               |              | \n>  | ]            |                         |    |          |               |              |            |               |              | \n>  +--------------+-------------------------+----+----------+---------------+--------------+------------+---------------+--------------+\n> ```\n> \n>\n\n## Visualizing Anomalies and Interpreting the Results ¶\n\nUse Snowsight to review and visualize the results of anomaly detection. In Snowsight, when\nyou call the <model\\_name>!DETECT\\_ANOMALIES method, the results are displayed in a table under the worksheet.\n\nTo visualize the results, you can use the chart feature in Snowsight.\n\n1. After calling the <model\\_name>!DETECT\\_ANOMALIES method, select Charts above the results table.\n2. In the Data section on the right side of the chart:\n   \n    1. Select the Y column, and under Aggregation , select None .\n    2. Select the TS column, and under Bucketing , select None .\n3. Add the LOWER\\_BOUND and UPPER\\_BOUND columns, and under Aggregation , select None .\n4. To display the initial visualization, select Chart .\n5. Select Add Column on the right side of the page, and select the columns you want to visualize:\n   \n    + LOWER\\_BOUND\n    + UPPER\\_BOUND\n    + IS\\_ANOMALY\n   \n   Results:\n6. Hover over the high spike to see that Y lies outside of the upper bound and is tagged with a 1 in the IS\\_ANOMALY field.\n\nTip\n\nTo better understand your results, try Top Insights .\n\n## Automate Anomaly Detection with Snowflake Tasks and Alerts ¶\n\nYou can create an automated anomaly detection pipeline, both for retraining the model and for monitoring your data for anomalies, by using Anomaly Detection functions within Snowflake Tasks or Alerts.\n\n* Recurring Training with a Snowflake Task\n* Monitoring with a Snowflake Task\n* Monitoring with a Snowflake Alert\n\n### Recurring Training with a Snowflake Task ¶\n\nYou can update your model to reflect the most up-to-date data using Snowflake Tasks .\n\nTo create a task that refreshes the anomaly detection object every hour, run following statement, replacing `_your_warehouse_name_` with your warehouse name:\n\n```\nCREATE OR REPLACE TASK ad_model_retrain_task \n WAREHOUSE = < your_warehouse_name > \n SCHEDULE = '60 MINUTE' \n AS \n EXECUTE IMMEDIATE \n $$ \n BEGIN \n  CREATE OR REPLACE SNOWFLAKE.ML.ANOMALY_DETECTION model_trained_with_labeled_data ( \n    INPUT_DATA => TABLE ( view_with_labeled_data_for_training ), \n    TIMESTAMP_COLNAME => 'date' , \n    TARGET_COLNAME => 'sales' , \n    LABEL_COLNAME => 'label' \n  ); \n END ; \n $$;\n```\n\nCopy\n\nBy default, newly created tasks are suspended.\n\nTo resume the task, execute the ALTER TASK … RESUME command:\n\n```\nALTER TASK ad_model_retrain_task RESUME ;\n```\n\nCopy\n\nTo pause the task, execute the ALTER TASK … SUSPEND command:\n\n```\nALTER TASK ad_model_retrain_task SUSPEND ;\n```\n\nCopy\n\n### Monitoring with a Snowflake Task ¶\n\nYou can also use Snowflake Tasks to monitor your data at a given frequency.\n\nFirst, create a table to hold the results\nof anomaly detection:\n\n```\nCREATE OR REPLACE TABLE anomaly_res_table ( \n  ts TIMESTAMP_NTZ , y FLOAT , forecast FLOAT , lower_bound FLOAT , upper_bound FLOAT , \n  is_anomaly BOOLEAN , percentile FLOAT , distance FLOAT );\n```\n\nCopy\n\nCreate a task to store the results of a recurring anomaly detection operation in the table.\nThis example sets the `WAREHOUSE` parameter to `snowhouse` . You can replace that with\nyour own warehouse:\n\n```\nCREATE OR REPLACE TASK ad_model_monitoring_task \n WAREHOUSE = snowhouse \n SCHEDULE = '1 minute' \n AS \n EXECUTE IMMEDIATE \n $$ \n BEGIN \n  INSERT INTO anomaly_res_table ( ts , y , forecast , lower_bound , upper_bound , is_anomaly , percentile , distance ) \n    SELECT * FROM TABLE ( \n      model_trained_with_labeled_data ! DETECT_ANOMALIES ( \n        INPUT_DATA => TABLE ( view_with_data_to_analyze ), \n        TIMESTAMP_COLNAME => 'date' , \n        TARGET_COLNAME => 'sales' , \n        CONFIG_OBJECT => { 'prediction_interval' :0 . 99 } \n    ) \n  ); \n END ; \n $$;\n```\n\nCopy\n\nTo resume the task, execute the ALTER TASK … RESUME command:\n\n```\nALTER TASK ad_model_monitoring_task RESUME ;\n```\n\nCopy\n\n`anomaly_res_table` then contains all the results for each task run.\n\nTo pause the task, execute the ALTER TASK … SUSPEND command:\n\n```\nALTER TASK ad_model_monitoring_task SUSPEND ;\n```\n\nCopy\n\n### Monitoring with a Snowflake Alert ¶\n\nYou can also use Snowflake Alerts to monitor your data at a given frequency and send you\nemail with detected anomalies. The following statements create an alert that detects anomalies every minute. First you\ndefine a stored procedure to detect anomalies, then create an alert\nthat uses that stored procedure.\n\nNote\n\nYou must set up email integration to send mail from a stored procedure; see Notifications in Snowflake .\n\n```\nCREATE OR REPLACE PROCEDURE extract_anomalies () \n  RETURNS TABLE () \n  LANGUAGE SQL \n  AS \n  $$ \n    BEGIN \n      let res RESULTSET := ( SELECT * FROM TABLE ( \n        model_trained_with_labeled_data ! DETECT_ANOMALIES ( \n          INPUT_DATA => TABLE ( view_with_data_to_analyze ), \n          TIMESTAMP_COLNAME => 'date' , \n          TARGET_COLNAME => 'sales' , \n          CONFIG_OBJECT => { 'prediction_interval' :0 . 99 } \n        )) \n        WHERE is_anomaly = TRUE \n      ); \n      RETURN TABLE ( res ); \n    END ; \n  $$ \n  ; \n\n CREATE OR REPLACE ALERT sample_sales_alert \n WAREHOUSE = < your_warehouse_name > \n SCHEDULE = '1 MINUTE' \n IF ( EXISTS ( CALL extract_anomalies ())) \n THEN \n CALL SYSTEM$SEND_EMAIL ( \n  'sales_email_alert' , \n  'your_email@snowflake.com' , \n  'Anomalous Sales Data Detected in data stream' , \n  CONCAT ( \n    'Anomalous Sales Data Detected in data stream \\n' , \n    'Value outside of prediction interval detected in the most recent run at ' , \n    current_timestamp ( 1 ) \n  ));\n```\n\nCopy\n\nTo start or resume the alert, execute the ALTER ALERT … RESUME command:\n\n```\nALTER ALERT sample_sales_alert RESUME ;\n```\n\nCopy\n\nTo pause the alert, execute the ALTER ALERT … SUSPEND command:\n\n```\nALTER ALERT sample_sales_alert SUSPEND ;\n```\n\nCopy\n\n## Understanding Feature Importance ¶\n\nAn anomaly detection model can explain the relative importance of all features used in your model, including any exogenous\nvariables that you choose, automatically generated time features (such as day of week or week of year), and\ntransformations of your target variable (such as rolling averages and auto-regressive lags). This information is useful\nin understanding what factors are really influencing your data.\n\nThe <model\\_name>!EXPLAIN\\_FEATURE\\_IMPORTANCE method counts the number of times the\nmodel’s trees used each feature to make a decision. These feature importance scores are then normalized to values\nbetween 0 and 1 so that their sum is 1. The resulting scores represent an approximate ranking of the features in your\ntrained model.\n\nFeatures that are close in score have similar importance. For extremely simple series (for example, when the target\ncolumn has a constant value), all feature importance scores may be zero.\n\nUsing multiple features that are very similar to each other may result in reduced importance scores for those features.\nFor example, if one feature is _quantity of items sold_ and another is _quantity of items in inventory_ , the values may be\ncorrelated because you can’t sell more than you have and because you try to manage inventory so you won’t have more in\nstock than you will sell. If two features are identical, the model may treat them as interchangeable when making\ndecisions, resulting in feature importance scores that are half of what those scores would be if only one of the\nfeatures were included.\n\nFeature importance also reports _lag features._ During training, the model infers the frequency (hourly, daily, or weekly)\nof your training data. The feature `lag _x_` (e.g. `lag24` ) is the value of the target variable _x_ time units ago.\nFor example, if your data is inferred to be hourly, `lag24` represents your target variable 24 hours ago.\n\nAll other transformations of your target variable (rolling averages, etc.) are summarized as `aggregated_endogenous_features` in the results table.\n\n### Limitations ¶\n\n* You cannot choose the technique used to calculate feature importance.\n* Feature importance scores can be helpful for gaining intuition about which features are important to your model’s\n  accuracy, but the actual values should be considered estimates.\n\n### Example ¶\n\nTo understand the relative importance of your features to your model, train a model, and then call <model\\_name>!EXPLAIN\\_FEATURE\\_IMPORTANCE . In this example, you first create random data with\ntwo exogenous variables: one that is random and therefore unlikely to be very important to your model, and one that\nis a copy of your target and therefore likely to be more important to your model.\n\nExecute the following statements to generate the data, train a model on it, and get the importance of the features:\n\n```\nCREATE OR REPLACE VIEW v_random_data AS SELECT \n  DATEADD ( 'minute' , ROW_NUMBER () over ( ORDER BY 1 ), '2023-12-01' ) ::TIMESTAMP_NTZ ts , \n  MOD ( SEQ1 (), 10 ) y , \n  UNIFORM ( 1 , 100 , RANDOM ( 0 )) exog_a \n FROM TABLE ( GENERATOR ( ROWCOUNT => 500 )); \n\n CREATE OR REPLACE VIEW v_feature_importance_demo AS SELECT \n  ts , \n  y , \n  exog_a \n FROM v_random_data ; \n\n SELECT * FROM v_feature_importance_demo ; \n\n CREATE OR REPLACE SNOWFLAKE.ML.ANOMALY_DETECTION anomaly_model_feature_importance_demo ( \n  INPUT_DATA => TABLE ( v_feature_importance_demo ), \n  TIMESTAMP_COLNAME => 'ts' , \n  TARGET_COLNAME => 'y' , \n  LABEL_COLNAME => '' \n ); \n\n CALL anomaly_model_feature_importance_demo ! EXPLAIN_FEATURE_IMPORTANCE ();\n```\n\nCopy\n\n**Output**\n\nBecause this example uses random data, do not expect your output to match this exactly.\n\n```\n+--------+------+--------------------------------------+-------+-------------------------+ \n | SERIES | RANK | FEATURE_NAME                         | SCORE | FEATURE_TYPE            | \n +--------+------+--------------------------------------+-------+-------------------------+ \n | NULL   |    1 | aggregated_endogenous_trend_features |  0.36 | derived_from_endogenous | \n | NULL   |    2 | exog_a                               |  0.22 | user_provided           | \n | NULL   |    3 | epoch_time                           |  0.15 | derived_from_timestamp  | \n | NULL   |    4 | minute                               |  0.13 | derived_from_timestamp  | \n | NULL   |    5 | lag60                                |  0.07 | derived_from_endogenous | \n | NULL   |    6 | lag120                               |  0.06 | derived_from_endogenous | \n | NULL   |    7 | hour                                 |  0.01 | derived_from_timestamp  | \n +--------+------+--------------------------------------+-------+-------------------------+\n```\n\n## Inspecting Training Logs ¶\n\nWhen you train multiple series with `CONFIG_OBJECT => _'ON_ERROR': 'SKIP'_` , individual time series models can\nfail to train without the overall training process failing. To understand which time series failed and why, call `<model_instance>!SHOW_TRAINING_LOGS` .\n\n### Example ¶\n\n```\nCREATE TABLE t_error ( date TIMESTAMP_NTZ , sales FLOAT , series VARCHAR ); \n INSERT INTO t_error VALUES \n  ( TO_TIMESTAMP_NTZ ( '2019-12-20' ), 1 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-21' ), 2 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-22' ), 3 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-23' ), 2 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-24' ), 1 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-25' ), 2 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-26' ), 3 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-27' ), 2 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-28' ), 1 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-29' ), 2 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-30' ), 3 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2019-12-31' ), 2 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-01' ), 2 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-02' ), 3 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-03' ), 3 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-04' ), 7 . 0 , 'A' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-06' ), 10 . 0 , 'B' ), -- the same timestamp used again and again \n  ( TO_TIMESTAMP_NTZ ( '2020-01-06' ), 13 . 0 , 'B' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-06' ), 12 . 0 , 'B' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-06' ), 15 . 0 , 'B' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-06' ), 14 . 0 , 'B' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-06' ), 18 . 0 , 'B' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-06' ), 12 . 0 , 'B' ); \n\n CREATE SNOWFLAKE.ML.ANOMALY_DETECTION model ( \n  INPUT_DATA => TABLE ( SELECT date , sales , series FROM t_error ), \n  SERIES_COLNAME => 'series' , \n  TIMESTAMP_COLNAME => 'date' , \n  TARGET_COLNAME => 'sales' , \n  LABEL_COLNAME => '' , \n  CONFIG_OBJECT => { 'ON_ERROR' : 'SKIP' } \n ); \n\n CALL model ! SHOW_TRAINING_LOGS ();\n```\n\nCopy\n\n**Output**\n\n```\n+--------+--------------------------------------------------------------------------+ \n | SERIES | LOGS                                                                     | \n +--------+--------------------------------------------------------------------------+ \n | \"B\"    | {   \"Errors\": [     \"At least two unique timestamps are required.\"   ] } | \n | \"A\"    | NULL                                                                     | \n +--------+--------------------------------------------------------------------------+\n```\n\n## Cost Considerations ¶\n\nFor details on costs for using ML functions, see Cost Considerations in the ML functions overview.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nLanguage: **English**\n\n* English\n* Français\n* Deutsch\n* 日本語\n* 한국어\n* Português"],"full_content":null},{"url":"https://docs.snowflake.com/en/user-guide/cost-anomalies","title":"Introduction to cost anomalies | Snowflake Documentation","publish_date":null,"excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nGuides Cost & Billing Visibility Cost anomalies\n\n# Introduction to cost anomalies ¶\n\nA cost anomaly occurs when daily consumption is above or below the expected range of consumption for the day. Snowflake uses an algorithm to\nautomatically detect these cost anomalies based on prior levels of consumption, which simplifies the process of identifying spikes or dips\nin costs so you can find ways to optimize your spend. Snowflake also provides tools to investigate these cost anomalies to identify root\ncauses.\n\nNote\n\nThe algorithm that detects cost anomalies requires at least 30 days of consumption before it can identify anomalies. If your consumption\nin the last seven days was less than 10 credits, Snowflake does not identify changes as an anomaly.\n\n## Account-level vs. organization-level cost anomalies ¶\n\nAn account-level cost anomaly occurs when the consumption in a single account falls outside the expected range of consumption for that\naccount.\n\nAn organization-level cost anomaly occurs when the consumption in the entire organization falls outside the expected range of consumption\nfor the organization. It is based on the aggregate consumption of all accounts in the organization. For example, if there is a significant\nconsumption spike in one account, but a dip in another, the two might offset each other such that it is not flagged as an organization-level\nanomaly. To help investigate organization-level anomalies, Snowflake provides tools to identify which accounts had the biggest increase or\ndecrease in consumption on a specific day.\n\nTo identify and investigate organization-level cost anomalies, you need to be signed in to the organization account or an ORGADMIN-enabled account .\n\n## Get started ¶\n\nTo identify and investigate cost anomalies using a user interface:\n\n2. In the navigation menu, select Admin » Cost management , and then select Anomalies .\n\n## Unit of measure for cost data ¶\n\nCost data can be shown with credits as the unit of measure or with a currency as the unit of measure. The unit of measure is a currency in\nthe following situations:\n\n* If you use the ACCOUNTADMIN or GLOBALORGADMIN system role to work with cost anomalies, cost data displays in a currency if you\n  are signed in to the organization account or one that has the ORGADMIN role enabled .\n* If you are not a system administrator, cost data displays in a currency if you are granted the ORGANIZATION\\_BILLING\\_VIEWER\n  application role or APP\\_ORGANIZATION\\_BILLING\\_VIEWER application role. For more information about these application roles, see Access control for cost anomalies .\n\n## Run queries against cost anomaly views ¶\n\nYou can run queries against views in the ACCOUNT\\_USAGE and ORGANIZATION\\_USAGE schemas to return historical data about account-level cost\nanomalies. Each row in the view includes the consumption on a specific day, and whether that consumption was a cost anomaly.\n\nCost anomalies for current account\n    Execute queries against the ANOMALIES\\_DAILY view in the ACCOUNT\\_USAGE schema to gain insights into whether cost anomalies occurred in the current account.\n\nThis view uses credits as the unit of measure for consumption.\nCost anomalies for all accounts in an organization\n    Execute queries against the ANOMALIES\\_IN\\_CURRENCY\\_DAILY view in the ORGANIZATION\\_USAGE schema to gain insights into whether cost anomalies occurred in accounts in\nthe organization. Note that not all accounts have access to the ORGANIZATION\\_USAGE schema.\n\nUse this view to see currency as the unit of measure rather than credits.\n\n## Learn more ¶\n\nFor information about how to work with cost anomalies, see the following:\n\n* Use Snowsight to work with cost anomalies\n* Programmatically work with cost anomalies\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Account-level vs. organization-level cost anomalies\n2. Get started\n3. Unit of measure for cost data\n4. Run queries against cost anomaly views\n5. Learn more\n\nRelated content\n\n1. Managing cost in Snowflake\n\nLanguage: **English**\n\n* English\n* Français\n* Deutsch\n* 日本語\n* 한국어\n* Português"],"full_content":null},{"url":"https://medium.com/snowflake/machine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee","title":"Machine Learning-Based Alerts for Snowflake FinOps | by Piotr Paczewski | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2023-12-04","excerpts":["Sitemap\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nWrite\n\nSearch\n\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-8ec640fb1cee---------------------------------------)\n\n·\n\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-8ec640fb1cee---------------------------------------)\n\nBest practices, tips & tricks from Snowflake experts and community\n\n# Machine Learning-Based Alerts for Snowflake FinOps\n\nPiotr Paczewski\n\n5 min read\n\n·\n\nDec 4, 2023\n\n\\--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nOverview of GenAI and LLM capabilities in Snowflake\n\n### Overview\n\nCost management is a key component of every successful cloud strategy. Snowflake provides a range of built-in tools to effectively manage cost, including:\n\n* [Cost exploration using Snowsight & account usage](https://docs.snowflake.com/en/user-guide/cost-exploring-overall)\n* [Resource monitors](https://docs.snowflake.com/en/user-guide/resource-monitors)\n* [Alerts & notifications](https://docs.snowflake.com/en/guides-overview-alerts)\n* [Budgets](https://docs.snowflake.com/en/user-guide/budgets)\n* [Object tagging](https://docs.snowflake.com/en/user-guide/object-tagging)\n\nThis article will demonstrate how you can leverage Snowflake alerts to automatically receive email notifications for anomalies detected in virtual warehouse compute usage using [Snowflake Cortex ML-Based Functions](https://docs.snowflake.com/en/guides-overview-ml-powered-functions) .\n\nThe cost of using Snowflake platform can be broken down into:\n\n* Compute resources (virtual warehouse compute, serverless compute, cloud service compute)\n* Storage\n* Data transfer\n\nVirtual warehouse compute usage can be monitored using [warehouse\\_metering\\_history](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history) view in [account usage schema](https://docs.snowflake.com/en/sql-reference/account-usage) . It shows hourly credit usage for virtual warehouses in your account within the last 365 days.\n\nPress enter or click to view image in full size\n\nSample result from querying [warehouse\\_metering\\_history](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history)\n\nTo automatically detect anomalies in virtual warehouse compute usage, Snowflake users can leverage machine learning. Snowflake Cortex ML-based functions simplify the complexities associated with using ML models and enable organizations to quickly gain insights into their data.\n\nTo train a machine learning using Snowflake Cortex ML-based functions, let’s start with splitting the data into a training dataset and a test dataset. Data from the last 12 months of virtual warehouse compute usage will be used, with the test dataset comprising the most recent 2 months and the remaining 10 months forming the training dataset.\n\n### Build ML model using Snowflake Cortex ML-based functions\n\nStep 1: Create the training dataset:\n\n```\ncreate or replace view warehouse_compute_usage_train as  \n  select  \n    to_timestamp_ntz(to_date(start_time)) as timestamp,  \n    sum(credits_used_compute) as credits_used  \n  from snowflake.account_usage.warehouse_metering_history  \n  where timestamp between dateadd(day,-365,current_date()) and dateadd(day,-61,current_date())  \n  group by all;\n```\n\nStep 2: Create the test dataset:\n\n```\ncreate or replace view warehouse_compute_usage_test as  \n  select  \n    to_timestamp_ntz(to_date(start_time)) as timestamp,  \n    sum(credits_used_compute) as credits_used  \n  from snowflake.account_usage.warehouse_metering_history  \n  where timestamp between dateadd(day,-60,current_date()) and current_date()  \n  group by all;\n```\n\nStep 3: Train an anomaly detection model using Snowflake Cortex ML-based functions:\n\n```\ncreate or replace snowflake.ml.anomaly_detection warehouse_usage_analysis(  \n  input_data => system$reference('view', 'warehouse_compute_usage_train'),  \n  timestamp_colname => 'timestamp',  \n  target_colname => 'credits_used',  \n  label_colname => ''  \n  );\n```\n\nStep 4: Run model inference using the trained model:\n\n```\ncall warehouse_usage_analysis!detect_anomalies(  \n  input_data => system$reference('view','warehouse_compute_usage_test')  \n  , timestamp_colname => 'timestamp'  \n  , target_colname => 'credits_used'  \n  );\n```\n\nPress enter or click to view image in full size\n\nResults of model inference\n\nStep 5: Visualize model results using Snowsight:\n\nPress enter or click to view image in full size\n\nVisualization of ML model inference using selected parameters.\n\nThe forecasted values are indicated by the red line, while the lower bounds and upper bounds of the forecasts are respectively represented by light blue and yellow lines.\n\nThe actual (observed) values are displayed using a dark blue line. Every point on this line that is not within the lower and upper bounds of the forecasts is marked an anomaly.\n\nBy running the SQL below, I am able to identify that they are 6anomalies detected in the test dataset.\n\n```\ncreate table warehouse_usage_anomalies   \n  as select * from table(result_scan(last_query_id()));  \n  \nselect * from warehouse_usage_anomalies   \n  where is_anomaly = true;\n```\n\nIn the code used to call the model inference, the _prediction\\_interval_ parameter value was not specified, therefore, the default value 0.99 was used. To mark more observations as anomalies, reduce the value of _prediction\\_interval_ and set it to, for instance, 0.9. On the other hand, to mark fewer observations as anomalies, the value of _prediction\\_interval_ parameter should be increased.\n\n### Create email notifications for newly detected anomalies using Snowflake Alerts and Tasks\n\nTo create automatic email notifications for new anomalies detected in virtual warehouse compute usage Snowflake Tasks & Alerts can be used.\n\nStep 1: Create a task to retrain ML model on a weekly basis at 5 AM every Sunday LA time:\n\n```\ncreate or replace task train_warehouse_usage_anomaly_task  \nwarehouse = demo_wh  \nschedule = 'USING CRON 0 5 * * 0 America/Los_Angeles'  \nas  \nexecute immediate  \n$$  \nbegin  \n  create or replace snowflake.ml.anomaly_detection warehouse_usage_analysis(  \n    input_data => system$reference('view', 'warehouse_compute_usage_train'),  \n    timestamp_colname => 'timestamp',  \n    target_colname => 'credits_used',  \n    label_colname => ''  \n    );  \nend;  \n$$;\n```\n\nStep 2: Create a task to call the anomaly detection model on a daily basis at 7 AM LA time and insert the result into warehouse\\_usage\\_anomalies table:\n\n```\ncreate or replace task inference_warehouse_usage_anomaly_task  \nwarehouse = demo_wh  \nschedule = 'USING CRON 0 7 * * * America/Los_Angeles'   \nas  \nexecute immediate  \n$$  \nbegin  \n  call warehouse_usage_analysis!detect_anomalies(  \n    input_data => system$reference('view','warehouse_compute_usage_test')  \n    , timestamp_colname => 'timestamp'  \n    , target_colname => 'credits_used'  \n    );  \ninsert into warehouse_usage_anomalies  \n  select * from table(result_scan(last_query_id()));  \nend;  \n$$;\n```\n\nStep 3: Set up an alert to check every day at 8 AM LA time if any new anomalies have been detected in warehouse compute usage:\n\n```\ncreate or replace alert warehouse_usage_anomaly_alert  \n  warehouse = demo_wh  \n  schedule = 'USING CRON 0 8 * * * America/Los_Angeles'  \n  if (exists (select * from warehouse_usage_anomalies where is_anomaly=True and ts > dateadd('day',-1,current_timestamp())))  \n  then  \n  call system$send_email(  \n    'warehouse_email_alert',  \n    'test@domain.com',  \n    'Warehouse compute usage anomaly detected',  \n    concat(  \n      'Anomaly detected in the warehouse compute usage. ',  \n      'Value outside of confidence interval detected.'  \n    )  \n  );\n```\n\nSimilarly to tasks, alerts are created in suspended state and need to be resumed in order to start running. After resuming the alert, this is the email I have received after some time:\n\nPress enter or click to view image in full size\n\nSample email notification received\n\n### Final note\n\nAnomaly detection using Snowflake Cortex ML-based functions works with both single-series and multi-series data. While this article focuses on single-series data, it’s important to note that it’s also possible to use anomaly detection functions with multi-series data to build separate, independent ML models, for example, for each virtual warehouse object.\n\nThere are various approaches that can be taken to anomaly detection in Snowflake. The decision to implement ML-based anomaly detection should be assessed on case-by-case basis. In certain scenarios, simple approaches, such as threshold-based anomaly detection, might be more practical than implementing ML-based methods.\n\n_I am currently a Snowflake Solutions Consultant at Snowflake. Opinions expressed in this post are solely my own and do not represent the views or opinions of my employer._\n\nSnowflake\n\nMlops\n\nFinops\n\nCost Optimization\n\nAI\n\n\\-- \n\n\\--\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--8ec640fb1cee---------------------------------------)\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--8ec640fb1cee---------------------------------------)\n\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--8ec640fb1cee---------------------------------------)\n\n10\\.9K followers\n\n· Last published 4 hours ago\n\nBest practices, tips & tricks from Snowflake experts and community\n\n## Written by Piotr Paczewski\n\n36 followers\n\n· 3 following\n\nSolutions Consultant at Snowflake. Opinions expressed are solely my own and do not represent views of my employer\n\n## No responses yet\n\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--8ec640fb1cee---------------------------------------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Status](https://status.medium.com/?source=post_page-----8ec640fb1cee---------------------------------------)\n\nAbout\n\nCareers\n\nPress\n\n[Blog](https://blog.medium.com/?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----8ec640fb1cee---------------------------------------)"],"full_content":null}],"errors":[],"warnings":[{"type":"warning","message":"Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.","detail":null}],"usage":[{"name":"sku_extract_excerpts","count":3}]}
{
  "search_id": "search_9427b7341f5e428fa1ac7f46881bef5c",
  "results": [
    {
      "url": "https://www.e6data.com/query-and-cost-optimization-hub/snowflake-cost-optimization-15-proven-tactics-to-cut-your-snowflake-cost",
      "title": "Snowflake Cost Optimization 2025: Code Hacks and Examples",
      "publish_date": "2025-08-14",
      "excerpts": [
        "Products\nProducts\nHybrid Data Lakehouse Real-time Streaming Ingest Lakehouse Query Engine Lakehouse Compute Engine\nProduct Release\ne6data\u2019s Hybrid Data Lakehouse: 10x Faster Queries, Near-Zero Egress, Sub-Second Latency\n[Read Launch Blog](https://www.e6data.com/blog/hybrid-data-lakehouse-10x-faster-queries-near-zero-egress-sub-second-latency)\nCompatibility\nCompatibility\nSnowflake Databricks Amazon SageMaker Microsoft Fabric\nCost Calculator\nGet the monthly compute costs on e6data vs. other engines\nEstimate Costs Now\nPricing\nCustomers\nCustomers\nFINTECH UNICORN\nUS-BASED GLOBAL BANK\nNASDAQ SAAS LEADER\nCase Study\ne6data pushes data engineering for world's leading companies\nRead Story\nBenchmarks About Us [Docs](https://docs.e6data.com/product-documentation)\nDevelopers\nDeveloperS\nBlog Optimization Hub Events\nLatest On the Blog\nBuilding a Modern Data Pipeline in Snowflake: From Snowpipe to Managed Iceb...\n ... \nSection Title: How to Optimize Snowflake Costs? {Updated 2025 Guide}\nContent:\nAugust 14, 2025\n/\ne6data Team\nSnowflake\nCost Optimization\nBeginner\nA Reddit user seeking help as Snowflake costs double\nSnowflake\u2019s elastic compute and practically bottomless storage make it one of the easiest platforms to scale analytics when your business takes off. However, that same elasticity means costs can ramp up faster than expected if usage isn\u2019t carefully managed, especially at enterprise scales.\nThe goal of this guide is to help you reclaim cost efficiency without throttling your users\u2019 needs by applying lessons learned from conversations with dozens of teams using Snowflake at scale (including those with >$10M annual spend).\n-> We\u2019ll dive deep into cost optimization techniques categorized by workload type: BI dashboards, ad-hoc analytics, and ETL/data pipelines.\nSection Title: How to Optimize Snowflake Costs? {Updated 2025 Guide}\nContent:\n-> For each category, we outline the purpose of the optimization, how to implement it (with SQL configuration or code examples), alternatives to consider (including Snowflake edition-specific features), and a key takeaway.\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 4. Aggressively auto-suspend idle warehouses\nContent:\nAfter you\u2019ve slowed dashboard refreshes and squeezed maximum value from caches, most of a reader warehouse\u2019s lifetime is waiting rather than working. Snowflake bills right down to the second, so each extra minute the warehouse sits idle is pure waste. Dropping AUTO_SUSPEND to 60 seconds means the meter stops almost immediately after the last query finishes, yet cold-start latency stays tolerable because queries are now infrequent and well-cached.\n**How to implement:**\n```\nALTER  WAREHOUSE BI_DASH\u00a0\u00a0 SET  AUTO_SUSPEND  = 60 , \u00a0  -- warehouse sleeps after one idle minute\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0AUTO_RESUME\u00a0 = TRUE; -- wakes automatically on the next query\u00a0\n```\n\u200d\nWhat changes?\nActive refresh \u2192 warehouse runs, answers queries, and costs credits.\n60 seconds of quiet \u2192 warehouse suspends; billing stops.\nNext dashboard hit \u2192 auto-resume spins it up in a few seconds.\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data > ... > **5. Let Snowflake store the heavy aggregates**\nContent:\n```\n-- Repeated heavy aggregate? Create an MV. CREATE  MATERIALIZED  VIEW  mv_sales_by_day  AS SELECT  order_date,\n SUM (amount)  AS  rev\n FROM    raw_sales\n GROUP BY  order_date;\n -- Dashboard filters on one customer or order at a time? -- Add SOS so Snowflake jumps straight to the matching partitions. ALTER TABLE  big_orders  ADD SEARCH  OPTIMIZATION;\n```\n\u200d\nMVs refresh automatically\u2014only the changed partitions are recomputed, so maintenance credits stay modest. SOS is serverless: you pay based on index size and updates, not for warehouse time.\nDocs: [Materialised Views](https://docs.snowflake.com/en/user-guide/views-materialized) , [Search Optimization Service](https://docs.snowflake.com/en/user-guide/search-optimization-service)\n**Alternatives:**\n ... \nSection Title: ... > 1. Encourage sampling and LIMIT for exploratory queries\nContent:\nAnalysts often want to \u201cpeek,\u201d not scan 20 TB. A culture of TABLESAMPLE and LIMIT 1000 shrinks cost burn without slowing discovery.\n**How to implement:**\nAdd SQL linters or IDE snippets that default to LIMIT.\nRun bi-weekly show-and-tell sessions on *why* a smaller sample usually answers the question.\n**Alternatives**\n**\u200d** 1 *.* Use Snowflake\u2019s [Preview App](https://docs.snowflake.com/en/release-notes/preview-features) (beta), which shows estimated credits before a query runs. If available, enabling this \u201cQuery Preview\u201d can be very eye-opening for users. It will display an estimated $$ or credit usage for the query, which often prompts analysts to reconsider running that 200 GB join and maybe apply a filter or sample first.\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 3. Tag everything for data-driven cost audits\nContent:\nIf your BI tool or query interface can\u2019t easily set session tags, one workaround is to use warehouse-level cost attribution. You can dedicate warehouses to specific teams or workloads (and reflect that in the warehouse name or in a separate meta-tag) and then use the warehouse_name in usage logs as a proxy for attribution. Snowflake also supports Resource Monitors that can be assigned per warehouse to cap credit usage, which indirectly separates costs. Another alternative is using [**Snowflake\u2019s Organization features**](https://docs.snowflake.com/en/user-guide/organizations) (if you have a multi-account setup): Snowflake Org allows you to roll up usage across accounts, and you can label accounts by department or environment. Finally, consider integrating Snowflake usage data with your company\u2019s monitoring/BI.\n ... \nSection Title: ... > 4. Leverage access control to restrict costly operations\nContent:\n2. Another alternative (if organizationally feasible) is to separate workloads into different Snowflake accounts/projects with different cost centers and permissions, though that introduces data synchronization overhead. his is a heavier solution, since it introduces data replication between accounts and administrative overhead, but it absolutely walls off costs between groups.\n**Takeaway** : By limiting who can spin up big compute or alter warehouse settings, and by guiding users to appropriate warehouses, you encourage cost-conscious behavior by design.\nSection Title: Reduce Snowflake costs by 60% with e6data > Ad-hoc Querying > 5. Enable query timeouts\nContent:\nSnowflake allows queries to run for days by default (up to 48 hours!), which means a runaway query or forgotten join can rack up a massive bill before anyone notices. Enabling query timeouts puts a cap on how long any single query can run \u2013 and thus how many credits it can burn. If a query exceeds the timeout (say 1 hour), Snowflake will automatically cancel it.\n**How to implement:**\nDecide on an appropriate timeout based on your workload (e.g., 1 hour for ad-hoc warehouses, maybe longer for ETL warehouses if needed). Then set the STATEMENT_TIMEOUT_IN_SECONDS parameter for the warehouse or at the account level. For example, to set a 30-minute timeout on an ad-hoc warehouse:\n```\nALTER  WAREHOUSE ANALYST_WH \n SET  STATEMENT_TIMEOUT_IN_SECONDS  = 1800 ;\n```\n\u200d\nDocs: [Parameters](https://docs.snowflake.com/en/sql-reference/parameters)\n**Alternatives:**\nSection Title: Reduce Snowflake costs by 60% with e6data > Ad-hoc Querying > 5. Enable query timeouts\nContent:\nIf you\u2019re hesitant to cancel queries automatically, you can start softer: use [**Resource Monitors**](https://docs.snowflake.com/en/user-guide/resource-monitors) to watch credit usage per query or per user. While resource monitors typically operate on a monthly or daily credit consumption basis (and can abort warehouses or sessions if a budget is exceeded), they\u2019re not as immediate as a timeout.\nAnother approach is to enable alerts: e.g., if a query runs longer than 30 minutes, have an alert (via SNOWSQL or AWS Lambda polling query history) that notifies the team to check it.\nOne more nuanced approach: set timeouts on a **session basis** for specific roles. For instance, you might allow the data engineering service account (running ETL) a longer timeout, but any session initiated by an analyst role gets a shorter timeout.\n ... \nSection Title: ... > 1. Process only new/changed data (streams and tasks for CDC)\nContent:\n**Alternative**\n1. If you need sub-second latency, switch to [Snowpipe Streaming](https://medium.com/snowflake/snowpipe-streaming-demystified-e1ee385c6d9c) . It pushes row changes directly into Snowflake and charges per row, bypassing the 60-second minimum a warehouse task would incur. The trade-off is complexity and possibly higher cost per row for very large volumes, so Snowpipe Streaming makes sense for low-latency requirements on moderate data volumes.\n2.  Use another real-time ingest engine that can directly land event streams to Iceberg tables in a queryable format in a zero-ETL fashion in less than a minute.\ne6data\u2019s zero-ETL real-time ingest flow\n\u200d\n**Takeaway** : Capture changes once, merge them once, and spend credits only on data that actually arrived since the last run.\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 3. Avoid frequent small DML operations\nContent:\nSnowflake tables are stored in immutable micro-partitions (~16 MB compressed each), so each time you update or delete a single record, Snowflake has to recreate the entire micro-partition that the record lives in. A single small update could rewrite hundreds of thousands of rows. Likewise, inserting a handful of rows at a time causes \u201csmall file\u201d overhead as Snowflake tries to combine them into existing micro-partitions. Beyond the compute cost of these rewrites, frequent DML also bloats your Time Travel and Fail-safe storage, since Snowflake must retain previous versions of each modified micro-partition.\n**How to implement:**\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 3. Avoid frequent small DML operations\nContent:\nIf truly real-time, row-by-row updates are needed, consider whether that portion of the workload belongs in Snowflake or if it\u2019s better suited for an OLTP database or a caching layer. Snowflake is optimized for analytical batch operations.\nAnother alternative is Snowflake\u2019s Apache Iceberg external table feature or similar, which might be more efficient for high-churn datasets by separating storage layers (though this comes with complexity). At minimum, if you must do frequent DML on a table, try to partition the hot data so that only a portion of micro-partitions get rewritten (e.g. cluster by date, and only update recent partitions). This mitigates the impact. You can also extend full native Iceberg support in Snowflake through e6data\u2019s compute engine.\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data\nContent:\nIngest, ETL, Query in Snowflake with e6data compute engine- with zero migration.\n[Get Started for Free](https://e6data-design.webflow.io/contact/demo)\nBack\nTable of contents:\nShare this article\n[](https://www.linkedin.com/company/e6data/) [](https://x.com/e6data)\n[](https://www.linkedin.com/company/e6data/) [](https://wellfound.com/company/evix)\nAvailable at\n[](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/e6data.e6data) [](https://aws.amazon.com/marketplace/pp/prodview-qqkfcpg43eq2w)\nBlog Events [Docs](https://docs.e6data.com/product-documentation)\n[Terms and Conditions](https://docs.e6data.com/product-documentation/terms-and-condition) [Privacy Policy](https://docs.e6data.com/product-documentation/privacy-policy) [Cookie Policy](https://docs.e6data.com/product-documentation/privacy-policy/cookie-policy)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history",
      "title": "WAREHOUSE_METERING_HISTORY view | Snowflake Documentation",
      "excerpts": [
        "Reference General reference SNOWFLAKE database Account Usage WAREHOUSE\\_METERING\\_HISTORY\n\nSchemas:\n    ACCOUNT\\_USAGE , READER\\_ACCOUNT\\_USAGE\n\n# WAREHOUSE\\_ METERING\\_ HISTORY view \u00b6\n\nThis Account Usage view can be used to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within the last 365 days (1 year).\n\n## Columns \u00b6\n\n|Column Name |Data Type |Description |\n| --- | --- | --- |\n|READER\\_ACCOUNT\\_NAME |VARCHAR |Name of the reader account where the warehouse usage took place. Column only included in view in READER\\_ACCOUNT\\_USAGE schema. |\n|START\\_TIME |TIMESTAMP\\_LTZ |The date and beginning of the hour (in the local time zone) in which the warehouse usage took place. |\n|END\\_TIME |TIMESTAMP\\_LTZ |The date and end of the hour (in the local time zone) in which the warehouse usage took place. |\n|WAREHOUSE\\_ID |NUMBER |Internal/system-generated identifier for the warehouse. |\n|WAREHOUSE\\_NAME |VARCHAR |Name of the warehouse. |\n|CREDITS\\_USED |NUMBER |Total number of credits used for the warehouse in the hour. This is a sum of CREDITS\\_USED\\_COMPUTE and CREDITS\\_USED\\_CLOUD\\_SERVICES. This value does not take into account the adjustment for cloud services , and may therefore be greater than the credits that are billed. To determine how many credits were actually billed, run queries against the METERING\\_DAILY\\_HISTORY view . |\n|CREDITS\\_USED\\_COMPUTE |NUMBER |Number of credits used for the warehouse in the hour. |\n|CREDITS\\_USED\\_CLOUD\\_SERVICES |NUMBER |Number of credits used for cloud services in the hour. |\n|CREDITS\\_ATTRIBUTED\\_COMPUTE\\_QUERIES |NUMBER |Number of credits attributed to queries in the hour. . . Includes only the credit usage for query execution and doesn\u2019t include warehouse idle time usage. |\n\n## Usage notes \u00b6\n\n* In the ACCOUNT\\_USAGE schema, latency for the view is up to 180 minutes (3 hours), except for the CREDITS\\_USED\\_CLOUD\\_SERVICES column. Latency for\n  CREDITS\\_USED\\_CLOUD\\_SERVICES is up to 6 hours.\n* In the READER\\_ACCOUNT\\_USAGE schema, latency for the view is up to 24 hours.\n* Warehouse idle time is not included in the CREDITS\\_ATTRIBUTED\\_COMPUTE\\_QUERIES column.\n  \n  See Examples for a query that calculates the cost of idle time.\n\n* If you want to reconcile the data in this view with a corresponding view in the ORGANIZATION USAGE schema , you must first set the timezone of the session to UTC. Before querying the Account Usage view, execute:\n  \n  > ```\n  > ALTER SESSION SET TIMEZONE = UTC ;\n  > ```\n  > \n  > Copy\n  > \n  >\n\n## Examples \u00b6\n\nFor example, to determine the cost of idle time for each warehouse for the last 10 days, execute the following statement:\n\n```\nSELECT \n  ( SUM ( credits_used_compute ) - \n    SUM ( credits_attributed_compute_queries )) AS idle_cost , \n  warehouse_name \n FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n WHERE start_time >= DATEADD ( 'days' , - 10 , CURRENT_DATE ()) \n  AND end_time < CURRENT_DATE () \n GROUP BY warehouse_name ;\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Columns\n2. Usage notes\n3. Examples\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-attributing",
      "title": "Attributing cost | Snowflake Documentation",
      "excerpts": [
        "Section Title: Attributing cost \u00b6 > Types of cost attribution scenarios \u00b6\nContent:\nIn this case, each query executed by the application is assigned a query tag that identifies\nthe team or cost center of the user on whose behalf the query is being made.\n ... \nSection Title: Attributing cost \u00b6 > ... > Resources shared by users from different departments \u00b6\nContent:\nSuppose that users in different departments share the same warehouses and you want to break down the credits used by each\ndepartment. You can tag the users with a `cost_center` tag to identify the department that they belong to, and you can join\nthe TAG_REFERENCES view with the QUERY_ATTRIBUTION_HISTORY view .\nNote\nYou can only get this data for a single account at a time. You cannot execute a query that retrieves this data across\naccounts in an organization.\nThe next sections provide examples of SQL statements for attributing costs for shared resources.\nCalculating the cost of user queries for the last month\nCalculating the cost of user queries by department without idle time\nCalculating the cost of queries by users without idle time\nCalculating the cost of queries by users without tags\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of user queries for the last month \u00b6\nContent:\nThis following SQL statement calculates the costs for the last month.\nIn this example, idle time is distributed among the users in proportion to their usage.\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of user queries by department without idle time \u00b6\nContent:\nThe following example attributes the compute cost to each department through the queries executed by users in that department.\nThis query depends on the user objects having a tag that identifies their department.\n```\nWITH joined_data AS ( \n  SELECT \n      tr . tag_name , \n      tr . tag_value , \n      qah . credits_attributed_compute , \n      qah . start_time \n    FROM SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES tr \n      JOIN SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n        ON tr . domain = 'USER' AND tr . object_name = qah . user_name \n ) \n SELECT \n    tag_name , \n    tag_value , \n    SUM ( credits_attributed_compute ) AS total_credits \n  FROM joined_data \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY tag_name , tag_value \n  ORDER BY tag_name , tag_value ;\n```\nCopy\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of user queries by department without idle time \u00b6\nContent:\n```\n+-------------+-------------+-----------------+ \n | TAG_NAME    | TAG_VALUE   |   TOTAL_CREDITS | \n |-------------+-------------+-----------------| \n | COST_CENTER | engineering |   0.02493688426 | \n | COST_CENTER | finance     |    0.2281084988 | \n | COST_CENTER | marketing   |    0.3686840545 | \n |-------------+-------------+-----------------|\n```\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries by users without idle time \u00b6\nContent:\nThis following SQL statement calculates the costs per user for the past month (excluding idle time).\n```\nSELECT user_name , SUM ( credits_attributed_compute ) AS credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE \n    start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY user_name ;\n```\nCopy\n```\n+-----------+--------------------+ \n | USER_NAME | ATTRIBUTED_CREDITS | \n |-----------+--------------------| \n | JSMITH    |       17.173333333 | \n | MJONES    |         8.14444444 | \n | SYSTEM    |         5.33985393 | \n +-----------+--------------------+\n```\n ... \nSection Title: ... > Resources used by applications that need to attribute costs to different departments \u00b6\nContent:\nThe examples in this section calculate the costs for one or more applications that are powered by Snowflake.\nThe examples assume that these applications set query tags that identify the application for all queries executed. To set the\nquery tag for queries in a session, execute the ALTER SESSION command. For example:\n```\nALTER SESSION SET QUERY_TAG = 'COST_CENTER=finance' ;\n```\nCopy\nThis associates the `COST_CENTER=finance` tag with all subsequent queries executed during the session.\nYou can then use the query tag to trace back the cost incurred by these queries to the appropriate departments.\nThe next sections provide examples of using this approach.\nCalculating the cost of queries by department\nCalculating the cost of queries (excluding idle time) by query tag\nCalculating the cost of queries (including idle time) by query tag\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries by department \u00b6\nContent:\nThe following example calculates the compute credits and the credits used for the query acceleration service for the finance department. This depends on the `COST_CENTER=finance` query tag being applied to the original queries that were executed.\nNote that the costs exclude idle time.\n```\nSELECT \n    query_tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_tag = 'COST_CENTER=finance' \n  GROUP BY query_tag ;\n```\nCopy\n```\n+---------------------+-----------------+------+ \n | QUERY_TAG           | COMPUTE_CREDITS | QAS  | \n |---------------------+-----------------|------| \n | COST_CENTER=finance |      0.00576115 | null | \n +---------------------+-----------------+------+\n```\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (excluding idle time) by query tag \u00b6\nContent:\nThe following example calculates the cost of queries by query tag and includes queries without tags (identified as \u201cuntagged\u201d).\n```\nSELECT \n    COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n  GROUP BY tag \n  ORDER BY compute_credits DESC ;\n```\nCopy\n```\n+-------------------------+-----------------+------+ \n | TAG                     | COMPUTE_CREDITS | QAS  | \n |-------------------------+-----------------+------+ \n | untagged                | 3.623173449     | null | \n | COST_CENTER=engineering | 0.531431948     | null | \n |-------------------------+-----------------+------+\n```\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (including idle time) by query tag \u00b6\nContent:\nThe following example distributes the idle time that is not captured in the per-query cost across departments in proportion\nto their usage of the warehouse.\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (including idle time) by query tag \u00b6\nContent:\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n      AND start_time < CURRENT_DATE \n  ), \n  tag_credits AS ( \n    SELECT \n        COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n        SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n      GROUP BY tag \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n      FROM tag_credits \n  ) \n SELECT \n    tc . tag , \n    tc . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM tag_credits tc , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\nCopy\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (including idle time) by query tag \u00b6\nContent:\n```\n+-------------------------+--------------------+ \n | TAG                     | ATTRIBUTED_CREDITS | \n +-------------------------+--------------------| \n | untagged                |        9.020031304 | \n | COST_CENTER=finance     |        1.027742521 | \n | COST_CENTER=engineering |        1.018755812 | \n | COST_CENTER=marketing   |       0.4801370376 | \n +-------------------------+--------------------+\n```\nSection Title: Attributing cost \u00b6 > Viewing cost by tag in Snowsight \u00b6\nContent:\nYou can attribute costs by reporting on the use of resources that have the `cost_center` tag. You can access this data in Snowsight .\nSwitch to a role that has access to the ACCOUNT_USAGE schema .\nIn the navigation menu, select Admin \u00bb Cost management .\nSelect Consumption .\nFrom the Tags drop-down, select the `cost_center` tag.\nTo focus on a specific cost center, select a value from the list of the tag\u2019s values.\nSelect Apply .\nFor more details about filtering in Snowsight, see Filter by tag .\nSection Title: Attributing cost \u00b6 > About the QUERY_ATTRIBUTION_HISTORY view \u00b6\nContent:\nYou can use the QUERY_ATTRIBUTION_HISTORY view to attribute cost based on queries. The cost per\nquery is the warehouse credit usage for executing the query. This cost does not include any other credit usage that is incurred\nas a result of query execution. For example, the following are not included in the query cost:\nData transfer costs\nStorage costs\nCloud services costs\nCosts for serverless features\nCosts for tokens processed by AI services\nFor queries that are executed concurrently, the cost of the warehouse is attributed to individual queries based on the weighted\naverage of their resource consumption during a given time interval.\nThe cost per query does not include warehouse *idle time* . Idle time is a period of time in which no queries are running in the\nwarehouse and can be measured at the warehouse level.\nSection Title: Attributing cost \u00b6 > Additional examples of queries \u00b6\nContent:\nThe next sections provide additional queries that you can use for cost attribution:\nGrouping similar queries\nAttributing costs of hierarchical queries\nSection Title: Attributing cost \u00b6 > Additional examples of queries \u00b6 > Grouping similar queries \u00b6\nContent:\nFor recurrent or similar queries, use the `query_hash` or `query_parameterized_hash` to group costs\nby query.\nTo find the most expensive recurrent queries for the current month, execute the following statement:\n```\nSELECT query_parameterized_hash , \n       COUNT (*) AS query_count , \n       SUM ( credits_attributed_compute ) AS total_credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n  AND start_time < CURRENT_DATE \n  GROUP BY query_parameterized_hash \n  ORDER BY total_credits DESC \n  LIMIT 20 ;\n```\nCopy\nFor an additional query based on query ID, see Examples .\nSection Title: Attributing cost \u00b6 > Additional examples of queries \u00b6 > Attributing costs of hierarchical queries \u00b6\nContent:\nFor stored procedures that issue multiple hierarchical queries, you can compute the attributed query costs for the\nprocedure by using the root query ID for the procedure.\nTo find the root query ID for a stored procedure, use the ACCESS_HISTORY view . For example,\nto find the root query ID for a stored procedure, set the `query_id` and execute the following statements:CopyFor more information, see Example: Ancestor queries with stored procedures .\nTo sum the query cost for the entire procedure, replace `<root_query_id>` and execute the following statements:Copy\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)"
      ]
    },
    {
      "url": "https://medium.com/@angelamarieharney/cortex-ai-cost-queries-in-snowflake-07331811d42d",
      "title": "Cortex AI Cost Queries in Snowflake | by Angela Harney | Jan, 2026 | Medium",
      "publish_date": "2026-02-10",
      "excerpts": [
        "Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nSearch\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > Introduction\nContent:\nI recently had to provide a breakdown of the AI services costs that are listed on the monthly Snowflake invoice so that it was clearly understood where AI costs were being attributed to help make planning decisions.\nSnowflake\u2019s integration of AI services like Cortex AI introduces new dimensions to cost management and understanding AI costs.\nAI costs fall into four main categories: ingest, inference, standard compute, and egress.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > Cortex Usage History Views > Cortex Analyst\nContent:\n*Usage is metered at a secondary level in the* `CORTEX_ANALYST_USAGE_HISTORY` *view and is aggregated hourly, showing credits consumed and number of messages per user. It does not include* `_query_id_` *, but offers per-user visibility into AI interaction costs. Costs are incurred per successful response (message) not by token count.*\nSection Title: Cortex AI Cost Queries in Snowflake > Cortex Usage History Views > Cortex Search\nContent:\n*Usage is metered at a secondary level in the* `CORTEX_SEARCH_DAILY_USAGE_HISTORY` *view and breaks down costs daily by consumption type (* `_serving_` *,* `_embed_text_tokens_` *) and includes costs for storage, embedding, and serving compute as follows:*\n`CORTEX_SEARCH_DAILY_USAGE_HISTORY` (serving vs. embedding)\n`CORTEX_SEARCH_SERVING_USAGE_HISTORY` (hourly serving credits)\nSection Title: Cortex AI Cost Queries in Snowflake > Cortex Usage History Views > Metering History\nContent:\n*A broader view that can be filtered by* `_SERVICE_TYPE = 'AI_SERVICES'_` *to track overall AI credit usage across all Cortex services, including AI functions, Analyst, and Search.*\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > My Cost Queries > Invoice Summary\nContent:\nUse the following query to isolate warehouse costs at an Organizational billing level to tie-out to the invoice cost for AI INFERENCE and AI SERVICES that are shown in the Snowflake Monthly Invoice Statement.\n```\nSELECT  \n      TO_DATE(USAGE_DATE) AS USAGE_DATE,  \n      'INVOICE SUMMARY' AS SERVICE_GROUP,  \n      SERVICE_TYPE,  \n      SUM(CREDITS_USED) as CREDITS_USED,  \n      SUM(CREDITS_USED) * << your credit price >> AS INVOICE_COST,  \n      0 as TOKENS  \nFROM snowflake.organization_usage.metering_daily_history h  \nWHERE service_type IN ('AI_SERVICES','AI_INFERENCE')   \nAND TO_DATE(USAGE_DATE) >= DATE_TRUNC('MONTH',DATEADD(day, -60, CURRENT_DATE))  \nGROUP BY ALL  \nORDER BY 1  \n;\n```\nSection Title: Cortex AI Cost Queries in Snowflake > My Cost Queries > Invoice and First-Level Details\nContent:\nIn my invoice detail query, AI Inference breaks out to document processing, AI Services breaks out to function usage and there is an aggregation section for Compute as standard compute for virtual warehouse costs. See `APPENDIX I \u2014 Invoice Cost Details.`\n*Note: Additional Cortex Usage History views may be added as first-level charges that aggregate to invoice line-items over time as Snowflake enhances their offerings.*\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > My Cost Queries > Second-Level Details\nContent:\nSecond-level charges provided by Snowflake for tracking of specific activities can be found in my query in Appendix II- `APPENDIX III \u2014 Second-Level Cost Details.`\nFor example, although the `cortex_search_history_usage` view is consolidated to an invoice line item, the secondary-level `cortex_search_serving_usage_history` view is next-level details for Cortex Search activities.\n*Note: Additional second-level Cortex Usage History views may be added over time as Snowflake enhances their offerings.*\nSection Title: Cortex AI Cost Queries in Snowflake > Other Cost Considerations and Monitoring\nContent:\nQuerying AI costs using Snowflake\u2019s native monitoring tools covers several features and areas of functionality.\nSection Title: Cortex AI Cost Queries in Snowflake > Other Cost Considerations and Monitoring > Notebooks\nContent:\nThe `ACCOUNT_USAGE` schema provides granular visibility into AI-related credit consumption. For example, running a query against `SNOWFLAKE.ACCOUNT_USAGE.CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY` allows you to identify which Notebooks or users trigger the highest AI processing costs.\nSimilarly, `NOTEBOOKS_CONTAINER_RUNTIME_HISTORY` helps track AI workload duration and credit usage for Snowflake Notebooks with AI components.\nBy combining these views with object and query tags, teams can attribute AI costs to specific projects, departments, or users that enable accurate budgeting and chargeback models.\nSection Title: Cortex AI Cost Queries in Snowflake > Other Cost Considerations and Monitoring > Containers\nContent:\nContainers in Snowflake, powered by Snowpark Container Services (SPCS), directly impact AI costs by providing dedicated compute pools (ranging from CPU to GPU), that can be selected for running containerized AI/ML workloads such as fine-tuning LLMs, distributed embedding generation, or deploying custom AI models.\nUnlike standard virtual warehouses, container runtime costs are based on the uptime of the compute pool (small, medium, or GPU-enabled) and are billed per credit hour, with pricing varying by instance type and region.\nRunning AI notebooks or Streamlit apps using container runtime (instead of warehouse runtime) can be more cost-effective, especially for GPU-intensive tasks, since Snowflake manages the underlying infrastructure with no additional fees for registry, networking, or logs.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > ... > Query Patterns and Resource Allocation\nContent:\nEffective cost control still hinges on optimizing query patterns and resource allocation like all other data querying does.\nA common mistake is relying on simple duration-based cost estimates, which ignore concurrency and auto-suspend delays. Instead, use a normalized, warehouse-uptime-based method to allocate costs. Treat each warehouse runtime as a single event, then distribute costs proportionally based on query duration within that window.\nThis method accounts for idle time and concurrency, revealing true cost per query. For AI workloads, this approach helps identify inefficient models or redundant invocations that inflate spend.\nInference costs are especially sensitive to query complexity and model selection.\nSection Title: Cortex AI Cost Queries in Snowflake > ... > Query Patterns and Resource Allocation\nContent:\nReduce data scanned to answer AI prompts that have frequently used data chunks that are flattened and stored in separate columns. This takes advantage of Snowflake\u2019s natural columnar storage optimizations.\nSection Title: Cortex AI Cost Queries in Snowflake > Other Cost Considerations and Monitoring > Complexity\nContent:\nComplex AI models with high token counts or large context windows consume more credits per inference. Teams should evaluate model efficiency, use caching where possible, and avoid overusing AI for simple tasks.\nSection Title: Cortex AI Cost Queries in Snowflake > ... > Cost Anomalies Feature\nContent:\nSnowflake\u2019s Cost Anomalies feature (released May 2025) can flag unexpected spikes in AI usage, enabling proactive intervention.\nSection Title: Cortex AI Cost Queries in Snowflake > Other Cost Considerations and Monitoring > Query Tags\nContent:\nAdditionally, leveraging query tags on AI queries allows for real-time cost tracking and automated alerts when thresholds are breached.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > ... > Cost Insights and Cloud Services Optimizations Features\nContent:\nSnowflake\u2019s Cost Insights and Cloud Services Optimizations features provide recommendations to reduce inefficiencies, such as unused AI assets or poorly optimized queries.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > Cost Summary\nContent:\nTo query standard compute costs for AI workloads in Snowflake, focus on the `WAREHOUSE_METERING_HISTORY` view in the `SNOWFLAKE.ACCOUNT_USAGE` schema, which tracks credit consumption for virtual warehouses used during AI operations such as running Cortex Analyst queries, refreshing Cortex Search indexes, or executing embedding jobs.\nSince AI functions like `AI_EXTRACT` or `AI_EMBED` rely on standard warehouses for orchestration, even though the AI processing itself uses AI Services compute, their associated compute costs appear under `CREDITS_USED_COMPUTE` .\nSo, filter by `WAREHOUSE_NAME` and `START_TIME` to isolate usage during AI-related workloads and join with `QUERY_HISTORY` to attribute costs to specific AI queries, such as those using `SNOWFLAKE.CORTEX` functions.\nUse resource monitors or tagging to allocate these costs by team or project, ensuring visibility into AI-driven compute spend alongside general usage.\nSection Title: Cortex AI Cost Queries in Snowflake > Cortex Code Costs\nContent:\nCortex Code charges standard Cloud Services compute costs for accessing metadata in Snowflake such as using `DESCRIBE` or querying `SNOWFLAKE.ACCOUNT_USAGE` tables and these costs can be found in the regular `QUERY_HISTORY` view.\nAt the time of this writing ( *2026\u201302\u201309),* Snowflake is not charging for the use of Cortex Code. There are charges being incurred as AI Services for the LLMs that are executed. They will be billed once Snowflake starts charging for the use of Cortex Code with token pricing.\nThese costs show under AI_SERVICES in `WAREHOUSE_METERING_HISTORY` in the following query:\n```\nSELECT  \n  USAGE_DATE,  \n  SERVICE_TYPE,  \n  CREDITS_USED,  \n  CREDITS_USED * <your_credit_rate> AS estimated_cost_usd  \nFROM SNOWFLAKE.ORGANIZATION_USAGE.METERING_DAILY_HISTORY  \nWHERE SERVICE_TYPE IN ('AI_SERVICES', 'AI_INFERENCE')  \n  AND USAGE_DATE >= DATEADD('month', -3, CURRENT_DATE())  \nORDER BY USAGE_DATE DESC;\n```\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > Conclusion\nContent:\nIn conclusion, mastering AI cost querying in Snowflake requires a blend of technical precision and strategic governance.\nFocus on accurate cost attribution, optimized query patterns, smart resource sizing, and vigilance on egress.\nUse native tools like `ACCOUNT_USAGE` , `QUERY_ATTRIBUTION_HISTORY` , and `WAREHOUSE_METERING_HISTORY` to build a transparent cost model.\nCombine this with tagging, budgets, and anomaly detection to turn AI from a cost center into a value-driven capability.\nWith the right approach, Snowflake\u2019s AI Data Cloud can deliver powerful insights without breaking the bank.\nSection Title: Cortex AI Cost Queries in Snowflake > APPENDIX I \u2014 Invoice Cost Details\nContent:\nThe COST column in this query should equal the amount of AI INFERENCE and AI SERVICES line-items on the Snowflake Monthly Statement.\nA COMPUTE aggregation is also provided as a total for virtual warehouse cost.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > APPENDIX I \u2014 Invoice Cost Details\nContent:\nas needed  \n    TOKENS  \nFROM invoice_cost_details  \nORDER BY   \n;\n```\nSection Title: Cortex AI Cost Queries in Snowflake > APPENDIX II\u2014 Individual AI Query Details\nContent:\nAny of the Cortex Usage History views that return a Query ID can be applied to the join and query fields in this query.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > `APPENDIX III \u2014 Second-Level Cost Details`\nContent:\nThese second-level costs encompass all of the other Cortex Usage History views at the time of the writing of this article and were collected to be able to see the lower layers of activities in Snowflake where Cortex costs are being attributed for tracking purposes.\n ... \nSection Title: Cortex AI Cost Queries in Snowflake > `APPENDIX III \u2014 Second-Level Cost Details`\nContent:\n--\n--\nSection Title: Cortex AI Cost Queries in Snowflake > Written by Angela Harney\nContent:\n41 followers\n\u00b7 2 following"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/application_daily_usage_history",
      "title": "APPLICATION_DAILY_USAGE_HISTORY view | Snowflake Documentation",
      "excerpts": [
        "Reference General reference SNOWFLAKE database Account Usage APPLICATION\\_DAILY\\_USAGE\\_HISTORY\n\nSchema:\n    ACCOUNT\\_USAGE\n\n# APPLICATION\\_ DAILY\\_ USAGE\\_ HISTORY view \u00b6\n\nUse this view to return the daily credit and storage usage for Snowflake Native Apps in an account within the last 365 days\n(1 year).\n\n## Columns \u00b6\n\nThe following table provides definitions for the APPLICATION\\_DAILY\\_USAGE\\_HISTORY view columns.\n\n|Field |Data type |Description |\n| --- | --- | --- |\n|APPLICATION\\_NAME |VARCHAR |The application name. |\n|APPLICATION\\_ID |NUMBER |An internal, system-generated identifier for the application. |\n|LISTING\\_GLOBAL\\_NAME |VARCHAR |The listing global name that appears in Snowflake Marketplace or in the data exchange hosting the application. |\n|USAGE\\_DATE |DATE |The date the Snowflake Native App usage occurred. |\n|CREDITS\\_USED |NUMBER |The number of credits consumed by the Snowflake Native App in a day. |\n|CREDITS\\_USED\\_BREAKDOWN |ARRAY |An array of data objects that identify the Snowflake service that consumed daily credits. See CREDITS\\_USED\\_BREAKDOWN array for formatting. |\n|STORAGE\\_BYTES |NUMBER |The daily average of storage bytes used by the Snowflake Native App. |\n|STORAGE\\_BYTES\\_BREAKDOWN |ARRAY |An array of data objects that identify the type and number of storage bytes used. See STORAGE\\_BYTES\\_BREAKDOWN array for formatting. |\n\n## Usage notes \u00b6\n\n* The maximum latency for this view is one day.\n* Usage is attributed to the start day when usage events span multiple days.\n* The APPLICATION\\_DAILY\\_USAGE\\_HISTORY view and the Snowsight cost management tools can return different daily credit and storage usage values. This discrepancy is caused by the methods used to determine daily credit and storage usage. To determine these values, the APPLICATION\\_DAILY\\_USAGE\\_HISTORY view uses the current session\u2019s TIMEZONE parameter and the Snowsight cost management tools use Coordinated Universal Time (UTC). To resolve any discrepancies, Snowflake recommends setting the TIMEZONE parameter to UTC.\n\n### CREDITS\\_USED\\_BREAKDOWN array \u00b6\n\nThe CREDITS\\_USED\\_BREAKDOWN array provides details about the services that consumed daily credits.\n\nExample:\n\n```\n[ \n  { \n    \"credits\": 0.005840921,\n    \"serviceType\": \"AUTO_CLUSTERING\" \n  } ,\n  { \n    \"credits\": 0.115940725,\n    \"serviceType\": \"SERVERLESS_TASK\" \n  } ,\n  { \n    \"credits\": 6.033448041,\n    \"serviceType\": \"SNOWPARK_CONTAINER_SERVICES\" \n  } \n ]\n```\n\nCopy\n\nThe following table provides descriptions for the key-value pairs in the objects in the array.\n\n|Field |Data type |Description |\n| --- | --- | --- |\n|`credits` |DECIMAL |Number of credits consumed by the service type specified by `serviceType` on the usage date. |\n|`serviceType` |VARCHAR |The service type, which can be one of the following values:\n\n* `AUTO_CLUSTERING` \u2014 See Automatic Clustering .\n* `DATA_QUALITY_MONITORING` \u2014 See Introduction to data quality and data metric functions .\n* `MATERIALIZED_VIEW` \u2014 See Working with Materialized Views .\n* `PIPE` \u2014 See Snowpipe .\n* `SEARCH_OPTIMIZATION` \u2014 See Search optimization service .\n* `SERVERLESS_TASK` \u2014 See Introduction to tasks .\n* `SNOWPARK_CONTAINER_SERVICES` \u2014 See Snowpark Container Services .\n* `WAREHOUSE_METERING` \u2014 See Overview of warehouses . |\n\nThe following are used in the determination of credit consumption:\n\n* The credits used by objects in the Snowflake Native App. For example, auto-clustering on tables in the Snowflake Native App.\n* The credits used by the warehouses owned by the Snowflake Native App.\n* The credits used by the compute pools dedicated to the Snowflake Native App.\n\n### STORAGE\\_ BYTES\\_ BREAKDOWN array \u00b6\n\nThe STORAGE\\_BYTES\\_BREAKDOWN array provides details about the services that consumed storage.\n\nExample:\n\n```\n[ \n  { \n    \"bytes\": 34043221,\n    \"storageType\": \"DATABASE\" \n  } ,\n  { \n    \"bytes\": 109779541,\n    \"storageType\": \"FAILSAFE\" \n  } \n ]\n```\n\nCopy\n\nThe following table provides descriptions for the key-value pairs in the objects in the array.\n\n|Field |Data type |Description |\n| --- | --- | --- |\n|`bytes` |INTEGER |Number of storage bytes used. |\n|`storageType` |VARCHAR |The storage type, which can be one of the following values:\n\n* `DATABASE` : Database storage.\n* `FAILSAFE` : Fail-safe storage .\n* `HYBRID_TABLE` : Storage for hybrid tables . |\n\nOnly data stored in the Snowflake Native App is used to determine storage byte consumption. External databases created by the Snowflake Native App are not included in the determination of this value.\n\n## Examples \u00b6\n\nRetrieve the daily credit and storage usage for a Snowflake Native App in an account and order the results by usage date:\n\n```\nSELECT * \n  FROM SNOWFLAKE . ACCOUNT_USAGE . APPLICATION_DAILY_USAGE_HISTORY \n  ORDER BY usage_date DESC ;\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Columns\n2. Usage notes\n3. Examples\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://blog.greybeam.ai/snowflake-cost-per-query/",
      "title": "Deep Dive: Snowflake's Query Cost and Idle Time Attribution",
      "publish_date": "2024-10-22",
      "excerpts": [
        "Section Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query\nContent:\nSnowflake's new QUERY_ATTRIBUTION_HISTORY view\nSnowflake recently released a new feature for granular cost attribution down to individual queries through the `QUERY_ATTRIBUTION_HISTORY` view in `ACCOUNT_USAGE` . As a company focused on SQL optimization, we at Greybeam were eager to dive in and see how this new capability compares to our own custom cost attribution logic. What we found was surprising - and it led us down a rabbit hole of query cost analysis.\nSection Title: ... > The Promise and Limitations of QUERY_ATTRIBUTION_HISTORY\nContent:\nThe new view aims to provide visibility into the compute costs associated with each query. Some key things to note:\nData is only available from July 1, 2024 onwards\nShort queries (<100ms) are excluded\nIdle time is not included in the attributed costs\nThere can be up to a 6 hour delay in data appearing\nThere's also a `WAREHOUSE_UTILIZATION` view that displays cost of idle time. At the time of writing, this must be enabled by your Snowflake support team.\nSection Title: ... > Our Initial Findings\nContent:\nWe set up a test with an X-Small warehouse and 600 second auto-suspend to dramatically illustrate idle time. Running a series of short queries (mostly <500ms) over an hour, we expected to see a very small fraction of the total credits in that hour attributed to our queries, but we were very wrong.\nOn September 4th at the 14th hour, ~40 seconds of queries were executed and some how in the `QUERY_ATTRIBUTION_HISTORY` view it showed that nearly half of the total credits (0.43 of 0.88) attributed to query execution. This seemed impossibly high given the short query runtimes, yet the pattern continues.\nQUERY_ATTRIBUTION_HISTORY aggregated by the hour.\nThis may just be an anomaly in our Snowflake account, so try it yourself.\n ... \nSection Title: ... > Our Approach to Query Cost Attribution\nContent:\nWe use `WAREHOUSE_METERING_HISTORY` as our source of truth for warehouse compute credits. The credits billed here will reconcile with Snowflake\u2019s cost management dashboards.\nCredits here are represented on an hourly grain. We like to refer to this as *credits metered* , analogous to how most homes in North America are metered for their electricity. In our solution, we\u2019ll need to allocate queries and idle times into their metered hours.\nWe use a weighted time-based approach to attribute costs within the metered hour. In reality, Snowflake\u2019s credit attribution is likely much more complex, especially in situations with more clusters or warehouse scaling.\nHow we need to break down our queries and idle times.\nThe full SQL query will be available at the end of this blog.\n ... \nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > Conclusion\nContent:\nWhile Snowflake\u2019s new `QUERY_ATTRIBUTION_HISTORY` view is a promising step towards easier cost attribution, our initial testing reveals some potential issues that need to be addressed. For now, we recommend carefully validating the results against your own calculations and metering history.\nWe\u2019re excited to see how this feature evolves and will continue to monitor its accuracy. In the meantime, implementing your own cost attribution logic can provide valuable insights into query performance and resource utilization.\nBy accounting for idle time and carefully tracking query execution across hour boundaries, we\u2019re able to get a more complete and accurate picture of costs. This level of detail is crucial for optimizing Snowflake usage and controlling costs effectively."
      ]
    },
    {
      "url": "https://sedai.io/blog/guide-to-optimizing-snowflake-costs-in-2025",
      "title": "How to Optimize Snowflake Costs: Best Practices for 2025 - Sedai",
      "publish_date": "2025-02-24",
      "excerpts": [
        "Section Title: How to Optimize Snowflake Costs: Best Practices for 2025 > How to Optimize Snowflake Costs\nContent:\nThe optimization process unfolds through a series of tactical steps. Start by dissecting billing data to highlight areas of excessive expenditure, such as oversized virtual warehouses or overutilized serverless functions. Implement precise adjustments: calibrate warehouse sizes to reflect workload demands, employ auto-suspend capabilities to reduce idle costs, and strategize data transfers to minimize expenses. Incorporating automated cost management platforms ensures continuous oversight over spending trends and aids in maintaining budget discipline.\nAchieving optimal cost efficiency in Snowflake demands harmonizing technical refinements with vigilant monitoring and strategic planning. By consistently evolving usage strategies and staying abreast of Snowflake's pricing dynamics, organizations can secure a balance between performance and cost efficiency.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 1. Analyze Snowflake Usage Patterns\nContent:\nTo optimize Snowflake costs effectively, a detailed assessment of usage metrics is essential. Start by leveraging Snowflake's native cost insights available within Snowsight. These insights help illuminate patterns in data consumption and identify potential inefficiencies. For a broader perspective, complement these insights with third-party analytics tools that offer a more expansive view of usage trends and cost drivers.\nFocusing on high-expense elements such as virtual warehouses is crucial. Ensure that these warehouses are properly sized for their intended workloads and not left active when idle, which can lead to unnecessary costs. Similarly, evaluate the usage of serverless features, which can contribute to significant expenses if not monitored diligently. Regularly reviewing these components allows for timely adjustments, aligning resource use with actual operational needs.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 1. Analyze Snowflake Usage Patterns\nContent:\nDynamic monitoring is key to sustaining cost efficiency as workloads fluctuate. By setting up automated alerts for deviations in spending patterns, teams can swiftly address unexpected costs. This proactive approach allows for real-time adjustments to resource allocation, ensuring that Snowflake deployments remain efficient and within budgetary constraints.\n ... \nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 4. Implement Automated Cost Management\nContent:\nIncorporating automated solutions for cost management is essential for overseeing financial operations in Snowflake environments. Leveraging intelligent cost optimization tools allows organizations to fine-tune resource utilization effectively. These systems provide a robust framework that dynamically adjusts usage, freeing teams to devote time to strategic priorities over routine financial monitoring.\nEstablishing a system to monitor for cost anomalies is crucial. Implementing budgetary markers enables immediate notification when expenditures diverge from anticipated levels. This approach mitigates potential budget excesses and ensures unusual financial activities are swiftly identified. Such real-time alerts function as a critical safeguard, allowing for rapid interventions.\n ... \nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 5. Leverage Snowflake\u2019s Advanced Features\nContent:\nMaximizing Snowflake's advanced capabilities involves a nuanced approach to balancing performance enhancements with cost management. Employing features such as clustering keys can significantly improve query performance by arranging data more efficiently, thus speeding up retrieval times. While clustering can enhance operations, it also requires careful management to prevent unnecessary resource consumption. Selectively applying clustering keys to datasets with complex access patterns ensures that performance gains justify the associated resource use."
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/functions/warehouse_metering_history",
      "title": "WAREHOUSE_METERING_HISTORY | Snowflake Documentation",
      "excerpts": [
        "Reference Function and stored procedure reference Table WAREHOUSE_METERING_HISTORY\nCategories:\nInformation Schema , Table functions\nSection Title: WAREHOUSE_METERING_HISTORY \u00b6\nContent:\nThis table function can be used in queries to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within a specified date range.\nNote\nThis function returns credit usage within the last 6 months. However, if you are querying multiple warehouses over a lengthy time period,\nit might not return a complete data set. To obtain a complete data set, use the ACCOUNT_USAGE view instead.\nSee also:\nWAREHOUSE_LOAD_HISTORY\nSection Title: WAREHOUSE_METERING_HISTORY \u00b6 > Syntax \u00b6\nContent:\n```\nWAREHOUSE_METERING_HISTORY ( \n      DATE_RANGE_START => <constant_expr> \n      [ , DATE_RANGE_END => <constant_expr> ] \n      [ , WAREHOUSE_NAME => '<string>' ] )\n```\nCopy\n ... \nSection Title: WAREHOUSE_METERING_HISTORY \u00b6 > Usage notes \u00b6\nContent:\nReturns results only for the ACCOUNTADMIN role or any role that has been explicitly granted the MONITOR USAGE global privilege.\nWhen calling an Information Schema table function, the session must have an INFORMATION_SCHEMA schema in use or the function name must be fully-qualified. For more details, see Snowflake Information Schema .\nThe order and structure of the arguments depends on whether the argument keywords (e.g. `DATE_RANGE_START` ) are included:\nThe keywords are not required if the arguments are specified in order.\nIf the argument keywords are included, the arguments can be specified in any order.\nSection Title: WAREHOUSE_METERING_HISTORY \u00b6 > Output \u00b6\nContent:\nThe function returns the following columns, ordered by WAREHOUSE_NAME and START_TIME:\nSection Title: WAREHOUSE_METERING_HISTORY \u00b6 > Output \u00b6\nContent:\n| Column Name | Data Type | Description |\n| START_TIME | TIMESTAMP_LTZ | The beginning of the hour in which this warehouse usage took place. |\n| END_TIME | TIMESTAMP_LTZ | The end of the hour in which this warehouse usage took place. |\n| WAREHOUSE_NAME | VARCHAR | Name of the warehouse. |\n| CREDITS_USED | NUMBER | Number of credits billed for this warehouse in this hour. |\n| CREDITS_USED_COMPUTE | NUMBER | Number of credits used for the warehouse in the hour. |\n| CREDITS_USED_CLOUD_SERVICES | NUMBER | Number of credits used for cloud services in the hour. |\nSection Title: WAREHOUSE_METERING_HISTORY \u00b6 > Examples \u00b6\nContent:\nRetrieve hourly warehouse usage over the past 10 days for all warehouses that ran during this time period:\nCopy\nRetrieve hourly warehouse usage for the `testingwh` warehouse on a specified date:\nCopy\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nSyntax\nArguments\nUsage notes\nOutput\nExamples\nSection Title: WAREHOUSE_METERING_HISTORY \u00b6 > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\n ... \nSection Title: WAREHOUSE_METERING_HISTORY \u00b6 > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details\u200e\n ... \nSection Title: WAREHOUSE_METERING_HISTORY \u00b6 > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://medium.com/@jordandhill/a-practical-guide-to-snowflake-notebook-cost-attribution-f1e377244275",
      "title": "A Practical Guide to Snowflake Notebook Cost Attribution | by Jordan Hill | Medium",
      "publish_date": "2025-10-27",
      "excerpts": [
        "Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nSearch\nSection Title: A Practical Guide to Snowflake Notebook Cost Attribution\nContent:\nJordan Hill\n5 min read\n\u00b7\nOct 27, 2025\n--\n1\nListen\nShare\n ... \nSection Title: A Practical Guide to Snowflake Notebook Cost Attribution > 2. Idle Time Is Not Attributed\nContent:\nThe attribution system tracks active query execution time, not warehouse idle time. If your notebook keeps a warehouse running between executions, those idle credits won\u2019t appear in attribution reports. This is an important distinction when calculating total notebook costs.\nSection Title: A Practical Guide to Snowflake Notebook Cost Attribution > 3. Attribution Latency\nContent:\nQuery attribution data may have a latency of up to 3 hours, so real-time cost monitoring isn\u2019t possible through these views.\nSection Title: A Practical Guide to Snowflake Notebook Cost Attribution > The SQL Solution: DIY Cost Attribution\nContent:\nIf you prefer a SQL-only approach without additional tools, here\u2019s a query that extracts notebook cost data for Warehouse Compute:\n ... \nSection Title: A Practical Guide to Snowflake Notebook Cost Attribution > How This Query Works\nContent:\n**Extract Notebook Names** : The first CTE identifies all notebooks that have executed by looking for queries with text matching `'execute notebook%'` and extracting the `StreamlitName` from the query tag JSON.\n**Find Related Queries** : The second CTE joins back to `query_history` to find all queries that contain the notebook name in their query tags. This captures not just the initial execution but all downstream (aka child) queries.\n**Attribute Credits** : The final query joins with `query_attribution_history` to sum credits consumed, providing comprehensive cost information along with execution metadata.\n ... \nSection Title: A Practical Guide to Snowflake Notebook Cost Attribution > The Streamlit Dashboard Approach\nContent:\nWhile SQL queries are powerful, they require manual execution and lack visual context. For ongoing monitoring, consider deploying a Streamlit dashboard. And, hey, who doesn\u2019t love a little Streamlit in Snowflake.\nPress enter or click to view image in full size\nI\u2019ve created an open-source **Notebook Cost Attribution Dashboard** that provides:\nInteractive visualizations of notebook costs\nTime-range filtering (7\u2013180 days)\nDrill-down by specific notebooks\nCSV export for further analysis\nMulti-dimensional charts showing cost distribution\nSection Title: A Practical Guide to Snowflake Notebook Cost Attribution > Quick Deployment\nContent:\nThe streamlit dashboard code is available at: [https://github.com/jordandhill/Notebook_Cost_Attribution](https://github.com/jordandhill/Notebook_Cost_Attribution)\nThe core code is in streamlit_app.py.\nThe dashboard automatically refreshes data based on your selected time range and provides an intuitive interface for cost exploration.\n ... \nSection Title: A Practical Guide to Snowflake Notebook Cost Attribution > Conclusion: Turning on the Lights\nContent:\nSnowflake Notebook cost attribution transforms from a black box to a crystal-clear picture when you leverage:\n**Query tags** for notebook identification\n**Account usage views** for credit attribution\n**Object tags** for organizational cost allocation\n**Automated dashboards** for ongoing monitoring\nBy implementing these practices, you gain:\nComplete visibility into notebook costs\nThe ability to optimize expensive workflows\nData-driven justification for resource allocation\nAccurate chargeback to business units\nRemember the caveats: fast queries and idle time won\u2019t appear in attribution, but the vast majority of your notebook costs will be tracked accurately.\nStart with the SQL query, explore the data, and when you\u2019re ready for continuous monitoring, deploy the Streamlit dashboard. Your finance team will thank you."
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/",
      "title": "Cost Optimization - Snowflake",
      "excerpts": [
        "Section Title: Cost Optimization > Recommendations\nContent:\n**Business Impact**\n**Consider cost as a design constraint:** Integrate cost\nconsiderations into the architecture and design process from the\nvery beginning, making it a key non-functional requirement alongside\nperformance, security, and reliability. **Quantify value:** Develop metrics to quantify the business value\ndelivered by cloud resources (e.g., revenue per Snowflake Credit,\ncost per customer, efficiency gains). **Trade-off analysis:** Understand the inherent trade-offs between\ncost, performance, reliability, and security, and make informed\ndecisions that align with business priorities. **Measure business value KPIs baseline:** Once metrics to quantify\nbusiness value are identified and trade-offs between cost,\nperformance, and reliability are established, you need to document a\n\u201cbaseline measurement\u201d in order to track progress again.\nSection Title: Cost Optimization > Recommendations\nContent:\nFurthermore, you should establish a regular cadence for refreshing\nthis measurement to ensure value realization is in line with\nexpectations and business goals. **Visibility**\n**Understand Snowflake\u2019s resource billing models:** Review\nSnowflake\u2019s billing models to align technical and non-technical\nresources on financial drivers and consumption terminology. **Establish a consistent and granular cost attribution strategy:** Implement robust and organizationally consistent tagging and\nlabeling strategies across all resources (storage objects,\nwarehouses, accounts, queries) to accurately allocate costs to\nspecific teams, products, or initiatives. **Embed cost accountability into your organization's DNA:** Implement a feedback system to transparently show or charge back\ncloud costs to relevant business units or teams, increasing\naccountability.\nSection Title: Cost Optimization > Recommendations\nContent:\n**Deliver clear, historical consumption insights:** Utilize\nconsistent in-tool visualizations or custom dashboards to monitor\nconsumption and contextualize spend on the platform with unit\neconomics. **Investigate anomalous consumption activity:** Review anomaly\ndetection to identify unforeseen cost anomalies and investigate\ncause and effect trends. **Control**\n**Proactively monitor all platform usage:** Define and enforce\nbudgets for projects and services, setting soft quotas to limit\nresource consumption and prevent runaway spending. **Forecast consumption based on business needs:** Establish a\nforecast process to project future spend needs based on business and\ntechnical needs. **Enforce cost guardrails for organizational resources:** Set up\nautomated checks (e.g., Tasks, query insights) and resource\nguardrails (e.g., warehouse timeout, storage policies, resource\nmonitors) to identify unusual usage patterns and potential\noverspending as they occur.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\n**Snowpipe (file-based micro-batch ingestion)** : Use Snowpipe for\ncontinuous ingestion of files where near real-time availability is\nacceptable and ingestion frequency is moderate. File size guidance is\nthe same as COPY INTO (100-250MB). This leverages serverless compute\nto avoid warehouse management overhead.\n**Snowpipe Streaming (rowset-based, real-time ingestion)** : Use\nSnowpipe Streaming for very low-latency (sub-five-second) ingestion,\nhigh throughput, and streaming sources (e.g., Kafka, Kinesis, and\nevent hubs). You will want to ensure \u201cexactly-once guarantees\u201d with\noffset tokens for idempotency. Ideally, you would batch insert as much\nas possible to reduce API call overhead, but avoid building streaming\napplications just to load large \"batch\" files.\n**Data Transformation Optimization**"
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}

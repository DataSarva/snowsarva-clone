{
  "extract_id": "extract_94ce7ae7a2e64bf6b06d96af32e8dabc",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/release-notes/2025/other/2025-06-23-auto-privs-app-spec",
      "title": "Jun 23, 2025: Snowflake Native App Framework updates | Snowflake Documentation",
      "publish_date": "2025-08-04",
      "excerpts": [
        "Jun 23, 2025: Snowflake Native App Framework updates | Snowflake Documentation\n\n[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nEN\n\nEnglish\n\nFran\u00e7ais\n\nDeutsch\n\n\u65e5\u672c\u8a9e\n\n\ud55c\uad6d\uc5b4\n\nPortugu\u00eas\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\n1. Overview\n3. Release notes\n4. All release notes\n5. Snowflake server release notes and feature updates\n\n   * Upcoming (or in progress) server release notes\n   * Preview - 10.6\n   * Release notes overview: 10.3-10.6\n   * Recent server release notes\n   * Feb 16-19, 2026 - 10.5\n   * Feb 09-13, 2026 - 10.4\n   * Recent feature updates\n   * Feb 24, 2026 - Enforcement of privatelink-only access (General availability)\")\n   * Feb 24, 2026 - User-defined actions for budgets\n   * Feb 24, 2026 - View invoices in Snowsight\n   * Feb 24, 2026 - Snowflake Postgres (General availability)\")\n   * Feb 23, 2026 - Simplified setup for Data Quality Monitoring (Preview)\")\n   * Feb 23, 2026 - Grouped Query History in Snowsight (General availability)\")\n   * Feb 20, 2026 - Snowflake Native Apps: Configuration (Preview)\")\n   * Feb 20, 2026 - USE AI FUNCTIONS account privilege for Cortex AI Functions\n   * Feb 19, 2026 - Machine learning experiments (General availability)\")\n   * Feb 19, 2026 - Snowflake Data Clean Rooms updates\n   * Feb 18, 2026 - Snowflake Container Runtime versioning for ML Jobs (Preview)\")\n   * Feb 18, 2026 - CORTEX\\_AGENT\\_USAGE\\_HISTORY view (Preview)\")\n   * Feb 18, 2026 - SNOWFLAKE\\_INTELLIGENCE\\_USAGE\\_HISTORY view (Preview)\")\n   * Feb 18, 2026 - Row timestamps\n   * Feb 18, 2026 - Refresh dynamic tables with user privileges\n   * Feb 17, 2026 - Access history improvements\n   * Feb 16, 2026 - Sharing Streamlit apps (Preview)\")\n   * Feb 13, 2026 - Snowflake Native Apps Inter-App Communication (Preview)\")\n   * Feb 13, 2026 - Run Security Essentials scanners on demand\n   * Feb 12, 2026 - Strong Authentication Hub (Preview)\")\n   * Feb 12, 2026 - New checkout experience for private offers with flat-fee pricing (General availability)\")\n   * Feb 12, 2026 - Snowflake Data Clean Rooms updates\n   * Feb 10, 2026 - Snowflake Native Apps: Shareback (General availability)\")\n   * Feb 09, 2026 - Performance Explorer enhancements (Preview)\")\n   * Earlier server release notes and feature updates\n   * Earlier 2026 server release notes and feature updates\n   * 2025 server release notes and feature updates\n\n     + Server release notes\n     + Feature updates\n\n       - Dec 18, 2025 - Network rules and policies support Google Cloud Private Service Connect IDs (General availability)\")\n       - Dec 17, 2025 - Schema evolution support for Snowpipe Streaming with high-performance architecture\n       - Dec 17, 2025 - Snowflake Kafka High Performance connector (Preview)\")\n       - Dec 17, 2025 - Snowflake Postgres (Preview)\")\n       - Dec 16, 2025 - Cortex Search multi-indexing and custom vector embedding (Preview)\")\n       - Dec 16, 2025 - Notebooks in Workspaces (Preview)\")\n       - Dec 15, 2025: Account Usage: New CATALOG\\_LINKED\\_DATABASE\\_USAGE\\_HISTORY view\n       - Dec 15, 2025 - Vector aggregate functions\n       - Dec 12, 2025 - Private connectivity for internal stages on Google Cloud (General availability)\")\n       - Dec 11, 2025 - Default pipe for Snowpipe Streaming with high-performance architecture\n       - Dec 11, 2025 - Interactive tables and interactive warehouses (General availability)\")\n       - Dec 11, 2025 - Support for Streamlit in Snowflake container runtime\n       - Dec 11, 2025 - Snowflake Data Clean Rooms updates\n       - Dec 10, 2025 - WORM backups (General availability)\")\n       - Dec 10, 2025 - Cost anomalies (General availability)\")\n       - Dec 08, 2025 - Dynamic tables: dual warehouses\n       - Dec 08, 2025 - AI\\_REDACT (General availability)\")\n       - Dec 08, 2025 - Snowpipe simplified pricing\n       - Dec 04, 2025 - Snowflake Data Clean Rooms updates\n       - Dec 03, 2025 - Access history improvements\n       - Dec 02, 2025 - Optimize existing semantic views or models with verified queries\n       - Dec 02, 2025 - Auto-fulfillment for listings that span databases (General availability)\")\n       - Dec 02, 2025 - Private connectivity for Iceberg REST catalog integrations (General availability)\")\n       - Dec 01, 2025 - CORTEX\\_AISQL\\_USAGE\\_HISTORY (General availability)\")\n       - Nov 21, 2025 - Trust Center notifications in Snowsight\n       - Nov 21, 2025 - Import models from Hugging Face to Snowflake (Preview)\")\n       - Nov 21, 2025 - AI\\_COMPLETE function (GA)\")\n       - Nov 21, 2025: Tri-Secret Secure data protection for Snowpark Container Services block volumes (General availability)\")\n       - Nov 21, 2025 - External query engine support for Apache Iceberg\u2122 tables with Snowflake Horizon Catalog (Preview)\")\n       - Nov 20, 2025 - SnowConvert AI interface improvements\n       - Nov 20, 2025 - New versions of Streamlit supported in Streamlit in Snowflake\n       - Nov 20, 2025 - Snowflake Data Clean Rooms updates\n       - Nov 18, 2025 - Apache Iceberg\u2122 tables: Support for bi-directional data access with Microsoft Fabric (Preview)\")\n       - Nov 17, 2025 - Document Processing Playground (Preview)\")\n       - Nov 17, 2025 - Native Apps support for Snowpark Container Services on AWS Gov/FedRAMP (General availability)\")\n       - Nov 17, 2025 - Access control for cost anomalies\n       - Nov 14, 2025 - Cortex Analyst Routing Mode (Preview)\")\n       - Nov 13, 2025 - Improved stage volume implementation in Snowpark Container Services (General availability)\")\n       - Nov 13, 2025 - Exclude data from sensitive data classification (General availability)\")\n       - Nov 13, 2025 - Snowflake Data Clean Rooms updates\n       - Nov 10, 2025 - Snowpipe Streaming with high-performance architecture on GCP (General availability)\")\n       - Nov 07, 2025 - AI\\_REDACT function (Preview)))\")\n       - Nov 07, 2025 - Trust Center extensions (Preview)\")\n       - Nov 07, 2025 - Storage lifecycle policies (General availability)\")\n       - Nov 07, 2025 - Pricing plans and offers (General availability)\")\n       - Nov 06, 2025 - dbt Projects on Snowflake (General availability)\")\n       - Nov 06, 2025 - Snowflake Data Clean Rooms updates\n       - Nov 05, 2025 - Support for paid listings in Kingdom of Saudi Arabia (KSA) (General availability) (General availability)\")\n       - Nov 05, 2025 - Shared workspaces\n       - Nov 05, 2025 - Snowpipe Streaming with high-performance architecture on Azure (General availability)\")\n       - Nov 05, 2025 - Cortex Agents for Teams (General availability)\")\n       - Nov 04, 2025 - Interactive tables and interactive warehouses (Preview)\")\n       - Nov 04, 2025 - Performance Explorer (General availability)\")\n       - Nov 04, 2025 - Sharing semantic views (General availability)\")\n       - Nov 04, 2025 - Cortex AI Functions (GA)\")\n       - Nov 04, 2025 - Cortex AI\\_TRANSCRIBE function (GA)\")\n       - Nov 04, 2025 - Snowflake Intelligence (General availability)\")\n       - Nov 04, 2025 - Snowflake-managed MCP server (General availability)\")\n       - Nov 04, 2025 - Cortex Agents (General availability)\")\n       - Nov 04, 2025 - Snowflake Machine Learning Experiments\n       - Nov 04, 2025 - Snowflake Openflow: Snowflake Deployments (General availability)\")\n       - Oct 31, 2025 - Native App support for Google Cloud\n       - Oct 31, 2025 - Trust Center: Organization-level findings\n       - Oct 30, 2025 - Snowflake Data Clean Rooms updates\n       - Oct 29, 2025 - Guided account failover in Snowsight (General availability)\")\n       - Oct 29, 2025 - Native Apps Shareback\n       - Oct 29, 2025 - CLIENT\\_POLICY parameter for authentication policies\n       - Oct 23, 2025 - Snowflake Data Clean Rooms updates\n       - Oct 20, 2025 - Performance Explorer (Preview)\")\n       - Oct 17, 2025 - Externally managed Iceberg writes and catalog-linked databases (General availability)\")\n       - Oct 17, 2025 - Set a target file size for Iceberg tables (General availability)\")\n       - Oct 17, 2025 - Partitioned writes for Iceberg tables (General availability)\")\n       - Oct 16, 2025: Cross-region inference for US Commercial Gov\n       - Oct 16, 2025 - Snowflake Data Clean Rooms updates\n       - Oct 16, 2025 - Organization account in hybrid organizations\n       - Oct 16, 2025 - AI\\_EXTRACT function (General availability)\")\n       - Oct 15, 2025 - Directed joins (General availability)\")\n       - Oct 13, 2025 - CORTEX\\_EMBED\\_USER database role\n       - Oct 10, 2025: Cortex Search Component Scores (Preview)\")\n       - Oct 09, 2025 - Verified query suggestions (Preview)\")\n       - Oct 09, 2025 - Snowflake Data Clean Rooms updates\n       - Oct 09, 2025 - dbt Projects on Snowflake: Recent improvements (Preview)\")\n       - Oct 09, 2025 - Organization user groups with organizational listings (Preview)\")\n       - Oct 07, 2025 - Query insights in Snowsight (General availability)\")\n       - Oct 06, 2025 - Hybrid table support for Microsoft Azure (General availability)\")\n       - Oct 03, 2025 - Named scoring profiles for Cortex Search Services (General availability)\")\n       - Oct 03, 2025 - Lineage for tasks and stored procedures\n       - Oct 02, 2025 - Semantic views in Snowsight (General availability)\")\n       - Oct 02, 2025 - Snowflake-managed MCP server (Preview)\")\n       - Oct 02, 2025 - Snowflake Data Clean Rooms updates\n       - Oct 01, 2025 - New OBJECT\\_VISIBILITY property (Preview)\")\n       - Sep 30, 2025 - GRANT OWNERSHIP ON NOTEBOOK (General availability)\")\n       - Sep 30, 2025 - Semantic views: Support for derived metrics\n       - Sep 30, 2025 - Declarative sharing (Preview)\")\n       - Sep 30, 2025 - Cortex Agents for Teams (Preview)\")\n       - Sep 29, 2025 - Open Catalog: External OAuth support\n       - Sep 29, 2025 - SQL object descriptions\n       - Sep 26, 2025 - AI: COUNT\\_TOKENS() function function\")\n       - Sep 25, 2025 - Data Clean Rooms (DCR)\")\n       - Sep 25, 2025 - AI: parse document page filter\n       - Sep 25, 2025 - FILE data type (GA)\")\n       - Sep 25, 2025 - AI Translate: updates\n       - Sep 25, 2025 - Budget refresh interval\n       - Sep 23, 2025 - AI filter optimization\n       - Sep 23, 2025 - General availability of Snowpipe Streaming with high-performance architecture\n       - Sep 22, 2025 - Prevent data compaction on Snowflake-managed Iceberg tables\n       - Sep 19, 2025: Support for FedRAMP on AWS for apps with containers\n       - Sep 19, 2025: Read consistency mode\n       - Sep 19, 2025 - Position row-level deletes support when writing to remote Iceberg tables on Azure (Preview)\")\n       - Sep 19, 2025 - SnowConvert AI Verification (Preview)\")\n       - Sep 17, 2025 - Snowflake Openflow: Snowflake Deployments (Preview)\")\n       - Sep 17, 2025 - Data lineage for tasks\n       - Sep 17, 2025 - SYS\\_CONTEXT function\n       - Sep 16, 2025 - Support for Streamlit in Snowflake in the People's Republic of China\n       - Sep 15, 2025 - Snowflake Native Apps\n       - Sep 15, 2025 - Billing views for resellers\n       - Sep 15, 2025 - Multi-factor authentication: Support for one-time passcodes\n       - Sep 12, 2025 - Position row-level deletes support when writing to remote Iceberg tables on Amazon S3 or Google Cloud (Preview)\")\n       - Sep 11, 2025 - Cortex AI Functions support for dynamic tables\n       - Sep 11, 2025 - Workspaces (General availability)\")\n       - Sep 09, 2025 - Data quality in Snowsight (Preview)\")\n       - Sep 09, 2025 - Sensitive data classification\n       - Sep 09, 2025 - Hybrid table support for Microsoft Azure (Preview)\")\n       - Sep 02, 2025 - Cortex Agents: Admin object REST API (Preview)\")\n       - Sep 02, 2025 - Partitioned Iceberg writes\n       - Sep 02, 2025 - Document AI models in model registry\n       - Aug 29, 2025 - Native App support for restricted caller\u2019s rights\n       - Aug 28, 2025 - Model Registry model deployment UI\n       - Aug 28, 2025 - Monitoring events for Snowpipe\n       - Aug 28, 2025 - Hybrid table support for periodic rekeying (General availability)\")\n       - Aug 28, 2025 - Data Clean Rooms\n       - Aug 26, 2025 - Semantic views in Snowsight (Preview)\")\n       - Aug 25, 2025 - Microsoft Power Apps connector (General availability)\")\n       - Aug 22, 2025 - Organization profile updates\n       - Aug 22, 2025 - AI\\_EXTRACT\n       - Aug 21, 2025 - AI Parse Document layout mode (General availability) \")\n       - Aug 21, 2025 - Data Clean Rooms\n       - Aug 20, 2025: Distributed processing in Snowflake ML: Many Model Training and Distributed Partition Function\n       - Aug 20, 2025: Cortex Search Service replication (Preview)\")\n       - Aug 20, 2025: New stage volume implementation in Snowpark Container Services (Preview)\")\n       - Aug 19, 2025 - Trust Center email notifications (General availability)\")\n       - Aug 18, 2025 - Snowsight navigation menu updates (Gradual rollout)\")\n       - Aug 18, 2025 - Write Once, Read Many (WORM) snapshots (Preview) snapshots (Preview)\")\n       - Aug 14, 2025 - Using SQL for Cortex Powered Object Descriptions\n       - Aug 14, 2025 - Workload identity federation\n       - Aug 14, 2025 - Lineage for stored procedures (Preview)\")\n       - Aug 12, 2025 - Snowflake ML Jobs (General availability)\")\n       - Aug 12, 2025 - Support for Streamlit 1.46\n       - Aug 11, 2025 - CORS configuration to enable cross-origin requests to a Snowpark Container Services service (General availability)\")\n       - Aug 08, 2025 - Contacts\n       - Aug 07, 2025 - Snowpark Container Services batch jobs\n       - Aug 07, 2025 - Cortex Audio with AI\\_TRANSCRIBE\n       - Aug 07, 2025 - Directed joins (Preview)\")\n       - Aug 06, 2025 - Cortex Agents: admin configuration UI (Preview)\")\n       - Aug 06, 2025 - Support for custom components in Streamlit in Snowflake\n       - Aug 05, 2025 - Document AI table extraction\n       - Aug 04, 2025 - Hybrid table storage for Time Travel data\n       - Aug 01, 2025 - Snowpark Container Services in Google cloud (General availability)\")\n       - Aug 01, 2025 - Snowflake Intelligence\n       - Jul 31, 2025 - AI Observability in Snowflake Cortex (General availability)\")\n       - Jul 30, 2025 - External network access with private connectivity on Google Cloud\n       - Jul 29, 2025 - Cortex Agents integration for Microsoft Teams\n       - Jul 28, 2025 - Cortex Powered Object Descriptions\n       - Jul 28, 2025 - Snowflake OAuth single-use refresh tokens\n       - Jul 25, 2025 - Native Apps support for Snowflake ML models\n       - Jul 25, 2025 - AI\\_SENTIMENT function\n       - Jul 24, 2025 - Data Clean Rooms\n       - Jul 21, 2025 - CREATE\\_BILLING\\_EVENT and CREATE\\_BILLING\\_EVENTS system functions\n       - Jul 21, 2025 - Billing contact updates\n       - Jul 18, 2025 - Sensitive Data Classification\n       - Jul 18, 2025 - Alerts on new data\n       - Jul 18, 2025 - Externally managed Iceberg writes and catalog-linked databases\n       - Jul 17, 2025 - Data Clean Rooms\n       - Jul 16, 2025 - Track tag propagation conflicts\n       - Jul 15, 2025 - Support for Streamlit 1.45.1\n       - Jul 08, 2025 - ML Explainability visualizations\n       - Jul 08, 2025 - AI\\_EMBED multimodal embeddings\n       - Jul 07, 2025 - New CREDENTIALS view\n       - Jul 04, 2025 - Native App support for Google Cloud\n       - Jul 03, 2025 - Query insights\n       - Jul 01, 2025 - Snowflake Multi-Node ML Jobs\n       - Jun 27, 2025 - dbt Projects on Snowflake\n       - Jun 26, 2025 - Clone dynamic tables as tables\n       - Jun 24, 2025 - Premium views\n       - Jun 23, 2025 - Snowflake Native App Framework\n       - Jun 19, 2025 - Integration support in Snowsight\n       - Jun 19, 2025 - Data Clean Rooms\n       - Jun 18, 2025 - Preconfigured Notebook Runtimes\n       - Jun 16, 2025 - Budgets\n       - Jun 12, 2025 - Data Clean Rooms\n       - Jun 03, 2025 - Snowflake Copilot inline\n       - Jun 03, 2025 - Workspaces\n       - Jun 02, 2025 - AI\\_CLASSIFY function\n       - Jun 02, 2025 - Cortex AI Functions preview\n       - Jun 01, 2025 - Snowsight Templates\n       - May 30, 2025 - Snowflake Openflow (General availability)\")\n       - May 30, 2025 - New models for Cortex AI Images\n       - May 30, 2025 - Request Approval Workflow\n       - May 30, 2025 - Tags in Standard Edition\n       - May 29, 2025 - Table extraction in Document AI\n       - May 29, 2025 - Data Clean Rooms\n       - May 28, 2025 - Organization users\n       - May 27, 2025 - Data sharing in Kingdom of Saudi Arabia region\n       - May 27, 2025 - Support for Azure Private link in Snowflake Native Apps\n       - May 27, 2025 - Multi-factor authentication\n       - May 23, 2025 - Notebooks st.secrets support for Warehouse and Container Runtimes\n       - May 22, 2025 - New Snowsight navigation preview\n       - May 22, 2025 - Data Clean Rooms\n       - May 21, 2025 - Snowflake Openflow\n       - May 20, 2025 - Snowflake Copilot model level RBAC\n       - May 20, 2025 - Snowflake Openflow\n       - May 20, 2025 - Snowpark Container Services preview available in Google Cloud\n       - May 20, 2025 - Contacts for objects\n       - May 19, 2025 - JSON schema references for COMPLETE Structured Output\n       - May 19, 2025 - Data Connector for Container Runtime\n       - May 19, 2025 - Notebooks Container Runtime on Azure and Azure Private Link\n       - May 16, 2025 - Universal Search: support for pipes, tasks, and streams\n       - May 16, 2025 - Cost Anomalies\n       - May 15, 2025 - Organizational listings: discovery and access\n       - May 14, 2025 - Automatic tag propagation\n       - May 13, 2025 - Support for Streamlit 1.44.0\n       - May 08, 2025 - Dynamic tables: Support for IS\\_ROLE\\_IN\\_SESSION\n       - May 08, 2025 - Document AI\n       - May 05, 2025 - Provisioned Throughput for Snowflake Cortex\n       - May 05, 2025 - Generation 2 standard warehouses\n       - May 01, 2025 - Dynamic tables: Filter by current date or time\n       - May 01, 2025 - Data Clean Rooms\n       - Apr 30, 2025 - Programmatic access tokens\n       - Apr 28, 2025 - Disable reranking in Cortex Search queries\n       - Apr 28, 2025 - Boost Cortex Search results based on metadata signals\n       - Apr 28, 2025 - Snowflake Cortex LLM Role-based Access Control\n       - Apr 24, 2025 - Multi-Node Container Runtime\n       - Apr 24, 2025 - Data Clean Rooms\n       - Apr 22, 2025 - Trust Center email notifications\n       - Apr 18, 2025 - Support for st.query\\_params\n       - Apr 17, 2025 - Semantic views\n       - Apr 17, 2025 - Data Clean Rooms\n       - Apr 16, 2025 - Snowflake ML Jobs\n       - Apr 16, 2025 - Document AI\n       - Apr 15, 2025 - Snowflake Egress Cost Optimizer\n       - Apr 15, 2025 - Search optimization improves the performance of queries containing scalar functions\n       - Apr 15, 2025 - Cortex AI ENTITY\\_SENTIMENT function\n       - Apr 14, 2025 - Support for Streamlit 1.42.0\n       - Apr 14, 2025 - EMBED function added to the Cortex LLM REST API\n       - Apr 14, 2025 - Mistral AI's Pixtral Large multimodal LLM available in Cortex AI\n       - Apr 14, 2025 - Cortex COMPLETE multimodal (image) support support\")\n       - Apr 14, 2025 - FILE data type\n       - Apr 14, 2025 - PROMPT helper function\n       - Apr 11, 2025 - Snowsight replication configuration and monitoring GA\n       - Apr 10, 2025 - Data Clean Rooms\n       - Apr 07, 2025 - Google Cloud Private Service Connect in Streamlit in Snowflake\n       - Apr 04, 2025 - AI Observability in Snowflake Cortex\n       - Apr 04, 2025 - Cortex COMPLETE Structured Outputs - GA\n       - Mar 31, 2025 - Access history: Support for joins\n       - Mar 27, 2025 - Git integration and multi-file editing in Streamlit in Snowflake\n       - Mar 27, 2025 - Data Clean Rooms\n       - Mar 26, 2025 - Multiple semantic models for Cortex Analyst - GA\n       - Mar 24, 2025 - Support for st.experimental\\_audio\\_input and st.camera\\_input\n       - Mar 20, 2025 - Cortex Powered Object Descriptions\n       - Mar 20, 2025 - Snowflake ML Datasets\n       - Mar 19, 2025 - Additional file formats for Cortex AI Parse Document\n       - Mar 19, 2025 - Alerts on new data\n       - Mar 17, 2025 - Notebooks on SPCS - GA\n       - Mar 17, 2025 - Document AI\n       - Mar 12, 2025 - Support for st.file\\_uploader\n       - Mar 07, 2025 - Resource constraints for Snowpark-optimized warehouses - GA\n       - Mar 06, 2025 - OCR mode of PARSE\\_DOCUMENT - GA\n       - Mar 05, 2025 - Snowpark Container Services support for application metrics\n       - Mar 05, 2025 - Search optimization improves the performance of queries containing scalar subqueries\n       - Mar 04, 2025 - Universal Search ML model support\n       - Mar 03, 2025 - Collapsible navigation bar in Snowsight\n       - Mar 03, 2025 - Cortex Document Processing Usage History\n       - Mar 03, 2025 - Native Apps with Snowpark Container Services - support for AWS PrivateLink\n       - Mar 03, 2025 Native Apps with Snowpark Container Services - support for Azure Private Link\n       - Feb 28, 2025 - Increased MAX\\_CLUSTER\\_COUNT limits for multi-cluster warehouses\n       - Feb 27, 2025 - Snowflake Native Apps release channels\n       - Feb 27, 2025 - Data Clean Rooms\n       - Feb 26, 2025 - Generating connection settings for a client, driver, library, or third-party application\n       - Feb 24, 2025 - Native Apps Snowsight changes\n       - Feb 19, 2025 - Auto-suspend on SPCS Model Serving\n       - Feb 14, 2025 - Support for st.file\\_uploader\n       - Feb 14, 2025 - Document AI\n       - Feb 13, 2025 - SPCS Model Serving on Azure\n       - Feb 11, 2025 - Cortex COMPLETE Structured Outputs\n       - Feb 07, 2025 - Support for material icons\n       - Feb 07, 2025 - Snowflake Cortex Fine-tuning - GA\n       - Feb 03, 2025 - Snowflake Native Apps with Snowpark Container Services on Azure - GA\n       - Jan 31, 2025 - Support for future grants in Streamlit in Snowflake\n       - Jan 27, 2025 - Organization account - GA\n       - Jan 23, 2025 - Document AI on GCP - GA\n       - Jan 20, 2025 - AWS PrivateLink in Snowflake Native Apps with Snowpark Container Services - Preview\n       - Jan 16, 2025 - Snowsight enhancements to contact email management\n       - Jan 15, 2025 - Optimized COPY and INSERT bulk loads on empty hybrid tables\n       - Jan 15, 2025 - Custom instructions in Cortex Analyst\n       - Jan 07, 2025 - Snowflake Cortex AI LLM Playground\n       - Jan 06, 2025 - Notebooks warehouse runtime: AWS PrivateLink and Azure Private Link support - GA\n   * 2024 server release notes and feature updates\n   * 2023 server release notes and feature updates\n   * Release notes from 2015 - 2022\n6. Client, driver, and library release notes\n\n   - Monthly release notes\n   - Client versions & support policy\n7. Snowflake Connector release notes\n\n   - Snowflake Connector for Google Analytics Raw Data\n   - Snowflake Connector for Google Analytics Aggregate Data\n   - Snowflake Connector for ServiceNow V2\n   - Snowflake Connector for MySQL\n   - Snowflake Connector for PostgreSQL\n   - Snowflake Connector for Sharepoint\n   - Native SDK for Connectors\n\n     - Native SDK for Connectors Java library\n     - Native SDK for Connectors Java Test library\n     - Native SDK for Connectors Java Template\n     - Native SDK Example Java GitHub Connector\n9. Behavior changes\n10. Behavior change announcements\n12. Recent improvements\n13. Performance improvements\n14. SQL improvements\n16. Feature information\n17. Preview features\n\nRelease notesSnowflake server release notes and feature updates2025 server release notes and feature updatesFeature updatesJun 23, 2025 - Snowflake Native App Framework\n\nJun 23, 2025: Snowflake Native App Framework updates\u00b6\n========================================================================================================\n\nThe Snowflake Native App Framework now includes the following features that make it easier for providers\nto develop an app to create objects in a consumer account. These features also\nmake it easier for consumers to configure an app during installation and upgrade.\n\nFor general information on these features, see Create and access objects in a consumer account.\n\nAutomated granting of privileges (*Preview*)\u00b6\n-----------------------------------------------------------------------------------------------------------------\n\nThis feature allows providers to specify in the manifest file the privileges required by an app.\nWhen a consumer installs or upgrades an app, Snowflake grants these privileges to the app. For\nmore information, see Configure the privileges required by an app.\n\nConsumers can use feature policies to override the automatic grants for an app. For more\ninformation, see Use feature policies to limit the objects an app can create.\n\nApp specifications (*Preview*)\u00b6\n-------------------------------------------------------------------------------------\n\nApp specifications allow providers to request permission from the consumer to allow connections\noutside Snowflake that use external access integrations or security integrations. Consumers must\napprove the app specification for these objects when configuring the app after installation or\nupgrade.\n\nFor more information, see Overview of app specifications.\n\nWas this page helpful?\n\nYesNo\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Automated granting of privileges (Preview)\n2. App specifications (Preview)"
      ],
      "full_content": null
    },
    {
      "url": "https://www.snowflake.com/en/product/features/native-apps/",
      "title": "Snowflake Native Apps",
      "publish_date": null,
      "excerpts": [
        "## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n## Privacy Preference Center\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\n* checkbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    },
    {
      "url": "https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/",
      "title": "FinOps on Snowflake: Built-In Cost and Performance Control",
      "publish_date": "2026-01-01",
      "excerpts": [
        "Data for Breakfast Around the World\n\nDrive impact across your organization with data and agentic intelligence.\n\n[register now](https://www.snowflake.com/en/data-for-breakfast/)\n\npricing options\n\noverview\n\ncost & performance optimization\n\npricing calculator\n\n###### Resources\n\n[Pricing calculator overview](https://www.snowflake.com/en/pricing-options/calculator/overview/) [Pricing calculator FAQs](https://www.snowflake.com/en/pricing-options/calculator/feedback-faqs/) [Snowflake Performance Index](https://www.snowflake.com/en/pricing-options/performance-index/)\n\n[_Image Source_](https://squadrondata.com/org-impact-comparison-spark-based-saas-vs-snowflake/)\n\n###### COST MANAGEMENT AND PERFORMANCE OPTIMIZATION\n\n# FinOps on Snowflake\n\nTime is money \u2013 save both with Snowflake.\n\n[explore the economic impact of snowflake](https://www.snowflake.com/resource/forrester-the-total-economic-impact-of-the-snowflake-ai-data-cloud/?utm_cta=website-cost-and-performance-forrester-tei)\n\n## Save time on platform management. Spend it on what matters more.\n\n## Go from painstaking configurations to a proven, fully-managed service\n\nSince its founding in 2012, Snowflake has provided automated cluster management, maintenance and upgrades \u2014 all without downtime \u2014 so you can spend time on valuable data projects\n\nGet **out-of-the-box governance and security through Snowflake Horizon Catalog** without extra configurations or protocols\n\n## Go from piecemeal dashboards to built-in cost & performance management\n\n* Get granular visibility, control and optimization of Snowflake spend through a unified Cost Management Interface .\n* Check query performance easily to proactively save on costs.\n* Automatically benefit from regular rollouts of performance improvements across all workloads.\n\n## Maximize your Snowflake spend\n\n* Add flexibility in how you use funds committed in your Snowflake Capacity contract.\n* Deploy partner solutions faster by simplifying finance and procurement processes.\n* Bundle your spend to increase your buying power with Snowflake and partners.\n\nlearn more\n\n###### OUR CUSTOMERS\n\n## Saving time on platform admin. Getting to market faster.\n\nTravelpass CTC Natwest\n\nTravel and Hospitality \u201cNow, we aren\u2019t so focused on how to build things. We are focused more on what to build.\u201d Dan Shah  \nManager of Data Science Read the story * **1 week** for 130 Dynamic Tables to be in production after migration\n* **65%** cost savings switching from Databricks to Snowflake\n\nRead the case study Financial Services \u201cNow with fewer ephemeral failures and higher visibility in Snowflake, we have a platform that\u2019s much easier and cost-effective to operate than managed Spark.\u201d David Trumbell  \nHead of Data Engineering, CTC Read the story * **1st** data availability deadline was hit everyday for the 1st time\n* **54%** cost savings switching from managed Spark to Snowflake\n\nRead the case study Financial Services \u201cThe speed at which we\u2019ve delivered wouldn\u2019t have been possible with other providers.\u201d Kaushik Ghosh Dastidar  \nHead of ESG Cloud Solutions, NatWest Read the story * **6x** reduction in onboarding time from 3 months to 2 weeks\n* **$750K** saved in salaries & staff training costs\n\nRead the case study\n\n[Resource #### Snowflake Joins the FinOps Foundation Snowflake joins The FinOps Foundation as a Premier Enterprise Member to provide thought leadership and set industry financial best practices. Read more](https://www.finops.org/members/snowflake/)\n\nResource #### Snowflake Pricing Calculator Curious about Snowflake pricing? Our Snowflake pricing calculator shows credit usage, warehouse costs, and total expenses. Access calculator\n\n[Guide #### Definitive Guide to Managing Spend in Snowflake Learn about considerations for consumption models such as Snowflake's, frameworks for better managing spend, and more. Get the guide](https://www.snowflake.com/en/resources/white-paper/definitive-guide-to-managing-spend-in-snowflake/)\n\n## Even More To Explore\n\n#### Snowflake Documentation\n\nAccess documentation on Managing Costs and Optimizing Performance in Snowflake.\n\n[Read about Managing Costs](https://docs.snowflake.com/en/user-guide/cost-management-overview)\n\n[Read about Optimizing Performance](https://docs.snowflake.com/en/guides-overview-performance)\n\n#### On-Demand Cost Governance Training\n\nLearn how to successfully examine, control, and optimize Snowflake costs.\n\n[Register Now](https://learn.snowflake.com/en/courses/OD-FINOPS/)\n\n#### Professional Services\n\nEngage Snowflake\u2019s Professional Services for expert advice on optimizing your use of Snowflake.\n\n[Discover Professional Services](https://www.snowflake.com/snowflake-professional-services/)\n\n#### Priority Support\n\nLearn more about how our Priority Support team can help you reduce consumption spend through performance monitoring, observability, and management.\n\n[Learn about Priority Support](https://www.snowflake.com/support/priority-support/)\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\n[start for free](https://signup.snowflake.com/)\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\n\nLearn * Resource Library\n* Live Demos\n* Fundamentals\n* Training\n* Certifications\n* Snowflake University\n* Developer Guides\n* Documentation\n\n* Privacy Policy\n* Site Terms\n* Communication Preferences\n* Cookie Settings\n* Do Not Share My Personal Information\n* Legal\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")\n\n\\* Private preview, <sup>\u2020</sup> Public preview, <sup>\u2021</sup> Coming soon"
      ],
      "full_content": null
    },
    {
      "url": "https://www.flexera.com/blog/finops/snowflake-native-apps/",
      "title": "Snowflake Native Apps 101: Build and monetize data apps (2026)",
      "publish_date": "2026-01-27",
      "excerpts": [
        "Flexera Open Primary Navigation\n\n* Solutions\n  \n  Spend management by vendor\n  \n  [Flexera is a Leader in 2025 cloud financial management tools](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\n  \n  Discover recognized CFM vendors to watch in the 2025 Gartner\u00ae Magic Quadrant\u2122\n  \n  [View report](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\n* Products\n  \n  Flexera One\n  \n    + IT Visibility\n    + ITAM\n    + Snow Atlas\n    + Cloud License Management\n    + SaaS Management\n  \n    + FinOps\n    + Cloud Cost Optimization\n    + Cloud Commitment Management\n    + Container Optimization\n    + Virtual Machine Optimization\n    + Data Cloud Optimization\n  \n    + Application Readiness\n    + Security\n    + Integrations\n    + Technology Intelligence Platform\n    + All Products\n  \n  [Introducing Flexera One SaaS Management](https://www.flexera.com/products/flexera-one/saas-management)\n  \n  Discover comprehensive SaaS visibility for taming SaaS sprawl, wasted spend and compliance risks.\n  \n  [Book your personalized demo](https://www.flexera.com/products/flexera-one/saas-management)\n* Success\n  \n  Customer Success\n  \n  Services & Training\n  \n    + Services\n    + Training\n  \n  Support\n  \n    + [Flexera support portal](https://community.flexera.com/s/support-hub)\n    + [Flexera product documentation](https://docs.flexera.com)\n    + [Snow product documentation](https://docs.snowsoftware.io/)\n  \n    + Technology Intelligence Awards\n    + [Flexera community](https://community.flexera.com/s/)\n  \n  [2025 Technology Intelligence Awards](https://info.flexera.com/ITAM-REPORT-State-of-IT-Asset-Management)\n  \n  The results are in\u2014see how our 2025 winners and honorable mentions are shaping the future of Technology Intelligence.\n  \n  [See the winners](https://www.flexera.com/customer-success/awards)\n* Resources\n  \n  Resources\n  \n    + Webinars\n    + Videos\n    + Datasheets\n    + Whitepapers & reports\n  \n    + Blog\n    + Case studies\n    + Events\n    + Analyst research\n    + Glossary\n    + Demos & trials\n    + Business value calculator\n  \n  [Flexera 2026 IT Priorities Report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)\n  \n  AI ROI, sustainability, cost and risk: Discover the latest IT trends shaping tomorrow\u2019s IT landscape in Flexera\u2019s 2026 IT Priorities Report.\n  \n  [View report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)\n* About\n  \n  Company\n  \n    + About\n    + Careers\n    + Contact us\n    + Leadership\n  \n  Partners\n  \n    + Partner program\n    + Partner locator\n  \n  Press center\n  \n    + Press releases\n    + Articles\n    + Awards\n  \n  Social responsibility\n  \n    + ESG\n    + Belonging and inclusion\n  \n  [The Flexera 2025 State of the Cloud Report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)\n  \n  How are GenAI rising cloud costs, security and sustainability shaping your cloud strategies in 2025?\n  \n  [View report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)\n\nCustomers Open External Links\n\n* [Community](https://community.flexera.com/)\n* [Product login](https://app.flexera.com/login)\n* [Spot login](https://console.spotinst.com/auth/signIn)\n* [Partner Portal](https://partnerhub.flexera.com/)\n\nSearch\n\nBook a demo\n\n1. Home\n2. Blog\n3. [FinOps](https://www.flexera.com/blog/finops/)\n4. Snowflake Native Apps 101: Build and monetize data apps (2026)\n\n### [FinOps](https://www.flexera.com/blog/finops/)\n\nSubscribe\n\nTopics\n\nSaaS Management FinOps IT Visibility IT Asset Management Product News Application Readiness Security Perspectives\n\n[FinOps](https://www.flexera.com/blog/finops/)\n\n# Snowflake Native Apps 101: Build and monetize data apps (2026)\n\n[](https://www.flexera.com/blog/author/pramit/ \"Pramit Marattha\")\n\n[Pramit Marattha](https://www.flexera.com/blog/author/pramit/ \"Pramit Marattha\")\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-native-apps%2F&title=Snowflake%20Native%20Apps%20101%3A%20Build%20and%20monetize%20data%20apps%20%282026%29&source=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-native-apps%2F) [](https://twitter.com/intent/tweet?source=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-native-apps%2F&text=Snowflake%20Native%20Apps%20101%3A%20Build%20and%20monetize%20data%20apps%20%282026%29%20https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-native-apps%2F) \n\nThis post originally appeared on the chaosgenius.io blog. Chaos Genius has been [acquired by Flexera](https://www.flexera.com/more/ProsperOps-Chaos-Genius) .\n\nBuilding and deploying [data applications](https://www.snowflake.com/guides/applications/) often comes with unnecessary complexity. Snowflake has significantly reduced this by introducing **Snowflake Native Apps** . Snowflake Native Apps allow you to build, run and monetize data apps directly within the Snowflake ecosystem. No external integrations. No infrastructure headaches. Snowflake previously supported [Connected Apps](https://www.snowflake.com/guides/connected-apps) , which operated outside Snowflake, connected via [APIs](https://docs.snowflake.com/en/api-reference) and required manual data movement. Native Apps, by contrast, run entirely within Snowflake, leveraging its secure and scalable infrastructure . They\u2019re built using the _Snowflake Native App Framework_ , allowing you to develop secure, scalable and integrated Snowflake apps that leverage Snowflake\u2019s environment without leaving the Snowflake platform.\n\nIn this article, we will discuss the features, benefits and inner workings of Snowflake Native Apps and walk you through the step-by-step process of creating a Snowflake Native App and publishing and monetizing your Native Apps through the Snowflake Marketplace.\n\n## What Are Snowflake Native Applications?\n\n**Snowflake Native Apps** are applications built using the Snowflake Native App Framework, which allows developers to create, test and deploy applications directly within Snowflake\u2019s Data Cloud. These Native Apps leverage Snowflake\u2019s core features like stored procedures , user-defined functions (Snowflake UDFs) and the [Snowpark API](https://docs.snowflake.com/en/developer-guide/snowpark/index) , all while keeping the data secure by running the code on your data stored in Snowflake. Native apps are available on the Snowflake Marketplace . You can discover and install them quickly, just like downloading an app on your smartphone.\n\n**TL;DR:  \n** **What They Are** \u2014 Snowflake Native Apps run entirely within Snowflake, eliminating the need to move data out for processing.  \n**How They Work** \u2014 Snowflake Native Apps operate in your Snowflake account, using your existing data without external connections. Developers create and distribute them via the Snowflake Marketplace.  \n**Why Use Them** :\n\n* **Fast** \u2014 Snowflake Native Apps process data directly in Snowflake, reducing latency.\n* **Secure** \u2014 Snowflake Native Apps don\u2019t access or store data outside your account, relying on Snowflake\u2019s encryption and access controls.\n* **Scalable** \u2014 Snowflake Native Apps can grow alongside your Snowflake resources, handling larger datasets or more users.\n\nSnowflake Native Apps (Source: [Snowflake](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/) )\n\n### Key Technical Characteristics of Snowflake Native Apps\n\n#### 1) **Native Integration with Snowflake Services**\n\nSnowflake Native Apps work directly with Snowflake\u2019s core services. They use stored procedures , user-defined functions (Snowflake UDFs UDFs) and the Snowpark API, making them efficient and seamless.\n\n#### 2) **Simplified Development and Testing**\n\nYou can build Snowflake Native Applications using the Snowflake Native App Framework. Snowflake Native App Framework streamlines development and testing. You can create, test and deploy Snowflake apps within Snowflake, reducing development time.\n\n#### 3) **Monetization via Snowflake Marketplace**\n\nProviders can list and sell Snowflake apps in the Snowflake Marketplace . Consumers install these apps directly into their Snowflake accounts, simplifying deployment and making app monetization straightforward.\n\n#### 4) **Security and Governance**\n\nSnowflake Native Applications don\u2019t transfer data outside the platform. Providers can package their logic securely, protecting intellectual property. Users maintain control of access permissions, with data security managed by Snowflake\u2019s encryption and governance features .\n\n#### 5) **Data Sharing**\n\nSnowflake Native Apps leverage [Snowflake\u2019s secure data sharing](https://docs.snowflake.com/en/user-guide/data-sharing-intro) . Apps can access shared datasets without creating duplicates or requiring data to leave the user\u2019s Snowflake account.\n\n#### 6) **Advanced Feature Support**\n\nSnowflake Native Applications use Snowflake features like:\n\n* User-Defined Functions (Snowflake UDFs)\n* Stored Procedures\n* [Snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/index)\n\n#### 7) **Versioning and Patching**\n\nSnowflake Native App Framework supports versioning and patching, allowing you to manage updates easily. This keeps your apps up to date.\n\n#### 8) **Streamlit Integration**\n\nYou can integrate your apps with Streamlit , which allows you to create interactive dashboards within Snowflake. While the integration is still evolving, it supports embedding visualizations in your apps for end-user analytics.\n\n#### 9) **Encapsulation in Application Packages**\n\nSnowflake Native Apps are bundled into packages containing all necessary components (e.g., data content, application logic, metadata and setup scripts). This makes deployment simple and reduces compatibility issues.\n\n#### 10) **Source Control and Tool Integration**\n\nYou can integrate with external tools like [IDEs](https://en.wikipedia.org/wiki/Integrated_development_environment) , [CI/CD pipelines](https://www.redhat.com/en/topics/devops/what-cicd-pipeline) and source control systems. This flexibility helps teams adopt DevOps practices when developing Snowflake Native Applications.\n\n#### 11) **AI and ML Workflows**\n\nSnowflake Native Applications support machine learning and AI tasks through Snowpark. You can integrate external ML libraries, process training data and deploy models directly within Snowflake.\n\n#### 12) **Cross-Cloud Deployment (AWS + Azure + with limitations on GCP)**\n\nSnowflake Native Applications are compatible with multiple clouds ( [AWS](https://www.snowflake.com/en/why-snowflake/partners/all-partners/aws/) , [Azure](https://www.snowflake.com/en/why-snowflake/partners/all-partners/microsoft/) and limit on [GCP](https://www.snowflake.com/technology-partners/google-cloud-platform/) ). This feature enables users to run apps across different infrastructures, though GCP support has limitations.\n\n## What Are the Benefits of Snowflake Native Apps \u2014 For Providers?\n\nSnowflake Native Apps offer significant advantages for providers looking to build, distribute and monetize their applications within the Snowflake ecosystem. Here are some of the benefits:\n\n### \u27a5 Simplified Development\n\nProviders can use Snowflake\u2019s tools to build apps that leverage Snowflake\u2019s high availability and scalability. This completely eliminates the need for separate infrastructure, reducing development overhead and speeding up time to market\u200b.\n\nSnowflake Native Apps run within customers\u2019 Snowflake accounts, meaning data stays within their environment, which minimizes the complexity of data integration and security management\u200b.\n\n### \u27a5 **On-Platform Monetization Opportunities**\n\nProviders can easily sell and monetize their Snowflake apps directly within the Snowflake ecosystem via Snowflake Marketplace , bypassing the need for third-party systems.\n\n### \u27a5 **Easy Management**\n\nSince apps are hosted within Snowflake\u2019s infrastructure, you don\u2019t need to manage separate resources. Snowflake offers always-on availability, global reach, built-in governance and secure operations.\n\n### \u27a5 **Data Protection**\n\nSnowflake Native Apps interact directly with the customer\u2019s Snowflake account. This approach keeps customer data within their environment, reducing security risks and compliance burdens. You avoid the complexity of managing sensitive data, as Snowflake\u2019s internal security and governance capability can handle it for you.\n\n### \u27a5 **Zero Infrastructure Management and Lower Operational Costs**\n\nTraditional Snowflake apps often require providers to pay for their own compute and storage. With Snowflake Native Applications, you leverage the customer\u2019s compute resources, reducing your operating costs and improving profit margins.\n\n### \u27a5 **Access to New Customers**\n\nSnowflake Marketplace gives you exposure to a global Snowflake customer base. Here, customers can find, test and purchase your apps. This built-in distribution network simplifies how you reach and onboard users while enabling seamless deployment directly into customer environments.\n\n## What Are the Benefits of Snowflake Native Apps \u2014 For Consumers?\n\nSnowflake Native Apps offer significant advantages for end-users, enhancing their ability to integrate, utilize and manage data applications seamlessly within the Snowflake ecosystem. Here are some of the key benefits of Native Snowflake apps for consumers:\n\n### \u27a5 **Quick and Easy Access**\n\nConsumers can easily search for Snowflake Native Applications on the Snowflake Marketplace or in private listings and directly install and use them with a single click.\n\n### \u27a5 **Secure Data Use**\n\nSince Snowflake Native Applications run directly in the consumer\u2019s Snowflake account, data does not need to leave the platform. This eliminates the risks associated with data transfers and ensures compliance with Snowflake\u2019s robust governance and security controls.\n\n### \u27a5 **Enhanced Performance**\n\nSnowflake Native apps utilize the consumer\u2019s Snowflake compute resources, which ensures optimal performance tailored to the existing workload. Consumers benefit from Snowflake\u2019s underlying scalability and processing power, resulting in faster query execution and application responsiveness\u200b.\n\n### \u27a5 **Streamlined Trial and Deployment**\n\nConsumers can easily test applications via trial periods. Transitioning from trial to full versions is simple and data generated during the trial is retained for future use, making the adoption process seamless and risk-free.\n\n### \u27a5 **Simplified Application Management**\n\nConsumers can manage access privileges, event logging and app-related tasks directly within their Snowflake account.\n\n## How Do Snowflake Native Applications Work?\n\nSnowflake Native Applications leverage the _Snowflake Native App Framework_ to build and deploy data-driven applications directly within the Snowflake ecosystem. These Snowflake apps harness Snowflake\u2019s core features\u2014secure data sharing, analytics, compute and governance\u2014enabling seamless integration and monetization, without requiring data to move outside the platform. The framework supports applications ranging from analytical tools to fully containerized services.\n\nSnowflake Native App Framework allows:\n\n* Providers to share data, business logic and application interfaces (e.g., Streamlit apps, stored procedures) using [Snowpark API](https://docs.snowflake.com/en/developer-guide/snowpark/index) , [Python](https://www.python.org/) , [SQL](https://www.w3schools.com/sql/) and [JavaScript](https://www.w3schools.com/js/) .\n* Applications to be listed as free or paid offerings on the Snowflake Marketplace or shared privately with select accounts.\n* Developers to benefit from streamlined testing environments, version control via external repositories and detailed logging for troubleshooting.\n* Built-in support for structured and unstructured event logging to streamline troubleshooting and performance tracking.\n* Integration with Streamlit to build interactive, user-friendly visual interfaces.\n\nOn top of that, the Snowflake Native Framework also provides an enhanced developer experience, including:\n\n* A unified testing environment for app lifecycle management.\n* Integration with version control systems for seamless code and resource management.\n* Incremental app updates with versioning and patch support.\n\nSnowflake Native Apps extend the functionality of features like Secure Data Sharing and Collaboration, ensuring providers can offer scalable, governed applications to other Snowflake users.\n\n### Architecture of the Snowflake Native App Framework\n\nThe architecture of the Snowflake Native App Framework operates on a provider-consumer model:\n\n* **Provider** \u2014 Creates and shares data and application logic using the framework.\n* **Consumer** \u2014 Installs and interacts with applications shared by providers.\n\nSnowflake Native Applications are packaged as **Application Packages** , which contains the necessary logic, metadata and configuration to deploy a Snowflake Native App. This includes:\n\n* **Manifest file** : Configuration details, including setup script locations and versioning.\n* **Setup script** : Contains SQL commands for installation and updates.\n\nThe provider publishes the Snowflake Native app via:\n\n* **Marketplace Listings** \u2014 Accessible to all Snowflake users for broad distribution.\n* **Private Listings** \u2014 Targeted sharing with specific accounts across regions.\n\nUpon installation, Snowflake creates a corresponding database object for the app, which runs the setup script to establish necessary resources in the consumer\u2019s account. Additional configurations like logging or privilege grants can be applied post-installation.\n\nSnowflake Native App Architecture\n\n### How do Snowflake Native Applications work with Snowpark Container Services?\n\nFor advanced use cases, Snowflake Native Applications can utilize [**Snowpark Container Services**](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) , which enable Snowflake apps to manage containerized workloads within Snowflake. This approach supports high-performance applications, such as machine learning and AI-driven analytics, without externalizing data.\n\nComponents unique to containerized Snowflake apps:\n\n* **Services specification file** \u2014 Applications reference container images stored in the provider\u2019s repository.\n* **Compute pool** \u2014 A collection of virtual machine nodes where containerized workloads execute.\n\nSnowflake Native App with Snowpark Container Architecture\n\nFeatures of Snowpark Container Services include:\n\n* Support for AI/ML workloads with low latency.\n* Persistent data and model storage close to compute resources for efficiency.\n* Seamless deployment across GPU/CPU configurations.\n\n## Step-by-Step Guide to Create a Snowflake Native App\n\nNow that we have covered the features, benefits and inner workings/architecture of Snowflake Native Applications, let\u2019s dive into a step-by-step guide on how you can create a simple Snowflake Native App on Snowflake.\n\nBefore you start building a Snowflake Native App, make sure you meet these requirements:\n\n## Prerequisite:\n\n* **Snowflake CLI** : Install the Snowflake CLI on your machine.\n* [**Visual Studio Code**](https://code.visualstudio.com/) : If you haven\u2019t already installed VSCode, download it from here.\n* **ACCOUNTADMIN Role** : Use the ACCOUNTADMIN role for all steps in this tutorial.\n* **Second Snowflake Account** : If installing your app from a private listing, you need access to a second Snowflake account.\n* **Set a Current Warehouse** : Use the USE WAREHOUSE command to specify the active warehouse.\n* **SQL Command Session** : Run all SQL commands within the same session. The session context must remain consistent as the steps build on one another.\n\nLet\u2019s kick off by covering the main pieces you\u2019ll be working with.\n\n**Application Package**\n\nThink of this as a container for your entire application. It holds:\n\n* Shared data\n* Application logic\n* Deployment artifacts\n\n**Setup Script**\n\nThe blueprint of your application\u2019s initialization. This script:\n\n* Creates schemas\n* Sets up application roles\n* Defines initial objects and permissions\n\n**Manifest File**\n\nYour app\u2019s configuration metadata. It tells Snowflake:\n\n* Which files to include\n* How to structure the deployment\n* Runtime behaviors\n\n**Project Definition File**\n\nDefines deployment specifics:\n\n* Application package name\n* Stages\n* Artifacts to include\n\n### **Step 1** \u2014Configuring the Environment\n\n```\nCREATE DATABASE IF NOT EXISTS my_app_db;\nUSE DATABASE my_app_db;\nCREATE SCHEMA IF NOT EXISTS my_app_schema;\n```\n\nThis sets up a dedicated space for your app\u2019s data and logic.\n\nNow make sure you have an active warehouse set for executing your SQL commands. You can create a new warehouse or use an existing one:\n\n```\nCREATE WAREHOUSE IF NOT EXISTS <warehouse> WITH WAREHOUSE_SIZE = 'SMALL';\nUSE WAREHOUSE <warehouse>;\n```\n\n### **Step 2** \u2014Setting Up Snowflake CLI\n\nTo get started, you need to install and configure Snowflake CLI. It allows users to execute SQL queries and carry out a wide range of DDL and DML operations.\n\nTo download Snowflake CLI, first download from the [SnowSQL download page](https://www.snowflake.com/en/developers/downloads/snowsql/) and then open a new terminal window. Execute the following code to test your connection:\n\n```\nsnow connection add\n```\n\nOnce you have done that, enter all the credentials when prompted.\n\nAdding Snowflake CLI credentials\n\nThat\u2019s it!\n\n> For detailed instructions on installing Snowflake CLI, [refer to the official documentation](https://docs.snowflake.com/developer-guide/snowflake-cli/installation/installation) .\n> \n> \n\n### **Step 3** \u2014Initializing a New Project\n\nNow that your environment is set up, you will initialize a new project folder for your app using the Snowflake CLI. Open your terminal and execute:\n\n```\nsnow init --template app_basic snowflake_native_app_demo\n```\n\nInitializing Snowflake Native App project\n\nThis command creates a directory named **snowflake\\_native\\_app\\_demo** , which contains a basic structure for your Snowflake Native App project.\n\nFolder structure of Demo Snowflake Native App\n\nHere\u2019s what is inside the app directory:\n\nFolder structure of Demo Snowflake Native App\n\n**OR**\n\nYou can clone the starter project which is provided by Snowflake by running the following command:\n\n```\ngit clone https://github.com/Snowflake-Labs/sfguide-getting-started-with-native-apps.git\n```\n\nCloning Starter Snowflake Native App project\n\n### **Step 4** \u2014Creating Application Files\n\nNavigate to the **snowflake\\_native\\_app\\_demo** directory and create several essential files:\n\n**\u27a5 Configuring Setup Script**\n\nThis SQL script runs automatically when a consumer installs your app. Modify **app/setup\\_script.sql** with the following content (add comment for now, we will update the script later on):\n\n```\n-- Snowflake Native App Setup script\n```\n\n**\u27a5 Configuring Manifest File**\n\nThis YAML file contains configuration information about your app. Create/Update **app/manifest.yml** with this:\n\n**\u27a5 Configuring Project Definition File**\n\nThis YAML file defines objects that can be deployed to Snowflake. Create/Update **snowflake.yml** in the root of your project with this:\n\n```\ndefinition_version: 2\nentities:\n\t\tsnowflake_native_app_demo_package:\n\t\t\ttype: application package\n\t\t\t stage: stage_content.hello_snowflake_stage\n\t\t\t manifest: app/manifest.yml\n\t\t\t identifier: snowflake_native_app_demo_package\n\t\t\tartifacts:\n\t\t\t\t - src: app/*\n\t\t\t\t\t\tdest: ./\n\t demo_snowflake_native_app:\n\t\t\t type: application\n\t\t\tfrom:\n\t\t\t\t\ttarget: snowflake_native_app_demo_package\n\t\t\t debug: false\n```\n\nThis file is central to setting up a Snowflake Native App. It specifies key details about the app\u2019s configuration and how resources are managed within Snowflake.\n\nTo create an application package, you need the [CREATE APPLICATION PACKAGE](https://docs.snowflake.com/en/sql-reference/sql/create-application-package) privilege. If your role doesn\u2019t have this, you can grant it with the Snowflake CLI:\n\n```\nsnow sql -q \"GRANT CREATE APPLICATION PACKAGE ON ACCOUNT TO ROLE accountadmin\" -c <connection_name>\n```\n\nGranting Create Application Package to account admin\n\n> Replace **connection\\_name** with the connection name specified in your **config.toml** file.\n> \n> \n\n**snowflake.yml** file defines objects and configuration details. Here\u2019s what it includes:\n\n**a) Application Package**\n\nServes as the container for app-related objects. Example: snowflake\\_native\\_app\\_demo\\_package\n\n**b) Application Object**\n\nCreated from the application package. Example: **demo\\_snowflake\\_native\\_app**\n\n**c) Named Stage**\n\nHolds application files. Example: **stage\\_content.hello\\_snowflake\\_stage** . The stage name is schema-qualified and created inside the application package.\n\nIt stores files for setup scripts or runtime use.\n\n**Artifacts Section**\n\nSpecifies file rules for deployment. Example: Files in app/ are uploaded to the root of the stage.\n\nExample mappings:\n\n```\ntutorial/app/manifest.yml \u2192 @snowflake_native_app_demo_package.stage_content.hello_snowflake_stage\ntutorial/app/README.md \u2192 @snowflake_native_app_demo_package.stage_content.hello_snowflake_stage\ntutorial/app/setup_script.sql \u2192 @snowflake_native_app_demo_package.stage_content.hello_snowflake_stage\n```\n\n> Use <% \u2026 %> syntax for dynamic referencing. Example: <% ctx.entities.pkg.identifier %> accesses the package identifier.\n> \n> \n\n**Debug Mode**\n\nThe debug field in **snowflake.yml** is set to **false** for production. Debug mode is typically enabled by default during development.\n\n**\u27a5 Configuring README File**\n\nThis file provides a description of your application. Create **app/README.md** with a brief description:\n\n```\nThis is a demo Snowflake Native App.\n```\n\n### **Step 5** \u2014Writing the Snowflake Native Application Logic\n\nNow let\u2019s get to the essence of Snowflake Native App development\u2014adding application logic and installing your first app. This involves creating stored procedures, setting up application roles and configuring privileges for seamless execution. Here\u2019s how you can achieve this:\n\n**\u27a5 Add a Stored Procedure to the Setup Script**\n\nThe setup script is central to your app\u2019s functionality. By extending it, you can define key components like roles and stored procedures.\n\nFirst let\u2019s define an Application Role. To do so, add the following to your **setup\\_script.sql** file:\n\n```\nCREATE APPLICATION ROLE IF NOT EXISTS snowflake_native_app_public;\nCREATE SCHEMA IF NOT EXISTS native_app_core;\nGRANT USAGE ON SCHEMA native_app_core TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nAs you can see, this creates an application-specific role ( _snowflake\\_native\\_app\\_public_ ) and a schema (native\\_app\\_core) for the app. And the role is restricted to the app\u2019s context and manages access to app-specific objects.\n\nNext, add a stored procedure that your app can call:\n\n```\nCREATE OR REPLACE PROCEDURE native_app_core.HELLO()\n\tRETURNS STRING\n\tLANGUAGE SQL\n\tEXECUTE AS OWNER\n\tAS\nBEGIN\n\t RETURN 'Demo Snowflake Native App!';\nEND;\n```\n\nThis stored procedure outputs a simple greeting\u2014useful as a test or base for more complex logic.\n\nFinally, grant the application role permission to use the procedure:\n\n```\nGRANT USAGE ON PROCEDURE native_app_core.hello() TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nNow this role can access and execute the stored procedure.\n\nHere is how your **setup\\_script.sql** file should look like:\n\nWriting the Snowflake Native Application Logic\n\nOnce your setup script includes the necessary logic, install the app in stage dev mode using the Snowflake CLI. Dev mode lets you test app behavior before deploying it to production.\n\nTo test it, run the following Snowflake CLI command:\n\n```\nsnow app run -c connection_name\n```\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\nAs you can see, if the command runs successfully, it outputs a URL where you can see your app in Snowsight.\n\nChecking a Deployed Snowflake Native App in Snowsight\n\nTo run the stored procedure that you added to **setup\\_script.sql** in a previous section, run the following Snowflake CLI command:\n\n```\nsnow sql -q \"call demo_snowflake_native_app.native_app_core.hello()\" -c connection_name\n```\n\nYou should see the following result/output:\n\nRunning deployed Stored procedure via Snowflake CLI\n\n### **Step 6** \u2014Adding Data Content to Your Snowflake Native App\n\nIn this step, you\u2019ll enhance your Snowflake Native App by incorporating shared data content. This involves creating and sharing a table within your app package and granting access to app users. Additionally, you\u2019ll create a view for secure data access by consumers.\n\nFirst, let\u2019s create a table to share with the app. To do so, you can add a table to your app package by writing a SQL script and specifying its execution in the project definition file.\n\nSo let\u2019s create and populate the table. To do that, you need to create a folder **scripts** and inside that folder, create a file called **shared\\_content.sql** . Then, add the following code:\n\n```\nUSE APPLICATION PACKAGE <% ctx.entities.snowflake_native_app_demo_package.identifier %>;\n\nCREATE SCHEMA IF NOT EXISTS shared_data;\nUSE SCHEMA shared_data;\nCREATE TABLE IF NOT EXISTS wealthy_individuals (\n\t\tid INT,\n\t\t name VARCHAR,\n\t\tstatus VARCHAR\n);\n\nTRUNCATE TABLE wealthy_individuals;\n\nINSERT INTO wealthy_individuals VALUES\n\t\t (1, 'Elon', 'Billionaire'),\n\t\t(2, 'Bernard', 'Billionaire'),\n\t\t(3, 'Jeff', 'Billionaire'),\n\t\t (4, 'Warren', 'Billionaire'),\n\t\t(5, 'Larry', 'Billionaire'),\n\t\t(6, 'Sergey', 'Billionaire'),\n\t\t (7, 'Gautam', 'Billionaire'),\n\t\t(8, 'Carlos', 'Billionaire'),\n\t\t (9, 'Mukesh', 'Billionaire'),\n\t\t(10, 'Bill', 'Millionaire'),\n\t\t(11, 'Mark', 'Millionaire'),\n\t\t(12, 'Larry', 'Millionaire'),\n\t\t (13, 'Michael', 'Millionaire'),\n\t\t(14, 'Aman', 'Millionaire'),\n\t\t(15, 'Warren', 'Billionaire');\n```\n\nNow, grant access to the table and schema using:\n\n```\nGRANT USAGE ON SCHEMA shared_data TO SHARE IN APPLICATION PACKAGE <% ctx.entities.snowflake_native_app_demo_package.identifier %>;\nGRANT SELECT ON TABLE wealthy_individuals TO SHARE IN APPLICATION PACKAGE <% ctx.entities.snowflake_native_app_demo_package.identifier %>;\n```\n\nThe placeholder `<% ctx.entities.snowflake_native_app_demo_package.identifier %>` dynamically resolves to your application package identifier during deployment.\n\n**Add the Script to the Project Definition File**\n\nUpdate the **snowflake.yml** file to include the new script in post-deployment hooks:\n\n```\ndefinition_version: 2\nentities:\n\t snowflake_native_app_demo_package:\n\t\t\t type: application package\n\t\t\tstage: stage_content.hello_snowflake_stage\n\t\t\tmanifest: app/manifest.yml\n\t\t\tidentifier: snowflake_native_app_demo_package\n\t\t\t artifacts:\n\t\t\t\t\t- src: app/*\n\t\t\t\t\t dest: ./\n\t\t\t meta:\n\t\t\t\t post_deploy:\n\t\t\t\t\t\t - sql_script: scripts/shared_content.sql\n\t\tdemo_snowflake_native_app:\n\t\t\ttype: application\n\t\t\t from:\n\t\t\t\t target: snowflake_native_app_demo_package\n\t\t\tdebug: false\n```\n\nNext, modify the **setup\\_script.sql** file to create a view that app consumers can use to query the data.\n\n**Create a Versioned Schema:**\n\n```\nCREATE OR ALTER VERSIONED SCHEMA code_schema;\nGRANT USAGE ON SCHEMA code_schema TO APPLICATION ROLE snowflake_native_app_public;\n```\n\n**Create the View:**\n\n```\nCREATE VIEW IF NOT EXISTS code_schema.accounts_view\n\t AS SELECT roll_number, NAME, VALUE\n\t FROM shared_data.wealthy_individuals;\nGRANT SELECT ON VIEW code_schema.accounts_view TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nAfter adding the table and view, test the updated app to make sure everything works as expected. Let\u2019s deploy the updates:\n\n```\nsnow app run -c connection_name\n```\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\nThis uploads the edited files to the stage, runs **scripts/shared\\_content.sql** and deploys the app.\n\nNow, finally to verify the data access, fire the command below:\n\n```\nsnow sql -q \"SELECT * FROM demo_snowflake_native_app.code_schema.accounts_view\" -c connection_name\n```\n\nVerifying Snowflake data access\n\nThe output should list the sample data from the accounts table.\n\n### **Step 7** \u2014Adding Python Code to Your Snowflake Native App\n\nLet\u2019s enhance our Snowflake Native App by incorporating Python-based logic using User-Defined Functions (Snowflake UDFs). We\u2019ll add both an inline Python UDF and one that references an external Python module.\n\nInline Python UDFs enable you to embed Python logic directly within your setup script. To do so, update your setup script to include the following code:\n\n```\nCREATE OR REPLACE FUNCTION code_schema.squareroot(i INT)\nRETURNS INT\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.11'\nHANDLER = 'squareroot_py'\nAS\n$$\ndef squareroot_py(i):\n\t return i * i\n$$;\n\nGRANT USAGE ON FUNCTION code_schema.squareroot(INT) TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nAs you can see, this:\n\n* Creates a Python UDF named squareroot in the code\\_schema schema.\n* Uses Python 3.11 runtime.\n* Grants the necessary usage privilege to the snowflake\\_native\\_app\\_public role.\n\nNow let\u2019s reference a Python file for modular and reusable logic. First, let\u2019s add this code to your setup script:\n\n```\nCREATE OR REPLACE FUNCTION code_schema.cuberoot(i INT)\nRETURNS FLOAT\nLANGUAGE PYTHON\nRUNTIME_VERSION = 3.11\nIMPORTS = ('/python/cube_python.py')\nHANDLER = 'cube_python.cuberoot';\n\nGRANT USAGE ON FUNCTION code_schema.cuberoot(INT) TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nNow let\u2019s add the external Python file. To do this, in the project folder, create a subdirectory: **python/** . Inside this folder, create a file named **cube\\_python.py** with the following content:\n\n```\ndef cuberoot(i):\n\t return i * i * i\n```\n\nUpdate the project definition file to include the Python file:\n\n```\nartifacts:\n\t- src: python/cube_python.py\n```\n\nAfter adding the Python UDFs, deploy and validate their functionality.\n\n**Deploy the Updates:**\n\n```\nsnow app run -c connection_name\n```\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\nThis uploads the updated files to the stage and deploys the app.\n\n**Test the Inline Python UDF:**\n\n```\nsnow sql -q \"SELECT demo_snowflake_native_app.code_schema.squareroot(2)\" -c connection_name\n```\n\nOutput:\n\nTesting the Inline Python UDF \u2013 Snowflake Native App\n\n**Test the External Python UDF:**\n\n```\nsnow sql -q \"SELECT demo_snowflake_native_app.code_schema.cuberoot(2)\" -c connection_name\n```\n\nOutput:\n\nTesting the External Python UDF \u2013 Snowflake Native App\n\n### **Step 8** \u2014Adding a Streamlit App to Your Snowflake Native App\n\nNow, let\u2019s integrate a Streamlit-based user interface into your Snowflake Native App. Streamlit is an open-source framework designed for building interactive data applications, offering features for data visualization and user interaction.\n\nHead over to your project folder and create a subdirectory named **streamlit/** . Inside the streamlit folder, create a file named **streamlit\\_app.py** . Then, add the following Python code to **streamlit\\_app.py** :\n\n```\n# Import python packages\nimport streamlit as st\nfrom snowflake.snowpark import Session\n\nst.title(\"Demo Snowflake Native Application\")\nst.write(\n\t\t\"\"\"Demo of Snowflake Native Application\"\"\")\n\n# Get the current credentials\nsession = Session.builder.getOrCreate()\n\n# Create an example data frame\ndata_frame = session.sql(\"SELECT * FROM code_schema.accounts_view\")\n\n# Execute the query and convert it into a Pandas data frame\nqueried_data = data_frame.to_pandas()\n\n# Display the Pandas data frame as a Streamlit data frame.\nst.dataframe(queried_data, use_container_width=True)\n```\n\nNext, add the following to the artifacts section of the project definition file:\n\n```\nartifacts:\n\t - src: streamlit/streamlit_app.py\n```\n\nThis is how your **snowflake.yml** should look:\n\n```\ndefinition_version: 2\nentities:\n\t\tsnowflake_native_app_demo_package:\n\t\t\ttype: application package\n\t\t\t stage: stage_content.hello_snowflake_stage\n\t\t\t manifest: app/manifest.yml\n\t\t\t identifier: snowflake_native_app_demo_package\n\t\t\tartifacts:\n\t\t\t\t - src: app/*\n\t\t\t\t\t\tdest: ./\n\t\t\t\t - src: python/cube_python.py\n\t\t\t\t\t- src: streamlit/streamlit_app.py\t\t\t\t\n\t\t\tmeta:\n\t\t\t\t\tpost_deploy:\n\t\t\t\t\t\t- sql_script: scripts/shared_content.sql\n\t demo_snowflake_native_app:\n\t\t\t type: application\n\t\t\tfrom:\n\t\t\t\t\ttarget: snowflake_native_app_demo_package\n\t\t\t debug: false\n```\n\nThen, create the Streamlit object. To do that, add the following to the end of the **setup\\_script.sql** file:\n\n```\nCREATE STREAMLIT IF NOT EXISTS code_schema.snowflake_streamlit_native_app\n\tFROM '/streamlit'\n\t MAIN_FILE = '/streamlit_app.py'\n;\n```\n\nAdd the following statement to the same script to allow the application role to access the Streamlit object:\n\n```\nGRANT USAGE ON STREAMLIT code_schema.hello_snowflake_streamlit TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nDeploy the updates by running the following command to update the app:\n\n```\nsnow app run -c connection_name\n```\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\nAfter deployment, a URL will be printed in the console. Navigate to this URL to interact with the app. Click the **snowflake\\_streamlit\\_native\\_app** tab to view the Streamlit interface.\n\nChecking a Deployed Streamlit App in Snowsight\n\n### **Step 9** \u2014Versioning Your Snowflake Native App\n\nLets formalize your Snowflake Native App by creating a version. While previous steps utilized \u201cstage development\u201d mode for rapid iteration, adding a version is essential for listing the application package and sharing it with other Snowflake users.\n\nRun the following command to create version V1 for the **snowflake\\_native\\_app\\_demo\\_package** application package:\n\n```\nsnow app version create v1 -c connection_name\n```\n\nVersioning Snowflake Native App\n\nHere is how you would check the version of the app to see whether it was added successfully or not. To do so, run the following command:\n\n```\nsnow app version list -c connection_name\n```\n\nVersioning Snowflake Native App\n\n### **Step 10** \u2014Installing and Testing the Versioned App\n\nTo install and test the versioned app, run the following command to install the app based on the created version:\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\n### **Step 11** \u2014Viewing Snowflake Native App in Snowsight\n\nAfter finalizing your Snowflake Native App, you can use Snowsight, Snowflake\u2019s web interface, to explore your app visually, instead of relying solely on SQL commands.\n\nTo start, sign in to Snowsight using your Snowflake credentials. Once logged in, switch to the **ACCOUNTADMIN** role to ensure you have the necessary permissions. You can do this by selecting your username from the navigation menu, opening the account menu and switching from the current active role (e.g., PUBLIC) to **ACCOUNTADMIN** . Next, navigate to the **Data Products** section and select **Apps** from the menu.\n\nNavigating to Data Products > Apps section\n\nLocate your application, **DEMO\\_SNOWFLAKE\\_NATIVE\\_APP** , in the list.\n\nChecking a Deployed Snowflake Native App in Snowsight\n\nClicking on it opens the **Read Me** tab, which displays the content of the **README.md** file you created earlier in this tutorial.\n\nTo view the Streamlit interface, find and select **SNOWFLAKE\\_STREAMLIT\\_NATIVE\\_APP** .\n\nChecking a Deployed Streamlit App in Snowsight\n\n## Monetization and Distribution of Snowflake Native App\n\nNow that we have covered the detailed steps of how you can create and build Snowflake Native Applications from scratch, let\u2019s go through the process of how you can monetize your Snowflake Native Applications via Snowflake Marketplace. But before that, let\u2019s actually understand what Snowflake Marketplace is.\n\nThe Snowflake Marketplace is a platform where users can discover, evaluate and purchase a variety of products, including third-party data, data services, Snowflake Native Apps and AI products. It serves as a public data exchange integrated within the Snowflake Data Cloud, facilitating seamless and secure transactions between data providers and consumers.\n\nThe Snowflake marketplace offers various types of data products, such as:\n\n* Raw datasets\n* Refined and enriched data\n* Historical datasets for forecasting and machine learning\n* Real-time data streams (like weather or traffic updates)\n* Specialized identity or audience data for analytics\n* Snowflake Native Applications\n* Pre-built data pipelines and transformations\n\nSnowflake Marketplace leverages Snowflake\u2019s architecture to facilitate the secure sharing of data and applications. Transactions are managed natively, eliminating the need for third-party billing systems. Vendors can offer their products through various pricing models, such as pay-as-you-go, one-time payment, usage-based payment, or subscription-based plans, while benefiting from Snowflake\u2019s built-in analytics to track customer engagement.\n\nLet\u2019s jump right into the juicy part of the article: a step-by-step guide to monetizing Snowflake Native Applications via Snowflake Marketplace.\n\n### Step-By-Step Monetization Process of a Snowflake Native App via Snowflake Marketplace\n\n#### **Step 1** \u2014Prepare Your Snowflake Native Application Package\n\nBefore monetization, make sure your Snowflake Native App meets Snowflake\u2019s submission requirements:\n\n* The app must be fully functional upon installation, with all necessary resources and configurations included.\n* It must not depend on external systems for core functionality.\n* Should Leverage Snowflake-stored or shared data for operations.\n* Include the following files:\n  \n    + **manifest.yml** (outlines app permissions and dependencies).\n    + **readme.md** (describes app functionality, post-installation setup steps and sample usage SQL).\n    + Setup scripts and external components like Streamlit files or UDF code\n\n#### **Step 2** \u2014Define the Default Release Directive\n\nNow, set the release directive to specify the app version and patch available for distribution:\n\nList available versions and patches:\n\n```\nsnow app version list -c connection_name\n```\n\nThen, set the default release directive:\n\n```\nsnow sql -q \"ALTER APPLICATION PACKAGE <your_app_package> SET DEFAULT RELEASE DIRECTIVE VERSION = v1 PATCH = 0\"\n```\n\nVersioning Snowflake Native App\n\nAs you can see, this command sets version **v1** and patch **0** as the default for your app, ensuring it is ready for deployment\u200b.\n\n#### **Step 3** \u2014Create and Configure a Private Listing on the Snowflake Marketplace\n\nTo share your app via the Snowflake Marketplace, start by signing in to Snowsight and navigating to **Data Products > Provider Studio** .\n\nNavigating to Provider Studio in Snowflake\n\nClick **\\+ Listing** to create a new listing and proceed with configuration. Enter a name for the listing and specify the discovery permissions, choosing whether the listing will be public or restricted to specific consumers (e.g., select \u201c **Only specified consumers** \u201d for private sharing and select \u201c **Anyone on the Marketplace** \u201d for public listing).\n\nCreating a private listing for only specified consumers \u2013 Snowflake Native App\n\nAttach the application package you prepared earlier as the core data content for the listing. Provide a detailed description outlining your app\u2019s features and usage scenarios. If creating a private listing, add the account identifiers of intended consumers in the \u201c **Add** **Consumer accounts** \u201d section. Finally, publish your listing for approval.\n\nCreating a private listing for only specified consumers \u2013 Snowflake Native App\n\n#### **Step 4** \u2014Create and Configure a Public Paid Listing on the Snowflake Marketplace\n\nNow, to create a public listing, you need to first contact your Snowflake business development partner to approve your paid listing. If you don\u2019t have a business development partner, you\u2019ll need to [submit a case with Marketplace Operations](https://snowflakecommunity.force.com/s/provider-onboarding-case) . Before proceeding, verify that your [role has the required privileges to create a listing](https://other-docs.snowflake.com/en/collaboration/provider-becoming) .\n\nOnce everything is in place, you need to log in to Snowsight and go to Data Products > Provider Studio from the menu. Select **\\+ Listing** to open the Create Listing window. Here, name your listing and set its visibility. To make the listing publicly discoverable, choose \u201c **Anyone on the Marketplace** \u201d under the discovery settings.\n\nCreating a paid private listing for only specified consumers \u2013 Snowflake Marketplace\n\nNext, you need to decide how consumers will access your data product: choose \u201c **Free** \u201d for no-cost access, \u201c **Personalized/Limited Trial** \u201d to offer a trial version with full access upon request, or \u201c **Paid** \u201d if you plan to charge consumers directly.\n\nAfter setting the access type, click **Next** to generate a draft listing. Lastly, refine and configure the draft by including all necessary details to ready it for publication on the Snowflake Marketplace.\n\n#### **Step 5** \u2014Submit Listing for Approval\n\nAll listings on the Snowflake Marketplace must undergo a review and approval process before publication. If a listing is rejected, review the provided feedback, make the necessary updates and resubmit it for approval.\n\nBefore publishing make sure that your listing configuration is complete, you have the **ACCOUNTADMIN** role or **OWNERSHIP** privilege for the associated data product and all sample SQL queries in the listing are validated successfully. To submit your listing, sign in to Snowsight, navigate to **Data Products \u27a4 Provider Studio** , go to the Listings tab, select your draft listing and click **Submit for Approval** .\n\nSubmitting listing for final approval from Snowflake\n\n#### **Step 6** \u2014Final Approval and Publishing\n\nOnce submitted, Snowflake will review your listing and provide an **Approved** or **Denied** status. If denied, review the feedback, make the necessary updates and resubmit the listing. After receiving approval, return to the **Listings** tab, select your approved listing and click **Publish** . Upon publication, the listing will be visible to consumers in all current and future Snowflake Marketplace regions. Regional availability can be managed through cross-cloud auto-fulfillment settings and you can create referral links for direct access to your listing.\n\n## Further Reading\n\n* [Snowflake Native Apps](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/)\n* [What Are Native Apps?](https://www.snowflake.com/guides/what-are-native-apps/)\n* [About the Snowflake Native App Framework](https://docs.snowflake.com/en/developer-guide/native-apps/native-apps-about)\n* [Snowflake Native App Framework on AWS and Azure](https://www.snowflake.com/en/blog/native-app-framework-available-aws-azure/)\n* [Introducing the Snowflake Native App Framework](https://www.snowflake.com/en/blog/introducing-snowflake-native-application-framework/)\n* [Getting Started with Snowflake Native Apps](https://quickstarts.snowflake.com/guide/getting_started_with_native_apps/)\n* [Snowflake Native Apps Example](https://github.com/snowflakedb/native-apps-examples)\n\n## Conclusion\n\nAnd that\u2019s a wrap! Snowflake Native Apps are built using the Snowflake Native App Framework. This allows developers to create, test and launch apps right in Snowflake. The framework simplifies the process of building, launching and integrating advanced tools. It ensures security and governance by tapping into the Snowflake ecosystem. For providers, these apps provide an easy way to sell their solutions on the Snowflake Marketplace, reaching thousands of customers. Meanwhile, consumers get instant access to the apps without needing a complex setup.\n\nIn this article, we have covered:\n\n* What are Native Apps in Snowflake?\n* Key features and characteristics of Snowflake Native Apps\n* What are the benefits of Snowflake Native Apps for providers?\n* What are the benefits of Snowflake Native Apps for consumers?\n* How do Snowflake Native Apps work?\n* Step-by-step guide to create a Snowflake Native App\n* Monetization and distribution of Snowflake Native Apps\n* Step-by-step monetization process via Snowflake Marketplace\n\n\u2026 and so much more!\n\n## FAQs\n\n**What are Native Apps in Snowflake?**\n\nSnowflake Native Apps are designed specifically to operate within the Snowflake ecosystem without requiring external access or movement of sensitive data outside its environment.\n\n**How can I develop and test a Snowflake Native App locally?**\n\nDevelopers can set up their environments using tools like VSCode along with necessary extensions provided by Snowflakes such as CLI support.\n\n**Can I share my Snowflake Native App with other users?**\n\nYes! Once published on the marketplace after meeting compliance requirements.\n\n**Does the Snowflake Native App framework support logging and monitoring?**\n\nYes! Snowflake Native App framework includes telemetry tools that allow developers to monitor application performance post-deployment.\n\n**What is Streamlit\u2019s role in Snowflake Native apps?**\n\nStreamlit allows developers to create interactive web interfaces that enhance user engagement directly within their Snowflake Native Applications.\n\n**Can I update my Snowflake Native App after deployment?**\n\nYes! Providers have control over release cycles allowing them to push updates seamlessly even after deployment.\n\nRelated posts:\n\n* [Empowering users with intuitive, actionable data: introducing Data Explorer](https://www.flexera.com/blog/it-visibility/empowering-users-with-intuitive-actionable-data-introducing-data-explorer/ \"Empowering users with intuitive, actionable data: introducing Data Explorer\")\n* [From Spot Eco to Flexera One Cloud Commitment Management: A new era of automated cloud cost optimization](https://www.flexera.com/blog/finops/from-spot-eco-to-flexera-one-cloud-commitment-management-a-new-era-of-automated-cloud-cost-optimization/ \"From Spot Eco to Flexera One Cloud Commitment Management: A new era of automated cloud cost optimization\")\n* [The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)](https://www.flexera.com/blog/finops/the-practical-finops-roadmap-series-what-to-do-before-you-start-practicing-finops-1-4/ \"The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)\")\n* [FinOps and ITAM: A unified approach to optimizing technology investments](https://www.flexera.com/blog/finops/finops-and-itam-a-unified-approach-to-optimizing-technology-investments/ \"FinOps and ITAM: A unified approach to optimizing technology investments\")\n* [Elevate your packaging efficiency: Introducing \u2018My Requests\u2019 in AdminStudio](https://www.flexera.com/blog/application-readiness/elevate-your-packaging-efficiency-introducing-my-requests-in-adminstudio/ \"Elevate your packaging efficiency: Introducing \u2018My Requests\u2019 in AdminStudio\")\n* [What\u2019s new at Flexera: May 2025](https://www.flexera.com/blog/product/whats-new-at-flexera-may-2025/ \"What\u2019s new at Flexera: May 2025\")\n\n### Want to know more?\n\nTechnology is evolving rapidly\u2014and it's important to stay on top of the latest trends and critical insights. Check out the latest blogs related to FinOps below.\n\nFinOps\n\n## [2025 State of the Cloud](https://info.flexera.com/CM-REPORT-State-of-the-Cloud?lead_source=Website%20Visitor&id=Blog-Resources \"2025 State of the Cloud\")\n\nMarch 12, 2024\n\nFinOps\n\n## [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Request \"Cloud Cost Optimization demo\")\n\nFebruary 22, 2023\n\nFinOps\n\n## [Practical Guide for a Successful Cloud Journey](https://info.flexera.com/CM-GUIDE-Successful-Cloud-Journey?lead_source=Website%20Visitor&id=Blog-Resources \"Practical Guide for a Successful Cloud Journey\")\n\nFebruary 9, 2022\n\nFinOps\n\n## [Cloud Migration and Modernization Datasheet](https://www.flexera.com/sites/default/files/datasheet-foundation-cloudscape.pdf \"Cloud Migration and Modernization Datasheet\")\n\nFinOps\n\n## [New Flexera One Cloud Cost Optimization capabilities: Built for partners, designed for growth](https://www.flexera.com/blog/finops/new-flexera-one-cloud-cost-optimization-capabilities-built-for-partners-designed-for-growth/ \"New Flexera One Cloud Cost Optimization capabilities: Built for partners, designed for growth\")\n\nFebruary 24, 2026\n\nFinOps\n\n## [FinOps enters its technology value era: Insights from the State of FinOps 2026](https://www.flexera.com/blog/finops/finops-enters-its-technology-value-era-insights-from-the-state-of-finops-2026/ \"FinOps enters its technology value era: Insights from the State of FinOps 2026\")\n\nFebruary 20, 2026\n\n\u00d7\n\nGet updates delivered to your inbox\n\nSubscribe\n\n## How can we help?\n\nSales Team\n\n[Community](https://community.flexera.com/s/)\n\nSubscribe\n\nFlexera\n\n* [](https://www.linkedin.com/company/flexera?elqTrackId=62e00a6465d449b0824c83c70706dff9&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"LinkedIn\")\n* [](https://twitter.com/flexera?elqTrackId=ab8f06bd7aea498e807592d19ac2ab00&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"Twitter\")\n* [](https://www.instagram.com/weareflexera?elqTrackId=fcfa0064605a42baaebfabe8fedd5c50&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"Instagram\")\n* [](https://www.youtube.com/user/FlexeraSoftware?elqTrackId=c6e9107020754655a13aca3ac7aa3cd4&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"YouTube\")\n\n* Privacy policy\n* Terms and conditions\n* Site map"
      ],
      "full_content": null
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/",
      "title": "Cost Optimization",
      "publish_date": null,
      "excerpts": [
        "Data for Breakfast Around the World\n\nDrive impact across your organization with data and agentic intelligence.\n\n[register now](https://www.snowflake.com/en/data-for-breakfast/)\n\n \n\nProduct\n\nSolutions\n\nWhy Snowflake\n\nResources\n\nDevelopers\n\nPricing\n\nFeatured Capabilities\n\nFeatured Open Source Technologies\n\nINDUSTRIES\n\nDEPARTMENTS\n\nEnablement Solutions\n\nPARTNER SOLUTIONS\n\nConnect\n\nLearn\n\nBuild\n\nLearn\n\nConnect\n\n[Snowflake for Developers](https://www.snowflake.com/content/snowflake-site/global/en/developers) [Guides](https://www.snowflake.com/content/snowflake-site/global/en/developers/guides) Cost Optimization\n\n## Cost Optimization\n\nWell Architected Framework Team\n\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\n\n## Overview\n\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\n\n## Principles\n\n#### Business impact: Align cost with business value\n\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n\n#### Visibility: Understand & contextualize your consumption footprint\n\nYou can neither control nor optimize what you can't see. Gain deep and granular insight into all aspects of your cloud spending, fostering transparency, and attributing costs effectively.\n\n#### Control: Establish guardrails and governance\n\nEstablish policies and mechanisms to govern resource provisioning and consumption, preventing unnecessary costs and enforcing financial boundaries.\n\n#### Optimize: Maximize efficiency and value\n\nContinuously improve the efficiency of your resources and workloads to maximize the value derived from your platform investment.\n\n## Recommendations\n\nThe following key recommendations are covered within each principle of\nCost Optimization:\n\n* **Business Impact**\n  \n    + **Consider cost as a design constraint:** Integrate cost\n        considerations into the architecture and design process from the\n        very beginning, making it a key non-functional requirement alongside\n        performance, security, and reliability.\n    + **Quantify value:** Develop metrics to quantify the business value\n        delivered by cloud resources (e.g., revenue per Snowflake Credit,\n        cost per customer, efficiency gains).\n    + **Trade-off analysis:** Understand the inherent trade-offs between\n        cost, performance, reliability, and security, and make informed\n        decisions that align with business priorities.\n    + **Measure business value KPIs baseline:** Once metrics to quantify\n        business value are identified and trade-offs between cost,\n        performance, and reliability are established, you need to document a\n        \u201cbaseline measurement\u201d in order to track progress again.\n        Furthermore, you should establish a regular cadence for refreshing\n        this measurement to ensure value realization is in line with\n        expectations and business goals.\n* **Visibility**\n  \n    + **Understand Snowflake\u2019s resource billing models:** Review\n        Snowflake\u2019s billing models to align technical and non-technical\n        resources on financial drivers and consumption terminology.\n    + **Establish a consistent and granular cost attribution strategy:** Implement robust and organizationally consistent tagging and\n        labeling strategies across all resources (storage objects,\n        warehouses, accounts, queries) to accurately allocate costs to\n        specific teams, products, or initiatives.\n    + **Embed cost accountability into your organization's DNA:** Implement a feedback system to transparently show or charge back\n        cloud costs to relevant business units or teams, increasing\n        accountability.\n    + **Deliver clear, historical consumption insights:** Utilize\n        consistent in-tool visualizations or custom dashboards to monitor\n        consumption and contextualize spend on the platform with unit\n        economics.\n    + **Investigate anomalous consumption activity:** Review anomaly\n        detection to identify unforeseen cost anomalies and investigate\n        cause and effect trends.\n* **Control**\n  \n    + **Proactively monitor all platform usage:** Define and enforce\n        budgets for projects and services, setting soft quotas to limit\n        resource consumption and prevent runaway spending.\n    + **Forecast consumption based on business needs:** Establish a\n        forecast process to project future spend needs based on business and\n        technical needs.\n    + **Enforce cost guardrails for organizational resources:** Set up\n        automated checks (e.g., Tasks, query insights) and resource\n        guardrails (e.g., warehouse timeout, storage policies, resource\n        monitors) to identify unusual usage patterns and potential\n        overspending as they occur.\n    + **Govern resource creation and administration:** Establish clear\n        guidelines and automated processes for provisioning and maintaining\n        resources, ensuring that only necessary and appropriately sized\n        resources are deployed (e.g., warehouse timeout, storage policies,\n        resource monitors).\n* **Optimize**\n  \n    + **Compute workload-aligned provisioning:** Continuously monitor\n        resource health metrics to resize and reconfigure to match actual\n        workload requirements.\n    + **Leverage managed services:** Prioritize fully managed Snowflake\n        services (e.g., Snowpipe, Auto-clustering, [Query Acceleration Service](https://docs.snowflake.com/en/user-guide/query-acceleration-service) )\n        to offload operational overhead and often achieve better cost\n        efficiency.\n    + **Data storage types & lifecycle management:** Utilize appropriate\n        storage types and implement appropriate storage configuration to\n        right-size workloads to your storage footprint. Move data to cheaper\n        tiers or delete it when no longer needed.\n    + **Workload & architectural optimization:** Leverage architectural\n        decisions for cost-optimized resource utilization while meeting\n        business needs.\n    + **Limit data transfer:** Move, secure, and back up the appropriate\n        footprint across cloud regions.\n    + **Improve continually:** As new capabilities or usage patterns\n        emerge, establish a consistent testing framework to identify areas\n        for cost efficiency.\n\n## Business Impact\n\n#### Overview\n\nTo maximize organizational outcomes, Snowflake consumption must be\nexplicitly tied to business value. While cost optimization ensures\nefficiency, it does not guarantee that spend is aligned with outcomes\nthat matter to stakeholders. Alignment of business value to cost ensures\nworkloads, pipelines, dashboards, and advanced analytics are\ncontinuously evaluated not only for cost but also for the value they\ndeliver. This approach ensures Snowflake delivers as a strategic\nbusiness platform rather than a technical expense.\n\n#### Recommendations\n\nBusiness value to cost alignment represents a maturity step in FinOps on\nSnowflake. By embedding benchmarking, impact analysis, SLA definition,\nusage metrics, ROI measures, and business impact evaluation into daily\noperations, organizations can ensure that Snowflake consumption is\ncontinuously justified, optimized, and communicated in business terms.\nThis elevates the conversation with leadership from cost oversight to\nvalue realization and ensures that Snowflake is understood as a platform\nfor growth, innovation, and competitive advantage.\n\n#### Consider cost as a design constraint\n\nCost-Aware Architecting is the practice of embedding financial\naccountability directly into the design and development of Snowflake\nworkloads. By shifting left\u2014introducing cost considerations early in the\narchitecture lifecycle\u2014organizations ensure that ingestion,\ntransformation, analytics, and distribution workloads are not only\nperformant but also aligned with budget expectations. Many cost overruns\nin Snowflake originate from architectural decisions made without cost\nimplications in mind.\n\nFor example, designing ingestion with sub-second latency when daily\nfreshness is sufficient, or selecting inefficient table designs that\nincrease query scanning. These can lead to disproportionate spend.\nShifting cost awareness into architecture helps prevent inefficiencies\nbefore they occur and reinforces Snowflake\u2019s role as a cost-effective\nenabler of business value.\n\nAt the ingestion layer, best practices include balancing latency versus\ncost by selecting appropriate services (e.g., Snowpipe, Snowpipe\nStreaming, or third-party tools) and choosing the right storage format\n(e.g., native tables, Iceberg). For transformations, design with\nfrequency versus SLA in mind to ensure data freshness matches the\nbusiness need. For analytics, apply schema design best practices such as\nthoughtful clustering key choices and pruning strategies to reduce\nconsumed credits. In distribution, optimize data transfer by monitoring\negress patterns and applying cost-saving practices like the [Snowflake Data Transfer Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) .\n\n#### Quantify value\n\nTo maximize organizational outcomes, Snowflake consumption must be\nexplicitly tied to measurable business value and clearly communicated in\nterms that resonate with stakeholders. Establishing baselines using\nSnowflake\u2019s [Account Usage views](https://docs.snowflake.com/en/sql-reference/account-usage) creates a reference point, while tracking the current state highlights\ntrends in performance and consumption. Defining explicit goal\nstates\u2014such as reduced cost per decision, improved time-to-market, or\nbroader data access\u2014ties workloads directly to outcomes that matter to\nstakeholders. Outliers that diverge from these goals should be flagged\nfor review and optimization to prevent wasted resources. Best practices\ninclude applying unit economic measures related to your field (e.g. cost\nper terabyte analyzed or cost per fraud case prevented) and publishing\nROI dashboards that continuously link Snowflake consumption to business\noutcomes. By incorporating measurement into daily operations,\norganizations can move the conversation with leadership from cost\noversight to demonstrable value realization, positioning Snowflake as a\nclear enabler of enterprise growth and innovation.\n\n#### Trade-off analysis\n\nDefining SLAs or explicit business needs ensures that Snowflake\nworkloads are aligned with their intended purpose and that consumption\nlevels are justified by business outcomes. Some Snowflake workloads can\nbecome over-engineered or maintained without clear justification. Tying\neach workload to an SLA or business requirement prevents waste and\nensures that investment aligns with value. Before implementation, it is\ncrucial to document and align on the value of meeting an SLA,\nidentifying all stakeholders who rely on the workload. This includes\ndifferentiating between tangible outcomes, such as increased revenue,\nand intangible outcomes, such as compliance or data trust. Efficient\ncustomers use both [Snowflake Resource Monitors](https://docs.snowflake.com/en/user-guide/resource-monitors) and Budgets features to enforce guardrails that ensure workloads remain\nwithin acceptable cost-performance boundaries. All design decisions have\ntrade-offs, and explicitly calling out the expected outcomes leads to\nstreamlined decision-making in the future when outcomes are reviewed.\n\n#### Measure business value KPIs baseline\n\nBenchmarking establishes performance and cost baselines for Snowflake\nworkloads and compares them against internal standards as well as\nperformance and cost results from previous tech solutions. These\nbenchmarks can measure workload efficiency, the adoption of specific\nSnowflake features, and the alignment of workload costs to business\noutcomes. Without benchmarks, organizations lack the context to\ndetermine if their Snowflake consumption is delivering economies of\nscale or value back to the business. Benchmarking allows teams to\nidentify best practices, track improvements over time, and highlight\noutliers that may be driving unnecessary spend or delivering unexpected\nvalue.\n\nBest practices include measuring technical unit economic metrics (e.g.\ncredits per 1K queries, credits per 1 TB scanned), warehouse efficiency\nand utilization by workload type, and business unit economics (e.g.\ncredits per customer acquired, credits per partner onboarded, or credits\nper data product-specific KPIs). This provides a more comprehensive\npicture of consumption in relation to cost and value. Outliers should be\nhighlighted in executive communications as either success stories or\ncautionary examples. Benchmarking should be embedded in a continuous\nimprovement loop, where insights drive action, action improves\nefficiency, and those improvements are effectively measured.\n\n## Visibility\n\n#### Overview\n\nThe Snowflake Visibility principle is designed to transform opaque cloud\nspending into actionable insights, fostering financial accountability\nand maximizing business value within your Snowflake environment. It is\nfoundational to the FinOps framework, as you cannot control, optimize,\nor attribute business value to what you cannot see. To effectively\nmanage and optimize cloud costs in Snowflake, it's crucial to align\norganizationally to an accountability structure of spend, gain deep and\ngranular insight into all aspects of your cloud spending, and\ntransparently display it to the appropriate stakeholders to take action.\n\n#### Recommendations\n\nImplementing a robust FinOps visibility framework in Snowflake,\nsupported by cross-functional collaboration, enables each business\nfunction to access timely and relevant usage and cost data. This\nempowers them to understand the business impact of their consumption and\ntake prompt action when anomalies arise. To meet this vision, consider\nthe following recommendations based on industry best practices and\nSnowflake's capabilities:\n\n#### Understand Snowflake\u2019s resource billing models\n\nIt is essential to review Snowflake's billing models to align technical\nand non-technical resources on financial drivers and consumption\nterminology. Snowflake's elastic, credit-based consumption model charges\nseparately for compute (Virtual Warehouses, Compute Pools, etc),\nstorage, data transfer, and various serverless features (e.g., Snowpipe,\nAutomatic Clustering, Search Optimization, Replication/Failover, AI\nServices). Understanding the interplay of these billing types ensures\nyou can attribute costs associated with each category\u2019s unique usage\nparameters. High-level categories are below.\n\n* **Compute (Virtual Warehouses, Snowpark Container Services, Openflow):** This is often the most dynamic and largest portion of\n  Snowflake spend. Virtual Warehouses are billed per-second after an\n  initial 60-second minimum when active, with credit consumption\n  directly proportional to warehouse size (e.g., an \u201cX-Small\u201d Gen1\n  warehouse consumes one credit per hour, a 'Small' consumes two credits\n  per hour, doubling with each size increase) while SPCS (via [compute pools](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) )\n  are billed for all uptime with a minimum of five minutes. [Openflow](https://docs.snowflake.com/en/user-guide/data-integration/openflow/cost) is billed per second of runtime with a 60-second minimum.\n* **Storage:** Costs are based on the average monthly compressed data\n  volume stored, including active data, Time Travel (data retention),\n  and Fail-safe (disaster recovery) data. The price per terabyte (TB)\n  varies by cloud provider and region.\n* **Serverless features:** Snowflake Serverless features use resources\n  managed by Snowflake, not the user, which automatically scale to meet\n  the needs of a workload. This allows Snowflake to pass on efficiencies\n  and reduce platform administration while providing increased\n  performance to customers. The cost varies by feature and is outlined\n  in Snowflake\u2019s Credit Consumption Document .\n* **Cloud services layer:** This encompasses essential background\n  services, including query compilation, metadata management,\n  information schema access, access controls, and authentication. Usage\n  for cloud services is only charged if the daily consumption of cloud\n  services exceeds 10% of the daily usage of virtual warehouses.\n* **AI features:** Snowflake additionally offers artificial intelligence\n  features that run on Snowflake-managed compute resources, including\n  Cortex AISQL functions (e.g. COMPLETE, CLASSIFY, etc.), Cortex\n  Analyst, Cortex Search, Fine Tuning, and Document AI. The usage of\n  these features, often with tokens, are converted to credits to unify\n  with the rest of Snowflake\u2019s billing model. Details are listed in the\n  Credit Consumption Document.\n* **Data transfer:** Data transfer is the process of moving data into\n  (ingress) and out of (egress) Snowflake. This generally happens via\n  egress on cross-region [data replication](https://docs.snowflake.com/en/user-guide/account-replication-cost) , [copying into/out of stage, function calls](https://docs.snowflake.com/user-guide/cost-understanding-data-transfer) ,\n  and cross/same region [SPCS data transfer](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) .\n  Depending on the cloud provider and the region used during data\n  transfer, charges vary.\n* **Data sharing & rebates:** Snowflake offers an opt-out Data\n  Collaboration rebate program that allows customers to offset credits\n  by data consumed with shared outside organizations. This rebate is\n  proportional to the consumption of your shared data by consumer\n  Snowflake accounts. See the latest terms and more details here .\n\n#### Establish a consistent and granular cost attribution strategy\n\nImplementing robust and organizationally consistent tagging and labeling\nstrategies across all resources (e.g. storage objects, warehouses,\naccounts, queries) is crucial to accurately allocate costs to specific\nteams, products, or initiatives and linking actions to outcomes.\n\n**Tagging in Snowflake**\n\nTagging can be done at several levels:\n\n* **Snowflake object tagging:** Snowflake allows you to apply [object-level tags](https://docs.snowflake.com/en/user-guide/object-tagging/introduction) (key-value pairs) to accounts, warehouses, databases, schemas, users,\n  tables, and more. These tags are fundamental for apportioning costs\n  across departments, environments (dev, test, prod), projects, or lines\n  of business. Tags can also support [inheritance](https://docs.snowflake.com/en/user-guide/object-tagging/inheritance) and [propagation](https://docs.snowflake.com/en/user-guide/object-tagging/propagation) ,\n  simplifying tagging across dependent objects. For example, instead of\n  tagging each individual table underneath a schema, tagging the schema\n  will cause all tables to inherit the tag of the schema. This\n  significantly reduces the manual effort required for tagging and\n  ensures that new objects created within a tagged schema or propagated\n  workflow automatically inherit the correct cost attribution. Snowflake\n  strongly recommends tags for warehouses, databases, tables, and users\n  to enable granular cost breakdowns. You can use the [TAG\\_REFERENCES view](https://docs.snowflake.com/sql-reference/account-usage/tag_references) in SNOWFLAKE.ACCOUNT\\_USAGE to combine with common usage views like\n  WAREHOUSE\\_METERING\\_HISTORY and TABLE\\_STORAGE\\_METRICS to allocate usage\n  to relevant business groups. Object Tags are best utilized when\n  Snowflake objects are not shared across cost owners.\n* **Query tags for granular workload attribution:** [Query tags](https://docs.snowflake.com/en/user-guide/cost-attributing) can be set via session parameters (e.g., ALTER SESSION SET QUERY\\_TAG =\n  'your\\_tag';) or directly within SQL clients or ETL tools. This\n  associates individual queries with specific departments, projects, or\n  applications, even when using shared warehouses. This is extremely\n  valuable for shared warehouses where multiple teams or applications\n  use the same compute resource, allowing for granular showback. It is\n  also easy to programmatically make changes to query tags within\n  scripts or processes to allocate costs appropriately. Query tags can\n  be found in the QUERY\\_HISTORY view of the SNOWFLAKE.ACCOUNT\\_USAGE\n  schema.\n\n**Tagging models**\n\nIn the initial setup of a business unit or use case, it is important to\nconsider the [model for tagging](https://docs.snowflake.com/en/user-guide/cost-attributing) costs within the platform via shared or dedicated resources. These fall\ninto three large buckets:\n\n* **Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate warehouses with a\n  department. You can use object tags to attribute the costs incurred by\n  those warehouses to that department entirely.\n* **Resources shared by users from multiple departments:** An example of\n  this is a warehouse shared by users from different departments. In\n  this case, you use object tags to associate each user with a\n  department. The costs of queries are attributed to the users. Using\n  the object tags assigned to users, you can break down the costs by\n  department.\n* **Applications or workflows shared by users from different departments:** An example of this is an application that issues\n  queries on behalf of its users. In this case, each query executed by\n  the application is [assigned a query tag](https://docs.snowflake.com/en/sql-reference/sql/alter-session) that identifies the team or cost center of the user for whom the query\n  is being made.\n\nEach model has its pros and cons, including how to handle concepts such\nas idle time or whether to show/charge back attributed or billed\ncredits. Review each model before deploying resources. If an\norganization is caught between models, a common approach is to start in\na shared resource environment and graduate to dedicated resources as the\nworkload increases.\n\n**Tag enforcement**\n\nClear and consistent naming conventions for accounts, warehouses,\ndatabases, schemas, and tables facilitate immediate cost understanding.\nEnforcing robust tagging policies (e.g., requiring specific tags for new\nresource creation and using automated scripts to identify untagged\nresources) is crucial for accurate data interpretation and effective\ncost management. Without tag enforcement, it is difficult to accurately\nallocate all costs and can require manual effort, like extensive\ntag-mapping tables. Tag values are enforced within an account, but if a\nmulti-account strategy is needed for your organization, a tag [database can be replicated](https://docs.snowflake.com/en/user-guide/cost-attributing) and leveraged across all accounts to ensure consistent values are used.\nFor best-in-class visibility, it is recommended to have a tagging\nstrategy and tag all resources in an organization to allocate costs to\nrelevant owners.\n\n#### Embed cost accountability into your organization's DNA\n\nTo effectively manage Snowflake spend and align business structure to\ntechnical resources, you should implement a system of showback or\nchargeback. This approach is crucial for promoting accountability and\noptimizing resource usage as there is a single owner for each object\nwithin the platform.\n\n**Showback**\n\nIf cost accountability models have not been implemented previously,\nconsider a showback model. This involves transparently reporting\nSnowflake costs to different departments or projects to raise awareness\nof their costs. By showing each team their monthly consumption (broken\ndown by warehouse usage, query costs, and storage, etc.), it encourages\na cost-conscious culture. This initial step helps teams understand the\nfinancial impact of their actions without the immediate pressure of\nbudget cuts. Tools like Snowflake's built-in [Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) & [budget](https://docs.snowflake.com/en/user-guide/budgets) views, third-party cost management platforms, or custom dashboards can\nbe used to provide these reports.\n\n**Chargeback**\n\nFor more financially mature organizations, a chargeback model can be\nvery effective for managing costs. This system directly bills\ndepartments for their Snowflake usage. This creates a powerful financial\nincentive for teams to optimize their workloads. To make this transition\nsmooth and fair, you need to define clear rules for cost allocation. By\nimplementing chargeback, you turn each department into a financial\nstakeholder, encouraging them to right-size their warehouses, suspend\nthem during idle periods, and write more efficient queries. This shift\nin accountability leads to a more disciplined and cost-effective use of\nyour Snowflake environment.\n\nIn either case, having a centralized dashboard or visual for all\norganizations to review intra-period is critical for financial\naccountability and next-step actions.\n\n#### Deliver clear historical consumption insights\n\nThe most mature FinOps customers are those who programmatically and\nstrategically drive consumption insights across the business. This\ninvolves three core elements:\n\n* **Platform cost tracking:** Pinpoint specific Snowflake credit\n  consumption (compute, storage, serverless, AI, and data transfer),\n  usage patterns, and efficiency opportunities to deconstruct credit\n  usage, understand drivers, identify anomalies, and (eventually) drive\n  forecasting operations.\n* **Normalization of consumption:** Once consumption has been attributed\n  and aggregated to meaningful levels, normalizing it against relevant\n  business and technical metrics contextualizes it in relation to\n  organizational goals. It allows for the natural growth and seasonality\n  of platform usage to be put into context with business and technical\n  demand drivers.\n* **Clear reporting:** Presenting Snowflake cost data in an\n  understandable format for various stakeholders is vital. This enables\n  budgeting, forecasting, KPIs, and business value metrics directly tied\n  to Snowflake credit consumption.\n\n**Track usage data for all platform resources**\n\nTo deliver clear and actionable consumption insights, it is essential to\nleverage the rich usage data that Snowflake natively provides. The\nfoundation for all cost visibility is the **SNOWFLAKE** database, which\ncontains two key schemas for this purpose: [ACCOUNT\\_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) (for granular, account-level data) and [ORGANIZATION\\_USAGE](https://docs.snowflake.com/en/sql-reference/organization-usage) (for a consolidated view across all accounts).\n\n|Metric Category |Description |Key Metrics |Primary Data Sources |\n| --- | --- | --- | --- |\n|Compute & query metrics |Understand the cost of query execution, warehouse consumption, and overall compute health. These are often the most dynamic and largest portion of your spend. |\\- Credits used: total credits by warehouse  \n\\- Query performance: execution time, bytes scanned, compilation time, parameterized query hash  \n\\- Warehouse health: % idle time, queueing, spilling, concurrency |\\- `ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY` (hourly warehouse credit usage)  \n\\- `ACCOUNT_USAGE.QUERY_HISTORY` (detailed query metrics and associated warehouses) |\n|Storage metrics |Costs for compressed data, including active data, Time Travel, and Fail\u2011safe. |\\- Storage volume (avg monthly compressed GB/TB)  \n\\- Inactive storage (Time Travel, Fail\u2011safe)  \n\\- Storage growth rates  \n\\- Table access (stale/unused) |\\- `ACCOUNT_USAGE.TABLE_STORAGE_METRICS`  \n\\- `ACCOUNT_USAGE.DATABASE_STORAGE_USAGE_HISTORY`  \n\\- `ACCOUNT_USAGE.ACCESS_HISTORY` |\n|Serverless & AI metrics |Track credit consumption by Snowflake\u2011managed services and AI features. |\\- Credits used by service  \n\\- Cost per credit\u2011consuming events |\\- `ACCOUNT_USAGE.<Serverless Feature>_HISTORY`  \n\\- `ORGANIZATION_USAGE.METERING_DAILY_HISTORY`  \n\\- AI views such as `CORTEX_FUNCTIONS_USAGE_HISTORY` , `CORTEX_ANALYST_USAGE_HISTORY` , `DOCUMENT_AI_USAGE_HISTORY` |\n|Data transfer |Cost of moving data into (ingress) and out of (egress) Snowflake, especially cross\u2011region/cloud. |\\- Bytes transferred  \n\\- Transfer cost by destination  \n\\- Replication vs. egress |\\- `ACCOUNT_USAGE.DATA_TRANSFER_HISTORY`  \n\\- `ORGANIZATION_USAGE.DATA_TRANSFER_DAILY_HISTORY` |\n|Financial metrics |Translate credits to currency and provide org\u2011wide spend view. |\\- Overall dollar spend (daily)  \n\\- Spend by service type |\\- `ORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY`  \n\\- `ORGANIZATION_USAGE.RATE_SHEET_DAILY` |\n\n**Normalize consumption with unit economic metrics**\n\nFor organizations to achieve comprehensive financial visibility, it is\nrecommended best practice to move beyond tracking aggregate spend and\nimplement Unit Economics Metrics. Unit economics provides a powerful\nmethodology for normalizing cloud consumption by tying platform costs to\nspecific business or operational drivers. This per-unit approach helps\nyou understand cost efficiency, measure the ROI of your initiatives, and\nmake data-driven decisions about resource allocation and optimization.\nBy translating abstract credit consumption into tangible metrics, you\ncan empower technical and business teams with a shared language for\ndiscussing value and cost. These metrics are commonly tracked across\ntime to show changes in efficiency or business impact.\n\n**Efficiency metrics (technical KPIs)**\n\nEfficiency Metrics are technical Key Performance Indicators (KPIs) that\nconnect cloud costs directly to platform operations and workloads. They\nare crucial for engineering teams and platform owners to identify\ninefficiencies, optimize resource usage, and understand the cost drivers\nof the data platform itself. These metrics provide the granular,\noperational view needed to manage the platform's performance day-to-day.\nSome common examples include:\n\n* **Cost per 1000 executable queries:** Determines the average cost for\n  a batch of one thousand queries. This metric is useful for\n  understanding the overall cost profile of analytical activity on the\n  platform and, when trended, how efficiency has changed across time.\n* **Cost per TB scanned:** Represents the average scanning cost of data.\n  This metric can help understand the cost implications of table\n  configuration (data ordering/clustering keys, Search Optimization) and\n  query efficiency.\n* **Cost per user:** Measures the average cost to support each active\n  user on the platform. This helps in understanding the cost\n  implications of user growth and identifying expensive usage patterns.\n\n> Customers can track credits (warehouse) per thousand queries within a\n> use case to see how efficiency has evolved over time and determine if\n> they are achieving economies of scale.\n> \n> \n\n**Business metrics (business KPIs)**\n\nBusiness Metrics link cloud spending to meaningful business outcomes and\nvalue drivers. These KPIs are essential for executives, finance teams,\nand product managers to understand the return on investment (ROI) of\ncloud expenditure and to allocate costs accurately across different\nparts of the organization. They answer the critical question: \"What\nbusiness value are we getting for our cloud spend?\" Examples include:\n\n* **Cost per customer:** Attributes a portion of the total platform cost\n  to each of the company's end customers. This is a powerful metric for\n  understanding profitability and cost-to-serve at a customer level.\n* **Cost per project:** Allocates cloud costs to specific internal\n  projects, products, or initiatives. This enables accurate\n  project-based accounting and helps assess the financial viability of\n  new features or services.\n* **\"Bring Your Own Metric\":** Custom define unit economic metrics that\n  are unique to your organization's business model. Examples could\n  include Cost per Transaction, Cost per Shipment, or Cost per Ad\n  Impression. Creating these tailored metrics ensures the most accurate\n  alignment between cloud spend and core business value.\n\nIf Snowflake is in the value chain for orders, the cost per order can be a good metric to tie Snowflake consumption to Business Demand Drivers.\n\n**Visualize metrics with Snowsight tools and external BI tools**\n\nA critical component of cost governance is the effective visualization\nof spending and usage data. Raw data, while comprehensive, is often\ndifficult to interpret and act upon. By translating cost and usage\nmetrics into interactive dashboards and reports, you can empower\nstakeholders\u2014from engineers to executives\u2014to understand spending\npatterns, troubleshoot, and make informed decisions. A multi-layered\napproach can be used to track meaningful cost metrics.\n\n* **Snowsight's built-in cost management capabilities:** Snowsight\n  provides pre-built visuals for usage and credit monitoring directly\n  within the [Snowflake Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) .\n  It allows filtering by tags (e.g., view cost by department tag),\n  credit consumption by object types, and cost insights to optimize the\n  platform.\n* **Creating custom dashboards or Streamlit apps for different stakeholder groups:** Snowsight facilitates the creation of custom\n  dashboards using ACCOUNT\\_USAGE and ORGANIZATION\\_USAGE views. Custom\n  charts in the Dashboards feature and Streamlit apps can both be easily\n  shared. Combined with cost allocation and tagging, this allows for\n  tailored views for finance managers (aggregated spend), engineering\n  managers (warehouse utilization), or data analysts (query\n  performance).\n* **Integrating with third-party BI tools for advanced analytics:** Connecting to Snowflake from tools like Tableau, Power BI, Looker, or\n  custom applications offers highly customizable and extensive control\n  over cost data visualization. Cloud-specific third-party data programs\n  (FinOps platforms) offer easier setup and more out-of-the-box\n  Snowflake cost optimization insights.\n* **Leverage Cortex Code (In Preview):** This AI Assistant capability\n  allows users to query cost and usage data in ACCOUNT\\_USAGE views using\n  natural language natively in the Snowsight UI.\n\n#### Investigate anomalous consumption activity\n\nCost Anomaly Detection is a critical component of visibility that\nleverages machine learning to continuously monitor credit consumption\nagainst historical spending patterns, automatically flagging significant\ndeviations from the established baseline. This proactive monitoring is\nessential for preventing budget overruns and identifying inefficiencies,\nshifting the organization from a reactive to a proactive cost management\nposture to mitigate financial risk. As a best practice, you should\ninitially review anomaly detection on the entire account to gain a broad\nview, then dive deeper into a more granular review for individual\nhigh-spend warehouses. [This approach](https://docs.snowflake.com/en/user-guide/cost-anomalies) allows for more targeted analysis and assigns clear ownership for\ninvestigating any flagged anomalies. There are several methods for\nanomaly detection supported by Snowflake:\n\n**Cost Anomalies in Snowsight**\n\nSnowsight, Snowflake's primary web interface, offers a dedicated Cost\nManagement UI that allows users to visually identify and analyze the\ndetails of any detected cost anomaly. The importance of this intuitive\nvisual interface lies in its ability to make complex cost data\naccessible to a wide range of stakeholders, enabling rapid root cause\nanalysis by correlating a cost spike with specific query history or user\nactivity. One of the tabs in this UI is the Cost Anomaly Detection tab,\nwhich enables you to view cost anomalies at the organization or account\nlevel and explore the top warehouses or accounts driving this change. To\nfoster a culture of cost awareness and accountability, it is a best\npractice to ensure there is an owner for an anomaly detected in the\naccount and set up a [notification (via email)](https://docs.snowflake.com/en/user-guide/cost-anomalies-ui) in the UI itself to ensure that cost anomalies are quickly and\naccurately investigated.\n\n**Programmatic Cost Anomaly Detection**\n\nFor deeper integration and automation, organizations can review [anomalies programmatically](https://docs.snowflake.com/en/user-guide/cost-anomalies-class) using the SQL functions and views available within the SNOWFLAKE.LOCAL\nschema. This approach is important for enabling automation and\nscalability, allowing cost governance to be embedded directly into\noperational workflows, such as feeding anomaly data into third-party\nobservability tools or triggering automated incident response playbooks.\nA key best practice is to utilize this programmatic access to build\ncustom reports and dashboards that align with specific financial\nreporting needs and to create advanced, automated alerting mechanisms\nthat pipe anomaly data into established operational channels, such as\nSlack or PagerDuty.\n\n**Custom Anomaly Detection & Notification**\n\nAlthough anomalies are detected at the account and organization level,\nif you desire to detect anomalies at lower levels (e.g. warehouse or\ntable), it is recommended to leverage Snowflake\u2019s [Anomaly Detection](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) ML class and pair it with a Snowflake [alert](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) to notify owners of more granular anomalies that occur within the\necosystem. This ensures all levels of Snowflake cost can be monitored in\na proactive and effective way. As a best practice, [notifications](https://docs.snowflake.com/en/user-guide/notifications/about-notifications) should be configured for a targeted distribution list that includes the\nbudget owner, the FinOps team, and the technical lead responsible for\nthe associated Snowflake resources, ensuring all stakeholders are\nimmediately aware of a potential cost overrun and can coordinate a swift\nresponse.\n\n## Control\n\n#### Overview\n\nThe Control principle of the Cost Optimization framework is designed to\nmove organizations beyond cost reporting by establishing the necessary\nautomated guardrails and governance policies to manage and secure\nSnowflake consumption proactively. This framework enforces financial\ngovernance by transforming cost visibility into tangible action,\nutilizing features like budgets and resource monitors to prevent\nuncontrolled growth and ensure consumption aligns strictly with\norganizational financial policies. Control is foundational for\nmaximizing the value of the platform by ensuring disciplined and\ncost-effective resource utilization.\n\n#### Recommendations\n\nImplementing a comprehensive control framework, supported by features\nsuch as Resource Monitors, Budgets, and Tagging Policies, empowers\norganizations to enforce financial accountability and maintain budget\npredictability. By adopting these controls, teams can actively manage\nspend, quickly and automatically mitigate cost inefficiencies, and\nensure the disciplined, cost-effective utilization of the entire\nSnowflake environment. The culmination of all of these controls leads to\ngreater platform ROI and minimized financial risk. To meet this goal,\nconsider the following recommendations based on industry best practices\nand Snowflake's capabilities:\n\n#### Proactively monitor all platform usage\n\nTo effectively manage and [control Snowflake spend](https://docs.snowflake.com/en/user-guide/cost-controlling) ,\nit is essential to establish and enforce cost guardrails. Implementing a [budgeting system](https://docs.snowflake.com/en/user-guide/budgets) is a key\nFinOps practice that promotes cost accountability and optimizes resource\nusage by providing teams with visibility into their consumption and the\nability to set alerts and automated actions. Budgeting helps to prevent\nunexpected cost overruns and encourages a cost-conscious culture.\n\n**Set budgets permissions**\n\nTo establish effective budgets, it's crucial to define [roles and privileges](https://docs.snowflake.com/en/user-guide/budgets) by configuring the role, team, or user responsible for the resources.\nThis ensures that budget tracking aligns with specific business units or\nprojects, enabling accurate cost attribution and accountability. By\nlinking consumption to the relevant stakeholders, you can create a clear\nshowback or chargeback model, which is vital for fostering a sense of\nownership over spending. This configuration should be part of a broader,\nconsistent tagging strategy to ensure all costs are properly allocated\nto departments, environments, or projects.\n\n**Create budget categories**\n\nCategorizing costs is fundamental for granular budget management. You\ncan establish budgets based on the [account](https://docs.snowflake.com/en/user-guide/budgets/account-budget) or create [custom categories](https://docs.snowflake.com/en/user-guide/budgets/custom-budget) using Object Tags. Custom tags, such as those for a data product or cost\ncenter, are critical for accurately apportioning costs across different\ndepartments, lines of business, or specific projects. This granular\napproach provides a detailed breakdown of where spending occurs,\nenabling more precise control and informed decision-making regarding\nresource allocation. Implementing robust tagging policies and naming\nconventions ensures consistency and facilitates the interpretation of\ncost data. Because budgets are soft limit objects, objects can be part\nof more than one budget if different perspectives need to be tracked for\ncost (e.g., cost center & workload level budgeting).\n\n**Implement a notification strategy**\n\nEffective budget management relies on timely communication. Setting up\nalerting through emails or webhooks to collaboration tools like Slack\nand Microsoft Teams provides proactive [notification](https://docs.snowflake.com/en/user-guide/budgets/notifications) to key stakeholders when spending approaches or exceeds a defined\nthreshold. These alerts provide teams with an opportunity to review and\nadjust their usage before it leads to significant cost overruns. This\ncapability positions organizations for security success by mitigating\npotential threats through comprehensive monitoring and detection.\n\nNotifications are not limited to just budgets; [Snowflake alerts](https://docs.snowflake.com/en/user-guide/alerts) can also be\nconfigured to systematically notify administrators of unusual or costly\npatterns, such as those listed in the Control and Optimize sections of\nthe Cost Pillar. This ensures that key drivers of Snowflake consumption\ncan be tracked and remediated proactively, even as the platform\u2019s usage\ngrows.\n\n#### Forecast consumption based on business needs\n\nForecasting Snowflake consumption should be a strategic business\nfunction, not a mere technical prediction. The goal is to establish a\ntransparent basis for budgeting and optimizing ROI by linking\nconsumption directly to measurable business outcomes. In a dynamic,\nusage-based environment where compute costs are the most volatile\nelement of the bill, a robust framework must integrate quantitative\nanalysis of historical usage with qualitative insights into future\nbusiness drivers. The following framework outlines how to build and\nmaintain a comprehensive consumption forecast.\n\n**Establish the Baseline**\n\nThis phase focuses on understanding the source of spend and establishing\ngranular cost accountability.\n\n* **Identify demand drivers and unit economics:** To understand what\n  drives Snowflake spend, correlate historical credit, storage, and data\n  transfer usage with key business metrics like cost per customer or per\n  transaction. Use Snowflake's [ACCOUNT\\_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) schema, including the WAREHOUSE\\_METERING\\_HISTORY and QUERY\\_HISTORY\n  views, as the primary data sources for this analysis.\n* **Granular cost attribution:** Accurately tie costs back to business\n  teams or workloads by implementing a mandatory tagging strategy for\n  all warehouses and queries. Align these tags with your organization's\n  financial structure to provide clear cost segmentation.\n\n**Build the predictive model**\n\nThis phase integrates historical trends with strategic business inputs\nto create forward-looking projections.\n\n* **Historical trend analysis:** Analyze past usage for trends,\n  seasonality, and outliers to inform future projections. Start with\n  simple trend-based forecasting and progressively move to more\n  sophisticated models, leveraging Snowflake\u2019s built-in [SNOWFLAKE.ML.FORECAST function](https://docs.snowflake.com/en/user-guide/ml-functions/forecasting) for time-series forecasting.\n* **Driver-based forecasting:** Integrate planned business initiatives\n  and new projects directly into the model. Collaborate with business\n  leaders to gather strategic inputs such as projected customer growth,\n  new product launches, or increased data ingestion from marketing\n  campaigns.\n* **Scenario modeling:** Develop multiple forecast scenarios (e.g.,\n  \"conservative,\" \"base case,\" \"aggressive\") by applying varied growth\n  factors to key business drivers. This enables flexible planning and\n  helps mitigate financial risk.\n\n**Operationalize and optimize**\n\nThis phase links the forecast to continuous monitoring, governance, and\nproactive cost controls.\n\n* **Continuous monitoring and variance analysis:** Regularly compare\n  actual consumption against the forecast to identify and investigate\n  significant variances. This feedback loop is crucial for refining the\n  underlying model and adapting to evolving business needs.\n* **Collaborative governance:** Ensure a single source of truth for\n  consumption data by establishing a regular FinOps review session with\n  Finance, Engineering, and Business teams. Use customized dashboards to\n  present data in business-friendly terms.\n* **Implement predictive budget controls:** Shift from reactive spending\n  to a proactive model. Utilize Snowflake Resource Monitors and Budgets,\n  which employ monthly-level time-series forecasting, to define credit\n  quotas and trigger automated alerts or suspensions to prevent cost\n  overruns.\n\n#### Enforce cost guardrails for organizational resources\n\nTo effectively manage Snowflake expenditure and prevent unforeseen\ncosts, it is crucial to implement a robust framework of resource\ncontrols. These controls act as automated guardrails, ensuring that\nresource consumption for compute, storage, and other services aligns\nwith your financial governance policies. By proactively setting policies\nand remediating inefficiencies, you can maintain budget predictability\nand maximize the value of your investment in the platform.\n\n**Compute controls**\n\nControlling compute consumption is often the most critical aspect of\nSnowflake cost management, as it typically represents the largest\nportion of spend. Snowflake offers several features to manage warehouse\nusage and prevent excessive costs.\n\n* **Implement resource monitors** : [Resource monitors](https://docs.snowflake.com/en/user-guide/resource-monitors) are a powerful feature for tracking and controlling credit consumption\n  across virtual warehouses or for the entire account. Their primary\n  importance lies in their ability to enforce strict budget limits,\n  preventing cost overruns by automatically [triggering actions](https://docs.snowflake.com/en/user-guide/resource-monitors) ,\n  such as sending notifications and/or suspending warehouses when credit\n  usage reaches a defined quota. For effective governance, it is a best\n  practice to create multiple resource monitors at different\n  granularities (e.g., per-department, per-project) with escalating\n  actions, such as notifying administrators at 80% usage and suspending\n  all assigned warehouses at 100% usage, to cap spending. It is also\n  considered best practice to ensure there is a consistent action tied\n  to the resource monitors based on your organization\u2019s ways of working.\n  For example, it is worth considering that set resource monitors\n  perform actions like notify to specific admin(s) within your\n  organization.\n* **Reduce Runaway Queries:** Runaway or hung queries can lead to\n  significant cost overruns. Managing long-running queries is especially\n  important for environments with ad-hoc users or complex analytical\n  workloads, where a poorly written query can consume credits for hours.\n  \n    + **Statement timeout policies** automatically terminate any query\n        that runs longer than a specified time limit. This serves as an\n        essential guardrail to prevent individual queries from consuming\n        excessive resources and impacting both cost and performance for\n        other users. The best practice is to set the [STATEMENT\\_TIMEOUT\\_IN\\_SECONDS parameter](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) at different levels\u2014for the account, warehouse, specific users, or\n        individual sessions\u2014to tailor controls to different workload\n        patterns, such as allowing longer timeouts for ETL warehouses\n        compared to BI warehouses. [Queued timeout policies](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) can also help remove queries that eclipse a reasonable time\n        threshold and could have been run elsewhere by users trying to\n        receive a response.\n    + **Policy-based automation** can also cancel queries pre-emptively.\n        An example is using stored procedures that leverage the [SYSTEM$CANCEL\\_QUERY](https://docs.snowflake.com/en/sql-reference/functions/system_cancel_query) function to terminate statements that exceed predefined runtime\n        thresholds or contain ill-advised logic, such as exploding joins.\n        This approach allows you to more finely customize the types of\n        queries you want to cancel, as you have full control over defining\n        the stored procedure logic.\n* **Auto-suspend policies** : Auto-suspend policies are a foundational\n  cost control for virtual warehouses, automatically suspending a\n  warehouse after a defined period of inactivity. By default, all\n  warehouses have auto-suspend enabled, however this feature can be\n  disabled, so it\u2019s important to [monitor warehouse auto-suspend configuration](https://docs.snowflake.com/en/user-guide/warehouses-considerations) and ensure proper access controls are set to restrict users from disabling the auto-suspend setting. The best practice for balancing cost versus performance is to reduce the auto-suspend policy to the minimum possible (generally above 60 seconds) without affecting query caching and performance (SLA) expectations.\n\n**Storage Controls**\n\nWhile storage costs are generally lower than compute costs, they can\ngrow significantly over time. Understanding the different components of\nstorage cost and implementing policies to manage the types of storage is\nkey to keeping these costs in check.\n\n* **Staged files:** [Staged](https://docs.snowflake.com/en/user-guide/data-load-considerations-stage) files are files that have been prepped for bulk data loading/unloading\n  (stored in compressed or uncompressed format). They can be stored in\n  an [external stage](https://docs.snowflake.com/en/user-guide/data-load-s3-create-stage) using cloud provider\u2019s blob storage (such as Amazon S3) or an [internal stage](https://docs.snowflake.com/en/user-guide/data-load-local-file-system-create-stage) within the Snowflake platform. You are only charged for data stored in\n  internal stages.\n  \n+ To help control costs on staged files, as well as improve\n        performance of data loads, you can ensure successfully ingested\n        files are removed by using the PURGE = TRUE option of the [COPY INTO <table>](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table) command . Alternatively, use the [REMOVE](https://docs.snowflake.com/en/sql-reference/sql/remove) command to remove the files in the stage.\n* **Active Storage:** [Active Storage](https://docs.snowflake.com/en/sql-reference/info-schema/table_storage_metrics) consists of the data in a table that can be actively queried against\n  at the current point of time (i.e., without using Time-Travel\n  commands) **.** To control active storage costs, you can create a [Storage Lifecyle Policy](https://docs.snowflake.com/LIMITEDACCESS/storage-lifecycle-policy/storage-lifecycle-policies) to automatically archive or delete data based on an expiration policy\n  you create.\n* **Time Travel:** [Time Travel](https://docs.snowflake.com/en/user-guide/data-time-travel) data is data that is maintained for all historical changes of a table\n  through its retention time, allowing for easy recovery of changed or\n  deleted data.\n  \n    + Time Travel data retention is controlled using the [DATA\\_RETENTION\\_TIME\\_IN\\_DAYS](https://docs.snowflake.com/en/user-guide/data-time-travel) parameter , which can be set at a number of different object\n        levels (i.e. Account, Database, Schema, Table), so it is important\n        to monitor this setting to ensure you do not have excessive data\n        retention where it is not needed.\n    + Snowflake has a standard [data retention period](https://docs.snowflake.com/en/user-guide/data-time-travel) of one day. This is automatically enabled for all accounts, but for some tables, particularly [large, high churn tables](https://docs.snowflake.com/en/user-guide/tables-storage-considerations) , this one day can still result in excessive costs. To reduce costs in these cases, you can create these tables as transient with zero Time Travel retention (i.e., DATA\\_RETENTION\\_TIME\\_IN\\_DAYS=0) and periodically insert a copy of the table contents into a permanent table.\n* **Fail-safe:** In addition to Time-Travel, Snowflake retains\n  historical data for seven days after the Time-Travel retention period\n  expires as [Fail-Safe](https://docs.snowflake.com/en/user-guide/data-failsafe) data. This data can be requested for worst-case scenario data\n  recovery. Fail-safe applies to all tables that are created as the\n  permanent [table type](https://docs.snowflake.com/en/user-guide/tables-temp-transient) (which is the default table type).\n  \n+ However, for ETL or data modeling, it is common to have tables that\n        are created on a short-term basis to support transformations. For\n        these use cases, it is recommended that you use [Temporary and Transient tables](https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs) ,\n        which do not utilize fail-safe storage and thus do not incur\n        additional storage costs.\n* **Retained for clones:** This is data that is stored because it is\n  referenced by a clone, despite the data being deleted or outside the\n  retention period of the base table that was cloned. To control costs\n  related to these \u201cstale\u201d clones, it is recommended that you monitor [RETAINED\\_FOR\\_CLONE\\_BYTES](https://community.snowflake.com/s/article/How-to-manage-RETAINED-FOR-CLONE-BYTES) and drop clones that are no longer needed. You can leverage the [Alerts and Notifications features](https://docs.snowflake.com/en/guides-overview-alerts) to alert you when RETAINED\\_FOR\\_CLONE\\_BYTES exceeds a threshold, prompting you to take action.\n\n**Serverless Features**\n\nFor serverless features, which do not use warehouse compute and\ntherefore cannot leverage the Resource Monitor feature, we recommend\nsetting up a budget. [Budgets](https://docs.snowflake.com/en/user-guide/budgets) allow\nyou to define a monthly spending limit on the [compute costs](https://docs.snowflake.com/en/user-guide/cost-understanding-compute) for a Snowflake account or a custom group of Snowflake objects. When the\nspending limit is projected to be hit, a notification is sent. While\nBudgets do not explicitly allow you to suspend serverless features upon\nreaching a limit (the way that Resource Monitors do), Budgets can be\nconfigured to not only send emails, but also send [notifications](https://docs.snowflake.com/en/user-guide/budgets/notifications) to a cloud message queue or other webhooks (such as Microsoft Teams,\nSlack, or PagerDuty). This then gives you the ability to trigger other\nactions for remediation.\n\n#### Govern resource creation and administration\n\nTo prevent uncontrolled spend as organizations scale, it's essential to\nhave a clear management strategy for Snowflake resources, most notably,\nvirtual warehouses. This strategy should encompass a defined\nprovisioning process, ongoing object management, and automated platform\nenforcement to foster agility while maintaining financial discipline.\n\n**Centralized vs. decentralized management**\n\nOrganizations tend to adopt one of two primary approaches to managing\nSnowflake resources:\n\n* **Centralized management:** A dedicated team, such as a platform\n  Center of Excellence (CoE), handles resource creation and\n  administration policies. This ensures consistency, adheres to best\n  practices, and facilitates robust cost control. This model is ideal\n  for large enterprises where strict governance and chargeback are\n  paramount.\n* **Decentralized management:** Individual business units or teams\n  manage their own resources. This provides greater autonomy and speed\n  but can lead to resource sprawl, inconsistent practices, and\n  significant cost inefficiencies if not properly governed.\n\n**Striking the balance: the federated model**\n\nThe most effective strategy often lies in a hybrid, or federated, model.\nThis approach combines centralized governance (policies defined by a\nCoE) with decentralized execution (teams having the freedom to create\nresources within those guardrails). This balance enables agility while\nmitigating financial risk.\n\n**Core Principles for Governance**\n\nRegardless of the chosen model, these principles are essential for\neffective governance:\n\n* **Limit resource creation:** Restrict the ability to create or modify\n  virtual warehouses and other high-cost resources to a small number of\n  trusted roles to prevent uncontrolled growth.\n* **Establish a transparent workflow:** Create a clear, simple workflow\n  for provisioning resources, especially for larger warehouses. For\n  example, any request for a warehouse of medium size or larger should\n  require a business justification and an assigned cost center.\n* **Near real-time visibility:** Triggered visibility is non-negotiable\n  for monitoring resource creation and resizing. Configure alerts that\n  notify FinOps or CoE teams whenever a new warehouse is created or an\n  existing one is modified outside of a provisioning workflow. This\n  allows for immediate review and prevents overprovisioning.\n* **Enforce tagging:** Make a mandatory tagging strategy a prerequisite\n  for all resource creation. This ensures every credit spent can be\n  accurately attributed to the correct department or project, enabling\n  robust chargeback and accountability.\n* **Automate deactivation:** To prevent object sprawl, implement\n  policies that identify and deactivate stale resources after a\n  predetermined period of disuse.\n\n## Optimize\n\n#### Overview\n\nThe Optimization principle of the Cost Optimization framework focuses on continuously improving the efficiency of your Snowflake resources. This includes optimizing compute, storage, data transfer, and managed services by understanding their usage and identifying areas for improvement. The frequency of optimization efforts should be guided by the metrics established in the Visibility principle and monitored through the Control principle. All recommendations within this Optimize principle are to be reviewed on a regular cadence and balanced with business and performance needs.\n\n#### Recommendations\n\nSnowflake offers numerous optimization controls within its platform.\nThese features are designed to enhance efficiency and reduce\nadministrative overhead for your various workloads. Coupled with\noperational best practices that utilize features in a healthy manner,\nyou can balance performance goals with cost governance requirements to\nmeet business objectives.\n\nBy implementing these recommendations, you will be able to:\n\n* Reduce administration time\n* Increase workload efficiency\n* Balance cost controls and guardrails to meet service level agreements\n  and business objectives\n* Achieve healthy growth and economies of scale within your organization\n\nTo foster healthy growth and achieve economies of scale within your\norganization, we recommend the following, drawing upon industry best\npractices and Snowflake's capabilities.\n\n#### Compute workload-aligned provisioning\n\nCompute is the most significant part of any organization\u2019s Snowflake\nspend, typically accounting for 80% or more of spend. A good warehouse\ndesign should incorporate the principles below:\n\n* Separate warehouses by workload (e.g., ELT versus analytics versus\n  data science)\n* Workload size in bytes should match the t-shirt size of the warehouse\n  in the majority of the workloads\u2013larger warehouse size doesn\u2019t always\n  mean faster\n* Align warehouse size for optimal cost-performance settings\n* [Utilize Multi-Cluster Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-considerations) configuration to solve for high concurrency\n* Utilize [Query Acceleration Services](https://docs.snowflake.com/en/user-guide/query-acceleration-service) to help with infrequent, large data scans\n* For memory-intensive workloads, use a warehouse type of Snowpark\n  Optimized or higher memory resource constraint configurations as\n  appropriate\n* Set appropriate auto-suspend settings - longer for high cache use,\n  lower for no cache reuse\n* Set appropriate warehouse query timeout settings for the workload and\n  the use cases it supports.\n\nUsing the principles above ensures that your compute costs are well\nmanaged and balanced with optimal benefits.\n\n**Separate warehouses by workload**\n\nDifferent workloads (e.g., data engineering, analytics, AI and\napplications) have varying characteristics. [Separating these to be serviced by different virtual warehouses](https://docs.snowflake.com/en/user-guide/warehouses-considerations) can help ensure relevant features in Snowflake can be utilized.\n\nSome examples of this include:\n\n* Optimize dashboards and reports by [reusing the warehouse SSD cache](https://docs.snowflake.com/en/user-guide/warehouses-considerations) for repeated select queries. This can be achieved by configuring a\n  longer warehouse autosuspend setting.\n* Loading large volumes of data using [optimal file sizes](https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare) and utilizing all available threads in a virtual warehouse\n* Processing large data sets for AI workloads using memory-optimized\n  warehouses\n\n**Warehouse sizing**\n\nMapping the workload to the [right warehouse size](https://docs.snowflake.com/en/user-guide/performance-query-warehouse-size) and configuration is an important consideration of warehouse design.\nThis should consider several factors like query completion time,\ncomplexity, data size, query volume, SLAs, queuing, and balancing\noverall cost objectives. Warehouse sizing involves a cost-benefit\nanalysis that balances performance, cost, and human expectations. Humans\noften have expectations that their queries will not be queued or take a\nlong time to complete, so it is recommended to have dedicated warehouses\nfor teams.\n\nRecommendations for choosing the right-sized warehouse include:\n\n* Follow the principles outlined above and understand that this is a\n  continuous improvement process.\n* Choose a size based on the estimated or actual workload size and\n  monitor.\n* Utilize Snowflake's extensive telemetry data, such as [QUERY\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) and [WAREHOUSE\\_METERING\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history) ,\n  to validate that the warehouse size is impacting the metrics you care\n  about in the direction you intend.\n\n**Optimal warehouse settings**\n\nWhile Snowflake strives for minimal knobs and self-managed tuning, there\nare situations where selecting the right settings for warehouses can\nhelp with optimal cost and/or performance. Some of the key [warehouse settings](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse) include\n\n* Auto suspend\n* Multi-cluster settings\n* Warehouse resource constraints\n* Warehouse type\n\nTo maintain an optimal balance between cost and performance, regularly\nmonitor your resource usage (e.g., weekly or monthly) and set up\nresource monitors to alert you to high credit consumption. When workload\ndemands change, adjust your settings as needed.\n\n**Warehouse consolidation**\n\nIf you find yourself with an excess of provisioned warehouses or a shift\nin workloads necessitating consolidation, apply the aforementioned\nprinciples. Begin with the least utilized warehouses and migrate their\nworkloads to an existing warehouse that handles similar tasks.\n\nThe [WAREHOUSE\\_LOAD\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_load_history) view can help you assess the average number of queries running on a\nwarehouse over a specific period. A useful benchmark is to aim for a\nwarehouse running queries 80% of the time it's active. Continuously\nmonitor your key metrics to ensure they still meet SLA goals and adjust\nwarehouse settings as needed.\n\n#### Leverage Managed Services\n\nTo achieve significant operational efficiency and predictable costs,\nprioritize the use of serverless and managed services. These services\neliminate the need to manage underlying compute infrastructure, allowing\nyour organization to pay for results rather than resource provisioning\nand scaling. Evaluate the following servterless features to reduce costs\nand enhance performance in your environment.\n\n**Storage optimization**\n\nSnowflake offers several serverless features that automatically [manage and optimize your tables](https://docs.snowflake.com/en/user-guide/performance-query-storage) ,\nreducing the need for manual intervention while improving query\nperformance. The following features ensure your data is efficiently\norganized, allowing for faster and more cost-effective qeuerying without\nthe burden of user management.\n\n**Automatic Clustering** is a background process in Snowflake that\norganizes data within a table by sorting it according to predefined\ncolumns. This process is critical for optimizing query performance and\nreducing costs. Benefits include:\n\n* **Improved query pruning:** By sorting data, automatic clustering\n  enables more effective pruning in SQL WHERE clauses, meaning less data\n  needs to be scanned for a given query.\n* **Faster joins:** Clustering also results in quicker and more\n  efficient join operations.\n* **Cost-efficient queries:** These benefits ultimately result in faster\n  and more cost-effective query execution.\n\n**Considerations and Best Practices:**\n\n* **Careful tuning:** It's essential to tune automatic clustering\n  carefully. Overuse of clustering keys, use of highly selective keys,\n  or frequent data manipulation language (DML) operations can\n  significantly increase clustering costs.\n* **Infrequently queried, frequently updated tables:** Exercise caution\n  with tables that are often updated but rarely queried, as clustering\n  costs may outweigh performance improvements.\n* **Cost estimation:** Before enabling, estimate clustering costs for a\n  table using Snowflake's system function for a preliminary cost/benefit\n  analysis.\n* **Strategic cluster key selection:** Optimizing clustering key\n  selection is vital to strike a balance between cost and performance.\n\n**Search Optimization Service (SOS)** enhances the performance of point\nlookup searches by creating a persistent search access path. Its primary\nvalue lies in achieving better pruning for these specific query types,\nwhich is critical for applications requiring quick response times. They\ncan be used in combination with auto-clustering and [Snowflake Optima service](https://docs.snowflake.com/en/user-guide/snowflake-optima) .\n\n**Considerations for SOS:**\n\n* **Excessive indexed columns:** Avoid having too many indexed columns,\n  especially on high-churn tables, as maintenance costs can become\n  substantial.\n* **Inefficient cost-benefit ratio:** Tables that are frequently updated\n  but infrequently queried for point lookups can lead to an inefficient\n  cost/benefit ratio.\n* **Cost estimation:** Before enabling, estimate SOS costs on a given\n  table using a Snowflake system function to perform a preliminary\n  cost/benefit analysis. Index selection is crucial for balancing\n  performance and cost, a topic also addressed in the Workload\n  Optimization section.\n\n**Materialized Views (MVs)** are pre-computed query results stored as a\nseparate table and automatically maintained by Snowflake.\n\n**Benefits of MVs:**\n\n* **Cost efficiency:** It is often cheaper to update a materialized view\n  than to repeatedly execute full scans of large, complex base tables.\n* **Alternative table order:** MVs can provide an alternative table\n  order for queries that do not align with your existing clustering\n  design.\n\n**Considerations for MVs:**\n\n* **Infrequently queried, frequently updated tables:** Avoid creating\n  materialized views on tables that are frequently updated but rarely\n  queried, as automated maintenance costs will negate any potential\n  query cost savings.\n* **Clustering differences:** Be aware of clustering differences between\n  the base table and the materialized view, as this can lead to high\n  maintenance costs.\n\n**[Query Acceleration Service](https://docs.snowflake.com/en/user-guide/query-acceleration-service) (QAS):** QAS is a serverless feature that provides a burst of additional\ncompute resources to accelerate specific parts of a query, rather than\nreplacing an appropriately sized warehouse. It's particularly beneficial\nfor large I/O operation queries, eliminating the need to manually scale\nwarehouses up or down. QAS also helps speed up query execution when\ntable clustering cannot be altered due to other workload dependencies. A\ncost-benefit analysis should always be performed to ensure that the\ncredit consumption from QAS is justified by the performance improvement\nand the avoided cost of a larger warehouse.\n\n**Serverless tasks:** Serverless tasks enable the execution of SQL\nstatements or stored procedures on a user-defined schedule, eliminating\nthe need for a user-managed virtual warehouse. This is a cost-effective\nsolution for infrequent workloads where a warm cache offers minimal\nvalue, or for unpredictable workloads that don't utilize a minimum\n60-second usage.\n\n#### Data storage types & lifecycle management\n\nNext to compute, storage often represents the second-highest cost\ncomponent in Snowflake. Effective storage governance is a critical\nconcern for many industries due to federal and global regulations.\nSnowflake's default settings prioritize maximum data protection, which\nmay not always align with the requirements of every workload or\nenvironment. This section focuses on how to manage and configure\nstorage-related settings appropriately, ensuring that storage costs\nremain reasonable and deliver business value.\n\n**Optimizing table volume and auditing usage**\n\n* **Review policy-driven data lifecycle management:**\n  \n    + **Time Travel & Fail-safe:** Set the [DATA\\_RETENTION\\_TIME\\_IN\\_DAYS](https://docs.snowflake.com/en/sql-reference/parameters) parameter on a per-table or per-schema basis to the minimum required\n        for your business needs. For transient data, use [TRANSIENT](https://docs.snowflake.com/en/user-guide/tables-temp-transient) tables to eliminate Fail-safe costs.\n    + **Retained for clone:** Be mindful of cloning operations. While\n        zero-copy cloning is cost-effective initially, any subsequent DML\n        (Data Manipulation Language) operations on the clone will create new\n        micro-partitions, increasing storage costs. It is recommended to\n        drop clones when they are no longer needed.\n* **Be aware of high-churn tables:**\n  \n+ If a table is updated consistently, inactive storage (Time Travel &\n        Fail-safe data) can grow at a much faster rate than active storage.\n        A high churn table is generally characterized as one that has 40% or\n        more of its storage inactive. Therefore, aligning both the retention\n        time and the use of an appropriate table type with business and\n        recovery requirements is paramount to keeping costs under control.\n        Review High Churn tables on a consistent basis to ensure their\n        configuration is as desired.\n* **Proactively clean up unused objects:**\n  \n    + **Large tables that are never queried:** Establish a process to\n        identify, rename, and eventually drop tables that have not been\n        queried for an extended period. You can use the [ACCESS\\_HISTORY](https://docs.snowflake.com/en/user-guide/access-history) account usage view to review the last time a table was selected\n        from. Snowflake\u2019s [Cost Insights](https://docs.snowflake.com/en/user-guide/cost-insights) will also check weekly for unused tables on your behalf.\n    + **Short-lived permanent tables:** For staging or intermediate data\n        that is rebuilt frequently, use [TRANSIENT](https://docs.snowflake.com/en/user-guide/tables-temp-transient) or [TEMPORARY](https://docs.snowflake.com/en/user-guide/tables-temp-transient) tables instead of permanent tables to avoid unnecessary Time Travel\n        and Fail-safe storage fees.\n* **Perform catalog & stage management:**\n  \n+ **Data archiving:** For historical data that is rarely accessed,\n        consider moving it to cooler tiers with [storage lifecycle policies](https://docs.snowflake.com/LIMITEDACCESS/storage-lifecycle-policy/storage-lifecycle-policies) or deleting it altogether.\n\n**Optimizing Managed Data Structures and Access**\n\n* You do not always need to define cluster keys for all tables (unlike\n  many other relational database management systems) if Snowflake's\n  natural data loading maintains consistent micro-partition min/max\n  values relative to your query patterns. Additionally, you can disable\n  Auto-Clustering on a table while keeping its cluster key definition\n  without incurring extra costs.\n* **Infrequently used materialized views and search optimization paths:** Materialized Views and search optimization paths can incur\n  unnecessary storage and compute costs if they are no longer actively\n  utilized. Materialized Views are most effective for stable data tables\n  with repeated complex aggregations or joins, while search optimization\n  is designed for high-speed point lookup queries. Snowflake\u2019s [Cost Insights feature](https://docs.snowflake.com/en/user-guide/cost-insights) can help identify instances where these objects are rarely used,\n  prompting a review to determine if their performance benefits still\n  outweigh their associated costs.\n\n#### Limit data transfer\n\nData egress, the transfer of data from one cloud provider or region to\nanother, can incur substantial costs, particularly when handling large\ndata volumes. Implementing appropriate tools and best practices is\nessential to minimize these data transfer expenses and maximize business\nvalue when data egress is necessary.\n\n**Tooling: Enable proactive cost management**\n\nLeverage Snowflake's native features to gain visibility and control over\ndata transfer costs before they become a significant expense.\n\n* **Egress Cost Optimizer (ECO):** For providers of data products on the\n  Snowflake Marketplace or private listings, enabling the [Egress Cost Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) (ECO) at the organizational level is a critical best practice. ECO is\n  an automated feature for listings with Cross-Cloud Auto-Fulfillment.\n  It intelligently routes data through a Snowflake-managed cache,\n  allowing you to pay a one-time egress cost for the initial data\n  transfer. After that, expanding to new regions incurs zero additional\n  egress costs for the same dataset. This is a powerful tool for scaling\n  your data sharing without compounding data transfer fees.\n* **Monitoring and alerts:** To effectively manage data transfer costs,\n  utilize Snowflake's [DATA\\_TRANSFER\\_HISTORY](https://docs.snowflake.com/en/sql-reference/organization-usage/data_transfer_history) telemetry view. This view provides detailed insights into data\n  movement between different regions and clouds. Establish dashboards\n  and alerts to meticulously track this usage, enabling prompt detection\n  of any unexpected cost increases.\n\n**Architectural best practices: Design for minimal data movement**\n\nMinimizing data transfer costs for your workloads heavily depends on the\narchitecture of your data pipelines and applications. Adhere to the\nfollowing best practices to achieve this:\n\n* **Unloading Data:**\n  \n    + **Compress data:** When using the [COPY INTO <location>](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location) command to unload data, always use a compression format (e.g., GZIP,\n        BZIP2, or Brotli). This dramatically reduces the volume of data\n        transferred and directly lowers egress costs.\n    + **Filter before unloading:** Before unloading a large dataset, use\n        SQL to filter and transform the data. Unload only the required\n        columns and rows to minimize the volume of data that must leave the\n        Snowflake environment.\n* **Data replication:** Replicating a database to a Snowflake account in\n  a different geographical region or on a different cloud provider\n  incurs data transfer fees. While useful for disaster recovery, this\n  can become expensive if not managed carefully.\n  \n    + **Targeted replication:** If a full database replica is not\n        required, use [replication groups](https://docs.snowflake.com/en/user-guide/account-replication-intro) to replicate only the necessary databases or schemas. This granular\n        approach ensures you only pay for the data you absolutely need to\n        move.\n    + **Consider refresh cadence:** your frequency of refresh will affect\n        the amount of data that is replicated, since it acts like a snapshot\n        rather than every incremental change. Incremental ETL practices are\n        recommended even more with data that is being replicated vs full\n        table reloads.\n* **External network access and functions:**\n  \n    + **Minimize data egress:** [External functions](https://docs.snowflake.com/en/sql-reference/external-functions) and [external network access](https://docs.snowflake.com/en/developer-guide/external-network-access/external-network-access-overview) ,\n        which call remote services (e.g., APIs), transfer data out of\n        Snowflake. For best practice, filter data _before_ sending it to the\n        external service. Avoid writing functions that pass an entire large\n        table as an input; instead, pass only a small, pre-filtered subset.\n    + **Co-locate services:** If possible, deploy the remote service (like\n        an AWS Lambda or Azure Function) in the same cloud and region as\n        your Snowflake account to eliminate cross-region and cross-cloud\n        egress fees.\n* **Cross-cloud auto-fulfillment:**\n  \n+ **Embrace the cache:** As a data provider, enable [Cross-Cloud Auto-Fulfillment](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment) for your listings. As mentioned under Tooling, this feature\n        automates the replication process and works in conjunction with the\n        Egress Cost Optimizer to ensure you pay a single, upfront cost for\n        data transfer, even as your data product expands to new regions.\n* **Cross-region/cross-cloud Iceberg storage:**\n  \n+ **Centralize your catalog:** While Snowflake supports [Iceberg tables](https://docs.snowflake.com/en/user-guide/tables-iceberg-storage) that are external to its managed storage, be mindful of where the\n        data resides. If your Snowflake account and your Iceberg data are in\n        different regions, querying the Iceberg table will result in egress\n        costs from your cloud provider. For a well-architected solution,\n        keep your Iceberg data and your Snowflake account in the same region\n        to avoid these egress charges.\n\n#### Workload optimization\n\nWorkload optimization focuses on identifying the efficiency of your data\nprocessing activities within Snowflake. This involves a holistic\napproach encompassing the review of query syntax, data pipelines, table\nstructures, and warehouse configurations to minimize resource\nconsumption and improve performance. By addressing inefficiencies across\nthese areas, organizations can significantly reduce costs and accelerate\ndata delivery.\n\n**Query syntax optimization**\n\nInefficient queries often lead to excessive and hidden credit\nconsumption. Organizations can identify performance bottlenecks and\nunderstand the cost impact of specific SQL patterns by using Snowflake\nfeatures and adhering to SQL code best practices. This enables\ndevelopment teams to create more efficient and cost-effective code by\nhighlighting poor performing queries. Refer to the Performance\nOptimization Pillar of the Snowflake Well-Architected Framework for\ndetails on how to do this.\n\n**Utilize query history & insights for highlevel monitoring**\n\nFor broader visibility across all workloads, the Snowsight UI and the\nACCOUNT\\_USAGE schema are indispensable.\n\n* When looking for opportunities to improve, it is best to look for\n  queries that have excessive execution time across similarly run\n  queries. The newly created [Grouped Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) aggregates queries to their [parameterized query hash](https://docs.snowflake.com/en/user-guide/query-hash) and allows users to sort based on key performance indicators like\n  total queries, total latency, bytes scanned, and drill down into the\n  query execution across time. It is recommended to start with queries\n  with outsized latency and query runs.\n* The [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) page in Snowsight provides a high-level, filterable view of past and\n  currently running queries. It allows for you to drill into the Query\n  Profile for individual query statistics and investigation, even while\n  the query is running.\n* Snowflake's [Query Insights](https://docs.snowflake.com/en/user-guide/query-insights) feature (both a set of account usage tables and query profile\n  attributes) are also in Snowsight and can easily surface queries that\n  would benefit from optimization, such as queries with exploding joins\n  or ineffective filtering criteria that Snowflake will surface on your\n  behalf.\n\n**Leverage the query profile for deep-dive analysis**\n\nAfter identifying problematic queries, the [Query Profile](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) is an essential tool for understanding the execution plan of a query. It\nprovides a detailed, step-by-step breakdown of every operator involved,\nfrom data scanning to final result delivery. To gain visibility into\ninefficiencies, analysts and developers should regularly use the Query\nProfile to identify common anti-patterns like:\n\n* **Nodes with high execution time:** Pinpoint specific operations\n  (e.g., a complex join or a sort operation) that consume the majority\n  of the query's execution time.\n* **\"Exploding\" joins:** Identify Cartesian products or other\n  inefficient joins that create a much larger number of rows than\n  expected.\n* **Excessive table scanning:** In the profile, compare Partitions\n  scanned to Partitions total. A large number of scanned partitions on a\n  clustered table often indicates an opportunity to improve table\n  pruning by adding or refining cluster keys or modifying WHERE clause\n  predicates.\n* **Data spillage:** Look for operators spilling data to local or remote\n  disk. This indicates that the warehouse memory is insufficient for the\n  operation, resulting in significant performance degradation. This\n  might suggest a need to temporarily increase warehouse size for that\n  specific workload or rewrite the query to consume less memory.\n\n**Programmatically deconstruct queries for automated analysis**\n\nFor advanced use cases and automated monitoring, you can\nprogrammatically access query performance data. The [GET\\_QUERY\\_OPERATOR\\_STATS](https://docs.snowflake.com/en/sql-reference/functions/get_query_operator_stats) function can be used to retrieve the granular, operator-level statistics\nfor a given query ID, showing many of the steps and attributes available\nin the query profile view. This allows you to build automated checks\nthat, for instance, flag any query where a full table scan accounts for\nmore than 90% of the execution time or where data spillage exceeds a\ncertain threshold. This approach helps scale performance visibility\nbeyond manual checks.\n\n**Pipeline optimization**\n\nSnowflake pipeline optimization is about designing and managing data\ningestion and transformation processes that are efficient,\ncost-effective, scalable, and low-maintenance, while balancing business\nvalue and SLAs (service levels for freshness and responsiveness). Key\nlevers include architecture patterns (truncate & load versus incremental\nloads), use of serverless managed services (e.g., Snowpipe, Dynamic\nTables), and auditing loading practices to maximize cost and performance\nbenefits.\n\n**Batch loading**\n\nThe COPY INTO\n( [table](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table) or [location](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location) )\ncommand is a foundational and flexible method for bulk data loading from\nan external stage into a Snowflake table. Its importance lies in its\nrole as a powerful, built-in tool for migrating large volumes of\nhistorical data or loading scheduled batch files. The best practice is\nto use COPY INTO for one-time or large batch data loading jobs, which\ncan then be supplemented with more continuous ingestion methods like\nSnowpipe for incremental data. Additional information regarding COPY\nINTO and general data loading best practices can be found in our\ndocumentation [here](https://docs.snowflake.com/user-guide/data-load-considerations) .\nSome additional best practices are outlined below.\n\n* **File and batch sizing:** Optimal performance is achieved with files\n  sized 100\u2013250 MB compressed. Too few large files or too many very\n  small files reduce load parallelism and reduce efficiency.\n* **Parallelism:** Size and configure your warehouse cluster to match\n  the number and typical size of files to be loaded. (e.g., an XS\n  warehouse has eight threads; to utilize it fully, you need at least\n  eight files).\n* **File organization:** Partition files in external stages by logical\n  paths (date, region, etc.) to allow selective/cost-effective loads and\n  enable easy partition-level reloading.\n* **Pattern filtering:** Use COPY's pattern and files parameter to\n  precisely select the right files for each load, particularly to avoid\n  scanning entire stages.\n* **Resource management:** Use resource monitors and low auto-suspend\n  settings on load warehouses to minimize idle compute costs. For more\n  information here see our section on Resource Monitors.\\*\\* \\*\\*\n\n**Serverless ingestion**\n\nWhile named similarly, [Snowpipe](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro) and [Snowpipe Streaming](https://docs.snowflake.com/en/user-guide/snowpipe-streaming/data-load-snowpipe-streaming-overview) are different serverless methods to ingest data. You would utilize one\nversus the other depending on your SLA requirements for data delivery,\nand based on how data is landing for consumption.\n\n* **Snowpipe (file-based micro-batch ingestion)** : Use Snowpipe for\n  continuous ingestion of files where near real-time availability is\n  acceptable and ingestion frequency is moderate. File size guidance is\n  the same as COPY INTO (100-250MB). This leverages serverless compute\n  to avoid warehouse management overhead.\n* **Snowpipe Streaming (rowset-based, real-time ingestion)** : Use\n  Snowpipe Streaming for very low-latency (sub-five-second) ingestion,\n  high throughput, and streaming sources (e.g., Kafka, Kinesis, and\n  event hubs). You will want to ensure \u201cexactly-once guarantees\u201d with\n  offset tokens for idempotency. Ideally, you would batch insert as much\n  as possible to reduce API call overhead, but avoid building streaming\n  applications just to load large \"batch\" files.\n\n**Data Transformation Optimization**\n\nIn general, there are two major transformation strategies followed in\nSnowflake. One is \u201ctruncate & load,\u201d which involves full data\nreplacement and reloading, and one is incrementally loading new data\ninto an object, possibly requiring an upsert operation. Below is some\ngeneral guidance on when to use each.\n\n* **Truncate & load:** This is utilized when full data replacement is\n  acceptable and the dataset volume/size allows for fast reloads. In\n  this case, data change patterns are such that identifying incremental\n  deltas is expensive or unreliable and downstream consumers can\n  tolerate occasional brief periods of incomplete data (during load).\n  \n+ **Best practices:** Schedule loads during off-peak or agreed \u201cquiet\u201d\n        windows. Consider transient or temporary tables to stage incoming\n        data, maximize parallelism during the load, and then do an atomic\n        swap/rename to minimize downtime. Ensure that you carefully manage\n        object dependencies, constraints, and statistical metadata refresh\n        after reloads to ensure proper performance once data is loaded\n* **Incremental loads:** Incremental loads are best when datasets are\n  large and a full reload is too costly, slow, or would create\n  unacceptable latency. Change-data-capture (CDC), event streaming, or\n  other means are available to reliably identify deltas (inserts,\n  updates, deletes). Additionally, in these cases, downstream consumers\n  require near-continuous data availability or very low data latency and\n  freshness.\n  \n    + **Best practices:** Design pipelines to deduplicate, merge, and\n        correctly apply CDC deltas using staging tables, Streams, and merge\n        operations. Ideally, you use Stream objects on source tables to\n        track changes efficiently if utilizing Streams or Tasks, and use\n        Tasks to automate processing. Dynamic tables are also an option;\n        please see below. Additionally, for batch files: organize files by\n        partitioned paths and use file loading patterns that enable max\n        parallelism (see COPY INTO guidance below).\n    + Test workloads on [Snowflake\u2019s Generation 2 warehouses](https://docs.snowflake.com/en/user-guide/warehouses-gen2) that incorporate software improvements for data manipulation.\n\nA great example of truncate & load versus incremental can be seen in\nrefresh strategies for [Dynamic Tables (DTs)](https://docs.snowflake.com/en/user-guide/dynamic-tables-about) .\nThey are also a cost-effective and low-maintenance way to maintain data\npipelines. Dynamic tables provide a powerful, automated way to build\ncontinuous data transformation pipelines with SQL, eliminating the need\nfor manual task orchestration that was historically architected with\nstreams & tasks in Snowflake. [Streams & tasks](https://docs.snowflake.com/en/user-guide/data-pipelines-intro) still have their uses, but general guidance and ease of use see more\nSnowflake users leaning towards DTs for automated data pipelines since\nthe pipeline definitions are defined in one object or in a chain of\nobjects.\n\nThe [key concepts of dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh) are defined in our documentation. However, best practices and\ndetermining when to use DTs versus other methods of pipeline tooling in\nSnowflake still warrant discussion, and are compared in Snowflake\u2019s [documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison) .\n\nIn addition to Snowflake\u2019s published [best practices](https://docs.snowflake.com/en/user-guide/dynamic-table-performance-guide) ,\nconsider the following\n\n* **Default to AUTO refresh** : Override only as needed for predictable\n  SLAs.\n* **Keep incremental refresh-friendly queries simple:** Avoid complex\n  nested joins, CTEs, and limit the number of outer joins per DT. The\n  introduction of complexity for incremental refresh may result in\n  longer times for execution, which in turn could force Snowflake in\n  AUTO to perform a full refresh rather than incremental.\n* **Incremental refresh is optimal** when less than 5% of the rows\n  change between refresh cycles, and source tables are well clustered by\n  relevant keys.\n* **For very complex/large transformations:** Chain multiple DTs for\n  better incrementalization, rather than building one massive DT.\n* **Monitor actual lag and refresh metrics** to adjust lag or warehouse\n  sizing as cost and response time needs evolve.\n* **Prefer a dedicated warehouse for DT refresh** during pipeline\n  development and cost analysis to isolate consumption, then consider\n  sharing for production.\n* **Use transient DTs** for high-throughput, non-critical staging steps\n  to keep storage costs down.\n* **Avoid non-deterministic functions in incremental DTs** (e.g.,\n  random, volatile UDFs, queries depending on CURRENT\\_USER).\n\nMore information on dynamic tables versus streams & tasks versus\nmaterialized views can be found in the Snowflake documentation [here](https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison) .\n\n**Table pruning optimization**\n\nTable scanning operations are one of the most resource-intensive aspects\nof query execution. Minimizing the scan of data partitions in a table\n(called partition pruning) can provide significant improvements to both\nperformance and cost for data operations in Snowflake. The account usage\nviews [TABLE\\_QUERY\\_PRUNING\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/table_query_pruning_history) and [COLUMN\\_QUERY\\_PRUNING\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/column_query_pruning_history) provide aggregated data on query execution, showing metrics such as\npartitions scanned and rows matched, which helps identify tables with\npoor pruning efficiency. By analyzing this data, you can determine the\nmost frequently accessed columns that are leading to a high number of\nunnecessarily scanned micro-partitions. Common ways to optimize these\naccess patterns are by using Automatic Clustering and Search\nOptimization.\n\nTo determine tables that can most benefit from re-ordering how data is\nstored, you can review Snowflake\u2019s [best practice](https://medium.com/snowflake/supercharging-snowflake-pruning-using-new-account-usage-views-52530b24bf2e) on how to analyze the TABLE\\_QUERY\\_PRUNING\\_HISTORY and\nCOLUMN\\_QUERY\\_PRUNING\\_HISTORY account usage views. Fundamentally,\nreducing the percentage of partitions in each table pruned to the\npercentage of rows returned in a query will lead to the most optimized\ncost and performance for any given workload.\n\nA table\u2019s Ideal pruning state is scanning the same % of rows matched as\npartitions read, minimizing unused read rows.\n\n**Warehouse optimization**\n\nWarehouse concurrency, type, and sizing can impact the execution\nperformance and cost of queries within Snowflake. Review the compute\noptimization section for more information into the tuning of the\nwarehouse and its effect on cost and performance.\n\n#### Improve continually\n\nOptimization is a continuous process that ensures all workloads not only\ndrive maximum business value but also do so in an optimal manner. By\nregularly reviewing, analyzing, and refining your Snowflake environment,\nyou can identify inefficiencies, implement improvements, and adapt your\nplatform to the ever-evolving business needs. The following set of steps\nwill help you continue to improve your environment as you grow:\n\n**Step 1: Identify & investigate workloads to improve**\n\nBegin by regularly reviewing (usually on a weekly, bi-weekly, or monthly\ncadence) workloads that could benefit from optimization, using\nSnowflake's [Cost Insights](https://docs.snowflake.com/en/user-guide/cost-insights) ,\ndeviations in unit economics or health metrics (from the Visibility\nprinciple), or objects hitting control limits (e.g., queries hitting\nwarehouse timeouts from the Control principle). Once identified,\ninvestigate these findings through the Cost Management UI, Cost Anomaly\ndetection, Query History, or custom dashboards with Account Usage Views\nto pinpoint the root cause. Then, using the recommendations in the\nOptimize Pillar, make improvements to the workload or object.\n\n**Step 2: Estimate & test**\n\nBefore implementing changes, estimate the potential impact on cost and\nperformance. Estimation encompasses both the expected amount of time\nrequired to make a change (for instance, consolidating warehouses will\nnecessitate more coordination effort for teams using the resource than\naltering a configuration setting) as well as the hard cost of\nimplementation. Snowflake provides helpful cost estimation functions for\nserverless features, such as [auto clustering](https://docs.snowflake.com/en/sql-reference/functions/system_estimate_automatic_clustering_costs) and [search optimization service](https://docs.snowflake.com/en/sql-reference/functions/system_estimate_search_optimization_costs) ,\nto help make this a more data-driven process. If an estimation tool is\nnot available, making changes in a development or test environment on a\nsubset of the workload can provide an estimate and expected impact.\n\n**Step 3: Decide & implement**\n\nBased on your estimations and test results, decide whether to move\nforward with the change, ensuring the cost-benefit aligns with\nperformance or business needs. If approved, proceed to productionize the\nchange, integrating it into your live environment.\n\n**Step 4: Monitor & analyze**\n\nFinally, monitor and analyze the implemented changes to track and\nvalidate the change\u2019s success over a period of time. This involves using\nthe same investigation methods, like utilizing the Cost Management UI\nand Account Usage Views, and comparing cost and performance metrics\nbefore and after the change to articulate the business impact. Translate\nthe technical improvements into tangible business benefits. For example,\n\"Optimizing this query reduced monthly warehouse costs by $X and\nimproved report generation time by Y minutes, allowing business users to\nmake faster decisions.\" This helps to both demonstrate the value of your\noptimization efforts to stakeholders and business value to the company.\nFinally, course-correct as needed depending on the results of the\nmonitoring\n\nThis continual improvement framework is the culmination of all subtopics\nwithin the Cost Optimization Pillar and provides a consistent way for\nyou to grow healthily on Snowflake.\n\nUpdated Nov 24, 2025\n\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n\n##### On this page\n\nOverview\n\nPrinciples\n\nRecommendations\n\nBusiness Impact\n\nVisibility\n\nControl\n\nOptimize\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\n\nLearn * Resource Library\n* Live Demos\n* Fundamentals\n* Training\n* Certifications\n* Snowflake University\n* Developer Guides\n* Documentation\n\n* Privacy Policy\n* Site Terms\n* Communication Preferences\n* Cookie Settings\n* Do Not Share My Personal Information\n* Legal\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 5
    }
  ]
}
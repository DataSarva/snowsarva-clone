{
  "extract_id": "extract_b2a19c5a0ac24aa8b510c5b2312f8117",
  "results": [
    {
      "url": "https://www.snowflake.com/en/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/",
      "title": "Automatic Concurrency Scaling in Snowflake | Snowflake Auto Scaling",
      "publish_date": "2024-08-05",
      "excerpts": [
        "blog\n\n##### Category\n\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\n\nApplications\n\nApr 28, 2016 | 5 min read\n\n# Automatic Concurrency Scaling in Snowflake - Another Way the Cloud Changes the Game\n\n#### The challenge with concurrency\n\nToday we take a major step forward by extending our elastic architecture to solve another major pain point in existing on-premises and cloud data warehousing solutions: how to run massively concurrent workloads at scale in a single system. Have you had the following experiences when building mission-critical applications that incorporate data analytics: * My application can only support a certain level of user concurrency due to the underlying data warehouse, which only allows 32-50 concurrent user queries.\n* To build my application, I need to acquire multiple data warehouse instances in order to isolate numerous workloads and users from each other. This adds to costs and complexity.\n* We have built our own scheduling policies around the data warehouse. We use query queues to control and prioritize incoming queries issued by our numerous users.\n* During peak times, users are getting frustrated because their requests are getting queued or fail entirely.\n At Snowflake, we separate compute from storage by introducing the unique concept of virtual data warehouses. That concept makes it possible to instantly resize virtual warehouses or pause them entirely. In addition, because of that concept Snowflake is the only cloud data warehousing solution that allows concurrent workloads to run without impacting each other.\n\n\ufeff However, we saw the need to go a step further to offer a service that adapts to changing workloads and addresses concurrency at the same time: * Imagine you didn\u2019t have any concurrency limitations on your mission-critical business application.\n* Imagine you didn\u2019t need users to adjust their workloads to accommodate data warehouse bottlenecks.\n* Imagine the data warehouse itself could detect increasing workloads and add additional compute resources as needed or shut-down/pause compute resources when workload activities subside again.\n* Imagine your application could scale out-of-the-box with one single (virtual) data warehouse without the need to provision additional data warehouses.\n* Imagine a world without any scheduling scripts and queued queries - a world in which you can leverage a smart data warehousing service that ensures all your users get their questions answered within the application\u2019s SLA.\n With Snowflake, we allow you to do that all of this for real, not just in your imagination, with our new multi-cluster data warehouse feature.\n\n#### Multi-cluster data warehouses\n\nA virtual warehouse represents a number of physical nodes a user can provision to perform data warehousing tasks, e.g. running analytical queries. While a user can instantly resize a warehouse by choosing a different size (e.g. from small to 3X large), until now a virtual data warehouse in Snowflake always consisted of one physical cluster. With the recent introduction of multi-cluster warehouses, Snowflake supports allocating, either statically or dynamically, more resources for a warehouse by specifying additional clusters for the warehouse.\n\nThe figure above shows a multi-cluster DW that consists of three compute clusters. All compute clusters in the warehouse are of the same size. The user can choose from two different modes for the warehouse: * **Maximized:** When the warehouse is started, Snowflake always starts all the clusters to ensure maximum resources are available while the warehouse is running.\n* **Auto Scaling:** Snowflake starts and stops clusters as needed to dynamically manage the workload on the warehouse.\n As always, in Snowflake a user can either leverage the user interface or use SQL to specify the minimum/maximum number of clusters per multi-cluster DW:\n\n#### Create Warehouse UI wizard\n\nCreate Warehouse SQL script\n\nSimilar to regular virtual warehouses, a user can resize all additional clusters of a multi-cluster warehouse instantly by choosing a different size (e.g. XS, S, M, L, \u2026) either through the UI or programmatically via corresponding SQL DDL statements. In auto-scale mode, Snowflake automatically adds or resumes additional clusters (up to the maximum number defined by user) as soon as the workload increases. If the load subsides again, Snowflake shuts down or pauses the additional clusters. No user interaction is required - this all takes place transparently to the end user. For these decisions, internally, the query scheduler takes into account multiple factors. There are two main factors considered in this context: 1. The memory capacity of the cluster, i.e. whether clusters have reached their maximum memory capacity\n2. The degree of concurrency in a particular cluster, i.e. whether there are many queries executing concurrently on the cluster\n As we learn more from our customers\u2019 use cases, we will extend this feature further and share interesting use cases where multi-cluster data warehouses make a difference. Please stay tuned as we continue reinventing modern data warehousing and analytics by leveraging the core principles of cloud computing. _We would like to thank our co-authors, Florian Funke and Benoit Dageville, for their significant contributions as main engineer and architect, respectively, to making multi-cluster warehouses a reality. As always, keep an eye on the blog and our Snowflake Twitter feed [(@SnowflakeDB](https://twitter.com/SnowflakeDB) ) f or updates on Snowflake Computing._\n\n### Additional Links\n\n* **Concurrent Load and Query**\n* **Snowflake Architecture**\n\n###### Author\n\nArtin Avanes\n\n###### Share Article\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fauto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications&title=Automatic+Concurrency+Scaling+in+Snowflake+-+Another+Way+the+Cloud+Changes+the+Game)\n\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fauto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications&text=Automatic+Concurrency+Scaling+in+Snowflake+-+Another+Way+the+Cloud+Changes+the+Game)\n\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fauto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications)\n\nSubscribe to our blog newsletter\n\nGet the best, coolest and latest delivered to your inbox each week\n\n\\*\n\nSubscribe Now\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\n[start for free](https://signup.snowflake.com/)\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\n\n\\*\n\n\\*\n\n\\* Country United States Canada United Kingdom Germany France Australia Japan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo The Democratic Republic of The Cook Islands Costa Rica Cote D'Ivoire (Ivory Coast) Croatia (Hrvatska) Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Heard and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jordan Kazakhstan Kenya Kiribati Korea Republic of (South) Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia Federated States of Moldova Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Occupied Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Reunion Romania Russian Federation Saint Helena Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and The South Sandwich Island Spain Sri Lanka Suriname Svalbard and Jan Mayen Islands Swaziland Sweden Switzerland Taiwan Tajikistan Tanzania United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Venezuela Viet Nam Virgin Islands (British) Virgin Islands (U.S.) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe\n\n\\*\n\nAdd me to the list to receive dedicated product updates and general availability emails.\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\n\nSubscribe Now\n\nLearn * Resource Library\n* Live Demos\n* Fundamentals\n* Training\n* Certifications\n* Snowflake University\n* Developer Guides\n* Documentation\n\n* Privacy Policy\n* Site Terms\n* Communication Preferences\n* Cookie Settings\n* Do Not Share My Personal Information\n* Legal\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ],
      "full_content": null
    },
    {
      "url": "https://www.dataexpert.io/blog/optimize-query-concurrency-snowflake",
      "title": "How to Optimize Query Concurrency in Snowflake",
      "publish_date": "2026-01-31",
      "excerpts": [
        "How to Optimize Query Concurrency in Snowflake\n\nHome Blog Data Engineering\n\nPublishedJan 31, 2026 \u2981 17 min read\n\n# How to Optimize Query Concurrency in Snowflake\n\n**Struggling with slow queries in [Snowflake](https://www.snowflake.com/en/) ?** When multiple users or apps run queries at the same time, your Snowflake warehouse can hit performance bottlenecks. Here's the deal: Snowflake's compute resources are shared, and when too many queries compete for them, things slow down - or worse, queries get stuck in a queue.\n\nTo fix this, you need to manage **query concurrency** effectively. This guide covers the essentials:\n\n* **Adjust concurrency settings** ( `MAX_CONCURRENCY_LEVEL` ) to control how many queries run simultaneously.\n* Use **warehouse auto-scaling** to handle spikes in query load without wasting credits.\n* Monitor for issues like **memory spillage** or **query queuing** using Snowflake tools like [Snowsight](https://docs.snowflake.com/en/user-guide/ui-snowsight) and [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) .\n* Apply **clustering keys** and **materialized views** to speed up query execution.\n\nSnowflake Query Concurrency Optimization Decision Framework\n\n## What is Query Concurrency in [Snowflake](https://www.snowflake.com/en/)\n\n### Query Concurrency Defined\n\nQuery concurrency in Snowflake refers to **multiple queries running simultaneously on a single virtual warehouse** , all sharing the same compute resources. Think of it like a highway with a fixed number of lanes: as more cars (queries) enter, traffic slows down.\n\nEach virtual warehouse in Snowflake is equipped with a specific allocation of **CPU and memory** . For instance, if two queries are running, each gets about half of the compute resources; with ten queries, each query receives roughly one-tenth. While this setup works well under light workloads, performance can drop significantly as the number of concurrent queries increases.\n\nBy default, Snowflake limits **concurrent queries to 8 per warehouse** , controlled by the `MAX_CONCURRENCY_LEVEL` parameter. The workload on a warehouse is measured by dividing the total execution time of all queries by the monitoring interval. This helps assess how heavily the warehouse is being utilized. These dynamics often lead to specific challenges when concurrency levels rise.\n\n### Common Problems with High Concurrency\n\nSharing resources among multiple queries can lead to several performance issues.\n\nThe first challenge is **resource competition** . When too many queries compete for the same compute pool, execution slows down as resources are spread thin. This creates a bottleneck, forcing some queries to wait their turn instead of running immediately.\n\nAnother issue is **memory spillage** . If concurrent queries use up all available RAM, Snowflake shifts to writing data to disk or remote storage. This shift from in-memory processing to disk-based operations can significantly slow query performance. You can track this issue by monitoring the \"percentage of queries with bytes spilled\" metric in Performance Explorer.\n\n**Increased latency** is another symptom of high concurrency. As queries wait in line for resources, the time it takes to get results grows longer. Snowflake's Snowsight interface provides a visual representation of this \"queued load\" via the Warehouse Activity chart, helping identify when a warehouse is overwhelmed. Additionally, queries might become **blocked** due to transaction locks on specific resources, further delaying execution.\n\n|Status |Meaning |Impact |\n| --- | --- | --- |\n|**Running** |Actively using warehouse resources |High running load creates contention |\n|**Queued** |Waiting for resources due to warehouse overload |Increases total turnaround time |\n|**Queued (Provisioning)** |Waiting for the warehouse to start or resize |Temporary delay during spin-up |\n|**Blocked** |Waiting for a transaction lock on a resource |Indicates contention at the data level |\n\n## Tackling High Concurrency with Multi-Cluster Warehouses\n\n## How to Configure MAX\\_CONCURRENCY\\_LEVEL\n\nThe `MAX_CONCURRENCY_LEVEL` parameter determines how many queries can run simultaneously on a single warehouse cluster. Adjusting this setting redistributes resources, allowing fewer queries to access more CPU and memory. This setup is particularly useful for large or multi-statement queries but may result in queuing additional queries instead. According to Snowflake's documentation, any changes should be tested to confirm consistent performance improvements. This parameter is key to addressing the concurrency challenges mentioned earlier.\n\n### Steps to Adjust MAX\\_CONCURRENCY\\_LEVEL\n\nChanging the `MAX_CONCURRENCY_LEVEL` is straightforward using SQL commands. Start by checking the current settings with:\n\n```\nSHOW PARAMETERS IN WAREHOUSE <warehouse_name>;\n```\n\nThis command provides details on the current `MAX_CONCURRENCY_LEVEL` value.\n\nTo update the concurrency level for an existing warehouse, use the `ALTER WAREHOUSE` command. For instance, to set the concurrency level to 4 for a warehouse handling complex BI reports:\n\n```\nALTER WAREHOUSE my_wh SET MAX_CONCURRENCY_LEVEL = 4;\n```\n\nIf you're creating a new warehouse with a specific concurrency level, use:\n\n```\nCREATE WAREHOUSE new_wh WITH MAX_CONCURRENCY_LEVEL = 2;\n```\n\nAfter making changes, monitor the Warehouse Activity chart in Snowsight to observe running and queued queries.\n\n### Guidelines for Setting MAX\\_CONCURRENCY\\_LEVEL\n\nFocus on adjusting warehouses that handle resource-heavy workloads where query performance is impacted by contention. For warehouses running simple, quick queries, the default setting usually suffices. Lower the concurrency level to 4 or below only when large queries are struggling due to resource competition. Always test changes in a non-production environment to evaluate their impact on performance.\n\nIf reducing concurrency leads to excessive queuing, consider pairing the adjustment with the `STATEMENT_QUEUED_TIMEOUT_IN_SECONDS` parameter to cancel queries that remain in the queue for too long. Another option is to explore using a dedicated warehouse for resource-intensive tasks or enabling the Query Acceleration Service, which may address performance issues more effectively than limiting concurrency.\n\n|Parameter |Default Value |Purpose |\n| --- | --- | --- |\n|`MAX_CONCURRENCY_LEVEL` |8 |Limits concurrent queries per cluster to manage resource allocation |\n|`STATEMENT_QUEUED_TIMEOUT_IN_SECONDS` |0 (No timeout) |Sets the maximum wait time for queued queries before cancellation |\n\n## How to Use Warehouse Auto-Scaling and Sizing\n\nAuto-scaling and proper sizing are essential for managing fluctuating query loads in Snowflake. These features ensure your warehouses adapt to demand automatically, balancing performance and cost without manual adjustments.\n\n### How Auto-Scaling Works\n\nSnowflake's auto-scaling operates through multi-cluster warehouses, which dynamically add or remove clusters based on query load. This flexibility is controlled by setting `MIN_CLUSTER_COUNT` and `MAX_CLUSTER_COUNT` . When queries start queuing or existing clusters hit capacity, additional clusters are spun up. As the load decreases, the extra clusters are shut down.\n\n> \"In auto-scale mode, Snowflake automatically adds or resumes additional clusters... as soon as the workload increases. If the load subsides again, Snowflake shuts down or pauses the additional clusters.\" \u2013 Artin Avanes, Snowflake\n> \n> \n\nTwo scaling policies determine how quickly clusters are added:\n\n* **Standard Policy** : Starts new clusters immediately when queries queue or resources are insufficient. This is ideal for performance-critical workloads.\n* **Economy Policy** : Adds clusters only if the workload can sustain them for at least six minutes. This option is better for reducing credit usage.\n\nAs a starting point, set `MIN_CLUSTER_COUNT` to 1 and `MAX_CLUSTER_COUNT` to 2 or 3, and monitor your workload. If queuing frequently occurs during peak times, consider increasing the maximum cluster count. Additionally, align your warehouse size with the complexity of your queries and the volume of data being processed for optimal results.\n\n### How to Choose the Right Warehouse Size\n\nWhile auto-scaling adjusts the number of clusters, the warehouse size dictates the compute power of each cluster. Compute power is measured in credits per hour, with an X-Small warehouse consuming 1 credit per hour and a 4X-Large warehouse using 128 credits per hour.\n\n* **Scaling Up** : Increasing warehouse size boosts performance for complex queries by providing more compute power.\n* **Scaling Out** : Adding clusters is better for handling high concurrency.\n\nFor complex queries, larger warehouses are recommended to avoid memory spillage, which can slow performance significantly. For simpler queries, a Medium or smaller warehouse usually suffices. If you notice queries spilling data to local or remote storage, increasing the warehouse size can help by providing additional memory.\n\nFor production environments, a Large or X-Large warehouse is often a good choice. Pair this with per-second billing and auto-suspend settings to control costs. In testing environments or when using the Snowsight UI, smaller warehouses (X-Small, Small, or Medium) are typically enough. To identify resource bottlenecks, monitor the Query Overload % metric in Performance Explorer. If queuing is due to high query volume rather than complexity, a multi-cluster warehouse setup might be more effective than simply increasing warehouse size.\n\n|Warehouse Size |Credits Per Hour |Best For |\n| --- | --- | --- |\n|X-Small to Medium |1\u20134 |Simple queries, testing |\n|Large to X-Large |8\u201316 |Complex queries |\n|2X-Large to 4X-Large |32\u2013128 |Very large queries |\n\n## How to Manage Query Queuing and Priorities\n\n### How Query Queuing Works in Snowflake\n\nWhen a warehouse runs out of compute resources or hits its concurrency limit, Snowflake places additional queries in a queue. The number of simultaneous queries a cluster can handle is controlled by the `MAX_CONCURRENCY_LEVEL` setting, which defaults to 8. Once this cap is reached, any new queries must wait in line.\n\nThere are two main types of queuing in Snowflake:\n\n* **\"Queued (Provisioning)\"** happens when a query waits for a warehouse to start or scale up.\n* **\"Queued\"** occurs when the warehouse is already running but overloaded, requiring the query to wait for other queries to complete.\n\nFor multi-cluster warehouses, Snowflake can automatically add clusters in Auto-scale mode to handle queued queries. However, if the Economy scaling policy is enabled, new clusters will only be added if there\u2019s sufficient workload to keep them active for at least six minutes.\n\nTo prevent long waits during peak times, Snowflake cancels queries that exceed the timeout set by `STATEMENT_QUEUED_TIMEOUT_IN_SECONDS` . You can monitor query queuing through the \"Warehouse Activity\" chart in Snowsight or by querying the `QUERY_HISTORY` view, which helps identify spikes in \"Queued Load\". By understanding these queuing mechanisms, you can better allocate resources and prioritize critical workloads.\n\n### How to Set Query Priorities\n\nWhile Snowflake\u2019s auto-scaling feature adjusts compute resources dynamically, managing query priorities can further enhance performance during high-demand periods. One effective way to minimize delays is through **workload isolation** , which ensures critical tasks have the resources they need.\n\nWorkload isolation involves creating separate warehouses for different types of queries. For example, high-priority workloads should run on dedicated warehouses to avoid competing with ad-hoc or low-priority queries. This method is more effective than trying to prioritize within a single shared warehouse.\n\nFor critical workloads, configure multi-cluster warehouses with a higher `MAX_CLUSTERS` value and use the **Standard** scaling policy. This policy starts new clusters immediately when queries begin to queue, ensuring faster processing. Avoid the Economy scaling policy for essential tasks, as it prioritizes saving credits over performance, which can result in delays. Additionally, lowering the `MAX_CONCURRENCY_LEVEL` for warehouses handling complex queries can allocate more resources to each query, though this may increase queuing for other tasks.\n\nTo maintain availability during peak usage, assign critical warehouses to Resource Monitors with sufficient credit quotas. Set the monitors to \"Notify\" instead of \"Suspend\" to avoid unexpected disruptions when nearing credit limits. This ensures that high-priority workloads continue running without interruption.\n\n|Feature |Impact on Priority |Recommended Setting |\n| --- | --- | --- |\n|**Warehouse Isolation** |Prevents resource competition between tasks |Use dedicated warehouses for key workloads |\n|**Scaling Policy** |Determines when extra clusters are added |Use **Standard** for immediate scaling |\n|**MAX\\_CONCURRENCY\\_LEVEL** |Allocates resources to fewer queries at a time |Lower for complex, resource-intensive queries |\n|**Resource Monitors** |Prevents warehouse suspension |Use \"Notify\" for critical warehouses |\n\n###### sbb-itb-61a6e59\n\n## How to Use Clustering Keys and Micro-Partition Pruning\n\n### Using Clustering Keys to Speed Up Queries\n\nSnowflake organizes data into micro-partitions, each ranging from 50 to 500 MB in size. While data naturally clusters during loading, setting a clustering key explicitly groups related data, allowing Snowflake to bypass irrelevant partitions during filtering or sorting. This reduces the amount of data scanned and improves query performance, especially under high concurrency.\n\n> \"Clustering is generally most cost-effective for tables that are queried frequently and do not change frequently.\" - Snowflake Documentation\n> \n> \n\nTo make the most of clustering keys, stick to three or four columns, prioritizing those frequently used in selective filters (e.g., date columns in fact tables). Arrange these columns from lowest to highest cardinality. For columns with high cardinality, like nanosecond timestamps, you can simplify the data by applying an expression such as `TO_DATE(timestamp_col)` . This reduces distinct values and enhances data grouping. To keep tabs on clustering efficiency, use the `SYSTEM$CLUSTERING_INFORMATION` function - a lower average depth indicates better pruning.\n\nThis setup lays the groundwork for efficient micro-partition pruning, which further refines query performance.\n\n### How Micro-Partition Pruning Works\n\nWhen combined with clustering, micro-partition pruning takes query optimization a step further by skipping over partitions that don't match the query's filters. Each micro-partition includes metadata for every column - such as value ranges and distinct counts - that Snowflake uses to identify and exclude irrelevant partitions. Thanks to columnar storage, only the columns referenced in the query are scanned. For example, if a query filter targets 10% of a value range, only around 10% of the partitions will be scanned. This method is especially powerful for time-series data, enabling rapid responses even when querying specific time intervals across extensive historical datasets.\n\nOnce a clustering key is defined, Snowflake's Automatic Clustering service reorganizes data in the background without disrupting data manipulation operations (DML). To keep maintenance costs low, it\u2019s best to batch `INSERT` , `UPDATE` , and `DELETE` operations into larger, less frequent batches. This reduces the frequency of automatic reclustering while maintaining high query performance, even under heavy workloads.\n\n## How to Monitor and Profile Concurrent Queries\n\n### How to Use [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) and Execution Plans\n\nMonitoring query performance is a key step in managing concurrency effectively.\n\nSnowsight's **Query History** feature logs query details for 14 days, including status, duration, warehouse size, and bytes scanned. To identify potential bottlenecks, you can filter queries by **Status** to focus on those marked as \"Queued\" (indicating they are waiting for warehouse resources) or \"Blocked\" (indicating a transaction lock). For workloads with high query volumes, the **Grouped Query History** view is particularly useful. It groups queries by their parameterized hash ID, making it easier to pinpoint repeated query types that may be overloading resources or causing queuing issues.\n\nWhen you select a specific query, its **Query Profile** provides detailed insights. This includes a visualization of the execution plan, highlighting costly operator nodes. It also breaks down execution time into categories like Processing, Synchronization, and Throttling, and flags issues such as \"Bytes spilled\", which suggests memory overflow. Snowflake's automated **Query Insights** further assist by identifying problems like extended queuing or remote spillage and offering suggestions for improvement.\n\n|Query Status |Description |\n| --- | --- |\n|**Running** |Queries actively processing during the interval |\n|**Queued** |Queries waiting for resources due to warehouse overload |\n|**Queued (Provisioning)** |Queries waiting while the warehouse is starting up or resuming |\n|**Blocked** |Queries waiting due to a transaction lock |\n\nFor a more extensive historical view, you can query the `ACCOUNT_USAGE.QUERY_HISTORY` view, which stores up to 365 days of data, though it updates with a delay of up to 45 minutes. By using fields like `query_hash` or `query_parameterized_hash` , you can identify patterns in repeated, resource-intensive queries and optimize them to free up concurrency slots.\n\nThese tools enable proactive monitoring and help manage costs by identifying and addressing inefficiencies.\n\n### How to Set Up Resource Monitors\n\nResource monitors are essential for tracking credit usage and managing costs. They allow you to set thresholds and trigger actions when those thresholds are reached. These monitors can be configured at either the **Account** level (covering all warehouses) or the **Warehouse** level (specific to individual warehouses). However, a warehouse can only be assigned to one monitor at a time. By default, only users with the `ACCOUNTADMIN` role can create resource monitors, though they can delegate `MONITOR` or `MODIFY` privileges to other roles.\n\nTriggers can be set up to take specific actions at defined quota percentages. For example:\n\n* Set `NOTIFY` at 50% and 75% to receive warnings.\n* Use `SUSPEND` at 100% to block new queries while letting current ones finish.\n* Opt for `SUSPEND_IMMEDIATE` to halt all running queries instantly.\n\nTo stay informed, enable notifications in Snowsight, as these are turned off by default. It\u2019s also a good idea to set thresholds slightly below 100% (e.g., 90%) to ensure timely suspension of warehouses.\n\nResource monitors reset their credit tracking at 12:00 AM UTC daily, regardless of the configured start time. For workloads with high concurrency, assign a single warehouse to each monitor instead of grouping them. This ensures tighter cost control. Keep in mind that while resource monitors track both warehouse compute and cloud services, they cannot suspend serverless features like Snowpipe or automatic reclustering. For those, you\u2019ll need to manage costs using Budgets.\n\n## Advanced Methods: Query Acceleration and Materialized Views\n\nBuilding on earlier strategies, these advanced techniques focus on reducing compute demand while speeding up query results.\n\n### How to Use Result Caching for Query Acceleration\n\nSnowflake's **result caching** is a powerful tool that stores query results and delivers them instantly when the same query is run again - without using your warehouse at all. This means **no compute credits** are spent on cached queries, freeing up your warehouse for other tasks.\n\nResult caching works across your entire account. Even if a warehouse is suspended, the cached results remain accessible. Each time a cached query is used, the retention period resets, allowing results to stay available for up to 31 days from the original execution. For larger queries, security tokens expire after 6 hours, but you can retrieve a new token if the result is still cached.\n\nTo make the most of result caching, ensure your SQL syntax is consistent. The query must match exactly, including capitalization, table aliases, and even spacing. Avoid using non-deterministic functions like `RANDOM()` , `UUID_STRING()` , or `CURRENT_TIMESTAMP()` in queries where caching is important, as these functions prevent result reuse. For further processing, use the `RESULT_SCAN` table function to work with cached results instead of re-running the full query.\n\n|Feature |Result Cache |Warehouse Cache |\n| --- | --- | --- |\n|**Storage Location** |Global (Snowflake Managed) |Local (Virtual Warehouse SSD) |\n|**Persistence** |24 hours (up to 31 days) |Dropped when warehouse is suspended |\n|**Compute Cost** |No credits used for retrieval |Requires running warehouse (credits) |\n|**Primary Benefit** |Bypasses execution entirely |Speeds up data scanning from disk |\n\n### How to Use Materialized Views\n\nMaterialized views are another way to streamline query performance, especially for workloads with repetitive query patterns. A **materialized view** is essentially a pre-computed dataset stored as a database object, allowing you to retrieve results much faster than querying large base tables. Snowflake\u2019s query optimizer can even rewrite queries to use a materialized view automatically, saving compute resources during periods of high demand.\n\n> \"Materialized views are designed to improve query performance for workloads composed of common, repeated query patterns.\"\n> \n> * Snowflake Documentation\n> \n> \n\nThese views are particularly effective when query results involve fewer rows or columns than the base table or include resource-intensive operations like aggregations or analyzing semi-structured data. Snowflake keeps materialized views up-to-date through a background service that updates the view whenever the base table changes. If a query arrives while the view is being updated, Snowflake combines the pre-computed data with fresh updates from the base table to ensure accurate results.\n\nHowever, materialized views aren't always the right choice. If the base table changes frequently but the view is rarely queried, maintenance costs can outweigh the benefits. Carefully evaluate whether the performance gains justify the added storage and maintenance expenses. Keep in mind that materialized views are only available with **Snowflake Enterprise Edition or higher** and come with some limitations: they can only query a single table and don\u2019t support joins, window functions, or non-deterministic functions.\n\n## Conclusion\n\nBalancing query concurrency in Snowflake requires thoughtful management of compute resources, workload types, and cost considerations. Start by monitoring warehouse activity in Snowsight to identify queuing patterns. From there, decide whether to **scale out** with multi-cluster warehouses for handling high concurrency or **scale up** to improve performance for complex queries.\n\nQueuing patterns often point to misaligned workload distribution. To address this, assign similar workloads to dedicated warehouses. Mixing queries with vastly different complexities can make it challenging to configure the right warehouse size and concurrency settings. For larger accounts, dedicating an X\u2011Small warehouse for Snowsight UI tasks can help. This approach minimizes interference from internal metadata queries, ensuring they don\u2019t compete with production workloads.\n\nFine-tune auto-suspend settings based on workload behavior. For BI and SELECT-heavy use cases, set auto-suspend to at least 10 minutes to retain the warehouse cache. For ad-hoc tasks like DevOps or Data Science, a 5-minute setting works better since caching offers fewer benefits in these scenarios. Keep in mind that Snowflake charges for a minimum of 60 seconds each time a warehouse starts, so frequent suspensions can lead to wasted credits.\n\nThe default `MAX_CONCURRENCY_LEVEL` of 8 is suitable for many cases, but reducing it can improve performance for complex queries by allocating more resources per query. Test changes by re-running queries and analyzing performance metrics. For large scans involving selective filters, consider using Query Acceleration to offload processing without needing to increase warehouse size.\n\nFor multi-cluster setups, the Economy scaling policy is a cost-efficient option. It prioritizes fully utilizing existing clusters before spinning up new ones, making it ideal when cost management takes precedence over immediate query execution. Regularly track metrics like \"Query Overload %\" and \"Query Blocked %.\" Keeping these near zero ensures minimal wait times for users and smooth query execution.\n\n## FAQs\n\n### What is MAX\\_CONCURRENCY\\_LEVEL, and how does it improve query performance in Snowflake?\n\nThe **MAX\\_CONCURRENCY\\_LEVEL** setting determines how many queries can run at the same time on a Snowflake warehouse. By capping concurrency, it helps minimize resource competition, ensuring that each query gets more system resources. This approach can boost performance, especially for large or complex queries, as they benefit from having more compute power and fewer bottlenecks.\n\nTweaking this setting is especially handy when you're handling workloads with resource-heavy queries or need to prioritize specific tasks. It allows you to balance throughput and performance, making the most of your Snowflake setup.\n\n### What are the advantages of using auto-scaling for warehouses in Snowflake?\n\nSnowflake's auto-scaling feature is designed to handle workloads efficiently by dynamically adjusting compute resources based on demand. When query traffic spikes, auto-scaling ensures your system can handle the increased load without slowing down. This means even during peak usage, your queries run smoothly.\n\nAnother major advantage is cost optimization. Snowflake automatically pauses warehouses when they're idle and resizes them as needed. This way, you're only charged for the resources you actively use, helping you manage expenses more effectively.\n\nWhat sets Snowflake apart is its separation of compute and storage. This design keeps your system agile and responsive, even when workloads are heavy. It's a great fit for businesses that need to scale up or down without sacrificing performance or overspending.\n\n### How do clustering keys improve query performance in Snowflake?\n\nClustering keys enhance query performance in Snowflake by structuring data within its **micro-partitions** , streamlining the process of finding and retrieving relevant information. This technique, called **micro-partition pruning** , helps Snowflake avoid scanning irrelevant data, which cuts down on query execution time.\n\nTo get the most out of clustering keys, choose them based on the columns most often used for filtering or sorting. This targeted approach can significantly boost performance and conserve resources - especially when working with massive datasets where quick and efficient data access is essential.\n\nAnalytics Engineering Cost Optimization Data Engineering\n\n## Related posts\n\n* Databricks vs Snowflake: Cost Efficiency in Embedded Analytics\n* How Partitioning Impacts Query Performance\n* Case Study: Optimizing Analytics with dbt and Snowflake\n* Snowflake in Hybrid Cloud Data Architecture\n\nDataExpert.io Academy\n\n### Find Your Way!\n\n* \n* \n* About Me\n* Contact\n* Reviews\n* Search\n* Practice SQL\n\n### Connect\n\n[Substack](https://blog.dataexpert.io) [Linkedin](https://www.linkedin.com/in/eczachly) [Twitter](https://www.x.com/eczachly) [YouTube](https://www.YouTube.com/c/datawithzach)\n\n### Community\n\n[Github](https://www.github.com/EcZachly) [Instagram](https://www.instagram.com/eczachly) [Discord](https://discord.com/invite/JGumAXncAK)\n\nSubscribe to Our Free Newsletter"
      ],
      "full_content": null
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/metering_history",
      "title": "METERING_HISTORY view | Snowflake Documentation",
      "publish_date": null,
      "excerpts": [
        "Reference General reference SNOWFLAKE database Account Usage METERING\\_HISTORY\n\nSchemas:\n    ACCOUNT\\_USAGE , READER\\_ACCOUNT\\_USAGE\n\n# METERING\\_HISTORY view \u00b6\n\nThe METERING\\_HISTORY view in the ACCOUNT\\_USAGE schema can be used to return the hourly credit usage for an account within the last 365 days (1 year).\n\n## Columns \u00b6\n\n|Column Name |Data Type |Description |\n| --- | --- | --- |\n|SERVICE\\_TYPE |VARCHAR |Type of service that is consuming credits. The following list includes many, **but not all** , of the possible service types:\n\n* `AI_SERVICES` : See Snowflake Cortex AI Functions (including LLM functions) , Cortex Analyst , and Document AI .\n* `ARCHIVE_STORAGE_RETRIEVAL_FILE_PROCESSING` : See Billing for storage lifecycle policies .\n* `ARCHIVE_STORAGE_WRITE` : See Billing for storage lifecycle policies .\n* `AUTO_CLUSTERING` : See Automatic Clustering .\n* `BACKUP` : See Backups for disaster recovery and immutable storage .\n* `COPY_FILES` : See COPY FILES .\n* `DATA_QUALITY_MONITORING` : See Introduction to data quality checks .\n* `FAILSAFE_RECOVERY` : See Understanding and viewing Fail-safe .\n* `HYBRID_TABLE_REQUESTS` : See Hybrid tables .\n* `MATERIALIZED_VIEW` : See Working with Materialized Views .\n* `OPENFLOW_COMPUTE_BYOC` : See Openflow BYOC cost and scaling considerations .\n* `OPENFLOW_COMPUTE_SNOWFLAKE` : See Openflow Snowflake Deployment cost and scaling considerations .\n* `PIPE` : See Snowpipe .\n* `POSTGRES_COMPUTE` : See Snowflake Postgres .\n* `POSTGRES_COMPUTE_HA` : See Snowflake Postgres .\n* `QUERY_ACCELERATION` : See Using the Query Acceleration Service (QAS) .\n* `REPLICATION` : See Introduction to replication and failover across multiple accounts .\n* `SEARCH_OPTIMIZATION` : See Search optimization service .\n* `SENSITIVE_DATA_CLASSIFICATION` : See Introduction to sensitive data classification .\n* `SERVERLESS_ALERTS` : See Setting up alerts based on data in Snowflake .\n* `SERVERLESS_TASK` : See Introduction to tasks .\n* `SNOWPARK_CONTAINER_SERVICES` : See Snowpark Container Services .\n* `SNOWPIPE_STREAMING` : See Snowpipe Streaming .\n* `STORAGE_LIFECYCLE_POLICY_EXECUTION` : Compute cost to apply a policy on a target table and expire or archive rows (policy execution). See Storage lifecycle policies .\n* `TELEMETRY_DATA_INGEST` : See Event table overview .\n* `TRUST_CENTER` : See Trust Center .\n* `WAREHOUSE_METERING` : See Overview of warehouses .\n* `WAREHOUSE_METERING_READER` : See Manage reader accounts . |\n|START\\_TIME |TIMESTAMP\\_LTZ |The date and beginning of the hour (in the local time zone) in which the usage took place. |\n|END\\_TIME |TIMESTAMP\\_LTZ |The date and end of the hour (in the local time zone) in which the usage took place. |\n|ENTITY\\_ID |NUMBER |A system-generated identifier for the entity associated with the service.\n\nIn most cases, this is the internal ID of the monitored entity; for example, a pipe, task, or replication group.\n\nWhen the SERVICE\\_TYPE is COPY\\_FILES, this column shows the ID of the database, schema, or stage from which files are copied.\n\nIf the SERVICE\\_TYPE is an Openflow type, the value is NULL.\n\nIf the SERVICE\\_TYPE is Snowpipe Streaming, this shows the ID of the relevant pipe; which is the default pipe ID for the default pipe. |\n|ENTITY\\_TYPE |VARCHAR |Type of Snowflake resource that consumed credits, such as WAREHOUSE, TASK, or TABLE. Note that TABLE is used for all table-like objects. |\n|NAME |VARCHAR |The name of the service or object associated with the cost entry, which varies significantly based on the SERVICE\\_TYPE.\n\nStandard (General): This column shows the name of the service type itself; for example, REPLICATION, TASK.\n\nSNOWPIPE\\_STREAMING: This service type generates two distinct cost entries, and the NAME column varies for each:\n\n* Cost entry 1 (table name): The value is the name of the Snowflake target table. For the high-performance default pipe, the name is derived from the target table name and appended with -STREAMING; for example, MY\\_TABLE-STREAMING.\n* Cost entry 2 (client string): The value is a colon-separated string in the format: SNOWPIPE\\_STREAMING:CLIENT\\_NAME:SNOWFLAKE\\_PROVIDED\\_ID. This is used for tracking client-side costs.\n\nCOPY\\_FILES: The value is the name of the database from which the files are copied.\n\nOpenflow Types: The value is NULL. |\n|DATABASE\\_ID |NUMBER |Internal/system-generated identifier of the database associated with the resource of type `ENTITY_TYPE` . Contains a NULL value when the resource isn\u2019t associated with a specific database; for example, a warehouse or compute pool. |\n|DATABASE\\_NAME |VARCHAR |Name of the database associated with the resource of type `ENTITY_TYPE` . Contains a NULL value when the resource isn\u2019t associated with a specific database. |\n|SCHEMA\\_ID |NUMBER |Internal or system-generated identifier of the schema associated with the resource of type `ENTITY_TYPE` . Contains a NULL value when the resource isn\u2019t associated with a specific schema. |\n|SCHEMA\\_NAME |VARCHAR |Name of the schema associated with the resource of type `ENTITY_TYPE` . Contains a NULL value when the resource isn\u2019t associated with a specific schema. |\n|CREDITS\\_USED\\_COMPUTE |NUMBER |Number of credits used by warehouses, serverless compute, and Openflow resources in the hour. |\n|CREDITS\\_USED\\_CLOUD\\_\nSERVICES |NUMBER |Number of credits used for cloud services in the hour. Always `0` when the SERVICE\\_TYPE is one of the Openflow types. |\n|CREDITS\\_USED |NUMBER |Total number of credits used for the account in the hour. This is a sum of CREDITS\\_USED\\_COMPUTE and CREDITS\\_USED\\_CLOUD\\_SERVICES. This value does not take into account the adjustment for cloud services, and may therefore be greater than your actual credit consumption. |\n|BYTES |NUMBER |When the service type is `auto_clustering` , indicates the number of bytes reclustered during the START\\_TIME and END\\_TIME window. When the service type is `pipe` , indicates the number of bytes inserted during the START\\_TIME and END\\_TIME window. When the service type is `SNOWPIPE_STREAMING` , indicates the number of bytes migrated during the START\\_TIME and END\\_TIME window. When the service type is `COPY_FILES` , columns are aggregated at the database level. |\n|ROWS |NUMBER |When the service type is `auto_clustering` , indicates number of rows reclustered during the START\\_TIME and END\\_TIME window. When the service type is `SNOWPIPE_STREAMING` , indicates the number of rows migrated during the START\\_TIME and END\\_TIME window. |\n|FILES |NUMBER |When the service type is `pipe` , indicates number of files loaded during the START\\_TIME and END\\_TIME window. When the service type is `SNOWPIPE_STREAMING` , this is NULL. When the service type is `COPY_FILES` , columns are aggregated at the database level. |\n\n## Usage notes \u00b6\n\n* Latency for the view may be up to 180 minutes (3 hours), except for the CREDITS\\_USED\\_CLOUD\\_SERVICES column. Latency for\n  CREDITS\\_USED\\_CLOUD\\_SERVICES may be up to 6 hours.\n* Latency for showing the credit consumption of `SNOWPIPE_STREAMING` may be up to 12 hours.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Columns\n2. Usage notes\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/warehouses-overview",
      "title": "Overview of warehouses | Snowflake Documentation",
      "publish_date": null,
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nEN\n\nEnglish\n\nFran\u00e7ais\n\nDeutsch\n\n\u65e5\u672c\u8a9e\n\n\ud55c\uad6d\uc5b4\n\nPortugu\u00eas\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\n1. Overview\n2. Snowflake Horizon Catalog\n3. Applications and tools for connecting to Snowflake\n4. Virtual warehouses\n   \n        - Overview\n        - Multi-cluster\n        - Considerations\n        - Working with warehouses\n        - Next-generation standard warehouses\n        - Query Acceleration Service\n        - Monitoring load\n        - Snowpark-optimized warehouses\n        - Interactive tables and warehouses\n5. Databases, Tables, & Views\n6. Data types\n7. Data Integration\n   \n    + Snowflake Openflow\n    + Apache Iceberg\u2122\n         \n        - Apache Iceberg\u2122 Tables\n        - Snowflake Open Catalog\n8. Data engineering\n   \n    + Data loading\n    + Dynamic tables\n    + Streams and tasks\n    + Row timestamps\n    + dbt Projects on Snowflake\n    + Data Unloading\n9. Storage lifecycle policies\n10. Migrations\n11. Queries\n12. Listings\n13. Collaboration\n14. Snowflake AI & ML\n15. Snowflake Postgres\n16. Alerts & Notifications\n17. Security\n18. Data Governance\n19. Privacy\n20. Organizations & Accounts\n21. Business continuity & data recovery\n22. Performance optimization\n23. Cost & Billing\n\nGuides Virtual warehouses Overview\n\n# Overview of warehouses \u00b6\n\nWarehouses are required for queries, as well as all DML operations, including loading data into tables. In addition to being defined by its\ntype as either Standard or Snowpark-optimized, a warehouse is defined by its size, as well as the other properties that can be set to help\ncontrol and automate warehouse activity.\n\nWarehouses can be started and stopped at any time. They can also be resized at any time, even while running, to accommodate the need for more\nor less compute resources, based on the type of operations being performed by the warehouse.\n\n## Warehouse size \u00b6\n\nSize specifies the amount of compute resources available per cluster in a warehouse. Snowflake supports the following warehouse sizes:\n\n|Warehouse size |Credits / hour (Gen1 warehouses) |Credits / second (Gen1 warehouses) |Notes |\n| --- | --- | --- | --- |\n|X-Small |1 |0\\.0003 |Default size for warehouses created in Snowsight and using CREATE WAREHOUSE . |\n|Small |2 |0\\.0006 | |\n|Medium |4 |0\\.0011 | |\n|Large |8 |0\\.0022 | |\n|X-Large |16 |0\\.0044 |Default size for warehouses created using Snowsight. |\n|2X-Large |32 |0\\.0089 | |\n|3X-Large |64 |0\\.0178 | |\n|4X-Large |128 |0\\.0356 | |\n|5X-Large |256 |0\\.0711 |Generally available in Amazon Web Services (AWS) and Microsoft Azure regions, and in preview in US Government regions. |\n|6X-Large |512 |0\\.1422 |Generally available in Amazon Web Services (AWS) and Microsoft Azure regions, and in preview in US Government regions. |\n\nThe numbers in the preceding table refer to the first generation (Gen1) of Snowflake standard warehouses.\nFor usage information about the newer Gen2 warehouses, see Snowflake generation 2 standard warehouses .\nFor information about credit consumption for generation 2 standard warehouses,\nsee the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nGen2 warehouses aren\u2019t yet available for all cloud service providers or for all regions, and currently are not the default\nwhen you create a standard warehouse.\n\nTip\n\nFor information about cost implications of changing the RESOURCE\\_CONSTRAINT property, see considerations for changing RESOURCE\\_CONSTRAINT while a warehouse is running or suspended .\n\nAnother way that you can scale the capacity of Snowflake warehouses without changing the warehouse size is by using\nmulti-cluster warehouses. For more information about that feature, see Multi-cluster warehouses .\n\n### Larger warehouse sizes \u00b6\n\nLarger warehouse sizes 5X-Large and 6X-Large are generally available in all Amazon Web Services (AWS) and Microsoft Azure regions.\n\nLarger warehouse sizes are in preview in US Government regions (requires FIPS support on ARM).\n\n### Impact on credit usage and billing \u00b6\n\nAs shown in the above table, there is a doubling of credit usage as you increase in size to the next larger warehouse size for each full\nhour that the warehouse runs; however, note that Snowflake utilizes per-second billing (with a 60-second minimum each time the warehouse\nstarts) so warehouses are billed only for the credits they actually consume.\n\nThe total number of credits billed depends on how long the warehouse runs continuously. For comparison purposes, the following table shows\nthe billing totals for three different size Gen1 standard warehouses based on their running time (totals rounded to the nearest 1000th of a credit):\n\n|Running Time |Credits . (X-Small) |Credits . (X-Large) |Credits . (5X-Large) |\n| --- | --- | --- | --- |\n|0-60 seconds |0\\.017 |0\\.267 |4\\.268 |\n|61 seconds |0\\.017 |0\\.271 |4\\.336 |\n|2 minutes |0\\.033 |0\\.533 |8\\.532 |\n|10 minutes |0\\.167 |2\\.667 |42\\.668 |\n|1 hour |1\\.000 |16\\.000 |256\\.000 |\n\nNote\n\n* For a multi-cluster warehouse , the number of credits billed is calculated based on the\n  warehouse size and the number of clusters that run within the time period.\n  \n  For example, if a 3X-Large multi-cluster warehouse runs 1 cluster for one full hour and then runs 2 clusters for the next full hour, the total number of credits billed would be 192 (i.e. 64 + 128).\n  \n  Multi-cluster warehouses are an Enterprise Edition feature.\n\n### Impact on data loading \u00b6\n\nIncreasing the size of a warehouse does not always improve data loading performance. Data loading performance is influenced more by\nthe number of files being loaded (and the size of each file) than the size of the warehouse.\n\nTip\n\nUnless you are bulk loading a large number of files concurrently (i.e. hundreds or thousands of files), a smaller warehouse\n(Small, Medium, Large) is generally sufficient. Using a larger warehouse (X-Large, 2X-Large, etc.) will consume more credits and may not\nresult in any performance increase.\n\nFor more data loading tips and guidelines, see Data loading considerations .\n\n### Impact on query processing \u00b6\n\nThe size of a warehouse can impact the amount of time required to execute queries submitted to the warehouse, particularly for larger, more\ncomplex queries. In general, query performance scales with warehouse size because larger warehouses have more compute resources available to\nprocess queries.\n\nIf queries processed by a warehouse are running slowly, you can always resize the warehouse to provision more compute resources. The\nadditional resources do not impact any queries that are already running, but once they are fully provisioned they become available for use\nby any queries that are queued or newly submitted.\n\nTip\n\nLarger is not necessarily faster for small, basic queries.\n\nFor more warehouse tips and guidelines, see Warehouse considerations .\n\n## Auto-suspension and auto-resumption \u00b6\n\nYou can set a warehouse to automatically resume or suspend, based on activity:\n\n* By default, auto-suspend is enabled. Snowflake automatically suspends the warehouse if it is inactive for the specified period of time.\n* By default, auto-resume is enabled. Snowflake automatically resumes the warehouse when any statement that requires a warehouse is submitted and the warehouse is the current warehouse for the session.\n\nThese properties can be used to simplify and automate your monitoring and usage of warehouses to match your workload. Auto-suspend ensures\nthat you don\u2019t leave a warehouse running (and consuming credits) when there are no incoming queries. Similarly, auto-resume ensures that\nthe warehouse starts up again as soon as it is needed.\n\nNote\n\nAuto-suspend and auto-resume apply only to the entire warehouse and not to the individual clusters in the warehouse.\nFor a multi-cluster warehouse :\n\n* Auto-suspend only occurs when the minimum number of clusters is running and there is no activity for the specified period of time. The\n  minimum is typically 1 (cluster), but could be more than 1.\n* Auto-resume only applies when the entire warehouse is suspended (i.e. no clusters are running).\n\n## Query processing and concurrency \u00b6\n\nThe number of queries that a warehouse can concurrently process is determined by the size and complexity of each query. As queries are\nsubmitted, the warehouse calculates and reserves the compute resources needed to process each query. If the warehouse does not have enough\nremaining resources to process a query, the query is queued, pending resources that become available as other running queries complete.\n\nSnowflake provides some object-level parameters that can be set to help control query processing and concurrency:\n\n* STATEMENT\\_QUEUED\\_TIMEOUT\\_IN\\_SECONDS\n* STATEMENT\\_TIMEOUT\\_IN\\_SECONDS\n\nNote\n\nIf queries are queuing more than desired, another warehouse can be created and queries can be manually redirected to the new warehouse.\nIn addition, resizing a warehouse can enable limited scaling for query concurrency and queuing; however, warehouse resizing is primarily\nintended for improving query performance.\n\nTo enable fully automated scaling for concurrency, Snowflake recommends multi-cluster warehouses ,\nwhich provide essentially the same benefits as creating additional warehouses and redirecting queries, but without requiring manual\nintervention.\n\nMulti-cluster warehouses are an Enterprise Edition feature.\n\n## Warehouse usage in sessions \u00b6\n\nWhen a session is initiated in Snowflake, the session does not, by default, have a warehouse associated with it. Until a session has a\nwarehouse associated with it, queries cannot be submitted within the session.\n\n### Default warehouse for users \u00b6\n\nTo facilitate querying immediately after a session is initiated, Snowflake supports specifying a default warehouse for each individual user.\nThe default warehouse for a user is used as the warehouse for all sessions initiated by the user.\n\nA default warehouse can be specified when creating or modifying the user, either through the web interface or using CREATE USER / ALTER USER .\n\n### Default warehouse for client utilities/drivers/connectors \u00b6\n\nIn addition to default warehouses for users, any of the Snowflake clients (Snowflake CLI, SnowSQL, JDBC driver, ODBC driver, Python connector, etc.) can\nhave a default warehouse:\n\n* Snowflake CLI and SnowSQL support both a configuration file and command line options for specifying a default warehouse.\n* The drivers and connectors support specifying a default warehouse as a connection parameter when initiating a session.\n\nFor more information, see Applications and tools for connecting to Snowflake .\n\n### Default warehouse for notebooks \u00b6\n\n Preview Feature \u2014 Open\n\nAvailable to all accounts.\n\nTo enhance cost efficiency for notebook workloads, a multi-cluster X-Small warehouse, SYSTEM$STREAMLIT\\_NOTEBOOK\\_WH, is automatically\nprovisioned within each account. This warehouse, featuring a maximum of 10 clusters and a 60-second default timeout, uses improved bin\npacking. The ACCOUNTADMIN role has OWNERSHIP privileges.\n\n#### Recommendations for cost management \u00b6\n\n* Snowflake recommends using the SYSTEM$STREAMLIT\\_NOTEBOOK\\_WH warehouse exclusively for notebook workloads.\n* To improve bin-packing efficiency and reduce cluster fragmentation, direct SQL queries from Notebook apps to a separate customer-managed query warehouse. Using a single shared warehouse for all Notebook apps in an account further enhances bin-packing efficiency.\n* Separating notebook Python workloads from SQL queries minimizes cluster fragmentation. This approach optimizes overall costs by ensuring that notebook Python workloads are not co-located with larger warehouses, which are typically used for query execution.\n\n#### Access control requirements \u00b6\n\n|Privilege |Object |Notes |\n| --- | --- | --- |\n|USAGE |SYSTEM$STREAMLIT\\_NOTEBOOK\\_WH |By default, the PUBLIC role has USAGE privileges. ACCOUNTADMIN can grant and revoke USAGE privileges. |\n|MONITOR, OPERATE, APPLYBUDGET |SYSTEM$STREAMLIT\\_NOTEBOOK\\_WH |Available to the ACCOUNTADMIN and grantable by the ACCOUNTADMIN to other roles. |\n\n### Precedence for warehouse defaults \u00b6\n\nWhen a user connects to Snowflake and start a session, Snowflake determines the default warehouse for the session in the following order:\n\n1. Default warehouse for the user,\n   \n   \u00bb **overridden by\u2026**\n2. Default warehouse in the configuration file for the client utility (SnowSQL, JDBC driver, etc.) used to connect to Snowflake (if the\n   client supports configuration files),\n   \n   \u00bb **overridden by\u2026**\n3. Default warehouse specified on the client command line or through the driver/connector parameters passed to Snowflake.\n\nNote\n\nIn addition, the default warehouse for a session can be changed at any time by executing the USE WAREHOUSE command within the session.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Warehouse size\n2. Auto-suspension and auto-resumption\n3. Query processing and concurrency\n4. Warehouse usage in sessions\n\nRelated content\n\n1. Understanding compute cost\n2. Working with resource monitors"
      ],
      "full_content": null
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/warehouses-multicluster",
      "title": "Multi-cluster warehouses | Snowflake Documentation",
      "publish_date": null,
      "excerpts": [
        "Guides Virtual warehouses Multi-cluster\n\n# Multi-cluster warehouses \u00b6\n\n Enterprise Edition Feature\n\nTo inquire about upgrading to Enterprise Edition (or higher), please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\n\nMulti-cluster warehouses enable you to scale compute resources to manage your user and query concurrency needs as they change, such as during\npeak and off hours.\n\n## What is a multi-cluster warehouse? \u00b6\n\nBy default, a virtual warehouse consists of a single cluster of compute resources available to the\nwarehouse for executing queries. As queries are submitted to a warehouse, the warehouse allocates resources to each query and begins\nexecuting the queries. If sufficient resources are not available to execute all the queries submitted to the warehouse, Snowflake queues the\nadditional queries until the necessary resources become available.\n\nWith multi-cluster warehouses, Snowflake supports allocating, either statically or dynamically, additional clusters to make a larger pool\nof compute resources available. A multi-cluster warehouse is defined by specifying the following properties:\n\n* Maximum number of clusters, greater than 1. The highest value you can specify depends on the warehouse size.\n  For the upper limit on the number of clusters for each warehouse size,\n  see Upper limit on number of clusters for a multi-cluster warehouse (in this topic).\n* Minimum number of clusters, equal to or less than the maximum.\n\nAdditionally, multi-cluster warehouses support all the same properties and actions as single-cluster warehouses, including:\n\n* Specifying a warehouse size.\n* Resizing a warehouse at any time.\n* Auto-suspending a running warehouse due to inactivity; note that this does not apply to individual clusters, but rather the entire\n  multi-cluster warehouse.\n* Auto-resuming a suspended warehouse when new queries are submitted.\n\n### Upper limit on number of clusters for a multi-cluster warehouse \u00b6\n\nThe maximum number of clusters for a multi-cluster warehouse depends on the warehouse size. Larger warehouse sizes have lower limits on the number of clusters. The following table shows the maximum number of clusters for each warehouse size:\n\n|Warehouse size |Allowed maximum cluster count |Default maximum cluster count |\n| --- | --- | --- |\n|XSMALL |300 |10 |\n|SMALL |300 |10 |\n|MEDIUM |300 |10 |\n|LARGE |160 |10 |\n|XLARGE |80 |10 |\n|2XLARGE |40 |10 |\n|3XLARGE |20 |10 |\n|4XLARGE |10 |10 |\n|5XLARGE |10 |10 |\n|6XLARGE |10 |10 |\n\n### Maximized vs. auto-scale \u00b6\n\nYou can choose to run a multi-cluster warehouse in either of the following modes:\n\nMaximized :\n    This mode is enabled by specifying the same value for both maximum and minimum number of clusters (note that the\nspecified value must be larger than 1). In this mode, when the warehouse is started, Snowflake starts all the clusters so\nthat maximum resources are available while the warehouse is running.\n\nThis mode is effective for statically controlling the available compute resources, particularly if you have large numbers of concurrent\nuser sessions and/or queries and the numbers do not fluctuate significantly.\nAuto-scale :\n    This mode is enabled by specifying different values for maximum and minimum number of clusters. In this mode,\nSnowflake starts and stops clusters as needed to dynamically manage the load on the warehouse:\n\n* As the number of concurrent user sessions and/or queries for the warehouse increases, and queries start to queue due to\n  insufficient resources, Snowflake automatically starts additional clusters, up to the maximum number defined for the warehouse.\n* Similarly, as the load on the warehouse decreases, Snowflake automatically shuts down clusters to reduce the number of\n  running clusters and, correspondingly, the number of credits used by the warehouse.\n\nTo help control the usage of credits in Auto-scale mode, Snowflake provides a property, SCALING\\_POLICY, that determines the scaling policy\nto use when automatically starting or shutting down additional clusters. For more information, see Setting the scaling policy for a multi-cluster warehouse (in\nthis topic).\n\nTo create a multi-cluster warehouse, see Creating a multi-cluster warehouse (in this topic).\n\n* For auto-scale mode, the maximum number of clusters must be _greater_ than the minimum number of clusters.\n* For maximized mode, the maximum number of clusters must be _equal_ to the minimum number of clusters.\n\nTip\n\nWhen determining the maximum and minimum number of clusters to use for a multi-cluster warehouse, start with Auto-scale mode and start\nsmall (for example, maximum = 2 or 3, minimum = 1). As you track how your warehouse load fluctuates over time, you can increase the maximum and\nminimum number of clusters until you determine the numbers that best support the upper and lower boundaries of your user/query concurrency.\n\n### Multi-cluster size and credit usage \u00b6\n\nThe amount of compute resources in each cluster is determined by the warehouse size:\n\n* The total number of clusters for the multi-cluster warehouse is calculated by multiplying the warehouse size by the maximum number of\n  clusters. This also indicates the maximum number of credits consumed by the warehouse per full hour of usage (i.e. if all clusters run during the hour).\n  \n  For example, the maximum number of credits consumed per hour for a Medium-size multi-cluster warehouse with 3 clusters is 12 credits.\n* If a multi-cluster warehouse is resized, the new size applies to all the clusters for the warehouse, including\n  clusters that are currently running and any clusters that are started after the multi-cluster warehouse is resized.\n\nThe actual number of credits consumed per hour depends on the number of clusters running during each hour that the warehouse\nis running. For more details, see Examples of multi-cluster credit usage (in this topic).\n\nTip\n\nIf you use Query Acceleration Service (QAS) for a multi-cluster warehouse, consider adjusting the QAS scale\nfactor higher than for a single-cluster warehouse. That helps to apply the QAS optimizations across all the\nclusters of the warehouse.\nFor more information, see Adjusting the scale factor .\n\n### Benefits of multi-cluster warehouses \u00b6\n\nWith a standard, single-cluster warehouse, if your user/query load increases to the point where you need more compute resources:\n\n1. You must either increase the size of the warehouse or start additional warehouses and explicitly redirect the additional users/queries to\n   these warehouses.\n2. Then, when the resources are no longer needed, to conserve credits, you must manually downsize the larger warehouse or suspend the additional\n   warehouses.\n\nIn contrast, a multi-cluster warehouse enables larger numbers of users to connect to the same size warehouse. In addition:\n\n* In Auto-scale mode, a multi-cluster warehouse eliminates the need for resizing the warehouse or starting and stopping additional\n  warehouses to handle fluctuating workloads. Snowflake automatically starts and stops additional clusters as needed.\n* In Maximized mode, you can control the capacity of the multi-cluster warehouse by increasing or decreasing the number of clusters as\n  needed.\n\nTip\n\nMulti-cluster warehouses are best utilized for scaling resources to improve concurrency for users/queries. They are not as beneficial for\nimproving the performance of slow-running queries or data loading. For these types of operations, resizing the warehouse provides\nmore benefits.\n\n## Examples of multi-cluster credit usage \u00b6\n\nThe following four examples illustrate credit usage for a multi-cluster warehouse. Refer to Virtual warehouse credit usage for\nthe number of credits billed per full hour by warehouse size.\n\nNote\n\nFor the sake of simplicity, all these examples depict credit usage in increments of 1 hour, 30 minutes, and 15 minutes. In a real-world\nscenario, with per-second billing, the actual credit usage would contain fractional amounts, based on the number of seconds that each\ncluster runs.\n\n### Example 1: Maximized (2 Hours) \u00b6\n\nIn this example, a Medium-size Standard warehouse with 3 clusters runs in Maximized mode for 2 hours:\n\n| |Cluster 1 |Cluster 2 |Cluster 3 |**Total Credits** |\n| --- | --- | --- | --- | --- |\n|1st Hour |4 |4 |4 |**12** |\n|2nd Hour |4 |4 |4 |**12** |\n|**Total Credits** |**8** |**8** |**8** |**24** |\n\n### Example 2: Auto-scale (2 Hours) \u00b6\n\nIn this example, a Medium-size Standard warehouse with 3 clusters runs in Auto-scale mode for 2 hours:\n\n* Cluster 1 runs continuously.\n* Cluster 2 runs continuously for the 2nd hour only.\n* Cluster 3 runs for 30 minutes during the 2nd hour.\n\n| |Cluster 1 |Cluster 2 |Cluster 3 |**Total Credits** |\n| --- | --- | --- | --- | --- |\n|1st Hour |4 |0 |0 |**4** |\n|2nd Hour |4 |4 |2 |**10** |\n|**Total Credits** |**8** |**4** |**2** |**14** |\n\n### Example 3: Auto-scale (3 Hours) \u00b6\n\nIn this example, a Medium-size Standard warehouse with 3 clusters runs in Auto-scale mode for 3 hours:\n\n* Cluster 1 runs continuously.\n* Cluster 2 runs continuously for the entire 2nd hour and 30 minutes in the 3rd hour.\n* Cluster 3 runs for 30 minutes in the 3rd hour.\n\n| |Cluster 1 |Cluster 2 |Cluster 3 |**Total Credits** |\n| --- | --- | --- | --- | --- |\n|1st Hour |4 |0 |0 |**4** |\n|2nd Hour |4 |4 |0 |**8** |\n|3rd Hour |4 |2 |2 |**8** |\n|**Total Credits** |**12** |**6** |**2** |**20** |\n\n### Example 4: Auto-scale (3 Hours) with resize \u00b6\n\nIn this example, the same warehouse from example 3 runs in Auto-scale mode for 3 hours with a resize from Medium to Large:\n\n* Cluster 1 runs continuously.\n* Cluster 2 runs continuously for the 2nd and 3rd hours.\n* Warehouse is resized from Medium to Large at 1:30 hours.\n* Cluster 3 runs for 15 minutes in the 3rd hour.\n\n| |Cluster 1 |Cluster 2 |Cluster 3 |**Total Credits** |\n| --- | --- | --- | --- | --- |\n|1st Hour |4 |0 |0 |**4** |\n|2nd Hour |4+2 |4+2 |0 |**12** |\n|3rd Hour |8 |8 |2 |**18** |\n|**Total Credits** |**18** |**14** |**2** |**34** |\n\n## Creating a multi-cluster warehouse \u00b6\n\nYou can create a multi-cluster warehouse in Snowsight or by using SQL:\n\n> Snowsight :\n>     In the navigation menu, select Compute \u00bb Warehouses \u00bb \\+ Warehouse\n> \n> 1. Expand Advanced Options .\n> 2. Select the Multi-cluster Warehouse checkbox.\n> 3. In the Max Clusters field, select a value greater than 1.\n>    \n>    Note\n>    \n>    Currently, the highest value you can choose in Snowsight is 10.\n>    The maximum sizes shown in Upper limit on number of clusters for a multi-cluster warehouse apply to the CREATE WAREHOUSE and ALTER WAREHOUSE commands in SQL only.\n> 4. In the Min Clusters field, optionally select a value greater than 1.\n> 5. Enter other information for the warehouse, as needed, and click Create Warehouse .\n> \n> SQL :\n>     Execute a CREATE WAREHOUSE command with:\n> \n> * `MAX_CLUSTER_COUNT` set to a value greater than `1` . For the highest value\n>   you can specify depending on the warehouse size, see Upper limit on number of clusters for a multi-cluster warehouse (in this topic).\n> * `MIN_CLUSTER_COUNT` (optionally) set to a value greater than `1` .\n> \n> \n\nTo view information about the multi-cluster warehouses you create:\n\n> Snowsight :\n>     In the navigation menu, select Compute \u00bb Warehouses .\n> \n> The Clusters column displays the minimum and maximum clusters for each warehouse, as well as the number of\n> clusters that are currently running if the warehouse is started. You can sort by the Clusters column in\n> descending order to list the multi-cluster warehouses at the top.\n> SQL :\n>     Execute a SHOW WAREHOUSES command.\n> \n> The output includes three columns ( `min_cluster_count` , `max_cluster_count` , `started_clusters` )\n> that display the same information provided in the Clusters column in the web interface.\n> \n> Tip\n> \n> If the SHOW WAREHOUSES output is difficult to read because it includes so many columns, you can\n> use the pipe operator ( `->>` ) to show just the columns you want,\n> along with any other clauses for filtering and sorting. Use a query that is similar to the following example,\n> and adjust it to suit your needs. The column names are quoted because they\u2019re case-sensitive in\n> the SHOW WAREHOUSES output:\n> \n> ```\n> SHOW WAREHOUSES \n>   ->> SELECT \"name\" , \"state\" , \"size\" , \"max_cluster_count\" , \"started_clusters\" , \"type\" \n>         FROM $ 1 \n>         WHERE \"state\" IN ( 'STARTED' , 'SUSPENDED' ) \n>         ORDER BY \"type\" DESC , \"name\" ;\n> ```\n> \n> Copy\n> \n> \n\nAll other tasks for multi-cluster warehouses (except for the remaining tasks described in this topic) are identical to single-cluster warehouse tasks .\n\n## Setting the scaling policy for a multi-cluster warehouse \u00b6\n\nTo help control the credits consumed by a multi-cluster warehouse running in Auto-scale mode, Snowflake provides scaling policies.\nSnowflake uses the scaling policies to determine how to adjust the capacity of your multi-cluster warehouse\nby starting or shutting down individual clusters while the warehouse is running. You can specify a scaling policy\nto make Snowflake prioritize responsiveness and throughput for the queries in that warehouse, or to minimize costs\nfor that warehouse.\n\nThe scaling policy for a multi-cluster warehouse only applies if it is running in Auto-scale mode.\nIn Maximized mode, all clusters run concurrently, so there is no need to start or shut down individual clusters.\n\nSnowflake supports the following scaling policies:\n\n|Policy |Description |A new cluster starts\u2026 |An idle or lightly loaded cluster shuts down\u2026 |\n| --- | --- | --- | --- |\n|Standard (default) |Prevents/minimizes queuing by favoring starting additional clusters over conserving credits. |When a query is queued, or if Snowflake estimates the currently running clusters don\u2019t have\nenough resources to handle any additional queries, Snowflake increases the number of clusters\nin the warehouse.\n\nFor warehouses with a MAX\\_CLUSTER\\_COUNT of 10 or less, Snowflake starts one additional cluster.\n\nFor warehouses with a MAX\\_CLUSTER\\_COUNT greater than 10, Snowflake starts multiple clusters at once\nto accommodate rapid increases in workload. |After a sustained period of low load, Snowflake shuts down one or more of the least-loaded\nclusters when the queries running on them finish. When the cluster count is higher than 10,\nSnowflake might shut down multiple clusters at a time. When the cluster count is 10 or less,\nSnowflake shuts down the idle clusters one at a time. |\n|Economy |Conserves credits by favoring keeping running clusters fully-loaded rather than starting additional clusters, which may result\nin queries being queued and taking longer to complete. |Only if the system estimates there\u2019s enough query load to keep the cluster busy for at least 6 minutes. |Snowflake marks the least-loaded cluster for shutdown if it estimates the cluster has less\nthan 6 minutes of work left to do. Snowflake shuts down the cluster after finishing any\nqueries that are running on that cluster. When the cluster count is higher than 10,\nSnowflake might shut down multiple clusters at a time. When the cluster count is 10 or less,\nSnowflake shuts down the idle clusters one at a time. |\n\nNote\n\nA third scaling policy, Legacy, was formerly provided for backward compatibility. Legacy has been removed.\nAll warehouses that were using the Legacy policy now use the default Standard policy.\n\nYou can set the scaling policy for a multi-cluster warehouse when it is created or at any time afterwards,\neither in Snowsight or using SQL:\n\n> Snowsight :\n>     When you select Multi-cluster Warehouse under Advanced Options in the New Warehouse dialog,\n> you can select the scaling policy from the Scaling Policy drop-down list.\n> \n> For an existing multi-cluster warehouse, in the navigation menu, select Compute \u00bb Warehouses . Then select Edit under the More menu ( \u2026 ).\n> \n> In the Scaling Policy field, select the desired value from the drop-down list.\n> \n> Tip\n> \n> You only see the Scaling Policy drop-down list when the warehouse you selected is a multi-cluster warehouse,\n> and the maximum clusters value is higher than the minimum clusters value.\n> SQL :\n>     Execute a CREATE WAREHOUSE or ALTER WAREHOUSE command with `SCALING_POLICY` set to the desired value.\n> \n> \n\nFor example, in SQL:\n\n> ```\n> CREATE WAREHOUSE mywh WITH MAX_CLUSTER_COUNT = 2 , SCALING_POLICY = 'STANDARD' ; \n>  ALTER WAREHOUSE mywh SET SCALING_POLICY = 'ECONOMY' ;\n> ```\n> \n> Copy\n> \n>\n\n## Increasing or decreasing clusters for a multi-cluster warehouse \u00b6\n\nYou can increase or decrease the maximum and minimum number of clusters for a warehouse at any time,\neven while it is running and executing statements. You can adjust the maximum and minimum clusters\nfor a warehouse in Snowsight or using SQL:\n\n> Snowsight :\n>     In the navigation menu, select Compute \u00bb Warehouses .\n> \n> Click on the warehouse name to view its properties and historical activity.\n> Select Edit from More menu ( \u2026 ).\n> You can also deselect the Multi-cluster Warehouse checkbox to reset the maximum and minimum\n> cluster settings to 1, changing the warehouse to a single-cluster one.\n> SQL :\n>     Execute an ALTER WAREHOUSE command.\n> \n> \n\nNote\n\nCurrently, Snowsight supports updating MAX\\_CLUSTER\\_COUNT to a maximum of 10 clusters.\nTo increase MAX\\_CLUSTER\\_COUNT beyond 10, use the ALTER WAREHOUSE command in SQL.\n\nThe effect of changing the maximum and minimum clusters for a running warehouse depends on whether it is running in\nMaximized or Auto-scale mode:\n\n* Maximized:\n  \n  \u2191 max & min :\n      Specified number of clusters start immediately.\n  \u2193 max & min :\n      Specified number of clusters shut down when they finish executing statements and the auto-suspend period elapses.\n* Auto-scale:\n  \n  \u2191 max :\n      If `new_max_clusters > running_clusters` , no changes until additional clusters are needed.\n  \u2193 max :\n      If `new_max_clusters < running_clusters` , excess clusters shut down when they finish executing statements and the scaling policy conditions are met.\n  \u2191 min :\n      If `new_min_clusters > running_clusters` , additional clusters immediately started to meet the minimum.\n  \u2193 min :\n      If `new_min_clusters < running_clusters` , excess clusters shut down when they finish executing statements and the scaling policy conditions are met.\n\n## Monitoring multi-cluster warehouses \u00b6\n\nYou can monitor usage of multi-cluster warehouses through the web interface:\n\n1. In the navigation menu, select Compute \u00bb Warehouses .\n2. Select a warehouse name.\n   \n   > That way, you can monitor one warehouse in precise detail, such as viewing queries that are currently\n   > running or queued.\n   > \n   > Alternatively, in the navigation menu, select Monitoring \u00bb Query History .\n   > This page lets you view activity across multiple warehouses in your account.\n   > To see the activity only for one warehouse, select Warehouse under the Filters drop-down menu. Then choose a warehouse name from the list.\n   > \n   >\n\nWhen you monitor a multi-cluster warehouse, you can see all the queries the warehouse processed.\nFor each query, you can see details such as how long it took, how many bytes\nit scanned, and how many rows it returned. You can also see the cluster used to execute\neach statement that the warehouse processed. To choose which details to view, select\nthe items such as Cluster Number , Duration , Rows , and so on\nunder the Columns drop-down menu.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. What is a multi-cluster warehouse?\n2. Examples of multi-cluster credit usage\n3. Creating a multi-cluster warehouse\n4. Setting the scaling policy for a multi-cluster warehouse\n5. Increasing or decreasing clusters for a multi-cluster warehouse\n6. Monitoring multi-cluster warehouses\n\nRelated content\n\n1. Understanding compute cost\n2. Working with resource monitors\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 5
    }
  ]
}
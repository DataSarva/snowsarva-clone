{"extract_id":"extract_01768d0b5e014ae3b0bcb24be11f5468","results":[{"url":"https://docs.snowflake.com/en/developer-guide/logging-tracing/logging-tracing-overview","title":"Logging, tracing, and metrics | Snowflake Documentation","publish_date":"2026-01-01","excerpts":["Developer Logging, Tracing, and Metrics\nSection Title: Logging, tracing, and metrics ¶\nContent:\nYou can record the activity of your Snowflake function and procedure handler code (including code you write using Snowpark APIs ) by capturing log messages and trace events from the code as it executes.\nOnce you’ve collected the data, you can query it with SQL to analyze the results.\nLogging, tracing, and metrics are among the observability features Snowflake provides to make it easier for you to debug and optimize\napplications. Snowflake captures observability data in a structure based on the [OpenTelemetry](https://opentelemetry.io/) standard.\nIn particular, you can record and analyze the following:\nLog messages — Independent, detailed messages with information about the state of a\nspecific piece of your code.\nMetrics data — CPU and memory metrics that Snowflake generates.\nTrace events — Structured data you can use to get information spanning and grouping\nmultiple parts of your code.\nSection Title: Logging, tracing, and metrics ¶ > Get started ¶\nContent:\nUse the following high-level steps to begin capturing and using log and trace data.\nSection Title: Logging, tracing, and metrics ¶ > Get started ¶\nContent:\nEnsure that you have an active event table. You can do one of the following:Snowflake collects telemetry data from your code in the event table. Use the default event table that is active by default. Create and set as active an event table . Set telemetry levels so that data is collected.With levels, you can specify which data – and how much data – is collected. Make sure the levels are set correctly. Begin emitting log or trace data from handler code.Once you’ve created an event table and associated it with your account, you can use an API in your handler’s language to emit log\nmessages.\nSection Title: Logging, tracing, and metrics ¶ > Get started ¶\nContent:\nAfter you’ve captured log and trace data, you can query the data to analyze the results.For more information on instrumenting your code, see the following:\nLogging messages from functions and procedures\nTrace events for functions and procedures\nQuery the event table to analyze collected log and trace data.For more information, see the following:\nViewing log messages\nViewing metrics data\nViewing trace data\nSection Title: Logging, tracing, and metrics ¶ > Set telemetry levels ¶\nContent:\nYou can manage the level of telemetry data stored in the event table — such as log, trace, and metrics data — by setting the level\nfor each type of data. Use level settings to ensure that you’re capturing the amount and kind of data you want.\nFor more information, see Setting levels for logging, metrics, and tracing .\nSection Title: Logging, tracing, and metrics ¶ > Compare log messages and trace events ¶\nContent:\nThe following table compares the characteristics and benefits of log messages and trace events.\nSection Title: Logging, tracing, and metrics ¶ > Compare log messages and trace events ¶\nContent:\n| Characteristic | Log entries | Trace events |\n| Intended use | Record detailed but unstructured information about the state of your code. Use this information to understand what happened during |  |\n| a particular invocation of your function or procedure. | Record a brief but structured summary of each invocation of your code. Aggregate this information to understand behavior of your |  |\n| code at a high level. |  |  |\n| Structure as a payload | None. A log entry is just a string. | Structured with attributes you can attach to trace events. Attributes are key-value pairs that can be easily queried with a SQL query. |\n| Supports grouping | No. Each log entry is an independent event. | Yes. Trace events are organized into spans. A span can have its own attributes. |\nSection Title: Logging, tracing, and metrics ¶ > Compare log messages and trace events ¶\nContent:\n| Characteristic | Log entries | Trace events |\n| Intended use | Record detailed but unstructured information about the state of your code. Use this information to understand what happened during |  |\n| a particular invocation of your function or procedure. | Record a brief but structured summary of each invocation of your code. Aggregate this information to understand behavior of your |  |\n| code at a high level. |  |  |\n| Structure as a payload | None. A log entry is just a string. | Structured with attributes you can attach to trace events. Attributes are key-value pairs that can be easily queried with a SQL query. |\n| Quantity limits | Unlimited. All log entries emitted by your code are ingested into the event table. | The number of trace events per span is capped at 128. There is also a limit on the number of span attributes. |\nSection Title: Logging, tracing, and metrics ¶ > Compare log messages and trace events ¶\nContent:\n| Characteristic | Log entries | Trace events |\n| Intended use | Record detailed but unstructured information about the state of your code. Use this information to understand what happened during |  |\n| a particular invocation of your function or procedure. | Record a brief but structured summary of each invocation of your code. Aggregate this information to understand behavior of your |  |\n| code at a high level. |  |  |\n| Structure as a payload | None. A log entry is just a string. | Structured with attributes you can attach to trace events. Attributes are key-value pairs that can be easily queried with a SQL query. |\n| Complexity of queries against recorded data | Relatively high. Your queries must parse each log entry to extract meaningful information from it. | Relatively low. Your queries can take advantage of the structured nature of trace events. |\nSection Title: Logging, tracing, and metrics ¶ > Compare log messages and trace events ¶\nContent:\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nGet started\nSet telemetry levels\nCompare log messages and trace events\nRelated content\nEnabling telemetry collection\nLogging messages from functions and procedures\nTrace events for functions and procedures\nCollecting metrics data\nSection Title: Logging, tracing, and metrics ¶ > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\nSection Title: Logging, tracing, and metrics ¶ > Privacy Preference Center > Your Privacy\nContent:\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nSection Title: Logging, tracing, and metrics ¶ > ... > Your Privacy > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Details‎\nSection Title: Logging, tracing, and metrics ¶ > Privacy Preference Center > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details‎\nSection Title: Logging, tracing, and metrics ¶ > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details‎\nSection Title: Logging, tracing, and metrics ¶ > Privacy Preference Center > Your Privacy > Targeting Cookies\nContent:\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nCookies Details‎\nSection Title: Logging, tracing, and metrics ¶ > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"],"full_content":"Developer Logging, Tracing, and Metrics\n\n# Logging, tracing, and metrics ¶\n\nYou can record the activity of your Snowflake function and procedure handler code (including code you write using Snowpark APIs ) by capturing log messages and trace events from the code as it executes.\nOnce you’ve collected the data, you can query it with SQL to analyze the results.\n\nLogging, tracing, and metrics are among the observability features Snowflake provides to make it easier for you to debug and optimize\napplications. Snowflake captures observability data in a structure based on the [OpenTelemetry](https://opentelemetry.io/) standard.\n\nIn particular, you can record and analyze the following:\n\n* Log messages — Independent, detailed messages with information about the state of a\n  specific piece of your code.\n* Metrics data — CPU and memory metrics that Snowflake generates.\n* Trace events — Structured data you can use to get information spanning and grouping\n  multiple parts of your code.\n\n## Get started ¶\n\nUse the following high-level steps to begin capturing and using log and trace data.\n\n1. Ensure that you have an active event table. You can do one of the following:\n   \n    + Use the default event table that is active by default.\n    + Create and set as active an event table .\n   \n   Snowflake collects telemetry data from your code in the event table.\n2. Set telemetry levels so that data is collected.\n   \n   With levels, you can specify which data – and how much data – is collected. Make sure the levels are set correctly.\n3. Begin emitting log or trace data from handler code.\n   \n   Once you’ve created an event table and associated it with your account, you can use an API in your handler’s language to emit log\n   messages. After you’ve captured log and trace data, you can query the data to analyze the results.\n   \n   For more information on instrumenting your code, see the following:\n   \n    + Logging messages from functions and procedures\n    + Trace events for functions and procedures\n4. Query the event table to analyze collected log and trace data.\n   \n   For more information, see the following:\n   \n    + Viewing log messages\n    + Viewing metrics data\n    + Viewing trace data\n\n## Set telemetry levels ¶\n\nYou can manage the level of telemetry data stored in the event table — such as log, trace, and metrics data — by setting the level\nfor each type of data. Use level settings to ensure that you’re capturing the amount and kind of data you want.\n\nFor more information, see Setting levels for logging, metrics, and tracing .\n\n## Compare log messages and trace events ¶\n\nThe following table compares the characteristics and benefits of log messages and trace events.\n\n|Characteristic |Log entries |Trace events |\n| --- | --- | --- |\n|Intended use |Record detailed but unstructured information about the state of your code. Use this information to understand what happened during\na particular invocation of your function or procedure. |Record a brief but structured summary of each invocation of your code. Aggregate this information to understand behavior of your\ncode at a high level. |\n|Structure as a payload |None. A log entry is just a string. |Structured with attributes you can attach to trace events. Attributes are key-value pairs that can be easily queried with a SQL query. |\n|Supports grouping |No. Each log entry is an independent event. |Yes. Trace events are organized into spans. A span can have its own attributes. |\n|Quantity limits |Unlimited. All log entries emitted by your code are ingested into the event table. |The number of trace events per span is capped at 128. There is also a limit on the number of span attributes. |\n|Complexity of queries against recorded data |Relatively high. Your queries must parse each log entry to extract meaningful information from it. |Relatively low. Your queries can take advantage of the structured nature of trace events. |\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Get started\n2. Set telemetry levels\n3. Compare log messages and trace events\n\nRelated content\n\n1. Enabling telemetry collection\n2. Logging messages from functions and procedures\n3. Trace events for functions and procedures\n4. Collecting metrics data\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details‎\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details‎\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details‎\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details‎\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"},{"url":"https://docs.snowflake.com/en/developer-guide/builders/observability","title":"Observability in Snowflake apps | Snowflake Documentation","publish_date":null,"excerpts":["Developer Observability\nSection Title: Observability in Snowflake apps ¶\nContent:\nThrough observability built into Snowflake, you can ensure that your applications are running as efficiently as possible.\nUsing the practices and features described in this topic, you can make the most of observability features that show you where you\ncan improve your code.\nSection Title: Observability in Snowflake apps ¶ > What is observability? ¶\nContent:\nIn an observable system, you can understand what’s happening internally through external evidence generated by the system—evidence\nthat includes telemetry data, alerts, and notifications.\nThrough the evidence of internal functioning it provides, observability makes it easier for you to troubleshoot hard-to-understand behaviors\non a production system. This is especially true in a distributed system, where evidence collected from observation provides a view of\nbehavior across multiple components. Rather than disrupting a production environment to diagnose issues, you can analyze the collected\ndata from it.\nWith an observable system, you can start to answer questions such as the following:\nHow well is the system performing?\nWhere is there latency and what’s causing it?\nWhy is a particular component or process not working as it should?\nWhat improvements can be made?\nSection Title: Observability in Snowflake apps ¶ > Observability in Snowflake ¶\nContent:\nSnowflake supports a model that provides built-in observable data while also giving you ways to add more instrumentation where you need it.\nWhile Snowflake provides support for telemetry data such as logs, metrics, and traces (which are typical of observability), it also\nincludes other features you can use to keep track of system usage and performance.\nThe following lists features you can use to receive and analyze system performance and usage.\n|Collected telemetry data |As your application generates logs, metrics, and traces, Snowflake collects that telemetry data in an event table. Using\nSnowsight, you can explore the data, looking for patterns.\nYou can emit custom telemetry into the event table to provide contextual, domain-specific information to expedite debugging. |\n| --- | --- |\n|History Tables |Use the following views and their associated tables to monitor all usage in your account.\nSection Title: Observability in Snowflake apps ¶ > Observability in Snowflake ¶\nContent:\nQuery History\nCopy History\nTasks |\n|Alerts and notifications |Alerts allow for customizable triggering conditions, actions, and a schedule, in combination with notification integrations for proactive monitoring. |\n|Extensibility with third-party tools |The Snowflake event table adopts [OpenTelemetry](https://opentelemetry.io/docs/) standards, so your\nSnowflake telemetry can easily be consumed by other ecosystem tools. |\nSection Title: Observability in Snowflake apps ¶ > Telemetry data collected for analysis ¶\nContent:\nAs code in your application executes, you can have Snowflake collect data from the code that tells you about the application’s internal\nstate. Using this telemetry data—collected in a Snowflake event table (your account has one by default )—you can look for bottlenecks and other opportunities to optimize.\nTelemetry data must be emitted as your code executes. Snowflake emits some of this data on your code’s behalf without\nyou needing to instrument your code. You can use also APIs included with Snowflake to emit telemetry data from specific parts of your code.\nAs described below, you can analyze the collected data by querying the event table or using the visualizations that capture the data\nin Snowsight.\nSection Title: Observability in Snowflake apps ¶ > ... > Types of telemetry data ¶\nContent:\nTo ensure that the telemetry data you collect is broadly useful, Snowflake telemetry is built on the standard [OpenTelemetry](https://opentelemetry.io/docs/) (sometimes called OTel) framework, an incubating project of the Cloud Native Compute Foundation. Through this framework (and APIs and\ntools designed for it), you can reuse collected data with tools besides Snowflake .\nThrough OpenTelemetry, you can instrument application code to add observability where you want it.\nSnowflake event tables collect log, span, and metrics data in the OpenTelemetry data model. The following describes each type of telemetry\ndata collected in an event table.\n|Logs |Logs record individual operations performed by code. Each log message is generated at\na discrete point during the execution of the code.\nSection Title: Observability in Snowflake apps ¶ > ... > Types of telemetry data ¶\nContent:\n**Instrumenting code** You can log from your code using libraries standard for the language you’re using, as listed in Logging from handler code .\n**Viewing data** You can view log messages for analysis\neither by querying the event table or looking at the visualizations provided in Snowsight.\nThe following image from Snowsight shows a list of collected log messages for a two-hour period in a single database.\n|\n| --- | --- |\n|Metrics |Metrics are measurements calculated over a time period. These values include CPU and memory measurements.\n**Instrumenting code** Snowflake emits metrics data automatically as your code executes, so you don’t need to instrument your code.\n**Viewing data** You can view metrics data for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\nSection Title: Observability in Snowflake apps ¶ > ... > Types of telemetry data ¶\nContent:\nThe following image from Snowsight shows changes in collected metrics data for the execution of a user-defined function.\n|\n|Traces |Traces show distributed events as data flows through a system. In a trace, you can see where time is spent as processing flows\nfrom component to component.\nYou can emit trace events—both within the default span Snowflake creates or from a custom span you create—using libraries\nstandard for the language you’re using, as listed in Logging from handler code .\n**Instrumenting code** You can emit trace events from your code using libraries standard for the language you’re using, as listed in Event tracing from handler code .\n**Viewing data** You can view trace events for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\nThe following image from Snowsight shows the spans resulting as a UDF executes.\n|\nSection Title: Observability in Snowflake apps ¶ > Telemetry best practices ¶\nContent:\nUse the following best practices to get the most out of observablity in Snowflake.\nSet up your environment to capture telemetry data before you need it\nOptimize procedures with query telemetry\nCache redundant DataFrame operations\nManage the amount of telemetry data received for UDFs\nOptimize user-defined functions with query telemetry\nSection Title: ... > Set up your environment to capture telemetry data before you need it ¶\nContent:\nYou can’t analyze data that you haven’t collected, so it’s best to start collecting telemetry data so you’ll have it when you need it.\nAs your deployment grows, your need to understand how your code is performing grows.\nUse the following best practices:\nSection Title: ... > Set up your environment to capture telemetry data before you need it ¶\nContent:\nEnable telemetry data collection for your Snowflake environment.To collect the data you’ll need, ensure that you have an active event table. To ensure you’re collecting telemetry data you want, set telemetry levels to\nuseful thresholds.At first, you’ll want to set these levels to ensure that you’re collecting data. For example, set log levels to at least WARN for any\nproduction or business critical jobs. Over time, you might adjust these levels to meet changing needs.Organize your production stored procedures, UDFs, and other objects under a database or schema so you can simply enable warning logs\nfor that database or schema. This saves the trouble of managing settings for separate objects.\nSection Title: ... > Set up your environment to capture telemetry data before you need it ¶\nContent:\nTo generate data for troubleshooting, add log statements or trace events to your production jobs.When you use standard logging libraries such as Java’s SLF4J or Python’s logging libraries, Snowflake routes logs from those packages to\nyour event table automatically.For tracing, you can use telemetry libraries included with Snowflake. To include in trace data parts of the handler’s processing that you want to measure, add custom spans to your stored procedure handler code.Along with the built-in spans from Snowflake objects, Snowflake represents custom spans you create in the trace diagram. With custom\nspans, you can capture data about arbitrary parts of your code’s processing to see how long those parts take to execute. You can also\nattach arbitrary metadata to custom spans to add descriptions to the data for troubleshooting and optimizing.\nSection Title: Observability in Snowflake apps ¶ > ... > Optimize procedures with query telemetry ¶\nContent:\nIn the Query Telemetry trace diagram, you’ll find data about all the spans emitted from a query.\nThe horizontal axis displays duration. A span that appears longer horizontally took longer to complete than a shorter\nspan.\nThe vertical axis displays the call hierarchy. In that hierarchy, any span that is directly under another span is a “child” of\nthe “parent” span above it.\nYou can use this diagram to find opportunities for optimization in stored procedures. Using what you see in the diagram as a starting\nplace, you can take steps to optimize your code.\nFor example, you might organize sequential operations so they execute in parallel using libraries like joblib. [Joblib](https://joblib.readthedocs.io/en/stable/) is a set of\ntools for adding pipelining to Python code. With it, you can more easily write parallel code.\nSection Title: Observability in Snowflake apps ¶ > ... > Cache redundant DataFrame operations ¶\nContent:\nWhen you have a chain of DataFrame operations that is used multiple times, you’ll see them in the trace diagram as a span for each\nDataFrame action. Depending on the query’s complexity, this span can be quite long.\nFor example, in the code below the same DataFrame chain is called in multiple contexts:\n```\ncount = session . table ( ... ) . select () . filter () . join () . count () \n\n if count > 0 : \n  session . table ( ... ) . select () . filter () . join () . write . save_as_table ( ... ) # same query as the count, this will execute again \n else : \n  session . table ( ... ) . select () . filter ( 'other criteria' ) . join () # nearly same query as the count\n```\nCopy\nUsing caching improves performance by caching the intermediate DataFrame as a temporary table, reducing redundant queries:\nSection Title: Observability in Snowflake apps ¶ > ... > Cache redundant DataFrame operations ¶\nContent:\n```\ncached_df = session . table ( ... ) . select () . filter () . join () . cache_result () \n count = cached_df . count () \n\n if count > 0 : \n  cached_df . write . save_as_table () # reuses the cached DF \n else : \n  cached_df\n```\nCopy\nSection Title: Observability in Snowflake apps ¶ > ... > Manage the amount of telemetry data received for UDFs ¶\nContent:\nWhen adding code to collect telemetry data with UDFs, remember that the UDF execution model can mean many more rows in the event table\nthan for a procedure.\nWhen a UDF is called on every input row, your handler code emits logging statements or span events for every row of the input dataset.\nFor example, a dataset of 10 million rows passed to a UDF would emit 10 million log entries.\nConsider using the following patterns when adding logs and span events to UDFs:\nSection Title: Observability in Snowflake apps ¶ > ... > Manage the amount of telemetry data received for UDFs ¶\nContent:\nInitially, use logging levels designed to reduce the number of entries recorded.Use DEBUG- or INFO-level logging statements and set the logging level to WARN in production. If an issue is found, you can lower the\nlogging level to DEBUG or INFO for the duration of the debugging session.\nUse try/catch blocks to isolate the code from which you want to emit logging data.Using try/catch can be useful to catch any unexpected UDF input, log it as a WARN-level log for awareness, and return a default value.\nUse condition statements to log only for scenarios that are meaningful to you.With if/else statements or other constraints, you can control the volume of logging output.\nSection Title: Observability in Snowflake apps ¶ > ... > Optimize user-defined functions with query telemetry ¶\nContent:\nWhen a UDF is called, Snowflake executes it in parallel by creating an instance of the handler code for each input row. You’ll see each of\nthese instances represented as its own span in a trace diagram.\nYou can use these spans to troubleshoot slow queries and find opportunities to improve performance. For example, you might see scenarios\nsuch as the following:\nOne or more instances of your UDF code might receive a row with data that is significantly larger or otherwise unlike the rest of your\ndata. When this happens, that instance might take much longer to complete, and therefore its span is much longer.\nDepending on your query’s input partitioning and preceding clauses, a minority of the instances might receive an outsized amount of\ninput data.\nSection Title: Observability in Snowflake apps ¶ > ... > Optimize user-defined functions with query telemetry ¶\nContent:\nThe following image shows a span for each row passed to a UDF, where one span’s longer duration suggests that the row might have larger\ndata than the others.\nSection Title: Observability in Snowflake apps ¶ > Alerts and notifications for time-sensitive response ¶\nContent:\nYou can use Snowflake alerts and notifications to have your system reveal what’s going on inside, then take action or send information\nabout system state. Unlike telemetry data, which you collect and analyze later, alerts and notifications are useful when you want an\nimmediate response to what’s happening in the system.\nSection Title: Observability in Snowflake apps ¶ > Alerts and notifications for time-sensitive response ¶\nContent:\nWith an alert , you can specify a condition, action, and schedule, then specify that the action should take\nplace when the condition and schedule details are met.For example, you might use an alert to monitor complex conditions that you specify in SQL. The most common action after an alert\ncondition is met is to send a notification. Snowflake supports sending notifications to email, cloud service provider queues, Slack,\nPagerDuty, and Microsoft Teams.\nWith a notification , you can use included stored procedures to send messages to\ndestinations such as email addresses , webhooks (for client tool integrations such as a chat tool), or to a queue hosted by a cloud service .\nSection Title: Observability in Snowflake apps ¶ > ... > Alerts and notifications best practices ¶\nContent:\nUse the following practices to improve observability by refining and increasing the amount of information you receive from the system.\nSection Title: Observability in Snowflake apps ¶ > ... > Alerts and notifications best practices ¶\nContent:\nAvoid duplicating event evaluation.You can avoid duplicating evaluation on events by accounting for the latency between the alert schedule and execution. To do this,\nspecify alert timestamps using SCHEDULED_TIME and LAST_SUCCESSFUL_SCHEDULED_TIME instead of using CURRENT_TIMESTAMP .For more information, see Specifying timestamps based on alert schedules . Enrich an alert action or notification with query results.You can check the results from the SQL statement specified by an alert condition. To obtain the query results, do the following:\nRetrieve the query ID for the alert condition’s SQL statement by calling GET_CONDITION_QUERY_UUID . Pass the query ID to RESULT_SCAN to obtain the query results. Log a result or take automated action in addition to sending a notification.You can specify that an alert action runs a task or logs a new row to a table whenever an alert\ncondition is met.\nSection Title: Observability in Snowflake apps ¶ > ... > Alerts and notifications best practices ¶\nContent:\nFor example, you might do this if you’ll take an action in Snowflake each time the alert condition is met.If you intend to perform a complex action after a condition is met, ensure that your warehouse is an appropriate size.\nSection Title: Observability in Snowflake apps ¶ > Tools for analysis and visualization ¶\nContent:\nYou can use the telemetry data collected in your event table with other tools that support the OpenTelemetry data model.\nThrough Snowflake support of OpenTelemetry, you can use APIs, SDKs, and other tools to instrument, generate, collect, and export telemetry\ndata. Using these tools, you can more thoroughly analyze software performance and behavior. Because a Snowflake event table uses this\nwidely-adopted standard, you might also be able to integrate your organization’s observability tools with event tables with little overhead.\nConsider integrating your external tools in one of the following ways:\nIf your observability tools can read from external sources, point them to the event table.\nIf your tools use a push model—in which telemetry data must be sent to the tool—consider using a stored procedure with external access to regularly read telemetry data from\nthe event table and emit it to your tool.\nSection Title: Observability in Snowflake apps ¶ > Tools for analysis and visualization ¶\nContent:\nThe following lists tools you might integrate with Snowflake event tables:\n[Snowflake integration for Datadog](https://docs.datadoghq.com/integrations/snowflake_web/)\nSnowflake integration for Grafana dashboardFor an introduction to using Grafana with Snowflake, see [How to monitor Snowflake with Grafana Cloud](https://grafana.com/blog/2023/05/24/how-to-monitor-snowflake-with-grafana-cloud/) .\n[Snowflake data source for Grafana](https://grafana.com/docs/plugins/grafana-snowflake-datasource/latest/)\n[Snowflake integration for Grafana Cloud](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/integrations/integration-reference/integration-snowflake/)\n[Observe for Snowflake](https://app.snowflake.com/marketplace/listing/GZTYZY3AR0U/observe-inc-observe-for-snowflake) , Observe’s native app for observability\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\nSection Title: Observability in Snowflake apps ¶ > Tools for analysis and visualization ¶\nContent:\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)"],"full_content":"Developer Observability\n\n# Observability in Snowflake apps ¶\n\nThrough observability built into Snowflake, you can ensure that your applications are running as efficiently as possible.\nUsing the practices and features described in this topic, you can make the most of observability features that show you where you\ncan improve your code.\n\n## What is observability? ¶\n\nIn an observable system, you can understand what’s happening internally through external evidence generated by the system—evidence\nthat includes telemetry data, alerts, and notifications.\n\nThrough the evidence of internal functioning it provides, observability makes it easier for you to troubleshoot hard-to-understand behaviors\non a production system. This is especially true in a distributed system, where evidence collected from observation provides a view of\nbehavior across multiple components. Rather than disrupting a production environment to diagnose issues, you can analyze the collected\ndata from it.\n\nWith an observable system, you can start to answer questions such as the following:\n\n* How well is the system performing?\n* Where is there latency and what’s causing it?\n* Why is a particular component or process not working as it should?\n* What improvements can be made?\n\n## Observability in Snowflake ¶\n\nSnowflake supports a model that provides built-in observable data while also giving you ways to add more instrumentation where you need it.\nWhile Snowflake provides support for telemetry data such as logs, metrics, and traces (which are typical of observability), it also\nincludes other features you can use to keep track of system usage and performance.\n\nThe following lists features you can use to receive and analyze system performance and usage.\n\n|Collected telemetry data |As your application generates logs, metrics, and traces, Snowflake collects that telemetry data in an event table. Using\nSnowsight, you can explore the data, looking for patterns.\n\nYou can emit custom telemetry into the event table to provide contextual, domain-specific information to expedite debugging. |\n| --- | --- |\n|History Tables |Use the following views and their associated tables to monitor all usage in your account.\n\n* Query History\n* Copy History\n* Tasks |\n|Alerts and notifications |Alerts allow for customizable triggering conditions, actions, and a schedule, in combination with notification integrations for proactive monitoring. |\n|Extensibility with third-party tools |The Snowflake event table adopts [OpenTelemetry](https://opentelemetry.io/docs/) standards, so your\nSnowflake telemetry can easily be consumed by other ecosystem tools. |\n\n## Telemetry data collected for analysis ¶\n\nAs code in your application executes, you can have Snowflake collect data from the code that tells you about the application’s internal\nstate. Using this telemetry data—collected in a Snowflake event table (your account has one by default )—you can look for bottlenecks and other opportunities to optimize.\n\nTelemetry data must be emitted as your code executes. Snowflake emits some of this data on your code’s behalf without\nyou needing to instrument your code. You can use also APIs included with Snowflake to emit telemetry data from specific parts of your code.\n\nAs described below, you can analyze the collected data by querying the event table or using the visualizations that capture the data\nin Snowsight.\n\n### Types of telemetry data ¶\n\nTo ensure that the telemetry data you collect is broadly useful, Snowflake telemetry is built on the standard [OpenTelemetry](https://opentelemetry.io/docs/) (sometimes called OTel) framework, an incubating project of the Cloud Native Compute Foundation. Through this framework (and APIs and\ntools designed for it), you can reuse collected data with tools besides Snowflake .\nThrough OpenTelemetry, you can instrument application code to add observability where you want it.\n\nSnowflake event tables collect log, span, and metrics data in the OpenTelemetry data model. The following describes each type of telemetry\ndata collected in an event table.\n\n|Logs |Logs record individual operations performed by code. Each log message is generated at\na discrete point during the execution of the code.\n\n**Instrumenting code** You can log from your code using libraries standard for the language you’re using, as listed in Logging from handler code .\n\n**Viewing data** You can view log messages for analysis\neither by querying the event table or looking at the visualizations provided in Snowsight.\n\nThe following image from Snowsight shows a list of collected log messages for a two-hour period in a single database.\n\n |\n| --- | --- |\n|Metrics |Metrics are measurements calculated over a time period. These values include CPU and memory measurements.\n\n**Instrumenting code** Snowflake emits metrics data automatically as your code executes, so you don’t need to instrument your code.\n\n**Viewing data** You can view metrics data for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\n\nThe following image from Snowsight shows changes in collected metrics data for the execution of a user-defined function.\n\n |\n|Traces |Traces show distributed events as data flows through a system. In a trace, you can see where time is spent as processing flows\nfrom component to component.\n\nYou can emit trace events—both within the default span Snowflake creates or from a custom span you create—using libraries\nstandard for the language you’re using, as listed in Logging from handler code .\n\n**Instrumenting code** You can emit trace events from your code using libraries standard for the language you’re using, as listed in Event tracing from handler code .\n\n**Viewing data** You can view trace events for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\n\nThe following image from Snowsight shows the spans resulting as a UDF executes.\n\n |\n\n## Telemetry best practices ¶\n\nUse the following best practices to get the most out of observablity in Snowflake.\n\n* Set up your environment to capture telemetry data before you need it\n* Optimize procedures with query telemetry\n* Cache redundant DataFrame operations\n* Manage the amount of telemetry data received for UDFs\n* Optimize user-defined functions with query telemetry\n\n### Set up your environment to capture telemetry data before you need it ¶\n\nYou can’t analyze data that you haven’t collected, so it’s best to start collecting telemetry data so you’ll have it when you need it.\nAs your deployment grows, your need to understand how your code is performing grows.\n\nUse the following best practices:\n\n* Enable telemetry data collection for your Snowflake environment.\n  \n  To collect the data you’ll need, ensure that you have an active event table.\n* To ensure you’re collecting telemetry data you want, set telemetry levels to\n  useful thresholds.\n  \n  At first, you’ll want to set these levels to ensure that you’re collecting data. For example, set log levels to at least WARN for any\n  production or business critical jobs. Over time, you might adjust these levels to meet changing needs.\n  \n  Organize your production stored procedures, UDFs, and other objects under a database or schema so you can simply enable warning logs\n  for that database or schema. This saves the trouble of managing settings for separate objects.\n* To generate data for troubleshooting, add log statements or trace events to your production jobs.\n  \n  When you use standard logging libraries such as Java’s SLF4J or Python’s logging libraries, Snowflake routes logs from those packages to\n  your event table automatically.\n  \n  For tracing, you can use telemetry libraries included with Snowflake.\n* To include in trace data parts of the handler’s processing that you want to measure, add custom spans to your stored procedure handler code.\n  \n  Along with the built-in spans from Snowflake objects, Snowflake represents custom spans you create in the trace diagram. With custom\n  spans, you can capture data about arbitrary parts of your code’s processing to see how long those parts take to execute. You can also\n  attach arbitrary metadata to custom spans to add descriptions to the data for troubleshooting and optimizing.\n\n### Optimize procedures with query telemetry ¶\n\nIn the Query Telemetry trace diagram, you’ll find data about all the spans emitted from a query.\n\n* The horizontal axis displays duration. A span that appears longer horizontally took longer to complete than a shorter\n  span.\n* The vertical axis displays the call hierarchy. In that hierarchy, any span that is directly under another span is a “child” of\n  the “parent” span above it.\n\nYou can use this diagram to find opportunities for optimization in stored procedures. Using what you see in the diagram as a starting\nplace, you can take steps to optimize your code.\n\nFor example, you might organize sequential operations so they execute in parallel using libraries like joblib. [Joblib](https://joblib.readthedocs.io/en/stable/) is a set of\ntools for adding pipelining to Python code. With it, you can more easily write parallel code.\n\n### Cache redundant DataFrame operations ¶\n\nWhen you have a chain of DataFrame operations that is used multiple times, you’ll see them in the trace diagram as a span for each\nDataFrame action. Depending on the query’s complexity, this span can be quite long.\n\nFor example, in the code below the same DataFrame chain is called in multiple contexts:\n\n```\ncount = session . table ( ... ) . select () . filter () . join () . count () \n\n if count > 0 : \n  session . table ( ... ) . select () . filter () . join () . write . save_as_table ( ... ) # same query as the count, this will execute again \n else : \n  session . table ( ... ) . select () . filter ( 'other criteria' ) . join () # nearly same query as the count\n```\n\nCopy\n\nUsing caching improves performance by caching the intermediate DataFrame as a temporary table, reducing redundant queries:\n\n```\ncached_df = session . table ( ... ) . select () . filter () . join () . cache_result () \n count = cached_df . count () \n\n if count > 0 : \n  cached_df . write . save_as_table () # reuses the cached DF \n else : \n  cached_df\n```\n\nCopy\n\n### Manage the amount of telemetry data received for UDFs ¶\n\nWhen adding code to collect telemetry data with UDFs, remember that the UDF execution model can mean many more rows in the event table\nthan for a procedure.\n\nWhen a UDF is called on every input row, your handler code emits logging statements or span events for every row of the input dataset.\nFor example, a dataset of 10 million rows passed to a UDF would emit 10 million log entries.\n\nConsider using the following patterns when adding logs and span events to UDFs:\n\n* Initially, use logging levels designed to reduce the number of entries recorded.\n  \n  Use DEBUG- or INFO-level logging statements and set the logging level to WARN in production. If an issue is found, you can lower the\n  logging level to DEBUG or INFO for the duration of the debugging session.\n* Use try/catch blocks to isolate the code from which you want to emit logging data.\n  \n  Using try/catch can be useful to catch any unexpected UDF input, log it as a WARN-level log for awareness, and return a default value.\n* Use condition statements to log only for scenarios that are meaningful to you.\n  \n  With if/else statements or other constraints, you can control the volume of logging output.\n\n### Optimize user-defined functions with query telemetry ¶\n\nWhen a UDF is called, Snowflake executes it in parallel by creating an instance of the handler code for each input row. You’ll see each of\nthese instances represented as its own span in a trace diagram.\n\nYou can use these spans to troubleshoot slow queries and find opportunities to improve performance. For example, you might see scenarios\nsuch as the following:\n\n* One or more instances of your UDF code might receive a row with data that is significantly larger or otherwise unlike the rest of your\n  data. When this happens, that instance might take much longer to complete, and therefore its span is much longer.\n* Depending on your query’s input partitioning and preceding clauses, a minority of the instances might receive an outsized amount of\n  input data.\n\nThe following image shows a span for each row passed to a UDF, where one span’s longer duration suggests that the row might have larger\ndata than the others.\n\n## Alerts and notifications for time-sensitive response ¶\n\nYou can use Snowflake alerts and notifications to have your system reveal what’s going on inside, then take action or send information\nabout system state. Unlike telemetry data, which you collect and analyze later, alerts and notifications are useful when you want an\nimmediate response to what’s happening in the system.\n\n* With an alert , you can specify a condition, action, and schedule, then specify that the action should take\n  place when the condition and schedule details are met.\n  \n  For example, you might use an alert to monitor complex conditions that you specify in SQL. The most common action after an alert\n  condition is met is to send a notification. Snowflake supports sending notifications to email, cloud service provider queues, Slack,\n  PagerDuty, and Microsoft Teams.\n* With a notification , you can use included stored procedures to send messages to\n  destinations such as email addresses , webhooks (for client tool integrations such as a chat tool), or to a queue hosted by a cloud service .\n\n### Alerts and notifications best practices ¶\n\nUse the following practices to improve observability by refining and increasing the amount of information you receive from the system.\n\n* Avoid duplicating event evaluation.\n  \n  You can avoid duplicating evaluation on events by accounting for the latency between the alert schedule and execution. To do this,\n  specify alert timestamps using SCHEDULED\\_TIME and LAST\\_SUCCESSFUL\\_SCHEDULED\\_TIME instead of using CURRENT\\_TIMESTAMP .\n  \n  For more information, see Specifying timestamps based on alert schedules .\n* Enrich an alert action or notification with query results.\n  \n  You can check the results from the SQL statement specified by an alert condition. To obtain the query results, do the following:\n  \n    1. Retrieve the query ID for the alert condition’s SQL statement by calling GET\\_CONDITION\\_QUERY\\_UUID .\n    2. Pass the query ID to RESULT\\_SCAN to obtain the query results.\n* Log a result or take automated action in addition to sending a notification.\n  \n  You can specify that an alert action runs a task or logs a new row to a table whenever an alert\n  condition is met. For example, you might do this if you’ll take an action in Snowflake each time the alert condition is met.\n  \n  If you intend to perform a complex action after a condition is met, ensure that your warehouse is an appropriate size.\n\n## Tools for analysis and visualization ¶\n\nYou can use the telemetry data collected in your event table with other tools that support the OpenTelemetry data model.\n\nThrough Snowflake support of OpenTelemetry, you can use APIs, SDKs, and other tools to instrument, generate, collect, and export telemetry\ndata. Using these tools, you can more thoroughly analyze software performance and behavior. Because a Snowflake event table uses this\nwidely-adopted standard, you might also be able to integrate your organization’s observability tools with event tables with little overhead.\n\nConsider integrating your external tools in one of the following ways:\n\n* If your observability tools can read from external sources, point them to the event table.\n* If your tools use a push model—in which telemetry data must be sent to the tool—consider using a stored procedure with external access to regularly read telemetry data from\n  the event table and emit it to your tool.\n\nThe following lists tools you might integrate with Snowflake event tables:\n\n* [Snowflake integration for Datadog](https://docs.datadoghq.com/integrations/snowflake_web/)\n* Snowflake integration for Grafana dashboard\n  \n    + [Snowflake data source for Grafana](https://grafana.com/docs/plugins/grafana-snowflake-datasource/latest/)\n    + [Snowflake integration for Grafana Cloud](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/integrations/integration-reference/integration-snowflake/)\n  \n  For an introduction to using Grafana with Snowflake, see [How to monitor Snowflake with Grafana Cloud](https://grafana.com/blog/2023/05/24/how-to-monitor-snowflake-with-grafana-cloud/) .\n* [Observe for Snowflake](https://app.snowflake.com/marketplace/listing/GZTYZY3AR0U/observe-inc-observe-for-snowflake) , Observe’s native app for observability\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)"},{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/monitoring","title":"Use monitoring for an app | Snowflake Documentation","publish_date":null,"excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nEN\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nBuilders\nSnowflake DevOps\nObservability\nSnowpark Library\nSnowpark API\nSpark workloads on Snowflake\nMachine Learning\nSnowflake ML\nSnowpark Code Execution Environments\nSnowpark Container Services\nFunctions and procedures\nLogging, Tracing, and Metrics\nSnowflake APIs\nSnowflake Python APIs\nSnowflake REST APIs\nSQL API\nApps\nStreamlit in Snowflake\nAbout Streamlit in Snowflake\nGetting started\nDeploy a sample app\nCreate and deploy Streamlit apps using Snowsight\nCreate and deploy Streamlit apps using SQL\nCreate and deploy Streamlit apps using Snowflake CLI\nStreamlit object management\nBilling considerations\nSecurity considerations\nPrivilege requirements\nUnderstanding owner's rights\nPrivateLink\nLogging and tracing\nApp development\nRuntime environments\nDependency management\nFile organization\nSecrets and configuration\nEditing your app\nMigrations and upgrades\nIdentify your app type\nMigrate to a container runtime\nMigrate from ROOT_LOCATION\nFeatures\nExternal access\nGit integration\nRow access\npolicies\nSharing Streamlit in Snowflake apps\nSleep timer\nLimitations and library changes\nTroubleshooting Streamlit in Snowflake\n[Streamlit open-source library documentation](https://docs.streamlit.io/)\nSnowflake Native App Framework\nSnowflake Declarative Sharing\nSnowflake Native SDK for Connectors\nExternal Integration\nExternal Functions\nKafka and Spark Connectors\nSnowflake Scripting\nSnowflake Scripting Developer Guide\nTools\nSnowflake CLI\nGit\nDrivers\nOverview\nConsiderations when drivers reuse sessions\nScala versions\nReference\nAPI Reference\nDeveloper Snowflake Native App Framework Monitoring\nSection Title: Use monitoring for an app ¶\nContent:\nFeature — Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nThis topic describes how providers can monitor consumer app health for a Snowflake Native App.\nSection Title: Use monitoring for an app ¶ > Monitor consumer application health ¶\nContent:\nYour application can report its health status to Snowflake, which allows you to\nmonitor the health of consumer instances of your app.\nTo report health status, your app uses the system-defined `SYSTEM$REPORT_HEALTH_STATUS(VARCHAR)` function, passing in the health status\nas an enum value:\n`OK` : The consumer instance is healthy.\n`FAILED` : The consumer instance is in an error state.\n`PAUSED` : The consumer manually paused the app.\nYou can use the `LAST_HEALTH_STATUS` and `LAST_HEALTH_STATUS_UPDATED_ON` fields\nof the APPLICATION_STATE view to monitor the health of consumer instances of your app. The `LAST_HEALTH_STATUS` field has the most recent value passed in by the app running in the consumer account.\nThe following code sample demonstrates using the APPLICATION_STATE view\nto retrieve the health status of all consumer instances of your app:\nSection Title: Use monitoring for an app ¶ > Monitor consumer application health ¶\nContent:\n```\nSELECT \n    CONSUMER_ORGANIZATION_NAME , \n    CONSUMER_ACCOUNT_NAME , \n    LAST_HEALTH_STATUS , \n    LAST_HEALTH_STATUS_UPDATE_TIME \n FROM \n    SNOWFLAKE . ACCOUNT_USAGE . APPLICATION_STATE \n WHERE \n    PROVIDER_ORG_NAME = '<your_provider_org_name>' \n    AND APPLICATION_NAME = '<your_app_name>' \n ORDER BY \n    LAST_HEALTH_STATUS_UPDATE_TIME DESC ;\n```\nCopy\nThe preceding query may return results similar to the following:\n```\nCONSUMER_ORG_NAME    CONSUMER_ACCOUNT_NAME    LAST_HEALTH_STATUS    LAST_HEALTH_STATUS_UPDATE_TIME \n ------------------   ---------------------    ------------------    ------------------------------- \n consumer_org_1       consumer_account_1       OK                    2024-01-15 10:30:00.000 \n consumer_org_2       consumer_account_2       FAILED                2024-01-15 09:45:00.000 \n consumer_org_3       consumer_account_3       PAUSED                2024-01-14 16:20:00.000\n```\nSection Title: Use monitoring for an app ¶ > Monitor consumer application health ¶\nContent:\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nMonitor consumer application health\nRelated content\nLogging, tracing, and metrics\nSection Title: Use monitoring for an app ¶ > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\nSection Title: Use monitoring for an app ¶ > Privacy Preference Center > Your Privacy\nContent:\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nSection Title: Use monitoring for an app ¶ > Privacy Preference Center > Your Privacy > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Details‎\nSection Title: Use monitoring for an app ¶ > Privacy Preference Center > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details‎\nSection Title: Use monitoring for an app ¶ > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details‎\nSection Title: Use monitoring for an app ¶ > Privacy Preference Center > Your Privacy > Targeting Cookies\nContent:\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nCookies Details‎\nSection Title: Use monitoring for an app ¶ > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"],"full_content":"[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nEN\n\nEnglish\n\nFrançais\n\nDeutsch\n\n日本語\n\n한국어\n\nPortuguês\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\n1. Overview\n2. Builders\n3. Snowflake DevOps\n4. Observability\n5. Snowpark Library\n6. Snowpark API\n7. Spark workloads on Snowflake\n8. Machine Learning\n9. Snowflake ML\n10. Snowpark Code Execution Environments\n11. Snowpark Container Services\n12. Functions and procedures\n13. Logging, Tracing, and Metrics\n14. Snowflake APIs\n15. Snowflake Python APIs\n16. Snowflake REST APIs\n17. SQL API\n18. Apps\n19. Streamlit in Snowflake\n    \n    + About Streamlit in Snowflake\n    + Getting started\n          \n        - Deploy a sample app\n        - Create and deploy Streamlit apps using Snowsight\n        - Create and deploy Streamlit apps using SQL\n        - Create and deploy Streamlit apps using Snowflake CLI\n    + Streamlit object management\n          \n        - Billing considerations\n        - Security considerations\n        - Privilege requirements\n        - Understanding owner's rights\n        - PrivateLink\n        - Logging and tracing\n    + App development\n          \n        - Runtime environments\n        - Dependency management\n        - File organization\n        - Secrets and configuration\n        - Editing your app\n    + Migrations and upgrades\n          \n        - Identify your app type\n        - Migrate to a container runtime\n        - Migrate from ROOT\\_LOCATION\n    + Features\n          \n        - External access\n        - Git integration\n        - Row access policies\n        - Sharing Streamlit in Snowflake apps\n        - Sleep timer\n    + Limitations and library changes\n    + Troubleshooting Streamlit in Snowflake\n    + [Streamlit open-source library documentation](https://docs.streamlit.io/)\n20. Snowflake Native App Framework\n    \n        - Get started\n        - Tutorial 1: Create a basic Snowflake Native App\n        - Tutorial 2: Create an app with containers\n        - Tutorial 3: Upgrade an app with containers\n        - Snowflake Native App workflow\n        - App with containers development workflow\n        - Understand limitations\n        - Costs associated with apps with containers\n        - Protect provider intellectual property\n        - Set up your app\n        - Install the Snowflake CLI\n        - Create the application package\n        - Create the manifest file\n        - Create the setup script\n        - Manifest file reference\n        - Add features to your app\n        - Add application logic\n        - Add shared data content\n        - Use owners rights and restricted callers rights in an app\n        - Add frontend experience with Streamlit\n        - Use application configuration\n        - Use inter-app communication\n        - Use callbacks\n        - Use Snowflake ML in a Snowflake Native App\n        - Monitoring\n        - Configure logging and event tracing for an app\n        - Add billable events to an app\n        - Access objects in a consumer account\n        - Python Permission SDK reference\n        - Add a compute pool to an app\n        - Add services to an app\n        - Add job services to an app\n        - Configure external access for services in an app with containers\n        - Set up the containers and services managed by an app\n        - Use versioned schemas to manage app state\n        - Test your app\n        - Install and test an app locally\n        - Manage versions and patches of your app\n        - About release channels and app versions\n        - Publish your app using release channels\n        - Publish your app to consumers\n        - Security requirements and guidelines\n        - Specify the resources required by an app\n        - Create a listing for an app\n        - Update and upgrade an app\n        - Guidelines for developing a new version of an app\n        - Upgrade an app using release channels\n        - Consumer-controlled maintenance policies\n        - Update an app from an application package (Legacy)\")\n21. Snowflake Declarative Sharing\n22. Snowflake Native SDK for Connectors\n23. External Integration\n24. External Functions\n25. Kafka and Spark Connectors\n26. Snowflake Scripting\n27. Snowflake Scripting Developer Guide\n28. Tools\n29. Snowflake CLI\n30. Git\n31. Drivers\n32. Overview\n33. Considerations when drivers reuse sessions\n34. Scala versions\n35. Reference\n36. API Reference\n\nDeveloper Snowflake Native App Framework Monitoring\n\n# Use monitoring for an app ¶\n\n Feature — Generally Available\n\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\n\nThis topic describes how providers can monitor consumer app health for a Snowflake Native App.\n\n## Monitor consumer application health ¶\n\nYour application can report its health status to Snowflake, which allows you to\nmonitor the health of consumer instances of your app.\n\nTo report health status, your app uses the system-defined `SYSTEM$REPORT_HEALTH_STATUS(VARCHAR)` function, passing in the health status\nas an enum value:\n\n* `OK` : The consumer instance is healthy.\n* `FAILED` : The consumer instance is in an error state.\n* `PAUSED` : The consumer manually paused the app.\n\nYou can use the `LAST_HEALTH_STATUS` and `LAST_HEALTH_STATUS_UPDATED_ON` fields\nof the APPLICATION\\_STATE view to monitor the health of consumer instances of your app. The `LAST_HEALTH_STATUS` field has the most recent value passed in by the app running in the consumer account.\n\nThe following code sample demonstrates using the APPLICATION\\_STATE view\nto retrieve the health status of all consumer instances of your app:\n\n```\nSELECT \n    CONSUMER_ORGANIZATION_NAME , \n    CONSUMER_ACCOUNT_NAME , \n    LAST_HEALTH_STATUS , \n    LAST_HEALTH_STATUS_UPDATE_TIME \n FROM \n    SNOWFLAKE . ACCOUNT_USAGE . APPLICATION_STATE \n WHERE \n    PROVIDER_ORG_NAME = '<your_provider_org_name>' \n    AND APPLICATION_NAME = '<your_app_name>' \n ORDER BY \n    LAST_HEALTH_STATUS_UPDATE_TIME DESC ;\n```\n\nCopy\n\nThe preceding query may return results similar to the following:\n\n```\nCONSUMER_ORG_NAME    CONSUMER_ACCOUNT_NAME    LAST_HEALTH_STATUS    LAST_HEALTH_STATUS_UPDATE_TIME \n ------------------   ---------------------    ------------------    ------------------------------- \n consumer_org_1       consumer_account_1       OK                    2024-01-15 10:30:00.000 \n consumer_org_2       consumer_account_2       FAILED                2024-01-15 09:45:00.000 \n consumer_org_3       consumer_account_3       PAUSED                2024-01-14 16:20:00.000\n```\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Monitor consumer application health\n\nRelated content\n\n1. Logging, tracing, and metrics\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details‎\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details‎\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details‎\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details‎\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"}],"errors":[],"warnings":null,"usage":[{"name":"sku_extract_excerpts","count":3},{"name":"sku_extract_full","count":3}]}

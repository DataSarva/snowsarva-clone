{
  "extract_id": "extract_c061f2af27664a388a8629bbccab4bb9",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/developer-guide/snowpark-container-services/monitoring-services",
      "title": "Snowpark Container Services: Monitoring Services | Snowflake Documentation",
      "publish_date": "2023-10-23",
      "excerpts": [
        "Developer Snowpark Container Services Monitoring Services\n\n# Snowpark Container Services: Monitoring Services \u00b6\n\n Feature \u2014 Generally Available\n\nSnowpark Container Services is available to accounts in AWS, Microsoft Azure, and Google Cloud Platform commercial regions , with some exceptions. For more information, see Available regions and considerations .\n\nSnowflake provides a variety of mechanisms for monitoring services, jobs, and compute pools. The following sections describe the details.\n\nA user needs the appropriate privileges on services, jobs, and compute pools to access the monitoring data. For more information, see Privileges needed to perform operations on the service and Compute pool privileges .\n\n## Publishing and accessing container logs \u00b6\n\nSnowflake automatically collects and stores container logs \u2014 whatever your application container emits to standard\noutput and standard error \u2014 to an event table for later analysis, unless you choose to opt out.\nEnsure that your code outputs useful information that can help with debugging your service or conducting retrospective analysis\nof your services and jobs.\n\nUse a combination of the following settings to control which container logs are sent to the event table:\n\n* In the service specification, use the logExporter field\n  to indicate which stream (stdout/stderr) should be sent to the event table.\n* In CREATE SERVICE or ALTER SERVICE command, specify the LOG\\_LEVEL parameter to indicate the severity at which logs are collected.\n\nWhen a service container is running, you can also retrieve the container log, without saving the logs to the event table by using the SYSTEM$GET\\_SERVICE\\_LOGS system function. This process is most useful during development and testing\nof your service code.\n\n### Publishing container logs \u00b6\n\nYour application containers can publish structured or unstructured logs:\n\n* **Unstructured logs:** Text that can\u2019t be parsed as JSON\n  that your application containers emit\n  to standard output and standard error. Snowflake persists these strings to the value\n  column in the event table.\n* **Structured logs:** These are JSON text that application containers emit to standard\n  output and standard error. Snowflake extracts JSON fields and saves them to specific\n  columns in the event table. Then, you can query the event table and apply filters\n  when exploring events.\n  \n  The following JSON structure shows supported fields that Snowflake stores in event\n  table columns. If your application emits JSON that includes unsupported fields,\n  as shown in the following example, Snowflake ignores those fields.\n  \n  ```\n  { \n    \"severity_text\": \"DEBUG\" ,    # \" <DEBUG, INFO, WARN, ERROR, FATAL> \",\n    \"body\": \"hello from SPCS\" ,   # <body text> \",\n    \"attributes\": { \n      \"attr_key1\": \"attr_value1\" ,\n      \"attr_key2\": { \"nested_key2\": \"nested_value2\" } } ,\n    } ,\n    \"scope\": { \"name\": \"val1\" } ,\n    \"timestamp\": \"2025-01-01T12:34:56.789Z\", # Format: RFC 3339\n    # Unsupported fields are dropped.\n    \"another_field_key1\": \"another_field_val1\" ,\n    \"another_field_key2\": \"another_field_val2\" ,\n   }\n  ```\n  \n  Copy\n  \n  The following table shows the JSON-log field names and the corresponding event-table\n  column names where Snowflake stores their values. For description of the log fields,\n  see the Event table columns descriptions.\n  \n  |JSON field |Event table column |Comment |\n  | --- | --- | --- |\n  |severity\\_text |RECORD |Snowflake saves both `severity_text` and the Snowflake-assigned `severity_number` as Object fields in this column. |\n  |attributes |RECORD\\_ATTRIBUTES . |The fields from structured log are copied as Object fields in this column. |\n  |scope |SCOPE |The fields from the structured log are copied as Object fields in this column. |\n  |timestamp |TIMESTAMP | |\n  \n  \n  The following example shows a container log stored in an event table:\n  \n  ```\n  +----------------------+--------------------------+-------------+----------------------------+-------------------+---------------------------------------------------------------------------------------------------+ \n   |        VALUE         |       TIMESTAMP          | RECORD_TYPE |           RECORD           |       SCOPE       |            RECORD_ATTRIBUTES           |                   RESOURCE_ATTRIBUTES                    | \n   +----------------------+--------------------------+-------------+----------------------------+-------------------+---------------------------------------------------------------------------------------------------+ \n   | \"hello from SPCS\"    | 2025-01-01T12:34:56.789Z | LOG         | {                          | {                 | {                                      | {                                                        | \n   |                      |                          |             |   \"severity_number\": 5,    |   \"name1\": \"val1\" |   \"attr_key1\": \"attr_value1\",          |   \"snow.account.name\": \"****\",                           | \n   |                      |                          |             |   \"severity_text\": \"DEBUG\" | }                 |   \"attr_key2\": {                       |   \"snow.compute_pool.id\": \"****\",                        | \n   |                      |                          |             | }                          |                   |     \"nested_key2\": \"nested_value2\"     |   \"snow.compute_pool.name\": \"MYPO****\",                  | \n   |                      |                          |             |                            |                   |   }                                    |   \"snow.compute_pool.node.id\": \"****\",                   | \n   |                      |                          |             |                            |                   | }                                      |   \"snow.compute_pool.node.instance_family\": \"CPU_****\",  | \n   |                      |                          |             |                            |                   |                                        |   \"snow.database.id\": \"****\",                            | \n   |                      |                          |             |                            |                   |                                        |   \"snow.database.name\": \"MYDB****\",                      | \n   |                      |                          |             |                            |                   |                                        |   \"snow.query.id\": \"****\",                               | \n   |                      |                          |             |                            |                   |                                        |   \"snow.schema.id\": \"****\",                              | \n   |                      |                          |             |                            |                   |                                        |   \"snow.schema.name\": \"MYSC****\",                        | \n   |                      |                          |             |                            |                   |                                        |   \"snow.service.container.instance\": \"0\",                | \n   |                      |                          |             |                            |                   |                                        |   \"snow.service.container.name\": \"main****\",             | \n   |                      |                          |             |                            |                   |                                        |   \"snow.service.container.run.id\": \"****\",               | \n   |                      |                          |             |                            |                   |                                        |   \"snow.service.id\": \"****\",                             | \n   |                      |                          |             |                            |                   |                                        |   \"snow.service.instance\": \"0\",                          | \n   |                      |                          |             |                            |                   |                                        |   \"snow.service.name\": \"TEST****\",                       | \n   |                      |                          |             |                            |                   |                                        |   \"snow.service.type\": \"Service\"                         | \n   |                      |                          |             |                            |                   |                                        | }                                                        | \n   +----------------------+--------------------------+-------------+----------------------------+-------------------+----------------------------------------+----------------------------------------------------------+\n  ```\n  \n  If you use Python for your application code, you can use\n  the [Snowflake-provided log formatter](https://pypi.org/project/snowflake-telemetry-python/) ( `SnowflakeLogFormatter` ) to emit structured logs, as shown in the following example:\n  \n  ```\n  from snowflake.telemetry.logs import SnowflakeLogFormatter \n  \n   handler = logging . StreamHandler ( stream = get_stream ( arguments . stream )) \n   handler . setFormatter ( SnowflakeLogFormatter ()) \n   logger . addHandler ( handler ) \n   logger . setLevel ( logging . DEBUG ) # info by default \n  \n   # Emit logs with record attributes (`extra` argument) \n   logger . warning ( \"warning log record with attributes\" , extra = { \"custom\" : True }) \n   logger . debug ( \"debug log with nested attributes\" , extra = { \"nested\" : { \"key1\" : [ 1 , 2 , 3 ]}})\n  ```\n  \n  Copy\n\n### Accessing container logs \u00b6\n\nYou can currently access container logs by using the following options:\n\n* **Use the service helper method:** We recommend calling the Using the <service-name>!SPCS\\_GET\\_LOGS function to retrieve container logs of the specified service or job, collected by Snowflake in the event table.\n* **Use the event table directly:** If you have full access to the event table, you can query the event table directly to get historical logs.\n* **Use the SYSTEM$GET\\_SERVICE\\_LOGS system function:** Call SYSTEM$GET\\_SERVICE\\_LOGS to retrieve the logs of the currently running service or job container.\n\n### Using the <service-name>!SPCS\\_ GET\\_ LOGS function \u00b6\n\nThe <service\\_name>!SPCS\\_GET\\_LOGS table function returns logs from the containers of the specified job. These logs are collected by Snowflake and are stored in the event table.\n\nThe following list explains the advantages of using this table function:\n\n* You can retrieve logs for a specific service.\n* You can retrieve logs within a specified time range.\n* The caller doesn\u2019t need access to the entire events table,\n  which can be beneficial for customers with strict information-security requirements. If the current session includes the service owner role, then they have access to these logs.\n\nFor `_service_name_` , you specify the name of the service. The function returns logs Snowflake collected from containers of that service (see Publishing and accessing container logs ).\n\nYou can optionally specify a date range. By default, the function returns one-day logs. For example, the query retrieved logs that Snowflake collected from containers of the `my_test_job` job over the past day, which is the default.\n\n> ```\n> SELECT * FROM TABLE ( mydb . myschema . my_test_job ! SPCS_GET_LOGS ());\n> ```\n> \n> Copy\n> \n> Example output:\n> \n> ```\n> +-------------------------+-------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+ \n>  | TIMESTAMP               | INSTANCE_ID | CONTAINER_NAME | LOG                                                                                                                                                                 | RECORD_ATTRIBUTES          | \n>  |-------------------------+-------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------| \n>  | 2025-06-26 00:23:40.281 |           0 | main           | job-tutorial - INFO - Job finished                                                                                                                                  | {                          | \n>  |                         |             |                |                                                                                                                                                                     |   \"log.iostream\": \"stdout\" | \n>  |                         |             |                |                                                                                                                                                                     | }                          | \n>  | 2025-06-26 00:23:38.787 |           0 | main           | job-tutorial - INFO - Executing query [select current_time() as time,'hello'] and writing result to table [results]                                                 | {                          | \n>  |                         |             |                |                                                                                                                                                                     |   \"log.iostream\": \"stdout\" | \n>  |                         |             |                |                                                                                                                                                                     | }                          | \n>  | 2025-06-26 00:23:38.787 |           0 | main           | job-tutorial - INFO - Connection succeeded. Current session context: database=\"TUTORIAL_DB\", schema=\"DATA_SCHEMA\", warehouse=\"TUTORIAL_WAREHOUSE\", role=\"TEST_ROLE\" | {                          | \n>  |                         |             |                |                                                                                                                                                                     |   \"log.iostream\": \"stdout\" | \n>  |                         |             |                |                                                                                                                                                                     | }                          | \n>  | 2025-06-26 00:23:36.852 |           0 | main           | job-tutorial - INFO - Job started                                                                                                                                   | {                          | \n>  |                         |             |                |                                                                                                                                                                     |   \"log.iostream\": \"stdout\" | \n>  |                         |             |                |                                                                                                                                                                     | }                          | \n>  +-------------------------+-------------+----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+\n> ```\n> \n> \n\nFor more information about calling this method, see <service\\_name>!SPCS\\_GET\\_LOGS .\n\n### Using event table \u00b6\n\nSnowflake can capture logs sent from containers to the standard output and standard error streams into the event table configured for your account.\nFor more information about configuring an event table, see Logging, tracing, and metrics .\n\nYou control which streams are collected (all, standard error only, or none) that you want stored in an event table by using the spec.logExporters field in the service specification file.\n\nYou can then query the event table for events. To find the active event table for the account, use the SHOW PARAMETERS command to check the value of the EVENT\\_TABLE parameter:\n\n```\nSHOW PARAMETERS LIKE 'event_table' IN ACCOUNT ;\n```\n\nCopy\n\nThe parameter specifies the active event table for the account.\n\nNext, query that event table. The following SELECT statement retrieves Snowflake service and job events recorded in the past hour:\n\n```\nSELECT TIMESTAMP , RESOURCE_ATTRIBUTES , RECORD_ATTRIBUTES , VALUE \n FROM < current_event_table_for_your_account > \n WHERE timestamp > dateadd ( hour , - 1 , current_timestamp ()) \n AND RESOURCE_ATTRIBUTES : \"snow.service.name\" = '<service_name>' \n AND RECORD_TYPE = 'LOG' \n ORDER BY timestamp DESC \n LIMIT 10 ;\n```\n\nCopy\n\nSnowflake recommends that you include a timestamp in the WHERE clause of event\ntable queries, as shown in this example. This is particularly important\nbecause of the potential volume of data generated by various Snowflake\ncomponents. By applying filters, you can retrieve a smaller subset\nof data, which improves query performance.\n\nThe event table includes the following columns, which provide useful information regarding the logs collected\nby Snowflake from your container:\n\n* **TIMESTAMP:** Shows when Snowflake collected the log.\n* **RESOURCE\\_ATTRIBUTES:** Provides a JSON object that identifies the Snowflake service and the container in the service that generated\n  the log message. For example, it furnishes details such as the service name, container name, and compute pool name that were specified\n  when the service was run.\n  \n  ```\n  { \n    \"snow.account.name\": \"SPCSDOCS1\" ,\n    \"snow.compute_pool.id\": 20,\n    \"snow.compute_pool.name\": \"TUTORIAL_COMPUTE_POOL\" ,\n    \"snow.compute_pool.node.id\": \"a17e8157\" ,\n    \"snow.compute_pool.node.instance_family\": \"CPU_X64_XS\" ,\n    \"snow.database.id\": 26,\n    \"snow.database.name\": \"TUTORIAL_DB\" ,\n    \"snow.schema.id\": 212,\n    \"snow.schema.name\": \"DATA_SCHEMA\" ,\n    \"snow.service.container.instance\": \"0\" ,\n    \"snow.service.container.name\": \"echo\" ,\n    \"snow.service.container.run.id\": \"b30566\" ,\n     \"snow.service.id\": 114,\n    \"snow.service.name\": \"ECHO_SERVICE2\" ,\n    \"snow.service.type\": \"Service\" \n   }\n  ```\n  \n  Copy\n* **RECORD\\_ATTRIBUTES:** For a Snowflake service, it identifies an\n  error source (standard output or standard error).\n  \n  ```\n  { \"log.iostream\": \"stdout\" }\n  ```\n  \n  Copy\n* **VALUE:** Standard output and standard error are broken into lines,\n  and each line generates a record in the event table.\n  \n  ```\n  \"echo-service [2023-10-23 17:52:27,429] [DEBUG] Sending response: {'data': [[0, 'Joe said hello!']]}\"\n  ```\n\n### Using SYSTEM$GET\\_ SERVICE\\_ LOGS \u00b6\n\nThe SYSTEM$GET\\_SERVICE\\_LOGS function returns logs of the currently running service container. After a container exits, you can continue to access the logs by using the system function for a short time. System functions are most useful during development and testing, when you are initially authoring a service or a job.\n\nYou provide the service name, instance ID, container name, and optionally the number of most recent log lines to retrieve. If only one service instance is running, the service instance ID is 0. For example, the following statement command retrieves the\ntrailing 10 lines from the log of a container named `echo` that belongs to instance 0 of a service named `echo_service` :\n\n```\nSELECT SYSTEM$GET_SERVICE_LOGS ( 'echo_service' , '0' , 'echo' , 10 );\n```\n\nCopy\n\nExample output:\n\n```\n+--------------------------------------------------------------------------+ \n | SYSTEM$GET_SERVICE_LOGS                                                  | \n |--------------------------------------------------------------------------| \n | 10.16.6.163 - - [11/Apr/2023 21:44:03] \"GET /healthcheck HTTP/1.1\" 200 - | \n | 10.16.6.163 - - [11/Apr/2023 21:44:08] \"GET /healthcheck HTTP/1.1\" 200 - | \n | 10.16.6.163 - - [11/Apr/2023 21:44:13] \"GET /healthcheck HTTP/1.1\" 200 - | \n | 10.16.6.163 - - [11/Apr/2023 21:44:18] \"GET /healthcheck HTTP/1.1\" 200 - | \n +--------------------------------------------------------------------------+ \n 1 Row(s) produced. Time Elapsed: 0.878s\n```\n\nIf you don\u2019t have the information about the service that you need to call the function \u2014 such as the instance ID or container name \u2014 you can first run the SHOW SERVICE CONTAINERS IN SERVICE command to get information\nabout the service instances and containers running in each instance.\n\nThe SYSTEM$GET\\_SERVICE\\_LOGS function has the following limitations:\n\n* It merges standard output and standard error streams. The function provides no indication of which stream the output came from.\n* It reports the captured data for a specific container in a single\n  service instance.\n* It only reports logs for a running container. The function can\u2019t fetch\n  logs from a previous container that was restarted or from a\n  container of a service that is stopped or deleted.\n* The function returns up to 100 KB of data.\n\n## Access platform metrics \u00b6\n\nSnowflake provides metrics for compute pools in your account and services running on those compute pools. These metrics, provided by Snowflake, are also referred to as platform metrics.\n\n* **Event-table service metrics:** Individual services publish metrics. These are a subset of the compute pool metrics that provide information specific to the service. The target use case for this is to observe the resource utilization of a specific service. In the service specification, you define which metrics you want Snowflake to record in the event table while the service is running.\n* **Compute pool metrics:** Each compute pool also publishes metrics that provide information about what is happening inside that compute pool. The target use case for this is to observe the compute pool utilization. To access your compute pool metrics, you will need to write a service that uses Prometheus-compatible API to poll the metrics that the compute pool publishes.\n\n### Accessing event-table service metrics \u00b6\n\nTo log metrics from a service into the event table configured for your account, include the following section in your service specification:\n\n```\nplatformMonitor : \n metricConfig : \n groups : \n - <group 1> \n - <group 2> \n - ...\n```\n\nCopy\n\nWhere each `_group N_` refers to a predefined metrics group that you are interested in; for example, `system` , `network` , or `storage` . For more information, see the spec.platformMonitor field section in the documentation on the service specification.\n\nWhile the service is running, Snowflake records these metrics to the event table in your account. You can read these metrics in the following ways:\n\n* **Using the service helper method:** The <service\\_name>!SPCS\\_GET\\_METRICS table function returns metrics Snowflake collected for the specified service. The following list explains advantages of using this table function:\n  \n    + You can retrieve metrics for a specific service.\n    + You can retrieve metrics within a specified time range.\n    + The caller doesn\u2019t need access to the entire events table, which can be beneficial for customers with strict information security requirements.\n  \n  The following SELECT statement uses the table function to retrieve platform events for the specified service that was recorded in the past hour:\n  \n  ```\n  SELECT * \n    FROM TABLE ( mydb . myschema . echo_service ! SPCS_GET_METRICS ( start_time => dateadd ( 'hour' , - 1 , current_timestamp ())));\n  ```\n  \n  Copy\n* **Query the events table directly:** You can query your event table to read the metrics. The following query retrieves the service metrics that were recorded in the past hour for the service `my_service` :\n  \n  ```\n  SELECT timestamp , value \n    FROM my_event_table_db . my_event_table_schema . my_event_table \n    WHERE timestamp > DATEADD ( hour , - 1 , CURRENT_TIMESTAMP ()) \n      AND RESOURCE_ATTRIBUTES : \"snow.service.name\" = 'MY_SERVICE' \n      AND RECORD_TYPE = 'METRIC' \n      ORDER BY timestamp DESC \n      LIMIT 10 ;\n  ```\n  \n  Copy\n  \n  If you don\u2019t know the name of the active event table for the account, run the SHOW PARAMETERS command to display the value of the account-level EVENT\\_TABLE parameter:\n  \n  ```\n  SHOW PARAMETERS LIKE 'event_table' IN ACCOUNT ;\n  ```\n  \n  Copy\n  \n  For more information about event tables, see Using event table .\n\n**Example**\n\nTo create an example service that records metrics to the event table that is configured for your account, complete the following steps.\n\n1. Create a service named `echo_service` by following the steps in Tutorial 1 , with one change. In step 3, where you create a service, use the following CREATE SERVICE command, which adds the `platformMonitor` field in the modified service specification:\n   \n   ```\n   CREATE SERVICE echo_service \n     IN COMPUTE POOL tutorial_compute_pool \n     FROM SPECIFICATION $$ \n    spec : \n    containers : \n - name : echo \n    image : /tutorial_db/data_schema/tutorial_repository/my_echo_service_image:latest \n    env : \n    SERVER_PORT : 8000 \n    CHARACTER_NAME : Bob \n    readinessProbe : \n    port : 8000 \n    path : /healthcheck \n    endpoints : \n - name : echoendpoint \n    port : 8000 \n    public : true \n    platformMonitor : \n    metricConfig : \n    groups : \n - system \n - system_limits \n         $$ \n       MIN_INSTANCES = 1 \n       MAX_INSTANCES = 1 ;\n   ```\n   \n   Copy\n\n> After the service is running, Snowflake starts recording the metrics in the specified metric groups to the event table.\n> \n> \n\n1. Access the metrics by calling the <service\\_name>!SPCS\\_GET\\_METRICS function or by querying the event table. For example, retrieve metrics reported in the last hour by the echo\\_service service:\n   \n   > + Use the <service\\_name>!SPCS\\_GET\\_METRICS helper function:\n   >       \n   >       ```\n   >       SELECT * \n   >        FROM TABLE ( mydb . myschema . echo_service ! SPCS_GET_METRICS ( START_TIME => DATEADD ( 'hour' , - 1 , CURRENT_TIMESTAMP ())));\n   >       ```\n   >       \n   >       Copy\n   >     + Query the event table directly:\n   >       \n   >       ```\n   >       SELECT timestamp , value \n   >        FROM my_events \n   >        WHERE timestamp > DATEADD ( hour , - 1 , CURRENT_TIMESTAMP ()) \n   >          AND RESOURCE_ATTRIBUTES : \"snow.service.name\" = 'ECHO_SERVICE' \n   >          AND RECORD_TYPE = 'METRIC' \n   >          AND RECORD :metric . name = 'container.cpu.usage' \n   >          ORDER BY timestamp DESC \n   >          LIMIT 100 ;\n   >       ```\n   >       \n   >       Copy\n   > \n   >\n\n### Access compute pool metrics \u00b6\n\nCompute pool metrics offer insights into the nodes in the compute pool and the services running on them. Each node reports node-specific metrics, such as the amount of available memory for containers, as well as service metrics, like the memory usage by individual containers. The compute pool metrics provide information from a node\u2019s perspective.\n\nEach node has a metrics publisher that listens on TCP port 9001. Other services can make an HTTP GET request with the path `/metrics` to port 9001 on the node. To discover the node\u2019s IP address, retrieve SRV records (or A records) from DNS for the `discover.monitor. _compute_pool_name_ .cp.spcs.internal` hostname. Then, create another service in your account that actively polls each node to retrieve the metrics.\n\nThe body in the response provides the metrics using the [Prometheus format](https://prometheus.io/docs/instrumenting/exposition_formats/) as shown in the following example metrics:\n\n```\n# HELP node_memory_capacity Defines SPCS compute pool resource capacity on the node \n # TYPE node_memory_capacity gauge \n node_memory_capacity{snow_compute_pool_name=\"MY_POOL\",snow_compute_pool_node_instance_family=\"CPU_X64_S\",snow_compute_pool_node_id=\"10.244.3.8\"} 1 \n node_cpu_capacity{snow_compute_pool_name=\"MY_POOL\",snow_compute_pool_node_instance_family=\"CPU_X64_S\",snow_compute_pool_node_id=\"10.244.3.8\"} 7.21397383168e+09\n```\n\nNote the following:\n\n* The response body starts with `# HELP` and `# TYPE` , which provide a short description and the type of the metric. In this example, the `node_memory_capacity` metric is of type `gauge` .\n* It is then followed by the metric\u2019s name, a list of labels describing a specific resource (data point), and its value. In this example, the metric (named `node_memory_capacity` ) provides memory information, indicating that the node has 7.2 GB available memory. The metric also includes metadata in the form of labels as shown:\n  \n  ```\n  snow_compute_pool_name=\"MY_POOL\", \n   snow_compute_pool_node_instance_family=\"CPU_X64_S\",snow_compute_pool_node_id=\"10.244.3.8\"\n  ```\n\nYou can process these metrics any way you choose; for example, you might store metrics in a database and use a UI (such as a Grafana dashboard) to display the information.\n\nNote\n\n* Snowflake does not provide any aggregation of metrics. For example, to get metrics for a given service, you must query all nodes that are running instances of that service.\n* The compute pool must have a DNS-compatible name for you to access the metrics.\n* The endpoint exposed by a compute pool can be accessed by a service using a role that has the OWNERSHIP or MONITOR privilege on the compute pool.\n\nFor a list of available compute pool metrics, see Available platform metrics .\n\n**Example**\n\nFor an example of configuring Prometheus to poll your compute pool for metrics, see the [compute pool metrics tutorials](https://github.com/Snowflake-Labs/spcs-templates/tree/main/user-metrics) .\n\n### Available platform metrics \u00b6\n\nThe following is a list of available platform metrics groups and metrics within each group. Note that `storage` metrics are currently only collected from block storage volumes.\n\n|Metric group . Metric name |Unit |Type |resource\\_attributes |record\\_attributes |Description |\n| --- | --- | --- | --- | --- | --- |\n|system . container.cpu.usage |cpu cores |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |Average number of CPU cores used since last measurement. 1.0 indicates full utilization of 1 CPU core. Max value is number of cpu cores available to the container. |\n|system . container.memory.usage |bytes |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |Memory used, in bytes. |\n|system . container.gpu.memory.usage |bytes |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |Per-GPU memory used, in bytes. The source GPU is denoted in the \u2018gpu\u2019 attribute. |\n|system . container.gpu.utilization |ratio |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id |gpu |Ratio of per-GPU usage to capacity. The source GPU is denoted in the \u2018gpu\u2019 attribute. |\n|system\\_limits . container.cpu.limit |cpu cores |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |CPU resource limit from the service specification. If no limit is defined, defaults to node capacity. |\n|system\\_limits . container.gpu.limit |gpus |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id |gpu |GPU count limit from the service specification. If no limit is defined, the metric is not emitted. |\n|system\\_limits . container.memory.limit |bytes |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |Memory limit from the service specification. If no limit is defined, defaults to node capacity. |\n|system\\_limits . container.cpu.requested |cpu cores |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |CPU resource request from the service specification. If no limit is defined, this defaults to a value chosen by Snowflake. |\n|system\\_limits . container.gpu.requested |gpus |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id |gpu |GPU count from the service specification. If no limit is defined, the metric is not emitted. |\n|system\\_limits . container.memory.requested |bytes |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id |gpu |Memory request from the service specification. If no limit is defined, this defaults to a value chosen by Snowflake. |\n|system\\_limits . container.gpu.memory.capacity |bytes |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id |gpu |Per-GPU memory capacity. The source GPU is denoted in the \u2018gpu\u2019 attribute. |\n|status . container.restarts |restarts |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |Number of times Snowflake restarted the container. |\n|status . container.state.finished |boolean |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |When the container is in the \u2018finished\u2019 state, this metric will be emitted with the value 1. |\n|status . container.state.last.finished.reason |boolean |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id |reason |If the container has restarted previously, this metric will be emitted with the value 1. The \u2018reason\u2019 label describes why the container last finished. |\n|status . container.state.last.finished.exitcode |integer |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |If a container has restarted previously, this metric will contain the exit code of the previous run. |\n|status . container.state.pending |boolean |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |When a container is in the \u2018pending\u2019 state, this metric will be emitted with the value 1. |\n|status . container.state.pending.reason |boolean |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id |reason |When a container is in the \u2018pending\u2019 state, this metric will be emitted with the value 1. The \u2018reason\u2019 label describes why the container was most recently in the pending state. |\n|status . container.state.running |boolean |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |When a container is in the \u2018running\u2019 state, this metric will have the value 1. |\n|status . container.state.started |boolean |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id | |When a container is in the \u2018started\u2019 state, this metric will have the value 1. |\n|network . network.egress.denied.packets |packets |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id | |Network egress total denied packets from service instance due to policy validation failures. |\n|network . network.egress.received.bytes |bytes |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id | |Network egress total bytes received by service instance from remote destinations. |\n|network . network.egress.received.packets |packets |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id | |Network egress total packets received by service instance from remote destinations. |\n|network . network.egress.transmitted.bytes |byte |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id | |Network egress total bytes transmitted by service instance out to remote destinations. |\n|network . network.egress.transmitted.packets |packets |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id | |Network egress total packets transmitted by service instance out to remote destinations. |\n|network . network.ingress.connections.active |connections |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id\nsnow.endpoint.name | |Number of active ingress connections for this endpoint.\nThis metric includes the resource attribute `snow.endpoint.name` to determine the value per endpoint. |\n|network . network.ingress.cps |connections/sec |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.service.container.name\nsnow.query.id\nsnow.endpoint.name | |Number of ingress connections to this endpoint per second.\nThis metric includes the resource attribute `snow.endpoint.name` to help you to determine the value per endpoint. |\n|storage . volume.capacity |bytes |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id |snow\\_volume\\_id\nsnow\\_volume\\_name\nsnow\\_volume\\_replica\nvolume\\_type |Size of the filesystem. The target volume is denoted in the `volume_name` attribute. |\n|storage . volume.io.inflight |operations |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id |snow\\_volume\\_id\nsnow\\_volume\\_name\nsnow\\_volume\\_replica\nvolume\\_type |Number of active filesystem I/O operations at current instant. The target volume is denoted in the `volume_name` attribute. |\n|storage . volume.read.throughput |bytes/sec |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id |snow\\_volume\\_id\nsnow\\_volume\\_name\nsnow\\_volume\\_replica\nvolume\\_type |Filesystem reads throughput in bytes per second since last measurement. The target volume is denoted in the `volume_name` attribute. |\n|storage . volume.read.iops |operations/sec |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id |snow\\_volume\\_id\nsnow\\_volume\\_name\nsnow\\_volume\\_replica\nvolume\\_type |Filesystem read operations per second since last measurement. The target volume is denoted in the `volume_name` attribute |\n|storage . volume.usage |bytes |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id |snow\\_volume\\_id\nsnow\\_volume\\_name\nsnow\\_volume\\_replica\nvolume\\_type |Total number of bytes used in the filesystem since last measurement. The target volume is denoted in the `volume_name` attribute. |\n|storage . volume.write.throughput |bytes/sec |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id |snow\\_volume\\_id\nsnow\\_volume\\_name\nsnow\\_volume\\_replica\nvolume\\_type |Filesystem write throughput in bytes per second since last measurement. The target volume is denoted in the `volume_name` attribute. |\n|storage . volume.write.iops |operations/sec |gauge |snow.account.name\nsnow.compute\\_pool.id\nsnow.compute\\_pool.name\nsnow.compute\\_pool.node.id\nsnow.compute\\_pool.node.instance\\_family\nsnow.database.id\nsnow.database.name\nsnow.schema.id\nsnow.schema.name\nsnow.service.id\nsnow.service.name\nsnow.service.type\nsnow.service.container.instance\nsnow.query.id |snow\\_volume\\_id\nsnow\\_volume\\_name\nsnow\\_volume\\_replica\nvolume\\_type |Filesystem write operations per second since last measurement. The target volume is denoted in the `volume_name` attribute. |\n\nAs shown in the preceding table, the platform metrics contain the following attributes. These attributes are stored in the event table resource\\_attributes and record\\_attributes columns. Snowflake exposes these attributes as Prometheus labels when scraped directly from the node.\n\n**Resource attributes**\n\n* `snow.account.name` : Name of the account that launched the service.\n* `snow.compute_pool.id` : Id of the compute pool where the service was scheduled.\n* `snow.compute_pool.name` : Name of the compute pool where service was scheduled.\n* `snow.compute_pool.node_id` : Id of the compute pool node running the container that produced this metric.\n* `snow.compute_pool.node.instance_family` : The type of the instance family of the compute pool that is running the service. For more information, see CREATE COMPUTE POOL .\n* `snow.database.id` : Id of the database that owns the service.\n* `snow.database.name` : Name of the database that owns the service.\n* `snow.schema.id` : Id of the schema that owns the service.\n* `snow.schema.name` : Name of the schema that owns the service\n* `snow.service.id` : Id of the service.\n* `snow.service.name` : Name of the service.\n* `snow.service.type` : Specifies whether the container is a job service or a long-running service.\n* `snow.service_container.instance` : Id of the container instance that produced the metric.\n* `snow.service.container.name` : Name of the container that produced the metric.\n* `snow.query.id` : The uuid of the query that created the service.\n\n**Record attributes**\n\n* `gpu` : Index of the gpu from which this metric originated, starting with 0.\n* `reason` : Explains the container state. This attribute appears only for metrics that end with reason suffix.\n  \n    + `spcs.container.state.pending.reason`\n        \n        - `FailedToPullImage` : Container cannot pull image.\n        - `FailingToStartContainer` : Container cannot be started. It is getting scheduled to the node, but then fails.\n        - `ServiceRunError` : Runtime error occurred resulting in the container eviction.\n        - `ServiceSpecError` : Container cannot be scheduled because error in service specification.\n        - `ServiceCreateError` : Error during container initialization.\n        - `Initializing` : Container is currently initializing.\n        - `Creating` : Container in process of creating, for example, pulling an image.\n    + `container.state.last.finished.reason`\n        \n        - `Done` : Container finished without error.\n        - `Failed` : Container terminated with an error.\n        - `FailedWithOOM` : Container terminated after exceeding memory limit from service specification.\n        - `FailedToStart` : Container did not start due to error.\n* `resource` : Node resource that the metric describes (cpu, memory, gpu, gpu\\_memory).\n* `snow_volume_id` : Id of the volume.\n* `snow_volume_name` : Name of the volume.\n* `snow_volume_replica` : Indicates the service instance\u2019s ordinal identity within a service. For example, `snow_volume_replica=\"3\"` represents the third instance of that service.\n* `volume_type` : Volume type (local, memory, block, and Snowflake stage).\n\n## Publishing and accessing application metrics \u00b6\n\nApplication metrics and traces are generated by your service in contrast to platform metrics that Snowflake generates. Your service\ncontainers can generate OLTP or Prometheus metrics and Snowflake publishes them to the event table configured for your account.\n\nNote that you should ensure that your service container code outputs metrics with the correct units, aggregation, and instrumentation\ntypes to generate metrics that are meaningful and effective for your analysis.\n\n### Publishing OTLP application metrics and traces \u00b6\n\nSnowflake runs an OTel collector that your service container can use to publish OTLP application metrics and traces. That is, a service container can push metrics to the OTel collector endpoints, which Snowflake then writes to the event table configured for your Snowflake account along with the originating service details.\n\nIt works as follows:\n\n* Snowflake automatically populates the following environment variables in your service container that provide the OTel collector endpoints where containers can publish application metrics and traces:\n  \n    + `OTEL_EXPORTER_OTLP_METRICS_ENDPOINT`\n    + `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`\n* The [standard OTLP client](https://opentelemetry.io/docs/languages/) looks for these environment variables to discover the OTel collector automatically. This enables your service container to publish metrics and traces using this client.\n\n#### Configuring OTLP application Trace IDs \u00b6\n\nTraces must use the Snowflake Trace ID format to be viewable in [Snowflake Trail](https://www.snowflake.com/en/product/features/snowflake-trail/) and allow for performant lookup.\n\nSnowflake provides Python and Java libraries to simplify Trace ID generator setup. The following examples show how to override the default OpenTelemetry trace ID generator with these libraries.\n\n```\nfrom opentelemetry.sdk.trace import TracerProvider \n from snowflake.telemetry.trace import SnowflakeTraceIdGenerator \n\n trace_id_generator = SnowflakeTraceIdGenerator () \n tracer_provider = TracerProvider ( \n    resource = Resource . create ({ \"service.name\" : SERVICE_NAME }), \n    id_generator = trace_id_generator \n )\n```\n\nCopy\n\nFor more information, see [snowflake-telemetry-python](https://pypi.org/project/snowflake-telemetry-python/) on PyPI.\n\n```\nimport io.opentelemetry.sdk.autoconfigure.AutoConfiguredOpenTelemetrySdk ; \n import com.snowflake.telemetry.trace.SnowflakeTraceIdGenerator ; \n\n static OpenTelemetry initOpenTelemetry () { \n return AutoConfiguredOpenTelemetrySdk . builder () \n . addPropertiesSupplier ( \n () -> \n Map . of (... config options ...) \n . addTracerProviderCustomizer ( \n ( tracerProviderBuilder , configProperties ) -> { \n tracerProviderBuilder . setIdGenerator ( SnowflakeTraceIdGenerator . INSTANCE ); \n return tracerProviderBuilder ; \n }) \n . build () \n . getOpenTelemetrySdk ();\n```\n\nCopy\n\nFor more information about installing `com.snowflake.telemetry` , see Setting up your Java and Scala environment to use the Telemetry class .\n\nA trace ID generator can be implemented for any other programming language as well. The 16-byte ID (big endian) must contain a timestamp in the four highest-order bytes. The other bytes should contain random bits. For more information, see [Python reference implementation](https://github.com/snowflakedb/snowflake-telemetry-python/blob/0c5b4faf024997d993f7cd1d00e6ae0cb0bb7d08/src/snowflake/telemetry/trace/__init__.py) .\n\n### Publishing Prometheus application metrics \u00b6\n\nSnowflake supports Prometheus metrics where instead of pushing OTLP metrics, your application might expose Prometheus metrics to be polled\nby a Snowflake-provided collector. For Snowflake to collect these application metrics from your service and publish them to the event\ntable, follow these steps:\n\n* Have your service listen on a port, which exposes your Prometheus metrics.\n* Include in your service a Snowflake-provided container (also referred to as \u201csidecar\u201d container), with necessary configuration to pull\n  the metrics from your service container.\n\nThe Prometheus sidecar pulls the application metrics from the container at a scheduled frequency, converts the Prometheus format to OTLP format, and pushes the metrics to the OTel collector. The OTel collector then publishes those metrics into the event table configured for your Snowflake account.\n\nNote\n\nSnowflake doesn\u2019t support Prometheus Summary [metric type](https://prometheus.io/docs/concepts/metric_types/) , as it is [deprecated by OpenTelemetry](https://opentelemetry.io/docs/specs/otel/metrics/data-model/) . Use the Histogram type instead.\n\nYou add the Prometheus sidecar container to the service specification as another container and include an argument to specify the HTTP endpoint exposed by your container, using the following format:\n\n```\nlocalhost:{PORT}/{METRICS_PATH}, {SCRAPE_FREQUENCY}\n```\n\nIt specifies a port number, path, and frequency at which the sidecar should pull the metrics.\n\nAn example service specification fragment shows the sidecar container scraping metrics every minute from your service container from port 8000 and pulling metrics from the path \u201c/metrics\u201d:\n\n```\nspec : \n containers : \n - name : <name> \n image : <image-name> \n ..... \n - name : prometheus \n image : /snowflake/images/snowflake_images/monitoring-prometheus-sidecar:0.0.1 \n args : \n - \"-e\" \n - \"localhost:8000/metrics,1m\"\n```\n\nCopy\n\nIn the specification:\n\n* `image` is the Snowflake-provided sidecar container image.\n* `args` provides necessary configuration for the prometheus container to scrape metrics:\n  \n    + From port 8000 provided by your container. The port is required in this prometheus container configuration.\n    + Using path \u201c/metrics\u201d. It is optional. If not specified, \u201c/metrics\u201d is the default path.\n    + Every minute. It is optional. If not specified, \u201c1m\u201d is the default.\n  \n  If you leverage the defaults, this is the equivalent configuration for scraping metrics:\n  \n  ```\n  spec : \n   ... \n   args : \n - \"-e\" \n - \"localhost:8000\"\n  ```\n  \n  Copy\n\nNote\n\nThe Prometheus sidecar container is only supported for services (not jobs). If you want to collect application metrics for a job, it must push the metrics to the OTel collector.\n\n### Accessing application metrics and traces in the event table \u00b6\n\nYou can query the event table to retrieve application metrics. The following query retrieves the application metrics collected in the past hour.\n\n```\nSELECT timestamp , record :metric . name , value \n  FROM < current_event_table_for_your_account > \n  WHERE timestamp > dateadd ( hour , - 1 , CURRENT_TIMESTAMP ()) \n    AND resource_attributes : \"snow.service.name\" = < service_name > \n    AND scope : \"name\" != 'snow.spcs.platform' \n    AND record_type = 'METRIC' \n  ORDER BY timestamp DESC \n  LIMIT 10 ;\n```\n\nCopy\n\nFor more information about event tables, see Event table overview . You can visualize these metrics in Snowflake dashboards .\n\nYou can also query your event table to view the application traces. For example, to retrieve application traces from the past hour, in the preceding query, replace the `record_type` condition as follows:\n\n```\nAND record_type = 'SPAN' OR record_type = 'SPAN_EVENT'\n```\n\nCopy\n\nTraces can be visualized in the [Snowflake trail](https://www.snowflake.com/en/data-cloud/snowflake-trail/) viewer.\n\nMetrics and traces contain both user-defined and Snowflake-defined attributes as resource and record attributes. Note that the `snow.` prefix is reserved for Snowflake-generated attributes, Snowflake ignores custom attributes that use this prefix. To see a list of Snowflake defined attributes see Available platform metrics .\n\n[Example code](https://github.com/Snowflake-Labs/spcs-templates/tree/main/application-observability) is provided in both Python and Java that demonstrates instrumenting an application with custom metrics and traces using the OTLP SDK. The examples show how to configure Snowflake Trace ID generation for compatibility with the Snowflake trail viewer for traces.\n\n## Accessing platform events \u00b6\n\nSnowflake records events that provide visibility into the status and history of services. These Snowflake-provided events are referred to as _platform events_ .\n\nFor example, if your service container is currently running but was restarted a day earlier due to a fatal error (such as an out-of-memory condition), you can use platform events to view this historical event.\n\nSnowflake logs these platform events in the event table in your account. By default, platform events are not logged. To enable the logging of platform events, set the LOG\\_LEVEL parameter when creating resources (for example, when running CREATE SERVICE) or use ALTER statements to update the log level for existing resources.\n\nNote\n\nIf the LOG\\_LEVEL parameter is not set at the resource level, Snowflake can inherit the value of the parameter that is set at a higher level. For a service, Snowflake can inherit the value of the LOG\\_LEVEL parameter that is set on the schema, database, or the account of the service. For more information, see How Snowflake determines the level in effect .\n\nYou can check the current log level set for a service by running SHOW PARAMETERS \u2026 IN SERVICE :\n\n```\nSHOW PARAMETERS LIKE 'LOG_LEVEL' IN SERVICE mydb . myschema . myservice ;\n```\n\nCopy\n\nThe value of the LOG\\_LEVEL parameter determines the severity of events you want recorded in the event table. In the current implementation, the supported LOG\\_LEVEL values are: `INFO` and `ERROR` .\n\n* If you want to record only ERROR events in the event table, set the LOG\\_LEVEL to `ERROR` .\n* If you want `INFO` and `ERROR` events recorded in the event table, set the LOG\\_LEVEL to `INFO` .\n* If you want to stop recording platform events in the event table, set the LOG\\_LEVEL to `OFF` .\n\nFor more information, see Setting telemetry levels .\n\n### Query platform events \u00b6\n\nAfter you configure the log level for your resource, Snowflake records the platform events to the active event table in your Snowflake account. You can access these events in the following ways:\n\n* **Using the service helper method:** The <service\\_name>!SPCS\\_GET\\_EVENTS table function returns events collected by Snowflake from the containers of the specified service.\n  \n  The following list explains the advantages of using this table function:\n  \n    + You can retrieve events for a specific service.\n    + You can retrieve events within a specified time range.\n    + The caller doesn\u2019t need access to the entire events table, which can be beneficial for customers with strict information security requirements.\n  \n  The following SELECT statement uses the table function to retrieve platform events for the specified service recorded in the past hour:\n  \n  ```\n  SELECT * \n   FROM TABLE ( mydb . myschema . echo_service ! SPCS_GET_EVENTS ( START_TIME => DATEADD ( 'hour' , - 1 , CURRENT_TIMESTAMP ())));\n  ```\n  \n  Copy\n* **Using the event table directly:** You can query the event table directly. To find the active event table for the account, use the SHOW PARAMETERS command to check the value of the EVENT\\_TABLE parameter:\n  \n  ```\n  SHOW PARAMETERS LIKE 'event_table' IN ACCOUNT ;\n  ```\n  \n  Copy\n  \n  The parameter specifies the active event table for the account.\n  \n  Next, query that event table. The following SELECT statement retrieves platform events for the specified service that was recorded in the past hour:\n  \n  ```\n  SELECT TIMESTAMP , RESOURCE_ATTRIBUTES , RECORD , VALUE \n    FROM < your_event_table > \n    WHERE TIMESTAMP > DATEADD ( hour , - 1 , CURRENT_TIMESTAMP ()) \n      AND RESOURCE_ATTRIBUTES : \"snow.service.name\" = '<your_service_name>' \n      AND RECORD_TYPE = 'EVENT' \n      AND SCOPE : \"name\" = 'snow.spcs.platform' \n    ORDER BY TIMESTAMP DESC \n    LIMIT 10 ;\n  ```\n  \n  Copy\n  \n  For more information about event tables, see Using event table .\n  \n  The following columns in the event table provide useful information about the platform events:\n  \n    + **TIMESTAMP:** Shows when the event was recorded.\n    + **RESOURCE\\_ATTRIBUTES:** Provides a JSON object with metadata about the event source, such as a service, a container, or a compute pool. The following example of a value in the `resource_attribute` column identifies a specific service for which the event is recorded\n        \n        ```\n        { \n         \"snow.compute_pool.name\" : \"TUTORIAL_COMPUTE_POOL\" , \n         \"snow.compute_pool.id\" : 123 , \n         \"snow.database.name\" : \"TUTORIAL_DB\" , \n         \"snow.database.id\" : 456 , \n         \"snow.schema.name\" : \"DATA_SCHEMA\" , \n         \"snow.schema.id\" : 789 , \n         \"snow.service.container.name\" : \"echo\" , \n         \"snow.service.name\" : \"ECHO_SERVICE2\" , \n         \"snow.service.id\" : 212 , \n         \"snow.service.type\" : \"Service\" \n         }\n        ```\n        \n        Copy\n    + **SCOPE:** Indicates the origin of the event. For platform events, the name of the scope is `snow.spcs.platform` , as shown in the following example:\n        \n        ```\n        { \"name\" : \"snow.spcs.platform\" }\n        ```\n        \n        Copy\n    + **RECORD\\_TYPE:** For platform events, EVENT is the RECORD\\_TYPE.\n    + **RECORD:** Provides metadata about the specific event. The following metadata shows the name and severity level of the platform event:\n        \n        ```\n        { \"name\" : \"CONTAINER.STATUS_CHANGE\" , \"severity_text\" : \"INFO\" }\n        ```\n        \n        Copy\n    + **VALUE:** Provides the event details. The following example shows the status and a message about the status of the container:\n        \n        ```\n        { \"message\" : \"Running\" , \"status\" : \"READY\" }\n        ```\n        \n        Copy\n\n### Supported events \u00b6\n\nCurrently, Snowflake supports only the container status change events.\n\nThe following table lists the platform events that Snowflake records. `RECORD` and `VALUE` in the column names refer to the columns in the event table (explained in the preceding section).\n\n|RECORD:name |RECORD:severity\\_text |VALUE:message |VALUE:status |\n| --- | --- | --- | --- |\n|CONTAINER.STATUS\\_CHANGE |INFO |Running |READY |\n|CONTAINER.STATUS\\_CHANGE |INFO |Readiness probe is failing at path: <path>, port: <port> |PENDING |\n|CONTAINER.STATUS\\_CHANGE |INFO |Waiting to start |PENDING |\n|CONTAINER.STATUS\\_CHANGE |INFO |Compute pool node(s) are being provisioned |PENDING |\n|CONTAINER.STATUS\\_CHANGE |ERROR |Failed to pull image |PENDING |\n|CONTAINER.STATUS\\_CHANGE |ERROR |Provided image name uses an invalid format |FAILED |\n|CONTAINER.STATUS\\_CHANGE |ERROR |Encountered fatal error, retrying |FAILED |\n|CONTAINER.STATUS\\_CHANGE |ERROR |Encountered fatal error |FAILED |\n|CONTAINER.STATUS\\_CHANGE |ERROR |Encountered fatal error while running, check container logs |FAILED |\n|CONTAINER.STATUS\\_CHANGE |ERROR |Container was OOMKilled due to resource usage |FAILED |\n|CONTAINER.STATUS\\_CHANGE |ERROR |User application error, check container logs |FAILED |\n|CONTAINER.STATUS\\_CHANGE |ERROR |Encountered fatal error while starting container |FAILED |\n|CONTAINER.STATUS\\_CHANGE |INFO |Completed successfully |DONE |\n\n## Guidelines and limitations \u00b6\n\n* Maximum throughput for logs ingested to the event table per node is 1 MB/second for Snowflake accounts on AWS and Azure.\n* Maximum combined throughput for metrics and traces ingested to the event table is 1 MB/second per node for both Azure and AWS.\n* Maximum record size for logs ingested to the event table is 16 KiB.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)"
      ],
      "full_content": null
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views",
      "title": "Snowpark Container Services costs | Snowflake Documentation",
      "publish_date": null,
      "excerpts": [
        "Developer Snowpark Container Services Snowpark Container Services Costs\n\n# Snowpark Container Services costs \u00b6\n\n Feature \u2014 Generally Available\n\nSnowpark Container Services is available to accounts in AWS, Microsoft Azure, and Google Cloud Platform commercial regions , with some exceptions. For more information, see Available regions and considerations .\n\nThe costs associated with using Snowpark Container Services can be categorized into storage cost, compute pool cost, and data\ntransfer cost.\n\n## Storage cost \u00b6\n\nWhen you use Snowpark Container Services, storage costs associated with Snowflake, including the cost of Snowflake stage usage\nor database table storage, apply. For more information, see Exploring storage cost . In addition, the\nfollowing cost considerations apply:\n\n* **Image repository storage cost:** The implementation of the image repository uses\n  a Snowflake stage. Therefore, the associated cost for using the Snowflake stage applies.\n* **Log storage cost:** When you store local container logs in event tables , event table storage\n  costs apply.\n* **Mounting volumes cost:**\n  \n    + When you mount a Snowflake stage as a volume, the cost of using the Snowflake stage applies.\n    + When you mount storage from the compute pool node as a volume, it appears as local storage in the container. But there is no\n        additional cost because the local storage cost is covered by the cost of the compute pool node.\n* **Block storage cost:** When you create a service that uses block storage , you are billed for block storage and snapshot storage. For more information about storage pricing, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) . The SPCS Block Storage Pricing table in this document provides the information.\n\n## Compute pool cost \u00b6\n\nA compute pool is a collection of one or more virtual machine (VM) nodes on which Snowflake\nruns your Snowpark Container Services jobs and services. The number and type (instance family) of the nodes in the compute pool\n(see CREATE COMPUTE POOL ) determine the credits it consumes and thus the cost you pay. For more information, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\n\nYou incur charges for a compute pool in the IDLE, ACTIVE, STOPPING, or RESIZING state, but not when it is in a STARTING or\nSUSPENDED state. To optimize compute pool expenses, you should leverage the AUTO\\_SUSPEND feature (see CREATE COMPUTE POOL).\n\nThe following views provide usage information:\n\n* **ACCOUNT\\_USAGE views**\n  \n  The following ACCOUNT\\_USAGE views contain Snowpark Container Services credit usage information:\n  \n    + The SNOWPARK\\_CONTAINER\\_SERVICES\\_HISTORY view offers\n        credit usage information (hourly consumption) exclusively for Snowpark Container Services.\n    + In the METERING\\_DAILY\\_HISTORY view , query for rows in which the `service_type` column contains the value `SNOWPARK_CONTAINER_SERVICES` .\n    + In the METERING\\_HISTORY view , query for rows in which the `service_type` column contains the value `SNOWPARK_CONTAINER_SERVICES` .\n* **ORGANIZATION\\_USAGE views**\n  \n    + In the METERING\\_DAILY\\_HISTORY view , use the `SERVICE_TYPE = SNOWPARK_CONTAINER_SERVICES` query filter.\n\n## Data transfer cost \u00b6\n\nData transfer is the process of moving data into (ingress) and out of (egress) Snowflake. For more information, see Understanding data transfer cost . When you use Snowpark Container Services, the following additional cost\nconsiderations apply:\n\n* **Outbound data transfer:** Snowflake applies the same data transfer rate for outbound data transfers from services and jobs\n  to other cloud regions and to the internet, consistent with the rate for all Snowflake outbound data transfers. For more\n  information, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) (table 4a).\n  \n  You can query the DATA\\_TRANSFER\\_HISTORY ACCOUNT\\_USAGE view for\n  usage information. The `transfer_type` column identifies this cost as the `SNOWPARK_CONTAINER_SERVICES` type.\n* **Internal data transfer:** This class of data transfer refers to data movements across compute entities within Snowflake, such as\n  between two compute pools or a compute pool and a warehouse, that resulted from executing a service function .\n  For more information, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) (tables 4(a) for AWS, 4(b) for Azure, and the column titled \u201cSPCS Data Transfer to Same Cloud Provider, Same Region\u201d).\n  \n  To view the costs associated with internal data transfer, you can do the following:\n  \n    + Query the INTERNAL\\_DATA\\_TRANSFER\\_HISTORY view in the ACCOUNT\\_USAGE schema.\n    + Query the DATA\\_TRANSFER\\_HISTORY view in the ACCOUNT\\_USAGE schema. The `transfer_type` column identifies this cost as the `INTERNAL` type.\n    + Query the DATA\\_TRANSFER\\_HISTORY view in the ORGANIZATION\\_USAGE schema.\n        The `transfer_type` column identifies this cost as the `INTERNAL` type.\n    + Query the DATA\\_TRANSFER\\_DAILY\\_HISTORY view in the ORGANIZATION\\_USAGE schema. The `service_type` column identifies this cost as the `INTERNAL_DATA_TRANSFER` type.\n    + Query the RATE\\_SHEET\\_DAILY view in the ORGANIZATION USAGE\n        schema. The `service_type` column identifies this cost as the `INTERNAL_DATA_TRANSFER` type.\n    + Query the USAGE\\_IN\\_CURRENCY\\_DAILY view in the ORGANIZATION USAGE\n        schema. The `service_type` column identifies this cost as the `INTERNAL_DATA_TRANSFER` type.\n\nNote\n\nData transfer costs are currently not billed for Snowflake accounts on Google Cloud.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Storage cost\n2. Compute pool cost\n3. Data transfer cost\n\nRelated content\n\n1. Snowpark Container Services\n2. Snowpark Container Services: Working with compute pools\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    },
    {
      "url": "https://www.flexera.com/blog/finops/snowpark-container-services/",
      "title": "Snowpark Container Services 101: A comprehensive overview (2026)",
      "publish_date": "2026-01-27",
      "excerpts": [
        "Flexera Open Primary Navigation\n\n* Solutions\n  \n  Spend management by vendor\n  \n  [Flexera is a Leader in 2025 cloud financial management tools](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\n  \n  Discover recognized CFM vendors to watch in the 2025 Gartner\u00ae Magic Quadrant\u2122\n  \n  [View report](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\n* Products\n  \n  Flexera One\n  \n    + IT Visibility\n    + ITAM\n    + Snow Atlas\n    + Cloud License Management\n    + SaaS Management\n  \n    + FinOps\n    + Cloud Cost Optimization\n    + Cloud Commitment Management\n    + Container Optimization\n    + Virtual Machine Optimization\n    + Data Cloud Optimization\n  \n    + Application Readiness\n    + Security\n    + Integrations\n    + Technology Intelligence Platform\n    + All Products\n  \n  [Introducing Flexera One SaaS Management](https://www.flexera.com/products/flexera-one/saas-management)\n  \n  Discover comprehensive SaaS visibility for taming SaaS sprawl, wasted spend and compliance risks.\n  \n  [Book your personalized demo](https://www.flexera.com/products/flexera-one/saas-management)\n* Success\n  \n  Customer Success\n  \n  Services & Training\n  \n    + Services\n    + Training\n  \n  Support\n  \n    + [Flexera support portal](https://community.flexera.com/s/support-hub)\n    + [Flexera product documentation](https://docs.flexera.com)\n    + [Snow product documentation](https://docs.snowsoftware.io/)\n  \n    + Technology Intelligence Awards\n    + [Flexera community](https://community.flexera.com/s/)\n  \n  [2025 Technology Intelligence Awards](https://info.flexera.com/ITAM-REPORT-State-of-IT-Asset-Management)\n  \n  The results are in\u2014see how our 2025 winners and honorable mentions are shaping the future of Technology Intelligence.\n  \n  [See the winners](https://www.flexera.com/customer-success/awards)\n* Resources\n  \n  Resources\n  \n    + Webinars\n    + Videos\n    + Datasheets\n    + Whitepapers & reports\n  \n    + Blog\n    + Case studies\n    + Events\n    + Analyst research\n    + Glossary\n    + Demos & trials\n    + Business value calculator\n  \n  [Flexera 2026 IT Priorities Report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)\n  \n  AI ROI, sustainability, cost and risk: Discover the latest IT trends shaping tomorrow\u2019s IT landscape in Flexera\u2019s 2026 IT Priorities Report.\n  \n  [View report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)\n* About\n  \n  Company\n  \n    + About\n    + Careers\n    + Contact us\n    + Leadership\n  \n  Partners\n  \n    + Partner program\n    + Partner locator\n  \n  Press center\n  \n    + Press releases\n    + Articles\n    + Awards\n  \n  Social responsibility\n  \n    + ESG\n    + Belonging and inclusion\n  \n  [The Flexera 2025 State of the Cloud Report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)\n  \n  How are GenAI rising cloud costs, security and sustainability shaping your cloud strategies in 2025?\n  \n  [View report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)\n\nCustomers Open External Links\n\n* [Community](https://community.flexera.com/)\n* [Product login](https://app.flexera.com/login)\n* [Spot login](https://console.spotinst.com/auth/signIn)\n* [Partner Portal](https://partnerhub.flexera.com/)\n\nSearch\n\nBook a demo\n\n1. Home\n2. Blog\n3. [FinOps](https://www.flexera.com/blog/finops/)\n4. Snowpark Container Services 101: A comprehensive overview (2026)\n\n### [FinOps](https://www.flexera.com/blog/finops/)\n\nSubscribe\n\nTopics\n\nSaaS Management FinOps IT Visibility IT Asset Management Product News Application Readiness Security Perspectives\n\n[FinOps](https://www.flexera.com/blog/finops/)\n\n# Snowpark Container Services 101: A comprehensive overview (2026)\n\n[](https://www.flexera.com/blog/author/pramit/ \"Pramit Marattha\")\n\n[Pramit Marattha](https://www.flexera.com/blog/author/pramit/ \"Pramit Marattha\")\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowpark-container-services%2F&title=Snowpark%20Container%20Services%20101%3A%20A%20comprehensive%20overview%20%282026%29&source=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowpark-container-services%2F) [](https://twitter.com/intent/tweet?source=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowpark-container-services%2F&text=Snowpark%20Container%20Services%20101%3A%20A%20comprehensive%20overview%20%282026%29%20https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowpark-container-services%2F) \n\nThis post originally appeared on the chaosgenius.io blog. Chaos Genius has been [acquired by Flexera](https://www.flexera.com/more/ProsperOps-Chaos-Genius) .\n\nWouldn\u2019t it be great to run [containers](https://cloud.google.com/learn/what-are-containers) right where your data lives, in a secure and governed space? Up until now, you\u2019ve had to rely on external container platforms that typically force you to copy data out of your database and into cloud storage or other servers, adding latency and risk. **_Snowpark Container Services (SPCS)_** changes that: you bring your containers to Snowflake instead. Now you can build, run, scale and deploy containerized applications and services all inside Snowflake, with security built in and direct access to your data. There\u2019s no need to manually shuffle data around. It\u2019s a simpler approach: instead of moving data to compute, you bring compute to the data, saving you from expensive data movement.\n\nIn this article, we\u2019ll cover everything you need to know about Snowpark Container Services, from core concepts to a hands-on setup guide. Here, you will learn how to set up Snowflake compute pools, push container images, define service specifications and deploy your first containerized workload.\n\n## What is Snowpark Container Services?\n\n_Snowpark Container Services (SPCS)_ is a fully managed container service integrated into Snowflake. You push [Open Container Initiative-compliant images](https://opencontainers.org/) to your account\u2019s private Open Container Initiative (OCI) image registry, then run those images as long-running services, finite Snowflake job services, or callable Snowflake service functions on Snowflake-managed compute pools.\n\nSnowflake SPCS guarantees that data remains secure and generally does not leave Snowflake\u2019s governed environment unless explicitly configured by the user for external access; your containers have fast local access to tables, stages and even secrets via Snowflake\u2019s security integrations. You also benefit from Snowflake\u2019s native features, such as role-based access control (RBAC) , governance , monitoring and auto-scaling.\n\nSnowpark Container Services Overview (Source: [Snowflake](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) )\n\nAs of 2025, Snowpark Container Services is generally available across AWS, Microsoft Azure and Google Cloud Platform commercial regions, with some exceptions (e.g., not available in most government regions or the Google Cloud me-central2 region). It is unavailable for trial accounts except for running notebooks. Check out [Snowflake\u2019s documentation for the latest region availability](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) .\n\nAccording to [Snowflake\u2019s official documentation](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) :\n\n> Snowpark Container Services is a fully managed container offering designed to facilitate the deployment, management and scaling of containerized applications within the Snowflake ecosystem. This service enables users to run containerized workloads directly within Snowflake, ensuring that data doesn\u2019t need to be moved out of the Snowflake environment for processing. Unlike traditional container orchestration platforms like Docker or Kubernetes, Snowpark Container Services offers an OCI runtime execution environment specifically optimized for Snowflake. This integration allows for the seamless execution of OCI images, leveraging Snowflake\u2019s robust data platform.\n> \n> \n\n### Snowpark Container Services Core Concepts Explained\n\nSnowflake manages the container runtime, scheduling, scaling and patching. You provide the container image and the service or job specification.  \nContainers can execute SQL or call Snowflake APIs to read from and write to tables and stages. Access is controlled via Snowflake roles and policies.  \nEach Snowflake account includes a private OCI-compliant image registry. You push images there and Snowflake pulls them when launching services or jobs.  \nSnowflake Compute pools are sets of VM nodes that host your services and jobs. Pools autoscale between a minimum and maximum node count, with choices for instance families (including GPU-enabled options where supported).  \nSupports any programming language or framework inside the container, including GPU acceleration for AI/ML workloads.  \nOffers three workload models: long-running services (auto-restarting), Snowflake job services (batch or one-off tasks that run to completion) and Snowflake service functions (callable endpoints for SQL-bound computations).\n\n### Key Features and Capabilities of Snowpark Container Services\n\nHere are all the key notable features of Snowpark Container Services:\n\n#### **Feature 1: Managed Container Runtime**\n\nSnowflake handles the orchestration and scaling of your containers. You don\u2019t have to run your own [Kubernetes cluster](https://kubernetes.io/docs/concepts/architecture/) you just tell Snowflake the compute you need (via a Snowflake compute pool) and it spins up nodes as needed.\n\n#### **Feature 2: Any language & libraries**\n\nYou\u2019re not limited to Snowpark Python or SQL. Use any programming language (Python, Node.js, C++, R, Java) and any Linux libraries inside your container. There\u2019s no need to rewrite code for Snowflake; just containerize it.\n\n#### **Feature 3: OCI Image Registry**\n\nSnowflake provides a built-in Open Container Initiative (OCI) v2 registry. You can push/pull images using Docker CLI or SnowCLI and Snowflake stores them (encrypted) in a stage. The registry is accessible only to your account roles.\n\n#### **Feature 4: Workload Types**\n\nSnowpark Container Services supports: _long-running services_ (APIs with auto-restarts), Snowflake _job services_ (batch tasks that exit upon completion) and Snowflake _service functions_ (SQL-callable endpoints).\n\n#### **Feature 5: Snowpark Data Integration**\n\nContainers can use Snowpark and Snowflake connectors inside them. Snowflake even provides a short-lived OAuth token ( `/snowflake/session/token` ) to easily connect from inside a container back to Snowflake.\n\n#### **Feature 6: GPU Acceleration**\n\nSnowflake provides GPU-enabled Snowflake compute pools for AI/ML workloads, so you can train or serve [large models (like LLMs)](https://en.wikipedia.org/wiki/Large_language_model) right in the platform.\n\n#### **Feature 7: Data Access**\n\nContainers can access Snowflake tables and stages directly. You can mount a Snowflake stage as a volume in a container (reads as regular files) or use Snowpark APIs inside the container to query data. You can even export query results directly into your app.\n\n#### **Feature 8: Security & Networking**\n\nSnowpark Container Services comes with built-in Snowflake security (IAM roles, private networking, secrets). Containers, by default, have no outbound Internet access; you must explicitly grant egress via [Snowflake External Access Integrations](https://docs.snowflake.com/en/sql-reference/sql/create-external-access-integration) . You can also mount Snowflake stages as volumes (subject to privileges).\n\n#### **Feature 9: Observability**\n\nYou can log container output and Snowflake can capture logs and metrics from services into event tables or system views.\n\n### Common Use Cases for Snowpark Container Services\n\nSnowpark Container Services excels in scenarios where applications need tight integration with Snowflake data:\n\n* **Advanced Analytics and ML/AI** \u2014 Run models or pipelines in-place. Deploy real-time inference services with REST endpoints or fine-tune LLMs on proprietary data using GPUs.\n* **Bring-Your-Own-Language** \u2014 Maybe you have C++ or R code that processes data. Package it into a container and run it on Snowflake data. Even COBOL or Java/.NET jobs can run inside Snowpark Container Services.\n* **External Caches or DBs** \u2014 If you use a specialized store (like a vector database, Redis cache, or any microservice), you can run it as a container next to your Snowflake data.\n* **ETL and Pipeline** \u2014 Instead of extracting to an external server, write your ETL logic as a container. It can pull raw data from a stage, transform it and load it into tables, all on Snowflake machines.\n* **Full-Stack Apps and Tools** \u2014 You could even run tools like Metabase or Grafana in Snowpark Container Services.\n* **Third-Party and Partner Apps** \u2014 Snowflake\u2019s own ecosystem (Native Apps marketplace) will host containerized apps from partners on Snowpark Container Services. As an engineer, you could also deploy your company\u2019s custom app via SPCS.\n\nTL;DR: Snowpark Container Services suits any containerizable app that interacts with Snowflake data or benefits from co-location. If your data team wants to run Python scripts, APIs, ML models, or even web UIs without leaving the Snowflake platform, Snowpark Container Services is the right tool.\n\nNext, we\u2019ll deep dive into how it works under the hood.\n\n## Snowpark Container Services Architecture Components and Technical Design\n\n**Snowpark Container Services Architecture TL;DR:** Snowpark Container Services (SPCS) runs OCI images inside Snowflake on managed compute pools. You push images to Snowflake\u2019s private OCI registry, create a compute pool with an instance family and node limits, then define services or job services via a YAML spec. Long-running services are restarted automatically. Job services run to completion. Service functions expose container endpoints as SQL-callable functions. Outbound networking is blocked by default and must be allowed via External Access Integrations or private connectivity. Secrets, caller\u2019s rights, volumes, logging and monitoring are built into the SPCS spec and control plane.\n\nSnowpark Container Services Architecture (Source: [Snowflake](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/working-with-services) )\n\n### How Snowpark Container Services(SPCS) pieces fit together?\n\nHere\u2019s how it all works together:\n\n### **Snowflake Compute Pools (the runtime hosts)**\n\nA compute pool is your container \u201cwarehouse\u201d. It is a collection of virtual machines (nodes) that Snowflake manages for you. Whenever you create a pool, you pick an instance family (CPU\\_X64\\_S for small CPU-optimized, or a GPU family for ML) and set the minimum and maximum number of nodes. Snowflake will auto-scale the pool between those limits based on workload. If your services need more containers, Snowflake spins up additional nodes (up to your max). If they sit idle, it can remove nodes down to the minimum.\n\n### **Image Registry & Repository**\n\nEach Snowflake account has a private OCI-compliant container registry. Within that registry you create image repositories (like namespaces or projects) to hold your Docker images. When you build a Docker image, you tag it with your Snowflake registry\u2019s hostname and push it there. This registry, by default, is private to your account. Snowflake provides commands (or a Docker login mechanism) to authenticate and upload images.\n\n### **Service types: Services vs Jobs vs Service Functions**\n\nOnce your image is in the registry, you run it by creating a **service** (long-running), executing a Snowflake **job service** (batch) or Snowflake **service functions** .\n\n**Snowflake Service** is like a forever running container. It might host an API or continuously process a queue. When you fire up the command `CREATE SERVICE` , you link it to a Snowflake compute pool and give a YAML spec for containers and endpoints. Snowflake ensures this service keeps running \u2013 if a container crashes, it restarts it.\n\n**Snowflake Job service** is one and done. You run it with Snowflake `EXECUTE JOB SERVICE` , Snowflake spins up the containers on the specified Snowflake compute pool, they run your workload, then exit and the job is marked complete. Jobs do not auto restart. You can request parallel replicas with the REPLICAS parameter; Snowflake will coordinate the data split (via special `SNOWFLAKE_JOB_INDEX` env var).\n\n**Snowflake Service functions** are function as a service endpoints exposed by a service. A service implements a REST POST endpoint; a SQL-level function is then defined to call that endpoint. Calls follow the external function call pattern but execute inside Snowflake\u2019s network boundary so your data does not leave Snowflake.\n\n**TL;DR:**\n\n* Service = always on and auto restarting.\n* Snowflake Job service = run to completion, optionally parallel.\n* Snowflake Service function = callable, data local function exposed to SQL.\n\n### **Service specification (YAML)**\n\nBoth services and jobs use a YAML spec to describe the container(s) to run, any environment variables, ports and endpoints. For a service, you often define an HTTP endpoint so external clients can call it. You might specify one container named \u201capp\u201d using your image, listening on port 80 and an endpoint named \u201chttp\u201d that maps to that port. Snowflake then takes care of load-balancing requests to your containers.\n\n### **Networking: ingress, egress, private connectivity**\n\n**Ingress**\n\n* You can expose endpoints as public. Snowflake provides a stable ingress URL and proxy for public endpoints. Use `SHOW ENDPOINTS IN SERVICE` to get the `ingress_url` .\n\n**Egress**\n\n* Containers have no unrestricted outbound internet access. To allow controlled outbound calls, you must create an External Access Integration with explicit network rules and secrets, then reference it when creating or executing a service or function (via `EXTERNAL_ACCESS_INTEGRATIONS` ). This lets you whitelist hostnames and permit specific secret usage.\n\n**Inter-service traffic**\n\n* Containers running inside Snowflake communicate over Snowflake\u2019s internal network. You can design multi-container apps and mount volumes for shared data. Different Snowflake compute pools can be used for isolation while allowing application containers to communicate as required.\n\n### **Identity and Access**\n\nWhen a container runs, Snowflake injects temporary credentials so it can connect back to the Snowflake account using a specified role. The container runs as the \u201c **owner** \u201d role of the service (the role that created it) by default. You can also reference Snowflake **secrets** in your spec (using `containers.secrets` ) to provide any credentials or API keys to the container securely.\n\n### **Autoscaling and Lifecycles**\n\nAfter creation, a Snowflake compute pool starts at your `MIN_NODES` . When more containers need space, new nodes come up automatically. If containers idle away, nodes scale down (but not below `MIN_NODES` ). You can configure the pool to suspend (free all nodes) after inactivity ( `AUTO_SUSPEND` ). A suspended pool incurs no compute cost. You can also set `AUTO_RESUME` so it restarts when you next run a service.\n\nNext, we\u2019ll go through step-by-step through setting Snowpark Container Service up from scratch.\n\n* * *\n\n## Step-by-Step Guide to Setting Up Snowpark Container Services\n\nNow, we will walk through the process of launching your first Snowpark Container Service. You\u2019ll need the `ACCOUNTADMIN` role (or an equivalent) to set this up. Before you begin, however, make sure you have the following prerequisites:\n\n### Prerequisites\n\n* A Snowflake account (a non-trial account in a supported AWS region). You need privileges to create Snowflake compute pools, image repositories and services; this typically requires [ACCOUNTADMIN](https://docs.snowflake.com/en/user-guide/security-access-control-considerations) or an equivalent role.\n* [Snow CLI (snow) / SnowCLI](https://docs.snowflake.com/en/developer-guide/snowflake-cli/index) installed for convenient SPCS commands (snow spcs). SnowCLI handles packaging and provides a push login helper.\n* [Docker Desktop](https://www.docker.com/products/docker-desktop/) (or an OCI builder) to build images locally.\n* [Python 3.10](https://www.python.org/downloads/) installed.\n* **Privileges** : `CREATE COMPUTE POOL` on account, `CREATE IMAGE REPOSITORY` on schema, `CREATE SERVICE` and privileges on any database objects you\u2019ll access. Create roles and grant privileges as needed.\n* **Networking Plan** : If containers need outbound access, plan External Access Integration rules and secrets.\n\nStart by logging into Snowflake via Snowsight or Snowflake CLI.\n\n> Check out this article to learn more about [how to install and configure Snowflake CLI](https://docs.snowflake.com/en/developer-guide/snowflake-cli/installation/installation) .\n> \n> \n\nJust make sure you have the `ACCOUNTADMIN` role active.\n\n### **Step 2** \u2014Create Core Snowflake Objects and Roles\n\nFirst, make sure you switch to a role that can create objects.\n\n```\nUSE ROLE ACCOUNTADMIN;\n```\n\nNext, create a role called `spcs_role` .\n\n```\nCREATE ROLE spcs_role;\n```\n\nThen, run the following SQL commands to grant all the necessary privileges for that role:\n\n```\nGRANT CREATE DATABASE ON ACCOUNT TO ROLE spcs_role;\nGRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE spcs_role;\nGRANT CREATE COMPUTE POOL ON ACCOUNT TO ROLE spcs_role;\nGRANT CREATE INTEGRATION ON ACCOUNT TO ROLE spcs_role;\nGRANT MONITOR USAGE ON ACCOUNT TO\tROLE\tspcs_role;\nGRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE spcs_role;\n```\n\nGranting necessary privileges to spcs\\_role Snowflake role \u2013 Snowpark Container Services\n\nAlso, do not forget to allow the role to bind service endpoints. This privilege is needed if you plan to create public endpoints for your services:\n\n```\nGRANT BIND SERVICE ENDPOINT ON ACCOUNT TO ROLE spcs_role;\n```\n\nThen, `grant spcs_role` to `ACCOUNTADMIN` .\n\n```\nGRANT ROLE spcs_role to role ACCOUNTADMIN;\n```\n\nGranting spcs\\_role to ACCOUNTADMIN role \u2013 Snowflake Snowpark Container Services\n\nOnce all that is done, create the Database, Warehouse and Image Spec Stage.\n\n```\nUSE ROLE spcs_role;\n\nCREATE\nOR REPLACE DATABASE spcs_demo_db;\n\nCREATE\nOR REPLACE WAREHOUSE spcs_demo_wh WAREHOUSE_SIZE = XSMALL AUTO_SUSPEND = 120 AUTO_RESUME = TRUE;\n\nCREATE STAGE IF NOT EXISTS specs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE');\n\nCREATE STAGE IF NOT EXISTS volumes ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = (ENABLE = TRUE);\n```\n\nCreating Database, Warehouse, & Image Spec Stage \u2013 Snowpark Container Services\n\nAdjust names as needed for your environment. From here on, we\u2019ll work under `spcs_role` .\n\n### **Step 3** \u2014Set Up Snowflake Compute Pool\n\nSnowflake **compute pool** is a new Snowflake object that provides the compute resources (VMs) for containers. To create one, use [CREATE COMPUTE POOL](https://docs.snowflake.com/en/sql-reference/sql/create-compute-pool) and specify the minimum and maximum nodes, plus an instance family:\n\n```\nUSE ROLE spcs_role;\n\nCREATE COMPUTE POOL IF NOT EXISTS spcs_compute_pool\n\t MIN_NODES = 1\n\tMAX_NODES = 3\n\t INSTANCE_FAMILY = 'CPU_X64_M'\n\tAUTO_RESUME = TRUE\n\tAUTO_SUSPEND_SECS = 300;\n```\n\n* `INSTANCE_FAMILY` chooses the VM size. Here, `CPU_X64_M` means 6 vCPUs and ~28 GiB RAM per node. You might pick `CPU_X64_S` (3 vCPU, 13 GiB) or `CPU_X64_L` (28 vCPU, 116 GiB) depending on need. For GPUs, choose families like `GPU_NV_S` or `GPU_NV_M` (AWS with A10G GPUs).\n* `MIN_NODES` is the number of nodes to start with (and the floor to scale down to). `MAX_NODES` is how far it can grow. Snowflake will auto-scale between them.\n* `AUTO_RESUME = TRUE` tells Snowflake to automatically start the pool if you submit work when it\u2019s suspended.\n* `AUTO_SUSPEND_SECS (optional)` is how long to wait with no activity before suspending the pool (turning off all nodes to save cost). Here we set 300 seconds. When suspended, the pool has no running nodes and incurs no compute charges.\n\n**Note on Autoscaling:** After you create the pool, Snowflake will start MIN\\_NODES nodes. As you run services, if more nodes are needed (beyond curr. capacity), Snowflake adds nodes automatically until MAX\\_NODES. If nodes sit idle, Snowflake removes them back down to MIN\\_NODES. You will be charged for the nodes that exist (in IDLE or ACTIVE state) as compute usage. Using AUTO\\_SUSPEND on the pool can eliminate idle charges by tearing down all nodes after inactivity.\n\n### **Step 4** \u2014Set Up Image Registry (Create an Image Repository)\n\nNext, create a Snowflake [**image repository**](https://docs.snowflake.com/en/sql-reference/sql/create-image-repository) in Snowflake\u2019s registry to hold your container images. This is like a folder for Docker images within Snowflake. For example:\n\n```\nCREATE IMAGE REPOSITORY spcs_demo_repo;\n```\n\nCreating image repository named spcs\\_demo\\_repo \u2013 Snowpark Container Services\n\nSnowflake encrypts images by default. Once the repository exists, note its URL. Each Snowflake account\u2019s registry has a unique hostname of the form.\n\nTo view Snowflake image repository details, run the following command:\n\n```\nSHOW IMAGE REPOSITORIES IN SCHEMA spcs_demo_db.public;\n```\n\nShowing image repositories in spcs\\_demo\\_db.public schema \u2013 Snowpark Container Services\n\nInternally, Snowflake will create a stage to hold the image layers. You don\u2019t need to manage the storage; Snowflake handles it. But remember: any image you push to spcs\\_demo\\_repo is stored in your Snowflake account and incurs normal Snowflake stage storage costs.\n\n(Replace `<org>` and `<acct>` with your Snowflake organization and account name). As you can see, combined with your database, schema and repository, your full image path will look like:\n\n```\n<org>-<acct>.registry.snowflakecomputing.com/spcs_demo_db/public/spcs_demo_repo\n```\n\n### **Step 5** \u2014Verify Snowflake Objects\n\nDouble-check that the objects exist:\n\n```\nSHOW COMPUTE POOLS;\t\t\t\t -- should list my_pool as IDLE\nSHOW WAREHOUSES LIKE '%WH%';\nSHOW DATABASES LIKE 'SPCS_DEMO_DB';\nSHOW IMAGE REPOSITORIES;\t -- should show tutorial_repo\n```\n\nYou should see spcs\\_demo\\_db in state IDLE (no containers running yet) and your warehouse and database listed. This step is just sanity-checking your setup.\n\nIf you see errors, make sure your role has the right privileges on each object. Once these are set up, your Snowflake account is ready to accept container images and run services.\n\n### **Step 6** \u2014Configure External Access (Optional)\n\nNote that containers in Snowflake, by default, cannot connect to the public internet. This is a security feature of Snowflake\u2019s network. If your app needs to call an external API or service (for example, fetching from S3 or calling a third-party endpoint), you must explicitly allow it via Snowflake\u2019s [External Access Integration (EAI)](https://docs.snowflake.com/en/developer-guide/external-network-access/creating-using-external-network-access) . In short, you create network rules specifying allowed addresses and then tie them to an integration.\n\n* [**Create a Network Rule**](https://docs.snowflake.com/en/sql-reference/sql/create-network-rule) to whitelist external hosts/ports.\n* [**Create an External Access Integration (EAI)**](https://docs.snowflake.com/en/sql-reference/sql/create-external-access-integration) that uses those rules and grant your role usage on it.\n\nFor example, to allow all outbound HTTPS and HTTP traffic, you can define broad network rules like:\n\n```\nCREATE OR REPLACE NETWORK RULE snowflake_network_rule\n\t TYPE = 'HOST_PORT'\n\t MODE = 'EGRESS'\n\tVALUE_LIST= ('0.0.0.0:443', '0.0.0.0:80');\n```\n\nCreating network rule for egress on ports 443 and 80 \u2013 Snowpark Container Services\n\nThen create an integration using that rule:\n\n```\nCREATE EXTERNAL ACCESS INTEGRATION spcs_demo_eai\n\t ALLOWED_NETWORK_RULES = (snowflake_network_rule)\n\t ENABLED = true;\n```\n\nCreating external access integration with network rule \u2013 Snowpark Container Services\n\nFinally, grant your role permission to use this integration:\n\n```\nGRANT USAGE ON INTEGRATION spcs_demo_eai TO ROLE spcs_role;\n```\n\nGranting usage on spcs\\_demo\\_eai to spcs\\_role \u2013 Snowflake Snowpark Container Services\n\nLater, when you create a service or job, you\u2019ll reference `spcs_demo_eai (in EXTERNAL_ACCESS_INTEGRATIONS=(...))` so the container can make outbound calls. In production, you would tighten `VALUE_LIST` to specific domains to follow least-privilege networking.\n\nIf you don\u2019t need external calls, skip this; internal Snowflake data access will still work normally.\n\n### **Step 7** \u2014Re-Verify Compute Pool and Image Repository\n\nOne more quick check: make sure to re-run `SHOW COMPUTE POOLS;` and `SHOW IMAGE REPOSITORIES;` to confirm your pool is running and registry is available. In Snowflake\u2019s web UI (Snowsight), you can also browse the **Compute** > **Pools** page to see your pool and the **Stages** .\n\nAlso, confirm which account/region you\u2019re on:\n\n```\nSELECT CURRENT_ACCOUNT(), CURRENT_REGION();\n```\n\nIf you created a public endpoint (the BIND SERVICE ENDPOINT grant), check under API Endpoints to make sure your role can bind one.\n\nAlso, make sure to check the network rule by heading over to **Admin > Security > Network Rules** .\n\nVerifying Snowflake Network rule \u2013 Snowflake Snowpark Container Services\n\n### **Step 8** \u2014Authenticate Docker to Snowflake\n\nNow, switch over to your local machine to prepare the Docker image. First things first, open your Docker Desktop app and also make sure your Snowflake CLI is successfully configured.\n\nThen authenticate your Docker client to Snowflake\u2019s registry. Snowflake CLI makes this easy:\n\n```\nsnow spcs image-registry login\n```\n\nLogging into Snowflake image registry via SnowCLI \u2013 Snowpark Container Services\n\nOr, you can retrieve the login credentials and run:\n\n```\ndocker login <org>-<acct>.registry.snowflakecomputing.com -u <username> -p <password>\n```\n\nOnce logged in, your Docker CLI can pull/push images to your Snowflake registry host.\n\n### **Step 9** \u2014Build and Test a Docker Image\n\nPrepare your application code and [Dockerfile](https://docs.docker.com/build/concepts/dockerfile/) .\n\nLet\u2019s create a simple Dockerfile for a test application. For this example, we will create a very basic Python app that echoes a simple message:\n\n```\n# Dockerfile\nFROM python:3.9-slim\nCOPY hello.py /app/hello.py\nENTRYPOINT [\"python\", \"/app/spcs_demo_app.py\"]\n```\n\nAnd spcs\\_demo\\_app.py:\n\n```\nprint(\"Hello from Snowpark Container Services (SPCS) !!!!\")\n```\n\nDOCKERFILE and Python script content \u2013 Snowflake Snowpark Container Services\n\nBuild and run it locally:\n\n```\ndocker build -t spcs_demo_app-app:latest .\ndocker run --rm spcs_demo_app-app:latest\n```\n\nBuilding and Running Docker image locally for testing \u2013 Snowpark Container Services\n\nIf this runs correctly, you\u2019ve got a working container image.\n\n### **Step 10** \u2014Tag and Push the Container Image\n\nNow tag the image for the Snowflake registry and push it. Suppose your org/account is `<myorg-myacct>` , your database is `spcs_demo_db` , schema public and repository `spcs_demo_repo` . Do:\n\n```\ndocker tag spcs_demo_app-app:latest \n\t<myorg-myacct>.registry.snowflakecomputing.com/spcs_demo_db/public/spcs_demo_repo/spcs_demo_app-app:latest\n\ndocker push <myorg-myacct>.registry.snowflakecomputing.com/spcs_demo_db/public/spcs_demo_repo/spcs_demo_app-app:latest\n```\n\nTagging Docker image for Snowflake registry \u2013 Snowflake Snowpark Container Services\n\nDocker will upload the image layers to Snowflake\u2019s registry. (Behind the scenes, Snowflake stores it in a Stage).\n\nThere\u2019s no visible \u201c **registry UI** \u201d, but you can verify from Snowflake:\n\n```\nSHOW IMAGES IN IMAGE REPOSITORY spcs_demo_repo;\n```\n\nThis should list your pushed image ( `<app>:latest` ) and a digest. (Keep track of the image name and tag for the next step).\n\nListing images in spcs\\_demo\\_repo repository \u2013 Snowflake Snowpark Container Services\n\n### **Step 11** \u2014Define the Service Specification (YAML)\n\nNow that we have image in place, let\u2019s write a service specification that tells Snowflake how to run it. In Snowpark Container Services, services are defined using YAML files. These YAML files set up all the necessary parameters and configurations required to run the containers within your Snowflake account. While these YAML files support a [wide range of configurable parameters](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/specification-reference) , we will only cover a subset of them here.\n\nFor our example, say we want one container (port 80) and expose it on endpoint spcs-demo-endpoint. A minimal spec might be:\n\n```\nspec:\n\t containers:\n\t- name: spcs\n\t\timage: <myorg-myacct>.registry.snowflakecomputing.com/spcs_demo_db/public/spcs_demo_repo/spcs_demo_app-app:latest\n\t\t ports:\n\t\t\t - containerPort: 80\n\t\tenv:\n\t\t\t- name: SNOWFLAKE_WAREHOUSE\n\t\t\t\t value: spcs_demo_wh\n\tendpoints:\n\t- name: spcs-demo-endpoint\n\t\tport: 80\n\t\tpublic: true\n```\n\nSave this to a local file, `service_spec.yaml` . Here\u2019s what it says:\n\n* Under containers, we define a container named hello using our image. We list its ports so Snowflake knows it listens on port 80. We set an environment variable `SNOWFLAKE_WAREHOUSE` , which in this case tells the container which Snowflake warehouse to use (Snowflake injects all the credentials automatically).\n* Under endpoints, we create spcs-demo-endpoint on port 80 and mark it public: true so external clients can reach it. (If you don\u2019t need external access, omit or set public: false).\n\nYou could add a readinessProbe or commands, but for a simple container, this is enough.\n\n> For reference, check out [Snowflake\u2019s docs](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/working-with-services) .\n> \n> \n\nNow that the service\\_spec file is updated, we need to push it to our Snowflake Stage so that we can reference it next in our create service statement. We will use SnowCLI to push the YAML file.\n\nTo do so:\n\n```\ncd <path>\n\nsnow stage copy ./service_spec.yaml @specs --overwrite --connection spec\n```\n\nUploading YAML spec to Snowflake stage via SnowCLI \u2013 Snowpark Container Services\n\nNow, let\u2019s verify that our YAML was pushed successfully.\n\n```\nUSE ROLE spcs_role;\nLS @spcs_demo_db.public.specs;\n```\n\nListing files in specs stage for verification \u2013 Snowflake Snowpark Container Services\n\n### **Step 12** \u2014Create and Run the Snowflake Service\n\nAfter successfully pushing our image and spec YAML, we will have all the necessary components uploaded to Snowflake to create our service. To create the service, you need three key components: a service name, a Snowflake compute pool for the service to run on and the spec file that defines the service. Use the following SQL to execute it:\n\n```\nCREATE SERVICE spcs_demo_service\n\t IN COMPUTE POOL spcs_compute_pool\n\tFROM @spcs\n\tSPECIFICATION_FILE = 'service_spec.yaml';\n```\n\nThis command registers your service with the name spcs\\_demo\\_service. Snowflake pulls the image, starts one container on the Snowflake compute pool node and exposes the spcs-demo-endpoint.\n\nCheck it with:\n\n```\nSHOW SERVICES \nDESCRIBE SERVICE spcs_demo_service;\n```\n\nYou should see the service in status **`RUNNING`** . If you made it public, you can invoke it from outside (Snowflake provides a DNS name or you can use CALL [SYSTEM$GET\\_SERVICE\\_LOGS](https://docs.snowflake.com/en/sql-reference/functions/system_get_service_logs) to find endpoints). For a quick test, use SnowSQL or Snowsight:\n\n```\nSELECT SYSTEM$GET_SERVICE_LOGS('spcs_demo_service');\n```\n\nOr simply do:\n\n```\nSELECT spcs_demo_service.spcs_demo_app()\n```\n\nNote that your container runs continuously. You can stop it with `ALTER SERVICE spcs_demo_service STOP;` if needed.\n\n### **Optional:** Beyond Services\u2014Jobs and Functions (Optional)\n\nOnce you have a service running, you can also use Snowpark Container Services for batch jobs and UDF-style calls.\n\n#### **1) Snowflake Job Services**\n\nIf, instead of a long-running service you want to run a one-time job, use Snowflake [EXECUTE JOB SERVICE](https://docs.snowflake.com/en/sql-reference/sql/execute-job-service) .\n\n```\nEXECUTE JOB SERVICE\n\t IN COMPUTE POOL spcs_compute_pool\n\tNAME = example_job\n\tFROM SPECIFICATION $$\n\t spec:\n\t\tcontainers:\n\t\t - name: worker\n\t\t\t image: <myorg-myacct>.registry.snowflakecomputing.com/spcs_demo_db/public/spcs_demo_repo/spcs_demo_app-app:latest\n\t\t\tenv:\n\t\t\t\tSNOWFLAKE_WAREHOUSE: spec_demo_wh\n\t\t\t args:\n\t\t\t- \"--do-something\"\t$$;\n```\n\nThis spins up the container, runs it with the given args and waits for it to finish. Snowflake treats each Snowflake `EXECUTE JOB SERVICE` as a new job instance. You can add REPLICAS = N to run multiple instances in parallel. After completion, check job status with `DESCRIBE SERVICE example_job;` or query `SPCS_GET_EVENTS()` .\n\n#### **2) Snowflake Service Functions**\n\nYou can call a service from SQL by creating a UDF that points to the service\u2019s endpoint. For example, if you have an endpoint /echo in your container, you could do:\n\n```\nCREATE FUNCTION my_echo_udf(text VARCHAR)\n\t RETURNS VARCHAR\n\tSERVICE = spcs_demo_service\n\t ENDPOINT = spcs-demo-endpoint\n\tAS '/echo';\n\nSELECT my_echo_udf('Snowpark Container Services!') as response;\n```\n\nThis makes Snowflake issue an HTTP POST to the container\u2019s /echo path, passing \u201cSnowpark Container Services!\u201d and return the result into SQL. Under the hood, Snowflake streams table data to your service as JSON.\n\nThat covers the basics. You now have a working service (or job) running as a container on Snowflake.\n\n* * *\n\n## Snowpark Container Services Cost Breakdown\n\nSnowpark Container Services costs come from three buckets (storage, Snowflake compute pool usage and data transfer) just like the rest of Snowflake.\n\n### Cost Categories in Snowpark Container Services\n\nSnowflake formally divides Snowpark Container Services costs into storage, compute and data transfer.\n\n* **Storage costs** : This includes any Snowflake storage you use (tables or stages) plus optional block storage for volumes or snapshots.\n* **Compute pool costs** : Snowpark Container Services runs your containers on compute pools \u2013 essentially clusters of VMs. You pay credits based on the number and size of those nodes. That rate depends on the instance \u201cfamily\u201d you choose (CPU vs high-memory vs GPU) and runs by the hour.\n* **Data transfer costs** : Moving data in or out of Snowflake (ingress/egress) follows Snowflake\u2019s standard rates. Snowflake also charges a small fee for data moving between compute in the same region (we\u2019ll detail that).\n\n### Snowpark Container Services **Compute Pool Costs**\n\nCompute pool is a cluster of one or more nodes running your containers. Each node is an instance with a fixed vCPU/memory profile. Snowflake assigns a **credit rate per hour** to each instance type. To get dollars per hour, multiply the credits by your Snowflake edition\u2019s price-per-credit. On-demand credits are:\n\n* **~$2** each in Standard US East;\n* **~$3** in Enterprise;\n* **~$4** in Business Critical;\n\nSnowpark Container Services offers several instance families.\n\n* **CPU instances (X64)** \u2013 general-purpose. ( `XS` , `S` , `M` , `SL` , `L` sizes.)\n* **High-Memory CPU** \u2013 more RAM for each CPU ( `S` , `M` , `SL` , `L` ).\n* **GPU instances** \u2013 for GPU-accelerated workloads (NVIDIA-based families like `NV_XS` , `NV_S` , `NV_SM` , `NV_L` , `NV_2M` , `NV_3M` , plus `Google L4/A100` variants).\n\nEach has a fixed credits-per-hour rate. From [Snowflake\u2019s consumption tables](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\n\n**\u27a5 CPU\\_X64 (standard CPU)**\n\nSnowflake Credit Table for Snowflake Snowpark Container Service Compute, CPU\n\n**\u27a5 HIGHMEM\\_X64 (high-memory CPU)**\n\nSnowflake Credit Table for Snowflake Snowpark Container Service Compute, High-Memory\n\n**\u27a5 GPU\\_NV (NVIDIA GPU & GCP)**\n\nSnowflake Credit Table for Snowflake Snowpark Container Service Compute, GPU Snowflake Credit Table for Snowflake Snowpark Container Service Compute, GPU\n\n_Example:_\n\n**Standard Plan (US-East, $2/credit):**\n\n* **CPU\\_X64\\_XS** : 0.06 \u00d7 $2 = **$0.12/hour**\n* **CPU\\_X64\\_L:** 0\\.83 \u00d7 $2 = **$1.66/hour**\n* **HIGHMEM\\_X64\\_S (Small High-Memory):** 0\\.28 \u00d7 $2 = **$0.56/hour**\n* **HIGHMEM\\_X64\\_L (Large High-Memory):** 4\\.44 \u00d7 $2 = **$8.88/hour**\n\n**GPU Instances:**\n\n* **NV\\_XS (NVIDIA T4, Tiny)** : 0.25 \u00d7 $2 = **$0.50/hour**\n* **NV\\_L (NVIDIA A100, Large)** : 14.12 \u00d7 $2 = **$28.24/hour**\n\n**Note for Enterprise or Business Critical Editions:** Replace the $2/credit rate with **$3 (Enterprise)** or **$4 (Business Critical)** and recalculate the hourly costs.\n\n**Bigger nodes burn credits faster** . There\u2019s also a small startup penalty: when a compute pool starts or resumes, Snowflake bills at least 5 minutes of credits up-front, then charges by the second thereafter. And note: you are charged for a pool as long as it\u2019s active (even if idle) so it\u2019s wise to configure AUTO\\_SUSPEND to shut nodes off when they\u2019re not needed.\n\n### Snowpark Container Services Storage and Volume Costs\n\nSnowpark Containers Service use standard Snowflake storage plus optional block storage.\n\n**\u27a5 Snowflake stage/table storage**\n\nAnything you store in a Snowflake stage or table (including container images or logs) is billed at Snowflake\u2019s normal storage rate. In US-East (Northern Virginia), standard storage is about **$23 per TB per month** (on-demand). (Rates vary by region; for example in Zurich it\u2019s **~$30 per TB per month** ). In short, _standard Snowflake data storage costs apply_ . So if you have 1 TB of images in a stage, expect roughly **$23/month** in that region.\n\n**\u27a5 Image repository**\n\nSnowpark Container Services uses a Snowflake-managed stage as the container image registry. That means your container images live in a Snowflake stage and incur normal stage storage fees. There\u2019s no special discount or charge \u2013 it\u2019s just paid as regular stage storage.\n\n**\u27a5 Logs**\n\nIf you configure services to store local container logs in Snowflake tables (so-called _event tables_ ), that data simply counts as table storage. You pay the usual storage rate on those logs.\n\n**\u27a5 Volume mounts**\n\nSnowpark lets you mount volumes into containers. If you mount a **Snowflake stage as a volume** , again you pay stage costs as above. If you mount the Snowflake **compute pool\u2019s local storage** as a volume, there\u2019s _no extra charge_ beyond the compute node cost. It\u2019s \u201cfree\u201d in the sense that you\u2019ve already paid for the node\u2019s cost.\n\n**Block storage (persistent volumes)**\n\nIf you want persistent disk beyond ephemeral, Snowpark Containers Service offers block storage volumes (backed by cloud block storage). These are metered in four ways: **Volume size (TB-month)** , **IOPS usage** , **Throughput** and **Snapshot storage.**\n\nSnowflake Snowpark Container Service Block Storage Pricing\n\nIn summary, for storage: **standard Snowflake storage** **prices apply** for anything on stages or tables. Container-specific extras come from block volumes. Those volumes are fairly expensive (on the order of $82\u2013$100 per TB-month) but give you persistent disk. Don\u2019t forget: all these storage costs run monthly (so divide by ~730 to get a \u201cper hour\u201d sense if needed).\n\nSo, a quick note on storage: if you\u2019ve got data stored on stages or tables, you\u2019ll be charged the standard rates. And if you\u2019re using containers, you\u2019ll also need to pay for block volumes on top of that. Those block volumes are fairly expensive, with a price tag of around **~$82-$100 per terabyte per month** . Just remember, these costs are monthly, so if you\u2019re looking for an hourly breakdown, you can divide by 730 to get an estimate.\n\n### Snowpark Container Services Data Transfer Costs\n\nData transfer (ingress/egress) follows Snowflake\u2019s usual rules, with a small twist for Snowpark Container Services.\n\n**\u27a5 Ingress Cost**\n\n**Ingress (loading data in)** is typically free or included in your storage/compute costs. Snowflake doesn\u2019t charge to upload data into a stage (aside from your cloud provider\u2019s charges, which Snowflake does not bill).\n\n**\u27a5 Egress to Internet or other clouds**\n\nSnowflake charges by the terabyte moved out.\n\n* On AWS, for example, it\u2019s about **$90 per TB to the public internet** (after the first 50\u202fGB which is free in AWS accounts).\n* On Azure, the outbound internet rate is about **$87.50/TB** .\n* Cross-cloud egress (from AWS Snowflake to Azure) is typically **$90\u2013$120/TB** on each end.\n\n**\u27a5 Intra-cloud (same cloud) transfers**\n\nMoving data between regions or within the same cloud has its own rates.\n\n* On AWS, Snowflake does **not** charge for data transfer within the same region (aside from a special SPCS fee, see below).\n* Transfer to a different AWS region is about **$20/TB** .\n* On Azure, inter-region (same continent) is **~$20/TB** , cross-continent up to **~$50/TB** .\n\n**\u27a5 Snowpark Container Services internal data transfer**\n\nWhen containers move data between compute (within Snowflake), Snowflake applies a nominal fee _even if staying in the same region_ . On AWS, Snowpark Container Services data transfers in the same region cost ~ **$3.07 per TB** . It\u2019s a small fee to account for internal network traffic. Also note that, Snowflake _caps_ these SPCS transfer fees: any given day, your SPCS data transfer charge will be reduced by up to 10% of that day\u2019s compute cost. In effect, you never pay SPCS transfer that exceeds 10% of compute spend, which keeps it modest.\n\nSnowflake Snowpark Container Service Data Transfer Cost, AWS Snowflake Snowpark Container Service Data Transfer Cost, Azure Snowflake Snowpark Container Service Data Transfer Cost, GCP\n\nThe true cost of Snowpark Container Services is a sum of all these. Snowflake also provides ACCOUNT\\_USAGE views (like [SNOWPARK\\_CONTAINER\\_SERVICES\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/snowpark_container_services_history) ) so you can query exactly how many credits each pool used. It also shows you any Snowpark Container Services block storage usage and data transfer costs.\n\n* * *\n\n## Quick Tips and Best Practices\n\n### 1) Check your nodes\n\nChoose the smallest instance family that meets your needs.\n\n* For light workloads or dev testing, CPU\\_X64\\_XS (1 vCPU, 6 GiB) or S (3 vCPUs) will cost less.\n* For heavy parallel tasks, larger families or GPUs may be worth it.\n\nAlways make sure to check the instance family table to see vCPU, memory and node limits. Remember each family has a max-node limit (XS family allows up to 50 nodes, GPUs often max at 10 nodes).\n\n### 2) Auto-suspend to save credits\n\nSet [AUTO\\_SUSPEND\\_SECS](https://docs.snowflake.com/en/sql-reference/sql/create-compute-pool) on your pool (and consider AUTO\\_RESUME=TRUE) so that it doesn\u2019t burn credits when idle. Just keep in mind that it takes a bit of time and credits to start nodes back up. You pay for them in the STARTING state. Usually, it\u2019s a good idea to suspend the pool after a few minutes of inactivity. This way, you avoid wasting credits, but you won\u2019t have to wait too long for it to start back up.\n\n### 3) Manage scaling carefully\n\nIf you\u2019re dealing with variable loads, bump up MAX\\_NODES, but keep MIN\\_NODES on the low side. For services that require low latency, you might keep MIN\\_NODES=1 or more can help with responsiveness. When it comes to batch jobs, you can set REPLICAS to process in parallel but make sure your pool MAX\\_NODES can accommodate it. Monitor usage with [SHOW COMPUTE POOLS](https://docs.snowflake.com/en/sql-reference/sql/show-compute-pools) and metrics from CloudWatch or Stackdriver if you\u2019re on AWS or GCP.\n\n### 4) Network and security\n\nUse Snowflake\u2019s roles and network policies to lock things down. For public services, only bind endpoints if needed. Always make sure to use a separate service role for each application, so you can revoke access easily. Be very careful of Snowflake\u2019s default outbound block, only open egress when absolutely necessary (via EAI).\n\n### 5) Secure Your Secrets\n\n**DO NOT add your secrets into your image.** Use Snowflake secrets integration. Store any API keys or passwords in Snowflake Secret objects and reference them in your service spec under containers.secrets. This injects the secret into the container (as an env var or file) without exposing it in code or image.\n\n### 6) Monitoring and logs\n\nMake use of Snowflake\u2019s built-in tables and functions for Snowpark Container Services. The views/functions like [SPCS\\_GET\\_EVENTS](https://docs.snowflake.cn/sql-reference/functions/spcs_get_events) , [SPCS\\_GET\\_LOGS](https://docs.snowflake.cn/sql-reference/functions/spcs_get_logs) and [SPCS\\_GET\\_METRICS](https://docs.snowflake.cn/sql-reference/functions/spcs_get_metrics) let you query service metrics and logs\n\n### 7) Test incrementally\n\nStart with a simple service (one container, no autoscaling) and confirm it works before adding more complexity.\n\n### 8) Always Clean up\n\nIf you run out of pool credits, you can ALTER COMPUTE POOL \u2026 SUSPEND; to stop everything. Or DROP SERVICE when done. Be very careful that services consume resources until stopped.\n\n### 10) Container Networking (private connectivity)\n\nFor enterprise use, consider using Snowflake\u2019s PrivateLink or Private Connectivity features. Snowflake now supports AWS and Azure PrivateLink for SPCS, letting containers access Snowflake without public internet.\n\nNow that you\u2019ve got the complete details on Snowpark Container Services, you\u2019re all set to start using it. You know the basics of setting it up and when to use it. Happy containerizing!\n\n## Conclusion\n\nAnd that\u2019s a wrap! Snowpark Container Services lets you run your containers inside Snowflake, so your code and data stay together under one security and governance umbrella. It\u2019s essentially a \u201cdata-center-less\u201d or \u201cserverless\u201d version of Kubernetes. Now that you\u2019ve thoroughly followed the steps above, you can now easily push images and run microservices, APIs, or ML inference without having to copy huge amounts of data to external storage. No more jumping between different platforms and environments or worrying about moving massive datasets. Snowpark Container Services has you covered.\n\nIn this article, we have covered:\n\n* What is Snowpark Container Services?\n* How Snowpark Container Services Works (Architecture & Core Components)\n* Step-by-Step Guide to Setting Up Snowpark Container Services\n* Snowpark Container Services Cost Breakdown\n* Quick Tips and Best Practices\n\n\u2026 and so much more!\n\n## Frequently Asked Questions (FAQs)\n\n**What is the difference between Snowflake and Snowpark?**\n\nSnowflake is the overall cloud data platform (storage + compute + services). **Snowpark** is Snowflake\u2019s developer framework (libraries and APIs) for running custom code inside Snowflake. _Snowpark Container Services_ is a feature of Snowpark that allows full Docker containers in the platform, extending beyond SQL or DataFrame UDFs.\n\n**What is Snowpark Container Services?**\n\nSnowpark Container Services (SPCS) is a fully managed container runtime within Snowflake that lets you deploy and scale containerized applications without moving data. It provides compute pools, image registries and container orchestration while maintaining Snowflake\u2019s security and governance model.\n\n**Do I need a separate cloud account (AWS/GCP/Azure) to use Snowpark Container Services?**\n\nNo, Snowpark Container Services runs entirely within your existing Snowflake account. You don\u2019t need AWS, Azure, or GCP accounts to use Snowpark Container Services. Snowflake manages all the underlying cloud infrastructure.\n\n**How is Snowpark Container Services different from running containers on Kubernetes or EC2?**\n\nSnowpark Container Services is like a \u201cserverless Kubernetes\u201d for Snowflake. You don\u2019t manage the control plane, pods, or networking. Snowflake does all of that. This makes it simpler and more secure for Snowflake-centric workloads. On Kubernetes or EC2, you would set up VPCs, clusters and manage scaling yourself. With SPCS, you only specify the desired compute (pool size) and Snowflake handles the orchestration. The main limitation is less flexibility: you can\u2019t run arbitrary OS tasks outside of containers and you\u2019re limited to Snowflake\u2019s supported instance types and regions. But the big advantage is data locality and built-in Snowflake security.\n\n**Which Snowflake editions support Snowpark Container Services?**\n\nSnowpark Container Services is available on all paid Snowflake editions in commercial regions. It is not available on trial or free-tier accounts.  In 2024 it launched on AWS, then Azure GA (Feb 2025) and Google Cloud GA (Aug 2025). So if you have a standard Enterprise or Business Critical account (non-trial) on AWS, Azure, or GCP, you should have SPCS.\n\n**What is a compute pool and how to size it?**\n\nSnowflake compute pool is a collection of virtual machine nodes that run your containers. You can size it up based on your workload\u2019s CPU, memory and GPU requirements. Start with CPU\\_X64\\_S instances and scale up based on actual resource utilization rather than over-provisioning from the start.\n\n**Does Snowflake charge for idle containers?**\n\nSnowflake charges for the Snowflake **compute pool** nodes, not individual containers. Any node that exists in IDLE or ACTIVE state costs Snowflake credits. So if you have 2 nodes idling (no container processing), you still pay for them. But, you can configure the pool to auto-suspend (eliminating all nodes) after inactivity. Whenever you suspend it, there are no nodes and you aren\u2019t charged.\n\nSo, if your pool is up (even idle) you pay; if it\u2019s fully suspended, you don\u2019t.\n\n**How do containers access Snowflake data?**\n\nInside your container, you typically use Snowflake\u2019s client libraries (Snowpark, Python Connector, JDBC, \u2026) just as you would externally. Snowflake automatically provides temporary credentials to the container under the role that owns the service. That lets your container call snowflake.cursor() or snowpark.session to run queries. You can query tables, load from a stage, or write results back.\n\n**Can I mount Snowflake stages as volumes in Snowpark Container Services?**\n\nYes. Snowflake supports mounting stages as virtual volumes inside a container. When you do this, the data in the stage is available as files/directories. (Under the hood, this uses Snowflake storage and block volumes). You pay standard Snowflake storage rates for that data. The cost is just stage storage; there\u2019s no separate container volume charge.\n\n**How do I give containers internet access?**\n\nOutbound internet is **disabled** for Snowflake containers (for security). To allow outbound calls, set up a Snowflake External Access Integration (EAI) with allowed network rules pointing to those domains or IPs.\n\n**Are container images private or public?**\n\nSnowflake image registry is **private to your account** . You must authenticate with your Snowflake credentials (or token) to docker pull or docker push. Only roles/users you grant access to the repository can read it. There are no public repositories (like Docker Hub) inside SPCS. If you want to use a public base image, Snowflake behind the scenes pulls it from Docker Hub when you build locally, but when you push to Snowflake, only your Snowflake repo is used.\n\n**Can I run multiple containers per service?**\n\nYes. A single service specification can define multiple containers, but more commonly you run multiple _instances_ (replicas) of the same container.\n\n**What\u2019s the difference between a service and a job service?**\n\nA **Service** is long-lived (like a web server). Snowflake will keep it running forever until you stop it. If a container crashes or node restarts, Snowflake restarts it. You create it with CREATE SERVICE. A **job service** (or just \u201cjob\u201d) is temporary work. You start it with Snowflake EXECUTE JOB SERVICE and Snowflake runs the container(s) until they exit on their own, then it stops.\n\n**How do I secure secrets and credentials inside containers?**\n\nDO NOT insert secrets into the Docker image. Instead, use Snowflake\u2019s Secret objects and the service spec\u2019s containers.secrets feature. As Snowflake docs explain, you can first [CREATE SECRET](https://docs.snowflake.com/en/sql-reference/sql/create-secret) (to store, say, an API key). Then in your YAML spec under the container, add:\n\n```\ncontainers:\n- name: main\n\t image: ...\n\t secrets:\n\t - name: MY_SECRET \n\t\t object_type: 'SECRET'\n\t\tobject_name: 'SNOWFLAKE_SECRET'\n....\n```\n\n**How do I roll back to a previous image in Snowpark Container Services?**\n\nSnowpark Container Services doesn\u2019t have a one-click rollback feature, but you can revert by re-deploying the service with an older image tag.\n\nRelated posts:\n\n* [Kubernetes pods vs containers: 4 key differences and how they work together](https://www.flexera.com/blog/finops/kubernetes-architecture-kubernetes-pods-vs-containers-4-key-differences-and-how-they-work-together/ \"Kubernetes pods vs containers: 4 key differences and how they work together\")\n* [AWS cost optimization tools and tips: Ultimate guide [2025]](https://www.flexera.com/blog/finops/aws-cost-optimization-8-tools-and-tips-to-reduce-your-cloud-costs/ \"AWS cost optimization tools and tips: Ultimate guide [2025]\")\n* [Optimize cloud costs: Using automation to avoid waste](https://www.flexera.com/blog/finops/optimize-cloud-costs-using-automation-to-avoid-waste/ \"Optimize cloud costs: Using automation to avoid waste\")\n* [FinOps for AI: Governing the unique economics of intelligent workloads](https://www.flexera.com/blog/finops/finops-for-ai-governing-the-unique-economics-of-intelligent-workloads/ \"FinOps for AI: Governing the unique economics of intelligent workloads\")\n* [The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)](https://www.flexera.com/blog/finops/the-practical-finops-roadmap-series-what-to-do-before-you-start-practicing-finops-1-4/ \"The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)\")\n\n### Want to know more?\n\nTechnology is evolving rapidly\u2014and it's important to stay on top of the latest trends and critical insights. Check out the latest blogs related to FinOps below.\n\nFinOps\n\n## [2025 State of the Cloud](https://info.flexera.com/CM-REPORT-State-of-the-Cloud?lead_source=Website%20Visitor&id=Blog-Resources \"2025 State of the Cloud\")\n\nMarch 12, 2024\n\nFinOps\n\n## [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Request \"Cloud Cost Optimization demo\")\n\nFebruary 22, 2023\n\nFinOps\n\n## [Practical Guide for a Successful Cloud Journey](https://info.flexera.com/CM-GUIDE-Successful-Cloud-Journey?lead_source=Website%20Visitor&id=Blog-Resources \"Practical Guide for a Successful Cloud Journey\")\n\nFebruary 9, 2022\n\nFinOps\n\n## [Cloud Migration and Modernization Datasheet](https://www.flexera.com/sites/default/files/datasheet-foundation-cloudscape.pdf \"Cloud Migration and Modernization Datasheet\")\n\nFinOps\n\n## [FinOps enters its technology value era: Insights from the State of FinOps 2026](https://www.flexera.com/blog/finops/finops-enters-its-technology-value-era-insights-from-the-state-of-finops-2026/ \"FinOps enters its technology value era: Insights from the State of FinOps 2026\")\n\nFebruary 20, 2026\n\nFinOps\n\n## [Agentic FinOps for AI: autonomous optimization for Snowflake, Databricks and AI cloud costs](https://www.flexera.com/blog/finops/agentic-finops-for-ai-autonomous-optimization-for-snowflake-databricks-and-ai-cloud-costs/ \"Agentic FinOps for AI: autonomous optimization for Snowflake, Databricks and AI cloud costs\")\n\nFebruary 12, 2026\n\n\u00d7\n\nGet updates delivered to your inbox\n\nSubscribe\n\n## How can we help?\n\nSales Team\n\n[Community](https://community.flexera.com/s/)\n\nSubscribe\n\nFlexera\n\n* [](https://www.linkedin.com/company/flexera?elqTrackId=62e00a6465d449b0824c83c70706dff9&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"LinkedIn\")\n* [](https://twitter.com/flexera?elqTrackId=ab8f06bd7aea498e807592d19ac2ab00&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"Twitter\")\n* [](https://www.instagram.com/weareflexera?elqTrackId=fcfa0064605a42baaebfabe8fedd5c50&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"Instagram\")\n* [](https://www.youtube.com/user/FlexeraSoftware?elqTrackId=c6e9107020754655a13aca3ac7aa3cd4&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"YouTube\")\n\n* Privacy policy\n* Terms and conditions\n* Site map"
      ],
      "full_content": null
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/native-apps/container-cost-governance",
      "title": "Costs associated with apps with containers | Snowflake Documentation",
      "publish_date": null,
      "excerpts": [
        "Developer Snowflake Native App Framework Costs associated with apps with containers\n\n# Costs associated with apps with containers \u00b6\n\n Feature \u2014 Generally Available\n\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\n\nThis topic describes the costs associated with developing, publishing and using a\nSnowflake Native App with Snowpark Container Services. It contains information for both providers and consumers.\n\n## Costs to consumers \u00b6\n\nA Snowflake Native App may incur costs in the consumer account. The total cost of running\na Snowflake Native App with Snowpark Container Services is determined by the following:\n\n* Costs determined by the provider\n* Infrastructure costs\n\n### Costs determined by the provider \u00b6\n\nA provider may monetize a Snowflake Native App using any of the paid listing pricing models that are available in the Snowflake Marketplace. These models include subscription based and usage based plans.\n\nThis cost to the consumer is determined by the provider. Consumers pay for provider software via the Snowflake Marketplace in addition to costs associated with running Snowflake\ninfrastructure, including warehouses and compute pools.\n\n### Infrastructure costs \u00b6\n\nAll infrastructure costs, including those related to compute pools, warehouse compute, storage, and\ndata transfer are the responsibility of the consumer of a Snowflake Native App.\n\nA consumer can use the IN ACCOUNT clause of the SHOW COMPUTE POOLS command to see all compute pools in their account\nand the current state of the compute pool. Costs are not incurred when a compute pool is suspended.\n\nA Snowflake Native App with Snowpark Container Services requires at least one compute pool and might require multiple compute pools to run as\nintended. A consumer has full control over the compute resources that the app requires, and may suspend a\ncompute pool or drop an application at any time.\n\nSeparate charges for compute pool compute related to the Snowflake Native App with Snowpark Container Services appear on the customer billing\nstatement. A consumer can determine the compute pool billing charges for a Snowflake Native App with Snowpark Container Services using the ACCOUNT USAGE views provided by\nSnowpark Container Services.\n\nFor more details, such as the consumption table for compute pools, contact your account representative.\n\n## Costs to providers \u00b6\n\nProviders can also incur costs when developing and maintaining a Snowflake Native App with Snowpark Container Services, including the\nfollowing:\n\n* Providers incur Snowpark Container Services compute costs associated with both initial development and\n  ongoing testing and support for their Snowflake Native App. The compute cost may be controlled through\n  orchestration of compute pools during provider-side development and testing.\n* The storage of container images can incur costs when a provider creates a new version or patch of\n  a Snowflake Native App with Snowpark Container Services. In this context, the Docker images that the app requires are copied into an image\n  repository that is not directly accessible or observable by the provider or the consumer.\n  \n  Services in the consumer account are created from the versioned images that are stored in this\n  repository. Providers are responsible for the storage costs for the images in this stage, which\n  appear on their Snowflake bill. These costs are aggregated with other storage costs that their account incurs.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Costs to consumers\n2. Costs to providers\n\nRelated content\n\n1. About the Snowflake Native App Framework\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 4
    }
  ]
}
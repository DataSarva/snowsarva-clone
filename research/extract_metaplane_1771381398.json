{
  "extract_id": "extract_2ac88a00e99a4dda8b916a4bdc9a6b3c",
  "results": [
    {
      "url": "https://www.metaplane.dev/blog/10-ways-to-optimize-and-reduce-your-snowflake-spend",
      "title": "10 ways to optimize (and reduce) your Snowflake spend in 2025 | Metaplane",
      "publish_date": "2023-05-23",
      "excerpts": [
        "Get the essential data observability guide\n\nDownload this guide to learn:\n\nWhat is data observability?\n\n4 pillars of data observability\n\nHow to evaluate platforms\n\nCommon mistakes to avoid\n\nThe ROI of data observability\n\nUnlock now\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\nAssess your company's data health and learn how to start monitoring your entire data stack.\n\nBook free workshop\n\nSign up for news, updates, and events\n\nSubscribe for free\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\nFree\n\nGetting started with Data Observability Guide\n\nMake a plan to implement data observability across your company\u2019s entire data stack\n\nDownload for free\n\nFree\n\nBook a data observability workshop with an expert.\n\nAssess your company's data health and learn how to start monitoring your entire data stack.\n\nBook free workshop\n\nAll Blog Data Observability Data Quality Data Culture Announcements Changelog Case Studies\n\n# 10 ways to optimize (and reduce) your Snowflake spend in 2025\n\nSnowflake costs can creep up in tons of different ways. The good news? That means there's lots of ways to reduce your Snowflake cost, too. Here are ten ways you can bring down your costs in Snowflake.\n\nBy\n\nWill Harris\n\nand\n\nFebruary 27, 2025\n\nWill Harris\n\nWriter / Data\n\nFebruary 27, 2025\n\nLet's face it\u2014Snowflake costs can sneak up on you. One day you're celebrating the power and flexibility of your data warehouse, and the next you're explaining to leadership why this month's bill is double what you forecasted.\n\nAs data engineers, we're caught in the middle: we need to provide powerful analytics capabilities for our organizations while keeping costs under control. I've seen teams struggle with this balancing act, which is why I've put together this guide based on real-world experience and best practices.\n\n## Understanding what components make up your Snowflake cost\n\nBefore diving into optimization strategies, let's break down what you're actually paying for.\n\nSnowflake\u2019s pricing model includes three main categories: compute resources, storage costs, and data transfer. Each category has unique characteristics affecting the total cost.\n\n**Compute Resources** : A significant portion of Snowflake costs stems from compute resources, particularly virtual warehouses. These virtual warehouses execute queries and load data, operating independently, which means their usage patterns can significantly affect the monthly bill.\n\n**Storage Costs** : Storage fees are another critical component. The typical storage cost per terabyte (TB) is about $23, but this can vary depending on the region and account type. Effective management of data storage can lead to substantial savings.\n\n**Data Transfer** : Although data ingress (upload) into Snowflake is free, data egress (download) across regions incurs charges. Understanding these costs is crucial for optimizing Snowflake spend.\n\nLet\u2019s dive a bit deeper into each one.\n\n### **Compute Resources**\n\nCompute resources are central to Snowflake\u2019s data processing. They\u2019re also typically where most of your bill comes from. Virtual warehouses execute queries and load data, with costs varying based on:\n\n\\- **Warehouse size** : From XS ($2/credit) to 4XL+ ($128/credit)\n\n\\- **Running time** : Per-second billing with a 60-second minimum\n\n\\- **Credit costs** : Ranging from $2.50 to $6.50 depending on your plan\n\n\\- **Concurrency** : Multi-cluster warehouses spinning up additional resources\n\nThe key insight here: an idle warehouse is literally burning money. Those warehouses running longer than needed for \"just in case\" scenarios? They're quietly draining your budget.\n\nRewriting queries and restricting access to larger warehouses are effective strategies. Optimizing query logic and controlling warehouse size can significantly reduce unnecessary compute costs. The pay-per-second pricing model, with credit costs from $2.50 to $6.50, highlights the need for careful management.\n\nResource monitors can help track and limit warehouse usage. Setting up resource monitors to alert on excessive usage or automatically suspend idle warehouses can prevent runaway costs and improve cost efficiency.\n\nUnderstanding and managing compute resources is key to optimizing Snowflake spend. These strategies help control costs and ensure efficient resource use.\n\n### **Storage costs**\n\nStorage costs in Snowflake are more predictable, but they can still sneak up. Here\u2019s how it works:\n\n* **Standard rate** : ~$23 per TB per month (varies by region)\n* **Time-travel and fail-safe storage** : Additional costs for keeping historical data\n* **Regional variations** : Can reach $50.50/TB in Zurich or $40/TB in Washington\n\nA quick calculation: 10TB of data in EST of the US = ~$230/month. It\u2019s not bank-breaking, but it scales with your data footprint, hence how it sneaks up.\n\nFrequent data load operations can increase storage consumption and overall costs. Managing data load frequency and volume is important for cost optimization. Tracking storage usage and minimizing unnecessary data can significantly reduce costs.\n\n### **Data transfer**\n\nData transfer costs in Snowflake are incurred when moving data across regions within the same cloud platform. These charges are based on a per-byte fee and can vary by region.\n\nData transfer costs are often overlooked until they surprise you on the bill. Here\u2019s how it works:\n\n* **Inbound data** : Free (a rare freebie in cloud services)\n* **Cross-region transfers** : Pay per byte moved\n* **Outbound data** : Charges apply when moving data out\n\nUnderstanding and efficiently planning data transfers can help manage and control overall Snowflake spend. Minimizing unnecessary data movement and leveraging Snowflake\u2019s features can optimize data transfer processes.\n\n## Best practices for optimizing your Snowflake spend and reducing costs\n\nOnce you understand where your Snowflake costs come from, you can start implementing strategies to optimize them. Here are the most effective ways to cut down on unnecessary spending:\n\n### Set up resource monitors (your first line of defense)\n\nResource monitors are non-negotiable for cost management. Think of them as the circuit breakers in your Snowflake house. I've seen teams burn through their entire quarterly budget in a single weekend because of a runaway query\u2014without monitors, you're flying blind.\n\nHere's a basic monitor that will save you countless headaches:\n\n```sql\n\nCREATE OR REPLACE RESOURCE MONITOR monthly\\_limit\n\nWITH CREDIT\\_QUOTA = 1000\n\nFREQUENCY = MONTHLY\n\nSTART\\_TIMESTAMP = IMMEDIATELY\n\nTRIGGERS ON 75 PERCENT DO NOTIFY\n\nON 90 PERCENT DO NOTIFY\n\nON 100 PERCENT DO SUSPEND;\n\n```\n\nTake it up a notch by setting separate monitors for different environments. For critical production workloads, consider using `SUSPEND\\_IMMEDIATE` at 100% instead of just `SUSPEND`\u2014this prevents half-completed queries from consuming extra credits. Create more aggressive monitors for development warehouses (50/75/90%) to catch issues early.\n\n### Implement auto-suspend and auto-resume (stop paying for idle time)\n\nThe default auto-suspend in Snowflake is 5 minutes\u2014in the cloud world, that's an eternity of burning money for nothing. A single medium warehouse left running idle costs ~$2/hour. Multiply that across multiple warehouses, and you're looking at thousands in waste annually.\n\nThis simple change makes a huge difference:\n\n```sql\n\nALTER WAREHOUSE analytics\\_wh\n\nSET AUTO\\_SUSPEND = 60;\n\n```\n\nFor development or intermittent-use warehouses, you can be even more aggressive (30 seconds). For BI tools that have constant but sporadic queries, you'll want to find the sweet spot between frequent suspend/resume overhead and idle time. Keep an eye on your warehouse \"cold start\" frequency\u2014if users complain about slowness, you might need to balance auto-suspend time with user experience.\n\nChanging from 5 minutes to 1 minute can save you serious money on your compute bill.\n\n### Right-size your warehouses (stop overprovisioning)\n\nBigger isn't always better. Too many teams default to XL or larger warehouses for everything, burning money with minimal performance gains. Each warehouse size doubles the cost\u2014rightsizing can literally cut your bill in half while maintaining nearly identical performance.\n\nFor BI dashboards and reporting, start with XS/S and only scale up if you consistently hit >60% resource utilization. For ELT processes, test representative loads at different sizes to find the sweet spot (often M). For data science workloads, consider dedicated warehouses that scale up when needed and down when idle.\n\nWant to get scientific about it? Use the `QUERY\\_HISTORY` view to analyze your patterns:\n\n```sql\n\nSELECT WAREHOUSE\\_SIZE,\n\nAVG(EXECUTION\\_TIME)/1000 as AVG\\_EXECUTION\\_SECONDS,\n\nCOUNT(\\*) as QUERY\\_COUNT\n\nFROM SNOWFLAKE.ACCOUNT\\_USAGE.QUERY\\_HISTORY\n\nWHERE START\\_TIME > DATEADD(month, -1, CURRENT\\_TIMESTAMP())\n\nAND WAREHOUSE\\_NAME = 'ANALYTICS\\_WH'\n\nGROUP BY WAREHOUSE\\_SIZE\n\nORDER BY AVG\\_EXECUTION\\_SECONDS;\n\n```\n\nThis helps you quantify the actual performance difference between sizes for your specific workloads. If there isn\u2019t a noticeable difference in performance, downsize and save.\n\n### Master query caching effectively\n\nSnowflake's 24-hour result cache is magical when used correctly. The best part? You don't pay compute credits for cached results. Teams that structure their queries for caching can see huge reductions in compute costs with little to no performance loss.\n\nStandardize dashboard queries to use identical SQL patterns whenever possible. Use parameterized queries in your BI tools\u2014different parameter values still use the cache. Avoid cache-busting patterns like `CURRENT\\_TIMESTAMP()` or other volatile functions, `RANDOM()` or non-deterministic functions, and session-specific variables that change query signatures.\n\nFor operations that require fresh data but have expensive joins/aggregations, try this two-step approach:\n\n```sql\n\n\\-- Step 1: Materialized fresh data in temp table\n\nCREATE OR REPLACE TEMPORARY TABLE fresh\\_daily\\_sales AS\n\nSELECT \\* FROM sales WHERE date = CURRENT\\_DATE();\n\n\u200d\n\n\\-- Step 2: Join with expensive but unchanging dimensions (this can be cached!)\n\nSELECT d.region, d.product\\_category, SUM(s.amount)\n\nFROM fresh\\_daily\\_sales s\n\nJOIN dim\\_product d ON s.product\\_id = d.product\\_id\n\nGROUP BY 1, 2;\n\n```\n\n### Implement query timeouts to prevent runaway queries\n\nWe've all been there\u2014you write a query, hit execute, and suddenly realize you forgot a `WHERE` clause on a billion-row table. Query timeouts are your safety net. A single runaway query can consume hundreds or thousands of credits before someone notices and kills it.\n\nSet reasonable timeouts at different levels depending on your needs:\n\nAccount-wide default (requires admin):\n\n```sql\n\nALTER ACCOUNT SET STATEMENT\\_TIMEOUT\\_IN\\_SECONDS = 14400; -- 4 hours max\n\n```\n\nPer-user setting (great for new team members):\n\n```sql\n\nALTER USER dataengineer SET STATEMENT\\_TIMEOUT\\_IN\\_SECONDS = 600; -- 10 minutes\n\n```\n\nOr per-session (useful in development):\n\n```sql\n\nALTER SESSION SET STATEMENT\\_TIMEOUT\\_IN\\_SECONDS = 600;\n\n```\n\nDifferent user types need different timeouts. For analysts and BI users, 5-15 minutes is usually sufficient. Data engineers running batch processes might need 30-60 minutes. Data scientists typically need around 30 minutes for interactive queries, longer for scheduled jobs.\n\nUse shorter timeouts during development, longer ones in production. Most bad queries show themselves quickly\u2014better to fail fast and fix than to let them drain your credits.\n\n### Leverage data observability for cost insights\n\nData observability tools (like [Metaplane](https://www.metaplane.dev/platform-overview) !) help you peek into the black box of credit consumption. With our Spend Analysis feature, you can immediately see your **daily total credit spend** , a **30-day spend aggregation** , and **your daily spend broken down by warehouse** and user.\n\nWarehouse and user-level credit consumption helps identify which teams or processes are driving costs. Query pattern analysis finds optimization opportunities in repeatedly executed expensive queries.\n\nMetaplane\u2019s spend analysis dashboard is powered by the same machine learning capabilities that power the rest of our monitors. This can help you catch and understand abnormal spikes or drops in credit usage. Your team will be able to:\n\n**Capture upstream issues** : For example, a misfiring pixel could lead to thousands of duplicate events in a table, directly increasing the length and credit usage of a modeling query merging that table. This is the sort of spike the spend analysis tool is made to capture.\n\n**Confirm proper Snowflake configuration(s)** : Imagine that your team updated the `AUTO\\_SUSPEND` setting, but accidentally added an extra \u201c0\u201d, leading to longer active warehouses, and higher spend for that warehouse. Catching an issue like this is a perfect use case for spend analysis monitoring.\n\n**Set up regular optimization efforts** : Incorporate the dashboard into your optimization workflow to understand the largest contributors towards your Snowflake spend and discuss if those contributors are worth reviewing.\n\n**Improve Total Cost of Ownership** for your whole data stack: By splitting out credit usage by users and warehouses, your team is able to better understand which service accounts and/or \u201cservice\u201d compute resources should be targeted for efficiency gains.\n\nYou can even use our [Snowflake native app](https://www.metaplane.dev/blog/introducing-metaplanes-snowflake-native-app-data-quality-at-the-source) to get started\u2014paid for by all the credits you save as a result of this article.\n\n### Master zero-copy cloning for test environments\n\nZero-copy cloning is one of Snowflake's superpowers, yet so many teams don't use it. Creating full copies of databases for testing or development can double or triple your storage costs. Zero-copy cloning gives you identical environments with minimal additional storage.\n\nIt's incredibly simple to implement:\n\nFor databases:\n\n```sql\n\nCREATE DATABASE dev\\_db CLONE prod\\_db;\n\n```\n\nFor individual tables:\n\n```sql\n\nCREATE TABLE analytics.customers\\_test CLONE analytics.customers;\n\n```\n\nYou can take this further with short-lived clones for testing:\n\n```sql\n\nCREATE TEMPORARY TABLE destructive\\_test CLONE production.important\\_table;\n\n```\n\nOr use clones for point-in-time analysis:\n\n```sql\n\nCREATE TABLE finance.month\\_end\\_snapshot\n\nCLONE finance.transactions BEFORE (TIMESTAMP => 'YYYY-MM-DD 23:59:59');\n\n```\n\nFor the truly ambitious, create automated weekly refreshes of development environments:\n\n```sql\n\nCREATE TASK refresh\\_dev\\_weekly\n\nWAREHOUSE = maintenance\\_wh\n\nSCHEDULE = 'USING CRON 0 0 \\* \\* SUN America/Los\\_Angeles'\n\nAS\n\nBEGIN\n\nCREATE OR REPLACE DATABASE dev\\_db CLONE prod\\_db;\n\nEND;\n\n```\n\n### Implement custom budgets for departments and teams\n\nIf you want to change spending behavior, make it visible. Custom budgets transform abstract \"cloud costs\" into tangible metrics teams care about. Cost management works best when it's decentralized and visible.\n\nCreate separate warehouses for different teams or functions, set up custom budgets with alerts using resource monitors, and share daily spend digests with team leads. When people see the costs associated with their work, behavior naturally shifts toward efficiency.\n\nWant to get fancy with cost attribution? Try query tagging:\n\n```sql\n\n\\-- First, set up a session context variable\n\nALTER SESSION SET QUERY\\_TAG = 'department=marketing,project=campaign\\_analysis';\n\n\u200d\n\n\\-- Then query the history to allocate costs\n\nSELECT split\\_part(QUERY\\_TAG, 'department=', 2) as DEPARTMENT,\n\nsum(CREDITS\\_USED) as CREDITS\n\nFROM SNOWFLAKE.ACCOUNT\\_USAGE.QUERY\\_HISTORY\n\nWHERE START\\_TIME > DATEADD(month, -1, CURRENT\\_TIMESTAMP())\n\nAND QUERY\\_TAG IS NOT NULL\n\nGROUP BY 1\n\nORDER BY 2 DESC;\n\n```\n\nIf you wanted to get really serious about it, you could create friendly competition with monthly leaderboards showing \"most efficient queries\" and \"biggest cost savings.\" Winners get [a deviled egg bar](https://mashable.com/article/severance-food-ranked-apple-tv-plus) , maybe?\n\n### Implement smart storage management\n\nStorage might seem cheap compared to compute, but it compounds over time\u2014especially with time travel and fail-safe retention.\n\nStart with appropriate retention periods:\n\n```sql\n\nALTER TABLE large\\_logs SET DATA\\_RETENTION\\_TIME\\_IN\\_DAYS = 30;\n\n```\n\nUse transient tables for temporary data:\n\n```sql\n\nCREATE TRANSIENT TABLE staging.import\\_buffer (data VARIANT);\n\n```\n\nImplement automated cleanup:\n\n```sql\n\nCREATE TASK cleanup\\_old\\_data\n\nWAREHOUSE = maintenance\\_wh\n\nSCHEDULE = 'USING CRON 0 0 \\* \\* \\* America/Los\\_Angeles'\n\nAS\n\nDELETE FROM logs.api\\_events WHERE event\\_date < DATEADD(days, -90, CURRENT\\_DATE());\n\n```\n\nFinding your storage hogs is half the battle. This query quickly shows you where to focus:\n\n```sql\n\nSELECT table\\_catalog,\n\ntable\\_schema,\n\ntable\\_name,\n\nactive\\_bytes / (1024\\*1024\\*1024) as active\\_storage\\_gb,\n\ntime\\_travel\\_bytes / (1024\\*1024\\*1024) as time\\_travel\\_gb,\n\nfailsafe\\_bytes / (1024\\*1024\\*1024) as failsafe\\_gb,\n\n(active\\_bytes + time\\_travel\\_bytes + failsafe\\_bytes) / (1024\\*1024\\*1024) as total\\_gb\n\nFROM snowflake.account\\_usage.table\\_storage\\_metrics\n\nORDER BY total\\_gb DESC\n\nLIMIT 20;\n\n```\n\nConsider a data tiering strategy. Keep hot data in standard tables, move warm data (3+ months old) to tables with shorter retention, and archive cold data (1+ year old) with COPY INTO @stage commands to S3/blob storage.\n\n### Build a cost-conscious engineering culture\n\nTechnical solutions only go so far. The most sustainable cost savings come from building a team culture that values efficiency.\n\nMake costs visible with [dashboards](https://www.metaplane.dev/blog/build-your-perfect-data-quality-view-with-metaplane-dashboards) in common areas. Include \"efficiency impact\" in code reviews. Create a #snowflake-optimization Slack channel to share wins. Celebrate cost reductions with the same enthusiasm as feature launches. Incorporate cost efficiency into team OKRs.\n\nDevelop a \"Snowflake Cost Efficiency\" onboarding module for all new data team members covering how to use `EXPLAIN PLAN` to analyze query performance, query optimization techniques specific to Snowflake, warehouse sizing guidelines, resource monitor usage, and how to use the query history to audit your own impact.\n\nThe ROI on cultural change can be massive\u2014often dwarfing the gains from any single technical optimization.\n\n## Snowflake spend management FAQs\n\n**1\\. What is the biggest contributor to Snowflake costs?**\n\nThe largest cost driver is typically compute (virtual warehouses), which charges per-second usage based on the size and activity of the warehouse.\n\n**2\\. How can I reduce compute costs in Snowflake?**\n\nUse auto-suspend, optimize queries, right-size warehouses, and set up resource monitors to track usage.\n\n**3\\. What role does data observability play in cost management?**\n\nData observability tools like Metaplane help monitor data quality, identify inefficiencies, and provide visibility into Snowflake spend to optimize usage and prevent waste.\n\n**4\\. How do I prevent unexpected Snowflake costs?**\n\nSet up spend alerts, enforce query limits, and regularly review usage patterns to catch cost anomalies before they become a problem.\n\n**5\\. Does Snowflake charge for storing old data?**\n\nYes, Snowflake charges per terabyte for storage, so managing retention policies and removing unnecessary data can help reduce costs.\n\n## Final thoughts\n\nManaging Snowflake costs doesn't require massive architecture changes. Often, the biggest savings come from small, consistent optimizations and building visibility into your spending patterns.\n\nThe key is balancing performance needs with cost efficiency\u2014and having the right tools to make informed decisions. With resource monitors as your guardrails, proper warehouse sizing as your foundation, and data observability tools like Metaplane providing insights, you can get the most value from your Snowflake investment without breaking the bank.\n\nWant to learn how to use Metaplane to not only track your Snowflake cost, but monitor your data across your entire pipeline? [Talk to our team](https://www.metaplane.dev/book-a-demo) .\n\n## Table of contents\n\n## Tags\n\n\\# Snowflake\n\n\\# data observability tool\n\nGet the best data engineering content delivered right to your inbox\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\nDownload our \\*free\\* data observability guide.\n\nDownload now\n\nGet the 2024 essential data observability guide\n\nA detailed guide to data observability that covers proven techniques for modern data teams\n\nView the guide\n\nStart monitoring your data today, for free.\n\nData observability across the entire data stack, in minutes.\n\nBook a demo [Get started for free](https://auth.metaplane.dev/u/signup/identifier?state=hKFo2SBFamo2X2pIU1JFMHRsUExKb2pUUXB5ODMzemlVODlqdqFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIGcxa3g2Slc2YUxsbS1scjZQYzlGdHkxWGg2M1BkdmM3o2NpZNkgZ1VhMWM4MGNBcml5ZWpJa0RnNGVKNzZZSEdBanZwbjc)\n\nGet the week's best data engineering content\n\nThank you! You have been subscribed.\n\nError: something went wrong.\n\nKevin on LinkedIn\n\n[Follow](https://www.linkedin.com/in/kevinzenghu/)\n\n[Data quality != data observability \ud83d\udc47 ... 323 \u00b7 25 comments](https://www.linkedin.com/feed/update/urn:li:activity:6968938872470732800) [\ud83d\udd35 or \ud83d\udd34 ? UI or Code? ... 122 \u00b7 26 comments](https://www.linkedin.com/feed/update/urn:li:activity:7061734371635978240) [**Snowflake** and **ClickHouse** are like a Cactus and a Hedgehog... 175 \u00b7 29 comments](https://www.linkedin.com/feed/update/urn:li:activity:6983452833056468992) [Great news: **Metaplane** is announcing our fundraise \ud83c\udf89 ... 413 \u00b7 176 comments](https://www.linkedin.com/feed/update/urn:li:activity:7018605738386161664) [\ud83d\ude80 Introducing End-to-End Column-Level Lineage Visualization \ud83c\udfa2 ... 142 \u00b7 33 comments](https://www.linkedin.com/feed/update/urn:li:activity:7043687272486989824) [We just wrapped up a team retreat in a faraway land called... 81 \u00b7 4 comments](https://www.linkedin.com/feed/update/urn:li:activity:7018605738386161664)\n\n[Follow on LinkedIn](https://www.linkedin.com/in/kevinzenghu/)\n\n## Build a data culture by increasing data literacy\n\nDownload your free guide to learn about the most common reasons business stakeholders don\u2019t use business intelligence dashboards and proven tips for you and your team to improve engagement.\n\nDOWNLOAD \n\n## Submit where you want to send your free guide for driving dashboard usage!\n\n## Please check your inbox for your guide to driving dashboard usage!\n\nOops! Something went wrong while submitting the form.\n\n## How to proactively prevent incidents\n\nLearn what data regression testing is, how to implement it, and associated benefits\n\nUnderstand where and how to setup unit tests to complement your regression tests\n\nFREE DOWNLOAD \n\n## Where should we send your 1-pager on incident prevention?\n\n## Please check your inbox for your 1-pager on incident prevention!\n\nOops! Something went wrong while submitting the form.\n\n## Getting started with data observability guide\n\nLearn about concepts benefits of data observability platforms to help you find, fix, and prevent data quality issues to improve trust\n\nLearn where to focus implementation within your data stack for the fastest results - whether using an in-house or vendor solution\n\nFREE DOWNLOAD \n\n## Let us know where to send your Guide to Data Observability\n\n## Please check your inbox for your guide to data observability!\n\nOops! Something went wrong while submitting the form.\n\n## Stay updated on the latest product updates\n\nWe\u2019re hard at work helping you improve trust in your data in less time than ever. We promise to send a maximum of 1 update email per week.\n\nYour email\n\nThank you for your submission!\n\nOops! Something went wrong while submitting the form.\n\nCREATED\\_AT 2023-05-23T18:15:01Z  \nUPDATED\\_AT 2023-05-23T18:32:12Z\n\nMetaplane ensures everyone trusts the data that powers your business.  \n  \nOur best-in-class data observability platform blankets your warehouse with monitors to catch data quality issues when they happen, and provides tools to help you prevent incidents before they occur. And, best of all, getting set up is as easy as flipping a switch.  \n\u200d  \nTalk to us to learn more or [set up a free account.](https://metaplane.dev/signup)\n\nSnowflake table types: Explained Discover how to choose the right table type in Snowflake based on your specific use case and data needs. November 15, 2024 \u00b7 7 min read\n\nHow to create a table in Snowflake Tables are the backbone of your data pipeline. Learn how to create a Snowflake table using a few simple commands. November 8, 2024 \u00b7 3 min read\n\nThe Definitive Guide to Snowflake Data Lineage Learn how to extract and use data lineage in Snowflake to optimize your data pipelines and improve data engineering workflows. May 15, 2023 \u00b7 15 min read\n\nEnsure trust in data\n\n### Start monitoring your data in minutes.\n\nConnect your warehouse and start generating a baseline in less than 10 minutes. Start for free, no credit-card required.\n\nGet started for free Book a demo\n\nData observability for high-leverage data teams. Save time and preserve trust by being the first to know of data quality issues.\n\n### Contact\n\nteam@metaplane.dev\n\n### Subscribe to stay updated\n\nStay up-to-date with the latest product updates and resources.\n\nEmail address\n\nSubscribe\n\nError subscribing.\n\nSubscribed! We'll email you with updates."
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 1
    }
  ]
}

{
  "search_id": "search_7933fc3852834e03afb1da9814eaf9b4",
  "results": [
    {
      "url": "https://www.youtube.com/watch?v=rRI_d-TmKwA",
      "title": "Using a Snowflake Native App for Snowflake Cost Optimization",
      "publish_date": "2023-08-09",
      "excerpts": [
        "... snowflake.com Join the Snowflake Community: \u2192 community.snowflake.com. Using a Snowflake Native App for Snowflake Cost Optimization. 2.8K ... Aug 9, 2023 \u00b7 ... Snowflake Native App Framework - a ... Cost Optimization Best Practices: Resource Monitors and Other Cost Control Features in Snowflake."
      ]
    },
    {
      "url": "https://www.snowflake.com/en/blog/cost-management-interface-generally-available/",
      "title": "Better Control Your Snowflake Spend with the Cost Management Interface",
      "publish_date": "2024-08-19",
      "excerpts": [
        "Product\nSolutions\nWhy Snowflake\nResources\nDevelopers\nPricing\nFeatured Capabilities\nFeatured Open Source Technologies\nINDUSTRIES\nDEPARTMENTS\nEnablement Solutions\nPARTNER SOLUTIONS\nConnect\nLearn\nBuild\nLearn\nConnect\nblog\nSection Title: Category\nContent:\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\nProduct and Technology\nMay 6, 2024 | 5 min read\nSection Title: Better See and Control Your Snowflake Spend with the Cost Management Interface, Now Generally Ava...\nContent:\nSnowflake is dedicated to providing customers with intuitive solutions that streamline their operations and drive success. As part of our ongoing commitment to helping customers in this way, we're introducing updates to the Cost Management Interface to make managing Snowflake spend easier at an organization level and accessible to more roles.\nSection Title: ... > Snowsight: Your Centralized Console for Cost Management\nContent:\nThe Cost Management Interface in Snowsight (Snowflake\u2019s web interface) is the centralized console for cost stewards and platform administrators. From monitoring expenditures to identifying cost optimization opportunities, the Cost Management Interface offers a comprehensive suite of insights and tools to help you take control of your finances.\nSection Title: ... > Gaining Visibility into Your Spend at Multiple Levels\nContent:\nCost management and visibility is needed at varying levels, from an entire organizational view down to individual teams. Our latest enhancements provide visibility into your spend at both the organization and account levels, ensuring you have the insights needed to make informed decisions and seek proactive measures. Here's what you can expect:\nSection Title: ... > Public preview of Organization Overview experience\nContent:\nWe\u2019re introducing the Organization Overview experience to give you a holistic view of spend across multiple accounts. With this intuitive interface, you can quickly access vital information, including: * **Spend summary** : Instant access to essential spend KPIs, such as total spend in USD, remaining balance, average monthly cost and total number of accounts.\n**Contract overview** : Detailed insights into contract consumption and forecast, including accumulated actual spend vs. forecasted spend, contract progress details and contract length.\n**Account spend summary** : A breakdown of top accounts by spend, with statistics on accumulated spend for the time period, current month spend, last month spend and monthly average.\nSection Title: ... > General availability of Account Overview experience\nContent:\nWe're thrilled to announce the general availability (GA) of the Account Overview experience, providing comprehensive insights into account-level spending. With new key features, users can now: * **Monitor account spend** : Get an overview of your account spend and track it against set budgets.\n**Forecast spend** : Predict future expenditures against budget allocations, ensuring financial plans remain on track.\n**Identify top areas by spend** : Pinpoint top areas of expenditure, including warehouses, queries and databases by storage.\n**Optimize spend** : Use Cost Insights (GA soon) to identify opportunities for cost optimization, maximizing every dollar spent.\nSection Title: ... > Inclusive access for all\nContent:\nRecognizing that account administration is a privileged role, we're pleased to announce that the GA of Account Overview extends access to more privileges. Users with varying levels of responsibility can now benefit from our financial management capabilities. In addition to the SQL interface, budgets are also accessible to anyone with budget_admin or budget_viewer privileges in the UI. For details on minimum access privileges required, please refer to our documentation here .\nSection Title: ... > Take cost controls with Budgets\nContent:\nWe are excited to also announce the Budgets feature in GA. As part of our public preview of Budgets, we received tremendous feedback, excitement and participation from customers. To recap, the Budgets feature is a cost control that monitors Snowflake compute credits across warehouses and serverless features (auto-clustering, replication, search optimization, etc.) within a Snowflake account. You can set up a budget for the entire account, which will monitor all of the compute resources within the account for the month against the spending limit set for that account, as seen below.\nOr, you can create a budget for a group of resources (Snowflake objects) within your account, representing either a business unit, department or application, and track its spend against the spending limit allocated for that month.\nSection Title: ... > Take cost controls with Budgets\nContent:\nCustomers like Anvilogic are already using Snowflake Budgets to achieve their goals: *\u201cSecurity teams use Anvilogic to analyze hundreds of terabytes of security data daily in Snowflake. Custom Budgets help us ensure that Data Cloud expenses are predictable, avoiding surprises while balancing cost and performance. All of which supports the shift from restrictive legacy pricing models to embracing consumption-based pricing for security operations.\u201d \u2014Serban Tanasa, Principal Engineer, Anvilogic* Based on the feedback we received during public preview, Budgets in GA will offer additional capabilities, as compared to our public preview release , including: * Removing account admin restrictions from Snowsight to manage budgets. Budgets will now be accessible to anyone with budget_admin or budget_viewer privileges. Learn more about what privilege you need.\nSection Title: ... > Take cost controls with Budgets\nContent:\nWe are excited to see how customers use the new cost management features and look forward to bringing new capabilities to manage, control and optimize Snowflake spend for our customers. To get started, watch this demo on [Snowflake\u2019s cost management and spend visibility](https://www.youtube.com/watch?v=6fAEc6Cx2hk) , or dive into this video on how to [understand, control and optimize spend in Snowflake more effectively](https://www.youtube.com/watch?v=Dwf11NxNqbw) . For general best practices, check out the Definitive Guide to Managing Spend in Snowflake for a detailed overview of how to use the platform efficiently, consume sustainably and deliver maximum business value.\nSection Title: ... > Definitive Guide to Managing Spend in Snowflake\nContent:\ndownload now\nSection Title: ... > Authors\nContent:\nShruti Anand\nPD Dutta\nTim Sander\nSection Title: ... > Share Article\nContent:\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcost-management-interface-generally-available&title=Better+See+and+Control+Your+Snowflake+Spend+with+the+Cost+Management+Interface%2C+Now+Generally+Available)\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcost-management-interface-generally-available&text=Better+See+and+Control+Your+Snowflake+Spend+with+the+Cost+Management+Interface%2C+Now+Generally+Available)\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcost-management-interface-generally-available)\nSection Title: ... > Just For You\nContent:\nProduct and Technology\nSection Title: ... > Better Manage and Optimize Your Snowflake Spend In One Place With the New Cost Management I...\nContent:\nSamartha Chandrashekar | Shruti Anand | PD Dutta\nNov 2, 2023 | 5 min read\nProduct and Technology\nSection Title: ... > More Effectively Control and Limit Your Spend With Budgets\nContent:\nPD Dutta\nNov 3, 2023 | 5 min read\nSubscribe to our blog newsletter\nGet the best, coolest and latest delivered to your inbox each week\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\nSection Title: ... > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\nstart for free\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\nIndustries * Advertising, Media & Entertainment\nFinancial Services\nHealthcare & Life Sciences\nManufacturing\nPublic Sector\nRetail & Consumer Goods\nTechnology\nLearn * Resource Library\nLive Demos\nFundamentals\nTraining\nCertifications\nSnowflake University\nDeveloper Guides\nDocumentation\nPrivacy Policy\nSite Terms\nCommunication Preferences\nCookie Settings\nDo Not Share My Personal Information\nLegal\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")\nSection Title: ... > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\nSection Title: ... > Your Privacy\nContent:\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nSection Title: ... > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Details\u200e\nSection Title: ... > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details\u200e\nSection Title: ... > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details\u200e\nSection Title: ... > Targeting Cookies\nContent:\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nCookies Details\u200e\nSection Title: ... > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/getting-started-cost-performance-optimization/",
      "title": "Getting Started with Cost and Performance Optimization",
      "excerpts": [
        "Data for Breakfast Around the World\nDrive impact across your organization with data and agentic intelligence.\n[register now](https://www.snowflake.com/en/data-for-breakfast/)\n[Snowflake for Developers](https://www.snowflake.com/content/snowflake-site/global/en/developers) [Guides](https://www.snowflake.com/content/snowflake-site/global/en/developers/guides) Getting Started with Cost and Performance Optimization\nSection Title: Getting Started with Cost and Performance Optimization\nContent:\nPraveen Purushothaman\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/getting-started-cost-performance-optimization)\nSection Title: Getting Started with Cost and Performance Optimization > **Overview**\nContent:\nBy completing this guide, you will be able to understand and implement various optimization features on Snowflake.\n**Setup Environment** : Use correct roles and sample datasets to use the optimization features\n**Account Usage** : Understand purpose of Account Usage schema and use it to uncover savings opportunities\n**Warehouse Controls** : Leverage settings on a virtual warehouse to optimize usage\n**Storage** : Determine cost savings with high-churn and short-lived tables\n**Optimization Features** : Utilize Snowflake optimization features to achieve cost or performance savings\nSection Title: Getting Started with Cost and Performance Optimization > **Overview** > **Prerequisites**\nContent:\nFamiliarity with [Snowflake platform](https://docs.snowflake.com/en/user-guide/intro-key-concepts)\nBasic understanding of [micro-partitions](https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions)\n[Accountadmin](https://docs.snowflake.com/en/user-guide/security-access-control-considerations) access on a Snowflake account\nIf you do not have access to a Snowflake account, you can sign up for a [free trial](https://signup.snowflake.com/?utm_source=snowflake-devrel&utm_medium=developer-guides&utm_cta=developer-guides)\nSection Title: Getting Started with Cost and Performance Optimization > **Overview** > **What You\u2019ll Learn**\nContent:\nHow to identify optimization patterns in your Snowflake account\nHow to implement performance improvements in your Snowflake environment\nSection Title: Getting Started with Cost and Performance Optimization > **Setup**\nContent:\nThis section contains the code that needs to be executed in your Snowflake account to enable understanding of content in this guide.\n-- WAREHOUSE CREATION --\nUSE ROLE ACCOUNTADMIN;\ncreate warehouse if not exists hol_compute_wh\nwith warehouse_size='SMALL'\nwarehouse_type='STANDARD'\ninitially_suspended=TRUE\nauto_resume=FALSE\n;\nuse warehouse hol_compute_wh;\n-- DATABASE SCHEMA CREATION --\ncreate database if not exists OPT_HOL;\nuse database OPT_HOL;\ncreate schema if not exists DEMO;\nuse schema OPT_HOL.DEMO;\ncreate or replace table lineitem as select * from snowflake_sample_data.tpch_sf100.lineitem order by L_PARTKEY;\ncreate or replace table orders as select * from snowflake_sample_data.tpch_sf100.orders;\ncreate or replace table part as select * from snowflake_sample_data.tpch_sf100.part;\ncreate or replace table lineitem_cl as select * from lineitem;\nalter table lineitem_cl cluster by linear(l_shipdate);\nSection Title: Getting Started with Cost and Performance Optimization > **Setup**\nContent:\ncreate or replace materialized view lineitem_mv as\nselect\nto_char(l_shipdate,'YYYYMM') as ship_month\n,l_orderkey\n,sum(l_quantity*l_extendedprice) as order_price\n,sum(l_quantity*l_discount) as order_discount\n,order_price-order_discount as net_selling_price\nfrom\nlineitem_cl\ngroup by\nto_char(l_shipdate,'YYYYMM')\n,l_orderkey\n;\ncreate or replace table DATE_DIM\nas\nselect * from SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.DATE_DIM\n;\ncreate or replace table CATALOG_RETURNS\nas\nselect\ncr.*\nfrom\nSNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.catalog_RETURNS cr\nJOIN SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.DATE_DIM d\nON cr.cr_returned_date_sk=d.d_date_sk\nwhere\nd.d_year in (2001,2002)\n;\n```\n\nCopy\n```\nSection Title: Getting Started with Cost and Performance Optimization > **Warehouse Controls**\nContent:\nThis section covers the code for controls that can be enforced on virtual warehouses.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Warehouse Controls** > SQL\nContent:\n-- Activating a resource monitor\nalter warehouse hol_compute_wh set resource_monitor=Credits_Quota_Monitoring;\n```\n\nCopy\n```\nSection Title: Getting Started with Cost and Performance Optimization > **Account Usage Queries**\nContent:\n[Account Usage](https://docs.snowflake.com/en/sql-reference/account-usage) is a powerful tool in an administrator's toolbox to identify optimization opportunites. Apart from metadata about objects in the Snowflake account, it contains usage metrics related to all services consumed in the account - Credits, Storage, Data Transfer.\nIn addition, [Query History](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) contains all information related to a query - metrics for each operation in a query, rows scanned, warehouse used, query text etc. The below queries are examples for viewing results of different views in Account Usage schema.\nSection Title: Getting Started with Cost and Performance Optimization > **Account Usage Queries** > SQL\nContent:\n-- SETTING CONTEXT FOR THE SESSION --\nUSE ROLE ACCOUNTADMIN;\nUSE SCHEMA SNOWFLAKE.ACCOUNT_USAGE;\nuse warehouse hol_compute_wh;\n-- SAMPLE ACCOUNT USAGE QUERIES\n-- Warehouse Usage Metrics\nselect * from snowflake.account_usage.warehouse_metering_history limit 10;\n-- Access History for objects used in queries\nselect * from snowflake.account_usage.access_history limit 10;\n-- Snowpipe Usage Metrics\nselect * from snowflake.account_usage.pipe_usage_history limit 10;\n-- Storage Metrics\nselect * from snowflake.account_usage.storage_usage limit 10;\n-- Table Storage Detailed Metrics\nselect * from snowflake.account_usage.table_storage_metrics limit 10;\n```\n\nCopy\n```\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring**\nContent:\nThis section covers the code to identify high churn tables - significant DML, short lived tables - tables truncated and reloaded everyday and tables not active in the past 90 days.\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\n-- SETTING CONTEXT FOR THE SESSION ----\nUSE ROLE ACCOUNTADMIN;\nUSE WAREHOUSE hol_compute_wh;\nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\n-- Identify high churn tables or short lived tables\nSELECT\nt.table_catalog||'.'||t.table_schema||'.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Storage Usage Monitoring** > SQL\nContent:\ndatabases\nAND\n(\nchurn_pct>=40\n ... \nSection Title: Getting Started with Cost and Performance Optimization > ... > SQL > Actions from query results\nContent:\nDecide on time travel setting for high churn tables\nConsider using transient table type for high churn tables and short lived tables\nInvestigate the business value of tables that haven't been used in the last 90 days\nSection Title: Getting Started with Cost and Performance Optimization > **Automatic Clustering**\nContent:\nThis section covers [Automatic Clustering](https://docs.snowflake.com/en/user-guide/tables-auto-reclustering) . Automatic Clustering is a Snowflake managed service that manages reclustering (as needed) of clustered tables. Reclustering is the process of reordering data in tables to colocate rows that have same cluster key values, which reduces the number of micro-partitions that need to be scanned during execution of a query thereby reducing execution times and help with efficient query execution on smaller sized warehouses.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > ... > SQL > Results Screenshot\nContent:\nUnclustered table\nClustered table\nSection Title: Getting Started with Cost and Performance Optimization > **Automatic Clustering** > SQL > Outcome\nContent:\nAutomatic clustering improves query performance by scanning less data (less micropartitions). Refer to [Clustering Considerations](https://docs.snowflake.com/en/user-guide/tables-clustering-keys) for clustering considerations and choosing the right clustering key for a table.\nSection Title: Getting Started with Cost and Performance Optimization > **Materialized Views**\nContent:\nThis section covers use of [Materialized Views](https://docs.snowflake.com/en/user-guide/views-materialized) as an option to optimize Snowflake workloads. A materialized view is a pre-computed data set derived from a query specification and stored for later use. Because the data is pre-computed, querying a materialized view is faster than executing a query against the base table of the view. This performance difference can be significant when a query is run frequently or is sufficiently complex. As a result, materialized views can speed up expensive aggregation, projection, and selection operations, especially those that run frequently and that run on large data sets.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Materialized Views** > SQL > Screenshot\nContent:\nThe image below shows the use of a materialized view even if a user query does not contain the materialized view.\nSection Title: Getting Started with Cost and Performance Optimization > **Materialized Views** > SQL > Outcome\nContent:\nRefer to [Materialized Views Best Practices](https://docs.snowflake.com/en/user-guide/views-materialized) for considerations on choosing materialized views.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Query Acceleration**\nContent:\nThe query acceleration service can handle these types of workloads more efficiently by performing more work in parallel and reducing the wall-clock time spent in scanning and filtering.\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Query Acceleration** > SQL\nContent:\n-- Find Queries that could be accelerated (for cost consistency, best to find an application workload with consistent query \"templates\").\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Query Acceleration** > SQL\nContent:\n-- Isolate the application queries that can be pulled together into a single warehouse\nSELECT\nqae.*\n,qh.USER_NAME\n,qh.ROLE_NAME\nFROM\nSNOWFLAKE.ACCOUNT_USAGE.QUERY_ACCELERATION_ELIGIBLE qae\nJOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY qh\nON qh.query_id = qae.query_id\nWHERE\nqae.WAREHOUSE_NAME IN ('')\nAND USER_NAME = ''\nAND ELIGIBLE_QUERY_ACCELERATION_TIME > 120\nAND qae.START_TIME >= CURRENT_DATE() - 7\nLIMIT 1000;\nSELECT SYSTEM$ESTIMATE_QUERY_ACCELERATION('');\n```\n\nCopy\n```\nSection Title: Getting Started with Cost and Performance Optimization > **Query Acceleration** > SQL > Outcome\nContent:\nRefer to [Evaluating Cost and Performance](https://docs.snowflake.com/en/user-guide/query-acceleration-service) to understand impact of Query Acceleration on workloads in your account.\nSection Title: Getting Started with Cost and Performance Optimization > **Search Optimization**\nContent:\nThis section covers use of [Search Optimization](https://docs.snowflake.com/en/user-guide/search-optimization-service) which can significantly improve the performance of certain types of lookup and analytical queries. The search optimization service aims to significantly improve the performance of certain types of queries on tables, such as:\nSelective point lookup queries on tables\nSubstring and regular expression searches\nQueries on fields in VARIANT, OBJECT, and ARRAY (semi-structured) columns that use the following types of predicates: EQUALITY, IN, ARRAY_CONTAINS, ARRAYS_OVERLAP etc.\nQueries that use selected geospatial functions with GEOGRAPHY values\n ... \nSection Title: Getting Started with Cost and Performance Optimization > **Search Optimization** > SQL > Outcome\nContent:\nRefer to [Search Optimization Cost Estimation and Management](https://docs.snowflake.com/en/user-guide/search-optimization/cost-estimation) for Search Optimization cost management considerations.\nSection Title: Getting Started with Cost and Performance Optimization > Conclusion And Resources\nContent:\nCongratulations! You have learned about optimization features and tools to assist in your quest to optimize workloads on your Snowflake account. Apart from the features and options discussed in this guide, the below mentioned resources are worth taking a look to get guidance to optimize workloads on Snowflake.\nSection Title: Getting Started with Cost and Performance Optimization > Conclusion And Resources > What You Learned\nContent:\nHow to set up warehouse controls to optimize warehouse usage\nHow to identify savings opportunities for table storage\nHow to implement automatic clustering, materialized views, query acceleration or search optimization service to improve performance of Snowflake workloads\nSection Title: Getting Started with Cost and Performance Optimization > ... > What You Learned > Call to Action\nContent:\nDefinitive Guide to managing spend in Snowflake .\n[Snowflake Education](https://learn.snowflake.com/en/)\nProfessional Services\nUpdated Dec 20, 2025\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\n*\n*\n ... \nSection Title: Getting Started with Cost and Performance Optimization > ... > What You Learned > Call to Action\nContent:\nVirgin Islands (British) Virgin Islands (U.S.) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe\nSection Title: Getting Started with Cost and Performance Optimization > ... > What You Learned > Call to Action\nContent:\n*\nAdd me to the list to receive dedicated product updates and general availability emails.\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\nSubscribe Now\nLearn * Resource Library\nLive Demos\nFundamentals\nTraining\nCertifications\nSnowflake University\nDeveloper Guides\nDocumentation\nPrivacy Policy\nSite Terms\nCommunication Preferences\nCookie Settings\nDo Not Share My Personal Information\nLegal\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/release-notes/2025/other/2025-05-16-cost",
      "title": "May 16, 2025: Cost Management release notes",
      "excerpts": [
        "16 May 2025 \u00b7 Snowflake can now automatically detect cost anomalies based on prior levels of consumption, which simplifies the process of identifying spikes or dips in costs."
      ]
    },
    {
      "url": "https://sedai.io/blog/guide-to-optimizing-snowflake-costs-in-2025",
      "title": "How to Optimize Snowflake Costs: Best Practices for 2025 - Sedai",
      "publish_date": "2025-12-31",
      "excerpts": [
        "Menu\nPlatform\nSolutions\nResources\nCustomers\nCompany\nBook Demo Demo\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025\nContent:\nJJ\nJohn Jamie\nContent Writer\nFebruary 24, 2025\nFeatured\nSnowflake's innovative cloud data platform has revolutionized data warehousing, offering unparalleled flexibility, scalability, and performance. However, as organizations increasingly rely on Snowflake to power their data-driven initiatives, managing and optimizing costs becomes a critical concern.\nIn the rapidly evolving world of cloud computing, staying ahead of the curve is essential for maximizing the value of your Snowflake investment. As we look towards 2025, it's crucial to understand the intricacies of Snowflake's pricing model and adopt effective cost optimization strategies.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025\nContent:\nThis comprehensive guide delves into the key aspects of Snowflake cost optimization, providing actionable insights and best practices to help you navigate the complexities of cloud data warehousing. By implementing these strategies, you can significantly reduce expenses, improve performance, and ensure the long-term success of your data initiatives.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > What is Snowflake Cost Optimization?\nContent:\nSnowflake cost optimization is the strategic approach to managing resources and usage within the Snowflake platform to minimize expenses while maintaining optimal performance. It involves a deep understanding of Snowflake's unique pricing model, which is based on a combination of storage, compute, and data transfer costs.\nBy carefully analyzing and controlling credit usage, organizations can identify areas of inefficiency and implement targeted optimizations. This may include right-sizing virtual warehouses, leveraging cost-effective storage options, and minimizing data transfer costs. Effective cost optimization requires a proactive, data-driven approach that considers both current usage patterns and future growth projections.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > What is Snowflake Cost Optimization?\nContent:\nThe benefits of Snowflake cost optimization extend beyond immediate cost savings. By aligning resource allocation with business requirements, organizations can improve query performance, reduce latency, and ensure a seamless user experience. Additionally, optimizing costs enables organizations to scale their Snowflake usage sustainably, accommodating increasing data volumes and workload demands without breaking the budget.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > How to Optimize Snowflake Costs\nContent:\nA thorough examination of existing resource utilization forms the cornerstone of reducing Snowflake expenses. Delving into usage patterns uncovers inefficiencies, allowing the alignment of resource consumption with actual operational needs. Utilizing Snowflake's cost analysis tools alongside advanced third-party solutions provides a holistic view of expenditure, enabling informed decisions to eliminate wasteful practices.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > How to Optimize Snowflake Costs\nContent:\nThe optimization process unfolds through a series of tactical steps. Start by dissecting billing data to highlight areas of excessive expenditure, such as oversized virtual warehouses or overutilized serverless functions. Implement precise adjustments: calibrate warehouse sizes to reflect workload demands, employ auto-suspend capabilities to reduce idle costs, and strategize data transfers to minimize expenses. Incorporating automated cost management platforms ensures continuous oversight over spending trends and aids in maintaining budget discipline.\nAchieving optimal cost efficiency in Snowflake demands harmonizing technical refinements with vigilant monitoring and strategic planning. By consistently evolving usage strategies and staying abreast of Snowflake's pricing dynamics, organizations can secure a balance between performance and cost efficiency.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 1. Analyze Snowflake Usage Patterns\nContent:\nTo optimize Snowflake costs effectively, a detailed assessment of usage metrics is essential. Start by leveraging Snowflake's native cost insights available within Snowsight. These insights help illuminate patterns in data consumption and identify potential inefficiencies. For a broader perspective, complement these insights with third-party analytics tools that offer a more expansive view of usage trends and cost drivers.\nFocusing on high-expense elements such as virtual warehouses is crucial. Ensure that these warehouses are properly sized for their intended workloads and not left active when idle, which can lead to unnecessary costs. Similarly, evaluate the usage of serverless features, which can contribute to significant expenses if not monitored diligently. Regularly reviewing these components allows for timely adjustments, aligning resource use with actual operational needs.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 1. Analyze Snowflake Usage Patterns\nContent:\nDynamic monitoring is key to sustaining cost efficiency as workloads fluctuate. By setting up automated alerts for deviations in spending patterns, teams can swiftly address unexpected costs. This proactive approach allows for real-time adjustments to resource allocation, ensuring that Snowflake deployments remain efficient and within budgetary constraints.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 2. Right-Size Virtual Warehouses\nContent:\nOptimizing the scale of your virtual warehouses requires matching them precisely to your workload requirements. Analyze the processing needs of different tasks to determine the most efficient warehouse size\u2014avoiding excess capacity that leads to inflated costs. Consider using smaller warehouses for routine operations while reserving larger configurations for peak processing periods to ensure cost-effective resource usage.\nIncorporating automation features to manage warehouse activity is another pivotal strategy. By setting virtual warehouses to automatically enter a suspended state during periods of inactivity, you prevent the accumulation of unnecessary expenses. This automated approach ensures that resources are utilized only during active periods, aligning costs closely with actual usage.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 2. Right-Size Virtual Warehouses\nContent:\nConsistent re-evaluation of warehouse performance and operational needs is essential to maintaining cost efficiency over time. Regularly review and adjust warehouse configurations to keep pace with shifts in demand and workload intensity, ensuring that resource allocation remains both effective and economical. This proactive management not only supports financial prudence but also facilitates the agility needed to adapt to business changes swiftly.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 3. Optimize Data Storage and Transfer\nContent:\nEmploying micro-partitioning is vital for managing storage expenses effectively while boosting query performance. This feature divides data into smaller, compressed segments, reducing the storage footprint and enhancing the speed of query processing. By optimizing data layout through strategic partitioning, organizations can substantially lower storage costs and improve system efficiency.\nA thorough understanding of the regional pricing disparities within Snowflake's framework is essential for cost management. Different geographic locations may incur varying storage charges, impacting the overall expenditure for global operations. By evaluating these regional differences, organizations can make informed decisions about where to store data, ensuring alignment with financial objectives.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 3. Optimize Data Storage and Transfer\nContent:\nTo mitigate excessive data transfer expenses, especially when moving data across regions or providers, careful planning is crucial. Egress costs can accumulate quickly if data movements are not strategically managed. By minimizing unnecessary transfers and planning data egress with precision, organizations can maintain a cost-effective approach to data architecture.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 4. Implement Automated Cost Management\nContent:\nIncorporating automated solutions for cost management is essential for overseeing financial operations in Snowflake environments. Leveraging intelligent cost optimization tools allows organizations to fine-tune resource utilization effectively. These systems provide a robust framework that dynamically adjusts usage, freeing teams to devote time to strategic priorities over routine financial monitoring.\nEstablishing a system to monitor for cost anomalies is crucial. Implementing budgetary markers enables immediate notification when expenditures diverge from anticipated levels. This approach mitigates potential budget excesses and ensures unusual financial activities are swiftly identified. Such real-time alerts function as a critical safeguard, allowing for rapid interventions.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 4. Implement Automated Cost Management\nContent:\nUtilizing platforms equipped with AI-driven cost management capabilities enhances expense oversight. These solutions employ predictive modeling to continuously refine resource distribution. They adapt automatically to evolving workload demands, maintaining performance efficiency while adhering to financial targets. By automating these processes, organizations can achieve a harmonious blend of cost-effectiveness and operational responsiveness.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 5. Leverage Snowflake\u2019s Advanced Features\nContent:\nMaximizing Snowflake's advanced capabilities involves a nuanced approach to balancing performance enhancements with cost management. Employing features such as clustering keys can significantly improve query performance by arranging data more efficiently, thus speeding up retrieval times. While clustering can enhance operations, it also requires careful management to prevent unnecessary resource consumption. Selectively applying clustering keys to datasets with complex access patterns ensures that performance gains justify the associated resource use.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 5. Leverage Snowflake\u2019s Advanced Features\nContent:\nSnowflake\u2019s query acceleration service offers another layer of optimization by dynamically managing resources to expedite query processing during peak loads. This service is designed to scale compute resources intelligently, providing a boost in performance when needed most. However, it\u2019s crucial to monitor its usage closely, as the automatic scaling can lead to increased costs if not aligned with actual demand. Evaluating the specific scenarios where query acceleration significantly reduces latency ensures that its deployment remains cost-effective.\nSection Title: How to Optimize Snowflake Costs: Best Practices for 2025 > 5. Leverage Snowflake\u2019s Advanced Features\nContent:\nWhen considering the deployment of Snowflake\u2019s task automation features, such as scheduled tasks for routine data transformations, it is important to balance their convenience with potential cost implications. These tasks automate repetitive data processing workflows, reducing manual intervention and improving efficiency. Yet, they can incur ongoing charges if not optimized for frequency and resource consumption. Aligning their scheduling with actual business needs optimizes their utility while maintaining budgetary control.\n\u200d\nSection Title: ... > 1. Regularly Review Billing Insights\nContent:\nConsistent evaluation of billing information is crucial for maintaining a clear view of financial outflows in Snowflake. Utilize Snowflake's comprehensive cost analysis tools to gain insights into spending patterns and identify opportunities for cost reduction. By staying attuned to these patterns, organizations can make informed adjustments to resource deployments, ensuring financial plans are adhered to without compromising on operational efficiency.\nSection Title: ... > 2. Foster a Culture of Cost Awareness\nContent:\nInstilling a mindset focused on cost efficiency across all levels of the organization supports long-term financial health. Encourage cross-functional teams to engage in discussions about the impact of their activities on overall expenses. By integrating cost considerations into the project planning and execution phases, teams can align their efforts with broader financial goals, promoting a balanced approach to utilizing Snowflake\u2019s capabilities.\nSection Title: ... > 2. Foster a Culture of Cost Awareness\nContent:\nAs you embark on your Snowflake cost optimization journey, remember that the key to success lies in continuous monitoring, strategic adjustments, and a commitment to financial discipline. By leveraging the insights and best practices outlined in this guide, you can position your organization to maximize the value of your Snowflake investment while minimizing expenses. If you're ready to take your cloud cost management to the next level, [start a free trial or book a demo to experience Sedai's autonomous cloud optimization platform](https://app.sedai.io/signup) and let us help you unlock the full potential of your Snowflake deployment.\nSedai is the world's first self-driving cloud.\u2122 Our platform optimizes your cloud resources to reduce costs, boost performance, & improve availability. All on autopilot.\nSection Title: ... > 2. Foster a Culture of Cost Awareness\nContent:\nSOC 2 Type 2Certified\nPartnerNetwork\nAzureMarketplace\nFinOpsFoundation\nCloud NativeComputing Foundation\nTop Cloud CostOptimization Platform 2025\n[](https://www.linkedin.com/company/sedai/)\n[](https://www.youtube.com/@sedaicloud)\n[](https://x.com/sedai_io)\nPrivacy Policy Terms of Use \u00a9 Copyright Sedai Inc. 2026"
      ]
    },
    {
      "url": "https://www.flexera.com/blog/finops/snowflake-native-apps/",
      "title": "Snowflake Native Apps 101: Build and monetize data apps (2026)",
      "publish_date": "2026-01-27",
      "excerpts": [
        "SolutionsSpend management by vendor[Flexera is a Leader in 2025 cloud financial management tools](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)Discover recognized CFM vendors to watch in the 2025 Gartner\u00ae Magic Quadrant\u2122[View report](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\nProductsFlexera One[Introducing Flexera One SaaS Management](https://www.flexera.com/products/flexera-one/saas-management)Discover comprehensive SaaS visibility for taming SaaS sprawl, wasted spend and compliance risks.\n[Book your personalized demo](https://www.flexera.com/products/flexera-one/saas-management)\nIT Visibility\nITAM\nSnow Atlas\nCloud License Management\nSaaS Management\nFinOps\nCloud Cost Optimization\nCloud Commitment Management\nContainer Optimization\nVirtual Machine Optimization\nData Cloud Optimization\nApplication Readiness\nSecurity\nIntegrations\nTechnology Intelligence Platform\nAll Products\nSuccessCustomer SuccessServices & TrainingSupport[2025 Technology Intelligence Awards](https://info.flexera.com/ITAM-REPORT-State-of-IT-Asset-Management)The results are in\u2014see how our 2025 winners and honorable mentions are shaping the future of Technology Intelligence.\n[See the winners](https://www.flexera.com/customer-success/awards)\nServices\nTraining\n[Flexera support portal](https://community.flexera.com/s/support-hub)\n[Flexera product documentation](https://docs.flexera.com)\n[Snow product documentation](https://docs.snowsoftware.io/)\nTechnology Intelligence Awards\n[Flexera community](https://community.flexera.com/s/)\nResourcesResources[Flexera 2026 IT Priorities Report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)AI ROI, sustainability, cost and risk: Discover the latest IT trends shaping tomorrow\u2019s IT landscape in Flexera\u2019s 2026 IT Priorities Report.\n[View report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)\nWebinars\nVideos\nDatasheets\nWhitepapers & reports\nBlog\nCase studies\nEvents\nAnalyst research\nGlossary\nDemos & trials\nBusiness value calculator\nAboutCompanyPartnersPress centerSocial responsibility[The Flexera 2025 State of the Cloud Report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)How are GenAI rising cloud costs, security and sustainability shaping your cloud strategies in 2025? [View report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)\nAbout\nCareers\nContact us\nLeadership\nPartner program\nPartner locator\nPress releases\nArticles\nAwards\nESG\nBelonging and inclusion\n ... \nSection Title: ... > What Are Snowflake Native Applications?\nContent:\n**Snowflake Native Apps** are applications built using the Snowflake Native App Framework, which allows developers to create, test and deploy applications directly within Snowflake\u2019s Data Cloud. These Native Apps leverage Snowflake\u2019s core features like stored procedures , user-defined functions (Snowflake UDFs) and the [Snowpark API](https://docs.snowflake.com/en/developer-guide/snowpark/index) , all while keeping the data secure by running the code on your data stored in Snowflake. Native apps are available on the Snowflake Marketplace . You can discover and install them quickly, just like downloading an app on your smartphone.\n ... \nSection Title: ... > 2) **Simplified Development and Testing**\nContent:\nYou can build Snowflake Native Applications using the Snowflake Native App Framework. Snowflake Native App Framework streamlines development and testing. You can create, test and deploy Snowflake apps within Snowflake, reducing development time.\n ... \nSection Title: ... > 7) **Versioning and Patching**\nContent:\nSnowflake Native App Framework supports versioning and patching, allowing you to manage updates easily. This keeps your apps up to date.\n ... \nSection Title: ... > 10) **Source Control and Tool Integration**\nContent:\nYou can integrate with external tools like [IDEs](https://en.wikipedia.org/wiki/Integrated_development_environment) , [CI/CD pipelines](https://www.redhat.com/en/topics/devops/what-cicd-pipeline) and source control systems. This flexibility helps teams adopt DevOps practices when developing Snowflake Native Applications.\n ... \nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > ... > \u27a5 **Data Protection**\nContent:\nSnowflake Native Apps interact directly with the customer\u2019s Snowflake account. This approach keeps customer data within their environment, reducing security risks and compliance burdens. You avoid the complexity of managing sensitive data, as Snowflake\u2019s internal security and governance capability can handle it for you.\nSection Title: ... > \u27a5 **Zero Infrastructure Management and Lower Operational Costs**\nContent:\nTraditional Snowflake apps often require providers to pay for their own compute and storage. With Snowflake Native Applications, you leverage the customer\u2019s compute resources, reducing your operating costs and improving profit margins.\nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > ... > \u27a5 **Access to New Customers**\nContent:\nSnowflake Marketplace gives you exposure to a global Snowflake customer base. Here, customers can find, test and purchase your apps. This built-in distribution network simplifies how you reach and onboard users while enabling seamless deployment directly into customer environments.\n ... \nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > ... > \u27a5 **Enhanced Performance**\nContent:\nSnowflake Native apps utilize the consumer\u2019s Snowflake compute resources, which ensures optimal performance tailored to the existing workload. Consumers benefit from Snowflake\u2019s underlying scalability and processing power, resulting in faster query execution and application responsiveness\u200b.\n ... \nSection Title: ... > How Do Snowflake Native Applications Work?\nContent:\nSnowflake Native Applications leverage the *Snowflake Native App Framework* to build and deploy data-driven applications directly within the Snowflake ecosystem. These Snowflake apps harness Snowflake\u2019s core features\u2014secure data sharing, analytics, compute and governance\u2014enabling seamless integration and monetization, without requiring data to move outside the platform. The framework supports applications ranging from analytical tools to fully containerized services.\nSnowflake Native App Framework allows:\nSection Title: ... > How Do Snowflake Native Applications Work?\nContent:\nProviders to share data, business logic and application interfaces (e.g., Streamlit apps, stored procedures) using [Snowpark API](https://docs.snowflake.com/en/developer-guide/snowpark/index) , [Python](https://www.python.org/) , [SQL](https://www.w3schools.com/sql/) and [JavaScript](https://www.w3schools.com/js/) .\nApplications to be listed as free or paid offerings on the Snowflake Marketplace or shared privately with select accounts.\nDevelopers to benefit from streamlined testing environments, version control via external repositories and detailed logging for troubleshooting.\nBuilt-in support for structured and unstructured event logging to streamline troubleshooting and performance tracking.\nIntegration with Streamlit to build interactive, user-friendly visual interfaces.\nOn top of that, the Snowflake Native Framework also provides an enhanced developer experience, including:\n ... \nSection Title: ... > Architecture of the Snowflake Native App Framework\nContent:\nThe architecture of the Snowflake Native App Framework operates on a provider-consumer model:\n**Provider** \u2014 Creates and shares data and application logic using the framework.\n**Consumer** \u2014 Installs and interacts with applications shared by providers.\nSnowflake Native Applications are packaged as **Application Packages** , which contains the necessary logic, metadata and configuration to deploy a Snowflake Native App. This includes:\n**Manifest file** : Configuration details, including setup script locations and versioning.\n**Setup script** : Contains SQL commands for installation and updates.\nThe provider publishes the Snowflake Native app via:\n**Marketplace Listings** \u2014 Accessible to all Snowflake users for broad distribution.\n**Private Listings** \u2014 Targeted sharing with specific accounts across regions.\nSection Title: ... > Architecture of the Snowflake Native App Framework\nContent:\nUpon installation, Snowflake creates a corresponding database object for the app, which runs the setup script to establish necessary resources in the consumer\u2019s account. Additional configurations like logging or privilege grants can be applied post-installation.\nSnowflake Native App Architecture\n ... \nSection Title: ... > **Step 8** \u2014Adding a Streamlit App to Your Snowflake Native App\nContent:\nNow, let\u2019s integrate a Streamlit-based user interface into your Snowflake Native App. Streamlit is an open-source framework designed for building interactive data applications, offering features for data visualization and user interaction.\nHead over to your project folder and create a subdirectory named **streamlit/** . Inside the streamlit folder, create a file named **streamlit_app.py** . Then, add the following Python code to **streamlit_app.py** :\n ... \nSection Title: ... > **Step 4** \u2014Create and Configure a Public Paid Listing on the Snowflake Marketplace\nContent:\nNext, you need to decide how consumers will access your data product: choose \u201c **Free** \u201d for no-cost access, \u201c **Personalized/Limited Trial** \u201d to offer a trial version with full access upon request, or \u201c **Paid** \u201d if you plan to charge consumers directly.\nAfter setting the access type, click **Next** to generate a draft listing. Lastly, refine and configure the draft by including all necessary details to ready it for publication on the Snowflake Marketplace.\n ... \nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > Further Reading\nContent:\n[Snowflake Native Apps](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/)\n[What Are Native Apps?](https://www.snowflake.com/guides/what-are-native-apps/)\n[About the Snowflake Native App Framework](https://docs.snowflake.com/en/developer-guide/native-apps/native-apps-about)\n[Snowflake Native App Framework on AWS and Azure](https://www.snowflake.com/en/blog/native-app-framework-available-aws-azure/)\n[Introducing the Snowflake Native App Framework](https://www.snowflake.com/en/blog/introducing-snowflake-native-application-framework/)\n[Getting Started with Snowflake Native Apps](https://quickstarts.snowflake.com/guide/getting_started_with_native_apps/)\n[Snowflake Native Apps Example](https://github.com/snowflakedb/native-apps-examples)\nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > Conclusion\nContent:\nAnd that\u2019s a wrap! Snowflake Native Apps are built using the Snowflake Native App Framework. This allows developers to create, test and launch apps right in Snowflake. The framework simplifies the process of building, launching and integrating advanced tools. It ensures security and governance by tapping into the Snowflake ecosystem. For providers, these apps provide an easy way to sell their solutions on the Snowflake Marketplace, reaching thousands of customers. Meanwhile, consumers get instant access to the apps without needing a complex setup.\nIn this article, we have covered:\n ... \nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > FAQs\nContent:\n**What are Native Apps in Snowflake?**\nSnowflake Native Apps are designed specifically to operate within the Snowflake ecosystem without requiring external access or movement of sensitive data outside its environment.\n**How can I develop and test a Snowflake Native App locally?**\nDevelopers can set up their environments using tools like VSCode along with necessary extensions provided by Snowflakes such as CLI support.\n**Can I share my Snowflake Native App with other users?**\nYes! Once published on the marketplace after meeting compliance requirements.\n**Does the Snowflake Native App framework support logging and monitoring?**\nYes! Snowflake Native App framework includes telemetry tools that allow developers to monitor application performance post-deployment.\n**What is Streamlit\u2019s role in Snowflake Native apps?**\n ... \nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > FAQs\nContent:\n[Empowering users with intuitive, actionable data: introducing Data Explorer](https://www.flexera.com/blog/it-visibility/empowering-users-with-intuitive-actionable-data-introducing-data-explorer/ \"Empowering users with intuitive, actionable data: introducing Data Explorer\")\n[From Spot Eco to Flexera One Cloud Commitment Management: A new era of automated cloud cost optimization](https://www.flexera.com/blog/finops/from-spot-eco-to-flexera-one-cloud-commitment-management-a-new-era-of-automated-cloud-cost-optimization/ \"From Spot Eco to Flexera One Cloud Commitment Management: A new era of automated cloud cost optimization\")\n[The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)](https://www.flexera.com/blog/finops/the-practical-finops-roadmap-series-what-to-do-before-you-start-practicing-finops-1-4/ \"The practical FinOps roadmap series: What to do before you start practicing FinOps\nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > FAQs\nContent:\n(1/4)\")\n[FinOps and ITAM: A unified approach to optimizing technology investments](https://www.flexera.com/blog/finops/finops-and-itam-a-unified-approach-to-optimizing-technology-investments/ \"FinOps and ITAM: A unified approach to optimizing technology investments\")\n[Elevate your packaging efficiency: Introducing \u2018My Requests\u2019 in AdminStudio](https://www.flexera.com/blog/application-readiness/elevate-your-packaging-efficiency-introducing-my-requests-in-adminstudio/ \"Elevate your packaging efficiency: Introducing \u2018My Requests\u2019 in AdminStudio\")\n[What\u2019s new at Flexera: May 2025](https://www.flexera.com/blog/product/whats-new-at-flexera-may-2025/ \"What\u2019s new at Flexera: May 2025\")\n ... \nSection Title: ... > [2025 State of the Cloud](https://info.flexera.com/CM-REPORT-State-of-the-Cloud?lead_source...\nContent:\nMarch 12, 2024\nFinOps\nSection Title: ... > [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Req...\nContent:\nFebruary 22, 2023\nFinOps\nSection Title: ... > [Practical Guide for a Successful Cloud Journey](https://info.flexera.com/CM-GUIDE-Successf...\nContent:\nFebruary 9, 2022\nFinOps\n ... \nSection Title: ... > [New Flexera One Cloud Cost Optimization capabilities: Built for partners, designed for gro...\nContent:\nFebruary 24, 2026\nFinOps"
      ]
    },
    {
      "url": "https://medium.com/snowflake/snowflake-native-apps-scalability-2587dea795e0",
      "title": "Benchmarking Snowflake Native Apps | by Uli Bethke | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium",
      "publish_date": "2023-11-21",
      "excerpts": [
        "Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nSearch\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-2587dea795e0---------------------------------------)\n\u00b7\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-2587dea795e0---------------------------------------)\nBest practices, tips & tricks from Snowflake experts and community\nSection Title: Benchmarking Snowflake Native Apps\nContent:\nUli Bethke\n4 min read\n\u00b7\nNov 20, 2023\n--\nListen\nShare\nPress enter or click to view image in full size\nAs a long time user of Snowflake we are impressed with the near unlimited scalability of the Snowflake Data Cloud.\nWe were wondering if the same scalability and performance magic also applies to Snowflake Native Apps.\nIn this blog post we give a brief overview on Snowflake Native Apps, benefits and use cases.\nWe dive deeper into scalability and performance of Native Apps.\nSection Title: Benchmarking Snowflake Native Apps > What are Snowflake Native Apps?\nContent:\nA Snowflake Native App is a self-contained data application that **runs directly** within the Snowflake Data Cloud.\nSection Title: Benchmarking Snowflake Native Apps > What are the benefits of Snowflake Native Apps?\nContent:\nSnowflake Native Apps offer a number of advantages over traditional approaches to building and deploying data-driven applications, including:\n**Quick App Access via Snowflake marketplace**\nExplore and buy Snowflake Native Apps on the Snowflake Marketplace. Try a full version before purchase. Once bought, the app is instantly in your Snowflake account, bypassing long vendor wait times. Snowflake Native Apps can be installed and used with just a few clicks.\n**Data safety assured**\nInstall apps with precise role-based controls. Your data stays put, avoiding extra data storage. While the software can access your data, the provider can\u2019t. Plus, Snowflake vets each public app release for security.\n**Varied billing methods**\nSection Title: Benchmarking Snowflake Native Apps > What are the benefits of Snowflake Native Apps?\nContent:\nPay for Snowflake Native Apps through integrated billing, including credit cards and ACH. There is also the Marketplace Capacity Drawdown Program where you can use your Snowflake credits to pay for Native App usage.\n**Performance and scalability:**\nSnowflake Native Apps are built on top of Snowflake\u2019s massively scalable architecture. Just like any other workloads [Native Apps scale linearly](https://sonra.io/snowflake/snowflake-native-apps-scalability/) with your requirements. We will dive deeper into this later on in this post.\n**IP**\nThe IP of Native App providers is protected. The source code is not exposed to consumers.\nSection Title: Benchmarking Snowflake Native Apps > What are Snowflake Native App use cases?\nContent:\nThe use cases of Native Apps are only limited by your imagination. Here are some common example use cases of Native Apps\nApps for Snowflake cost management, cost monitoring, and cost optimisation\nApps for lookups, e.g. geocoding and reverse geocoding.\nApps for data profiling and data quality\nBrian Hess has put together some less obvious use cases .\nSection Title: Benchmarking Snowflake Native Apps > Scalability and performance testing of Snowflake Native Apps\nContent:\nAs a long time user of Snowflake we are impressed with the near unlimited scalability of the Snowflake Data Cloud.\nWe were wondering if the same scalability and performance also applies to Snowflake Native Apps.\nThe short answer is yes!\nSection Title: Benchmarking Snowflake Native Apps > Test setup\nContent:\nWe installed the [FlowHigh SQL Analyser Native App](https://app.snowflake.com/marketplace/listing/GZSVZSYYPCC/sonra-flowhigh-sql-analyser?shareType=application) from the Snowflake App Marketplace.\n[FlowHigh SQL Analyser](https://flowhigh.sonra.io/analyse-sql) identifies issues in SQL code. It scans the [Snowflake query history](https://docs.snowflake.com/en/sql-reference/functions/query_history) and detects suboptimal SQL and anti-patterns affecting Snowflake\u2019s performance and query speed. It also finds SQL code that\u2019s hard to read and maintain. Most importantly it identifies SQL code that may yield incorrect or unintended results. Register for FlowHigh to [detect common anti patterns in your SQL Code](https://flowhigh.sonra.io/analyse-sql) .\nFor the performance test of Snowflake Native Apps we used the following FlowHigh UDTF to scan the Snowflake query history. We passed in a different number of records for each run.\nSection Title: Benchmarking Snowflake Native Apps > Test setup\nContent:\nWe used the following query:\n```\nWITH history_subset AS (  \n  SELECT *   \n  FROM snowflake.account_usage.query_history  \n  LIMIT 1000 -- Specify the umber of records   \n)  \nSELECT QUERY_ID  \n      ,QUERY_TEXT  \n      ,AP_TYPE  \n      ,POS  \n      ,STATUS  \n      ,STATUS_MSG  \n      ,AP_QUERY_PART  \n FROM history_subset  \n      ,TABLE(CODE_SCHEMA.get_ap(QUERY_TEXT))\n```\nSection Title: Benchmarking Snowflake Native Apps > Results\nContent:\nWe used three different Virtual Warehouse (VWH) sizes: Small (S), Medium (M), and Large (L).\nPerformance results are presented in detail for each Virtual Warehouse size. The evaluation spans from 1,000 to 2,000,000 queries that we passed to the [FlowHigh SQL Analyser](https://flowhigh.sonra.io/analyse-sql) app.\nPress enter or click to view image in full size Press enter or click to view image in full size\nAs we can see the execution time is pretty similar for anything up to 100K records. A Small or even X-Small VWH would have been sufficient to run these low volume queries.\nSection Title: Benchmarking Snowflake Native Apps > Results\nContent:\nOnce we hit 1M records we see that Virtual Warehouse Sizing makes a significant difference. We now see near linear scalability across Virtual Warehouses. We can cut processing time in half at the same price point. Scalability of Native Apps is near linear. The same level of scalability we are used to from other Snowflake use cases are also available for Snowflake Native Apps. Q.E.D.\nThe big advantage of the Snowflake Native Apps framework is that you don\u2019t have to worry about the nitty gritty details of building the infrastructure for scaling your app. It is taken care of for you by the Snowflake Data Cloud.\nSnowflake\nSnowflake Native App\nData Superhero\nScalability\n--\n--\n[](https://medium.com/snowflake?source=post_page---post_publication_info--2587dea795e0---------------------------------------)\n[](https://medium.com/snowflake?source=post_page---post_publication_info--2587dea795e0---------------------------------------)\nSection Title: Benchmarking Snowflake Native Apps > Results\nContent:\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--2587dea795e0---------------------------------------)\n10.8K followers\n\u00b7 Last published 13 hours ago\nBest practices, tips & tricks from Snowflake experts and community\nSection Title: Benchmarking Snowflake Native Apps > Written by Uli Bethke\nContent:\n873 followers\n\u00b7 31 following\nFollow me for data architecture and data engineering on LinkedIn https://www.linkedin.com/in/ulibethke\nSection Title: Benchmarking Snowflake Native Apps > No responses yet\nContent:\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--2587dea795e0---------------------------------------)\n[Help](https://help.medium.com/hc/en-us?source=post_page-----2587dea795e0---------------------------------------)\n[Status](https://status.medium.com/?source=post_page-----2587dea795e0---------------------------------------)\nAbout\nCareers\nPress\n[Blog](https://blog.medium.com/?source=post_page-----2587dea795e0---------------------------------------)\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2587dea795e0---------------------------------------)\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----2587dea795e0---------------------------------------)\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2587dea795e0---------------------------------------)\nSection Title: Benchmarking Snowflake Native Apps > No responses yet\nContent:\n[Text to speech](https://speechify.com/medium?source=post_page-----2587dea795e0---------------------------------------)"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/data-collaboration-native-app/",
      "title": "Getting Started With Model Sharing Using Native Apps - Snowflake",
      "excerpts": [
        "[Cortex AI Instant access to industry-leading LLMs](https://www.snowflake.com/en/product/features/cortex/)\n[Data Clean Rooms Privacy-preserving data collaboration](https://www.snowflake.com/en/product/features/data-clean-rooms/)\n[Native Apps End-to-end, Snowflake-native app creation and distribution](https://www.snowflake.com/en/product/features/native-apps/)\n[Horizon Built-in compliance, security, privacy and access](https://www.snowflake.com/en/product/features/horizon/)\n[Marketplace Third-party data sources connected within minutes](https://www.snowflake.com/en/product/features/marketplace/)\n[Notebooks Interactive dev environment for data and AI teams](https://www.snowflake.com/en/product/features/notebooks/)\n[Snowflake ML Streamlined model development and MLOps from a centralized UI](https://www.snowflake.com/en/product/features/end-to-end-ml-workflows/)\n[Snowpark Libraries and code execution environments that run Python and more](https://www.snowflake.com/en/product/features/snowpark/)\n[Streamlit Framework for transforming Python scripts into web apps](https://www.snowflake.com/en/product/features/streamlit-in-snowflake/)\n[Unistore Unify transactional and analytical workloads in Snowflake for enhanced simplicity](https://www.snowflake.com/en/product/features/unistore)\nFeatured Open Source Technologies\n[Arctic LLM An open, efficient LLM for enterprise AI apps](https://www.snowflake.com/en/product/features/arctic/)\n[Open Catalog Manage and govern data across many engines and storage locations](https://www.snowflake.com/en/product/features/open-catalog/)\nBack\nINDUSTRIES\nAdvertising, Media & Entertainment\nFinancial Services\nHealthcare & Life Sciences\nManufacturing\nPublic Sector\nRetail & Consumer Goods\nTechnology\nTelecom\nTravel & Hospitality\nDEPARTMENTS\n[Finance](https://www.snowflake.com/en/solutions/departments/finance/)\n ... \nThe AI Data Cloud Explained Learn how to connect, share and integrate the data and apps on the AI Data Cloud\nSecurity Hub Comprehensive security through built-in features, robust cloud infrastructure protection, and more\nCost and Performance Optimization Maximize economic value through minimizing TCO and continuously optimizing price for performance\nSnowflake for Startups Startups building applications in the AI Data Cloud\nBack\nConnect\nBlog\n[Events](https://www.snowflake.com/about/events/)\n[Support](https://www.snowflake.com/en/support/)\n[Contact us](https://www.snowflake.com/en/contact/)\nLearn\nResource Library Ebooks, videos, white papers and more\n[Training Overview of Snowflake's educational offerings](https://www.snowflake.com/en/resources/learn/training/)\nWebinars Expert-led discussions and demos across industries and use cases\n[Certifications Snowflake's technical industry professional certifications](https://www.snowflake.com/en/resources/learn/certifications/)\nLive Demos Weekly product demos showcasing key features and live Q&A\n[Snowflake University Training courses for all levels, on-demand or instructor-led](https://learn.snowflake.com/en/)\nHands-On Labs Instructor-led virtual workshops for exploring key Snowflake features\nSnowflake Research Publications Academic papers written by Snowflake researchers\nFundamentals Informative articles about AI and data topics\nBack\nBuild\nSnowflake for Developers Overview of the dev resources you need to build and scale\nDeveloper Guides Reference architectures, use cases and best practices\nDownloads The latest software versions, drivers, libraries and relevant docs\nLearn\n[Documentation Reference docs, guides, tutorials and announcements](https://docs.snowflake.com/)\nOpen Source Key projects Snowflake engineers maintain and support\nBuilder Education Online and in-person classes and workshops to upskill on Snowflake\nConnect\n[Engineering Blog Snowflake\u2019s technical leaders on what, why and how they build features](https://www.snowflake.com/engineering-blog/)\n[Community Tips, tricks and discussion with fellow Snowflake developers](https://community.snowflake.com/)\nBack\nBack\nBack\nPortugu\u00eas Italiano \ud55c\uad6d\uc5b4 \u65e5\u672c\u8a9e Espa\u00f1ol Deutsch Fran\u00e7ais English\n[Snowflake for Developers](https://www.snowflake.com/content/snowflake-site/global/en/developers) [Guides](https://www.snowflake.com/content/snowflake-site/global/en/developers/guides) Getting Started With Model Sharing Using Native Apps\nSection Title: Getting Started With Model Sharing Using Native Apps\nContent:\nNative Apps\nTim Buchhorn\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/data-collaboration-native-app)\nSection Title: Getting Started With Model Sharing Using Native Apps > Overview\nContent:\nThis Quickstart guide is the second in a series of Collaboration & Applications in Snowflake. Part One of the series can be found here\nIn Part One, we showed how using Snowflake's unique collaboration features, we could:\nshare data from an organisation to another\nscore that data using an ML model\nshare it back with the original provider\nall in an automated pipeline\nIn this Quickstart, we will show how we can leverage the Native Apps Framework to simplify this process even further.\nSection Title: Getting Started With Model Sharing Using Native Apps > Overview\nContent:\nThe Native App Framework is a collaboration framework that allows providers to share data and/or related business logic to other Snowflake customers. In the previous Quickstart guide, we set up a bi-directional share between two organisations. With the Native App framework, we can simplify this relationship by allowing one party to share its logic in the form of an application to the other. Therefore, we have brought the app to the data, as opposed to bringing the data to the app\nThere are numerous benefits to leveraging the Native App framework for our Use Case. For example:\nSection Title: Getting Started With Model Sharing Using Native Apps > Overview\nContent:\nData Sovereignty and Security for the Consumer. In the previous quickstart, it was necessary for the provider to share data with another entity. This could be potentially sensitive information, and therefore a bigger hurdle for security and governance teams to approve. With Native Apps, the app logic is brought to the consumers dataset, so no customer data is leaving their security perimeter\nCompliance Risk Reduction for the Provider. Similar to the above, the provider of the model may not want to have access to customer data. This requires extra consideration with how to securely handle this data, which is not necessary if the model is shared to their customers to run against their own data. Increased Margin. By sharing the model to the consumer, the infrastructure cost to execute the model are now with the consumer. This means the provider can monetise the model, without worrying about infrastructure costs.\n ... \nSection Title: Getting Started With Model Sharing Using Native Apps > Overview > What You Will Learn\nContent:\nHow to train an ML model in Snowflake\nHow to package the ML Model in an Application Package\nHow to privately list a Native Application\nHow to Consume a Native Application\n ... \nSection Title: Getting Started With Model Sharing Using Native Apps > Business Use Case and Context\nContent:\nThe Business Use case follows a similar scenario as Part One.\nIn this guide, we are playing the role of Zamboni and Snowbank. The Credit Risk team at Snowbank has noticed a rise in credit card default rates which affects the bottom line of the business. Previously, it has shared data with an external organisation (Zamboni) who assist with analysing the data and scoring which customers are most likely to default.\nThis time, the compliance team has said that it is too risky to share this customer data with an external party without proper procedures being followed, which could take a few months to complete. Zamboni have proposed a solution that utilises the Native Application Framework.\nSince both companies use Snowflake, Zamboni has proposed sharing their proprietary Credit Default Scoring Model via the Native Application Framework. The advantages of doing this for Snowbank are:\nSection Title: Getting Started With Model Sharing Using Native Apps > Business Use Case and Context\nContent:\nNo customer data leaves Snowbank's Snowflake Account\nThe Native App runs within the Snowbank Account. Since Snowflake has been approved for use internally, it is not subject to longer onboarding and cybersecurity checks\nThe advantages for Zamboni are:\nSection Title: Getting Started With Model Sharing Using Native Apps > Business Use Case and Context\nContent:\nSimple deployment. Zamboni can simply share the model using a similar listing paradigm as sharing a dataset.\nReduced compliance risk. Zamboni's security and compliance team are similarly pleased in being able to satisfy the business needs without needing Snowbank's data hosted on their servers\nIncreased Margin. Zamboni are now able to share their proprietary logic safely, without having to provision infrastructure to run the model on behalf of Snowbank.\nSpeed to Market. Zamboni can simply distribute the model via the Native App framework in a private listing. They no longer need to set up data pipelines. They can also build once, and distribute to lots of different customers by leveraging Snowflake's multitenancy framework.\n ... \nSection Title: Getting Started With Model Sharing Using Native Apps > Setup > Initial Set Up\nContent:\nNext, open up a new worksheet and run all following steps as the ACCOUNTADMIN role\n-- Change role to accountadmin\nUSE ROLE ACCOUNTADMIN;\nNext we create two [Virtual Warehouses](https://docs.snowflake.com/en/user-guide/warehouses-overview) that can be used to load the initial dataset, and do our model training respectively.\n-- Create a virtual warehouse for loading and ad-hoc queries\nCREATE OR REPLACE WAREHOUSE QUERY_WH WITH\nWAREHOUSE_SIZE = 'X-SMALL'\nWAREHOUSE_TYPE = 'STANDARD'\nAUTO_SUSPEND = 300\nAUTO_RESUME = TRUE\nMIN_CLUSTER_COUNT = 1\nMAX_CLUSTER_COUNT = 1;\n-- Create a virtual warehouse for training\nCREATE OR REPLACE WAREHOUSE training_wh with\nWAREHOUSE_SIZE = 'MEDIUM'\nWAREHOUSE_TYPE = 'snowpark-optimized'\nMAX_CLUSTER_COUNT = 1\nAUTO_SUSPEND = 300\nAUTO_RESUME = TRUE;\n ... \nSection Title: Getting Started With Model Sharing Using Native Apps > Create Model > Provider Account (Zamboni)\nContent:\nFor this section, make sure you download the corresponding [git repo](https://github.com/Snowflake-Labs/sfguide-getting-started-with-model-sharing-using-native-app) so you have the files referenced in this section.\n ... \nSection Title: Getting Started With Model Sharing Using Native Apps > Create Model > Train and Register Model\nContent:\nOpen up the Credit Card Default Native App notebook and follow the steps. Once you have completed those, you will have trained and deployed a ML Model in Snowflake that predicts credit card default risk. You will also have registered the model in the Snowflake Model Registry.\nStay in the Zamboni account for the next step.\nSection Title: Getting Started With Model Sharing Using Native Apps > Build Native App > Provider Account (Zamboni)\nContent:\nNow we have trained the model, we want to encapsulate it in an Application Package, so we can distribute it via the Native App Framework. More information on the Native Application framework can be found in the documentation [here](https://docs.snowflake.com/en/developer-guide/native-apps/native-apps-about) . We will be working through the [Native App Development Workflow](https://docs.snowflake.com/en/developer-guide/native-apps/native-apps-workflow) .\nContinue to work through the python notebook. The SQL commands below can be run from the python notebook.\n ... \nSection Title: Check that all our artefacts have been uploaded > Perform Local Testing > Provider Account (Zamboni)\nContent:\nCOPY INTO cc_default_unscored_data\nFROM @quickstart_cc_default_unscored_data\nFILE_FORMAT = (FORMAT_NAME= 'parquet_format')\nMATCH_BY_COLUMN_NAME=CASE_INSENSITIVE;\nALTER WAREHOUSE query_wh SET warehouse_size=XSMALL;\nThis app requires the consumer to explicitly provide a reference to a table in the Snowflake instance it is installed as an input. References are a way for Native App consumers to be explicit in their permissioning to any existing object in a Snowflake account that is required. This ensures that Native Applications run as \"isolated objects\" within a Consumer Account, and must be granted explicit permission to access other objects. More information can be foind [here](https://docs.snowflake.com/en/developer-guide/native-apps/requesting-refs) . We can achieve referencing through the UI, or running the following SQL.\n ... \nSection Title: Check that all our artefacts have been uploaded > Conclusion And Resources\nContent:\nCongratulations, you have just shared an ML Model over the Snowflake Marketplace via the Snowflake Native App framework. By sharing the ML Model to the Consumer account, we have realised the following benefits:\nSection Title: Check that all our artefacts have been uploaded > Conclusion And Resources\nContent:\nData Sovereignty and Security for the Consumer. With Native Apps, the app logic is brought to the consumers dataset, so no customer data is leaving their security perimeter\nCompliance Risk Reduction for the Provider. Similar to the above, the provider of the model may not want to have access to customer data. This requires extra consideration with how to securely handle this data, which is not necessary if the model is shared to their customers to run against their own data. Increased Margin. By sharing the model to the consumer, the infrastructure cost to execute the model are now with the consumer. This means the provider can monetise the model, without worrying about infrastructure costs. Similarly the consumer can set warehouse sizes appropriate to the SLAs and budgets of the consumer organisation. Faster Onboarding.\n ... \nSection Title: Check that all our artefacts have been uploaded > Conclusion And Resources > Resources\nContent:\n[GitHub Repo for Quickstart Assets](https://github.com/Snowflake-Labs/sfguide-getting-started-with-model-sharing-using-native-app)\n[Snowflake Developer Documentation for Native Applications](https://docs.snowflake.com/en/developer-guide/native-apps/native-apps-about)\n[CLI for Native Apps](https://docs.snowflake.com/en/developer-guide/snowflake-cli/native-apps/overview)\n[Developer Guide on End-to-End Machine Learning in Snowflake](https://docs.snowflake.com/en/developer-guide/snowpark-ml/index)\nUpdated2025-12-20\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\nProduct * [Platform](https://www.snowflake.com/en/product/platform/)"
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}

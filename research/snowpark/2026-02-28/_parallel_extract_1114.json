{
  "extract_id": "extract_89b3d02ad0e5409ab146293f49a4f972",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-attributing",
      "title": "Attributing cost | Snowflake Documentation",
      "publish_date": null,
      "excerpts": [
        "Guides Cost & Billing Visibility Attributing cost\n\n# Attributing cost \u00b6\n\nAn organization can apportion the cost of using Snowflake to logical units within the organization (for example, to different\ndepartments, environments, or projects). This chargeback or showback model is useful for accounting purposes and pinpoints\nareas of the organization that could benefit from controls and optimizations that can reduce costs.\n\nTo attribute costs to different groups like departments or projects, use the following recommended approach:\n\n* Use object tags to associate resources and users with departments or projects.\n* Use query tags to associate individual queries with departments or projects when the queries are\n  made by the same application on behalf of users belonging to multiple departments.\n\n## Types of cost attribution scenarios \u00b6\n\nThe following cost attribution scenarios are the most commonly encountered. In these scenarios, warehouses are used as an\nexample of a resource that incurs costs.\n\n* **Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate\n  warehouses with a department. You can use these object tags to attribute the costs incurred by those warehouses to that\n  department entirely.\n  \n  \n* **Resources that are shared by users from multiple departments:** An example of this is a warehouse shared by users from\n  different departments. In this case, you use object tags to associate each user with a department. The costs of queries are\n  attributed to the users. Using the object tags assigned to users, you can break down the costs by department.\n  \n  \n* **Applications or workflows shared by users from different departments:** An example of this is an application that issues\n  queries on behalf of its users. In this case, each query executed by the application is assigned a query tag that identifies\n  the team or cost center of the user on whose behalf the query is being made.\n  \n  \n\nThe next sections explain how to set up object tags in your accounts and provide the details for each of these cost attribution\nscenarios.\n\n## Setting up object tags for cost attribution \u00b6\n\nWhen you set up tags to represent the groupings that you want to use for cost attribution, you should determine if the\ngroupings apply to a single account or multiple accounts. This determines how you set up your tags.\n\nFor example, suppose that you want to attribute costs based on department.\n\n* If the resources used by the department are located in a single account, you create the tags in a database in that account.\n* If the resources used by the department span multiple accounts, you create the tags in a key account in your organization (for example, in your organization account ),\n  and you make those tags available in other accounts through replication .\n\nThe next sections explain how to create the tags, replicate the tags, and apply the tags to resources.\n\n* Creating the tags\n* Replicating the tag database\n* Tagging the resources and users\n\nNote\n\nThe examples in these sections use the custom role `tag_admin` , which is assumed to have been granted the privileges to\ncreate and manage tags. Within your organization, you can use more granular privileges for object tagging to develop a secure tagging strategy.\n\n### Creating the tags \u00b6\n\nAs part of designing the strategy, decide on the database and schema where you plan to create the tags.\n\n* You can create a dedicated database and schema for the tags.\n* If you want to tag resources in different accounts across your organization, you can create the tags in a key account in your\n  organization (for example, in your organization account ).\n\nThe following example creates a database named `cost_management` and a schema named `tags` for the tags that you plan to use:\n\n```\nUSE ROLE tag_admin ; \n\n CREATE DATABASE cost_management ; \n CREATE SCHEMA tags ;\n```\n\nCopy\n\nWith `cost_management` and `tags` selected as the current database and schema, create a tag named `cost_center` and set\nthe values allowed for the tag to the names of cost centers:\n\n```\nCREATE TAG cost_center \n  ALLOWED_VALUES 'finance' , 'marketing' , 'engineering' , 'product' ;\n```\n\nCopy\n\n### Replicating the tag database \u00b6\n\nIf you have an organization with multiple accounts and you want to make the tags available in these other accounts, set up your accounts for replication , and create a replication group in a main account (for example, in the organization account ). Set up this replication group to replicate the database\ncontaining the tags.\n\nFor example, to replicate the tags to the accounts named `my_org.my_account` and `my_org.my_account_2` , execute this\nstatement in your organization account:\n\n```\nCREATE REPLICATION GROUP cost_management_repl_group \n  OBJECT_TYPES = DATABASES \n  ALLOWED_DATABASES = cost_management \n  ALLOWED_ACCOUNTS = my_org . my_account_1 , my_org . my_account_2 \n  REPLICATION_SCHEDULE = '10 MINUTE' ;\n```\n\nCopy\n\nThen, in each account in which you want to make the tags available, create a secondary replication group, and refresh this\ngroup from the primary group:\n\n```\nCREATE REPLICATION GROUP cost_management_repl_group \n  AS REPLICA OF my_org . my_org_account . cost_management_repl_group ; \n\n ALTER REPLICATION GROUP cost_management_repl_group REFRESH ;\n```\n\nCopy\n\n### Tagging the resources and users \u00b6\n\nAfter creating and replicating the tags, you can use these tags to identify the warehouses and users belonging to each\ndepartment. For example, because the sales department uses both `warehouse1` and `warehouse2` , you can set the `cost_center` tag to `'SALES'` for both warehouses.\n\nTip\n\nIdeally, you should have workflows that automate the process of applying these tags when you create resources and users.\n\n```\nUSE ROLE tag_admin ; \n\n ALTER WAREHOUSE warehouse1 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse2 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse3 SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n\n ALTER USER finance_user SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n ALTER USER sales_user SET TAG cost_management . tags . cost_center = 'SALES' ;\n```\n\nCopy\n\n## Viewing cost by tag in SQL \u00b6\n\nYou can attribute costs within an account or across accounts in an organization:\n\n* **Attributing costs within an account**\n  \n  You can attribute costs within an account by querying the following views in the ACCOUNT\\_USAGE schema:\n  \n    + TAG\\_REFERENCES view : Identifies objects (for example, warehouses and users) that have tags.\n    + WAREHOUSE\\_METERING\\_HISTORY view : Provides credit usage for warehouses.\n    + QUERY\\_ATTRIBUTION\\_HISTORY view : Provides the compute costs for queries. The cost per query is\n        the warehouse credit usage for executing the query.\n        \n        For more information on using this view, see About the QUERY\\_ATTRIBUTION\\_HISTORY view .\n* **Attributing costs across accounts in an organization**\n  \n  Within an organization, you can also attribute costs for resources that are used **exclusively by a single department** by\n  querying views in the ORGANIZATION\\_USAGE schema from the organization account .\n  \n  Note\n  \n    + In the ORGANIZATION\\_USAGE schema, the TAG\\_REFERENCES view is only available in the organization account.\n    + The QUERY\\_ATTRIBUTION\\_HISTORY view is only available in the ACCOUNT\\_USAGE schema for an account. There is no\n        organization-wide equivalent of the view.\n\nThe next sections explain how to attribute costs for some of the common cost-attribution scenarios :\n\n* Resources not shared by departments\n* Resources shared by users from different departments\n* Resources used by applications that need to attribute costs to different departments\n\n### Resources not shared by departments \u00b6\n\nSuppose that you want to attribute costs by department and that each department uses a set of dedicated warehouses.\n\nIf you tag warehouses with a `cost_center` tag to identify the department that owns the warehouse, you can join the\nACCOUNT\\_USAGE TAG\\_REFERENCES view with the WAREHOUSE\\_METERING\\_HISTORY view on the `object_id` and `warehouse_id` columns to get usage\ninformation by warehouse, and you can use the `tag_value` column to identify the departments that own those warehouses.\n\nThe following SQL statement performs this join:\n\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\n\nCopy\n\n```\n+-------------+-------------+-----------------+ \n | TAG_NAME    | TAG_VALUE   |   TOTAL_CREDITS | \n |-------------+-------------+-----------------| \n | NULL        | untagged    |    20.360277159 | \n | COST_CENTER | Sales       |    17.173333333 | \n | COST_CENTER | Finance     |      8.14444444 | \n +-------------+-------------+-----------------+\n```\n\nYou can run a similar query to perform the same attribution for all the accounts in your organization using views in the\nORGANIZATION\\_USAGE schema from the organization account . The rest of the query\ndoes not change.\n\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ORGANIZATION_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ORGANIZATION_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n          AND tag_database = 'COST_MANAGEMENT' AND tag_schema = 'TAGS' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\n\nCopy\n\n### Resources shared by users from different departments \u00b6\n\nSuppose that users in different departments share the same warehouses and you want to break down the credits used by each\ndepartment. You can tag the users with a `cost_center` tag to identify the department that they belong to, and you can join\nthe TAG\\_REFERENCES view with the QUERY\\_ATTRIBUTION\\_HISTORY view .\n\nNote\n\nYou can only get this data for a single account at a time. You cannot execute a query that retrieves this data across\naccounts in an organization.\n\nThe next sections provide examples of SQL statements for attributing costs for shared resources.\n\n* Calculating the cost of user queries for the last month\n* Calculating the cost of user queries by department without idle time\n* Calculating the cost of queries by users without idle time\n* Calculating the cost of queries by users without tags\n\n#### Calculating the cost of user queries for the last month \u00b6\n\nThis following SQL statement calculates the costs for the last month.\n\nIn this example, idle time is distributed among the users in proportion to their usage.\n\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n  ), \n  user_credits AS ( \n    SELECT user_name , SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n      GROUP BY user_name \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n    FROM user_credits \n  ) \n SELECT \n    u . user_name , \n    u . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM user_credits u , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\n\nCopy\n\n```\n+-----------+--------------------+ \n | USER_NAME | ATTRIBUTED_CREDITS | \n |-----------+--------------------+ \n | FINUSER   | 6.603575468        | \n | SALESUSER | 4.321378049        | \n | ENGUSER   | 0.6217131392       | \n |-----------+--------------------+\n```\n\n#### Calculating the cost of user queries by department without idle time \u00b6\n\nThe following example attributes the compute cost to each department through the queries executed by users in that department.\nThis query depends on the user objects having a tag that identifies their department.\n\n```\nWITH joined_data AS ( \n  SELECT \n      tr . tag_name , \n      tr . tag_value , \n      qah . credits_attributed_compute , \n      qah . start_time \n    FROM SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES tr \n      JOIN SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n        ON tr . domain = 'USER' AND tr . object_name = qah . user_name \n ) \n SELECT \n    tag_name , \n    tag_value , \n    SUM ( credits_attributed_compute ) AS total_credits \n  FROM joined_data \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY tag_name , tag_value \n  ORDER BY tag_name , tag_value ;\n```\n\nCopy\n\n```\n+-------------+-------------+-----------------+ \n | TAG_NAME    | TAG_VALUE   |   TOTAL_CREDITS | \n |-------------+-------------+-----------------| \n | COST_CENTER | engineering |   0.02493688426 | \n | COST_CENTER | finance     |    0.2281084988 | \n | COST_CENTER | marketing   |    0.3686840545 | \n |-------------+-------------+-----------------|\n```\n\n#### Calculating the cost of queries by users without idle time \u00b6\n\nThis following SQL statement calculates the costs per user for the past month (excluding idle time).\n\n```\nSELECT user_name , SUM ( credits_attributed_compute ) AS credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE \n    start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY user_name ;\n```\n\nCopy\n\n```\n+-----------+--------------------+ \n | USER_NAME | ATTRIBUTED_CREDITS | \n |-----------+--------------------| \n | JSMITH    |       17.173333333 | \n | MJONES    |         8.14444444 | \n | SYSTEM    |         5.33985393 | \n +-----------+--------------------+\n```\n\n#### Calculating the cost of queries by users without tags \u00b6\n\nThe following example calculates the cost of queries by users who are not tagged. You can use this to verify that tags are\nbeing applied consistently to users.\n\n```\nSELECT qah . user_name , SUM ( qah . credits_attributed_compute ) as total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n    LEFT JOIN snowflake . account_usage . tag_references tr \n    ON qah . user_name = tr . object_name AND tr . DOMAIN = 'USER' \n  WHERE \n    start_time >= dateadd ( month , - 1 , current_date ) \n    AND qah . user_name IS NULL OR tr . object_name IS NULL \n  GROUP BY qah . user_name \n  ORDER BY total_credits DESC ;\n```\n\nCopy\n\n```\n+------------+---------------+ \n | USER_NAME  | TOTAL_CREDITS | \n |------------+---------------| \n | RSMITH     |  0.1830555556 | \n +------------+---------------+\n```\n\n### Resources used by applications that need to attribute costs to different departments \u00b6\n\nThe examples in this section calculate the costs for one or more applications that are powered by Snowflake.\n\nThe examples assume that these applications set query tags that identify the application for all queries executed. To set the\nquery tag for queries in a session, execute the ALTER SESSION command. For example:\n\n```\nALTER SESSION SET QUERY_TAG = 'COST_CENTER=finance' ;\n```\n\nCopy\n\nThis associates the `COST_CENTER=finance` tag with all subsequent queries executed during the session.\n\nYou can then use the query tag to trace back the cost incurred by these queries to the appropriate departments.\n\nThe next sections provide examples of using this approach.\n\n* Calculating the cost of queries by department\n* Calculating the cost of queries (excluding idle time) by query tag\n* Calculating the cost of queries (including idle time) by query tag\n\n#### Calculating the cost of queries by department \u00b6\n\nThe following example calculates the compute credits and the credits used for the query acceleration service for the finance department. This depends on the `COST_CENTER=finance` query tag being applied to the original queries that were executed.\n\nNote that the costs exclude idle time.\n\n```\nSELECT \n    query_tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_tag = 'COST_CENTER=finance' \n  GROUP BY query_tag ;\n```\n\nCopy\n\n```\n+---------------------+-----------------+------+ \n | QUERY_TAG           | COMPUTE_CREDITS | QAS  | \n |---------------------+-----------------|------| \n | COST_CENTER=finance |      0.00576115 | null | \n +---------------------+-----------------+------+\n```\n\n#### Calculating the cost of queries (excluding idle time) by query tag \u00b6\n\nThe following example calculates the cost of queries by query tag and includes queries without tags (identified as \u201cuntagged\u201d).\n\n```\nSELECT \n    COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n  GROUP BY tag \n  ORDER BY compute_credits DESC ;\n```\n\nCopy\n\n```\n+-------------------------+-----------------+------+ \n | TAG                     | COMPUTE_CREDITS | QAS  | \n |-------------------------+-----------------+------+ \n | untagged                | 3.623173449     | null | \n | COST_CENTER=engineering | 0.531431948     | null | \n |-------------------------+-----------------+------+\n```\n\n#### Calculating the cost of queries (including idle time) by query tag \u00b6\n\nThe following example distributes the idle time that is not captured in the per-query cost across departments in proportion\nto their usage of the warehouse.\n\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n      AND start_time < CURRENT_DATE \n  ), \n  tag_credits AS ( \n    SELECT \n        COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n        SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n      GROUP BY tag \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n      FROM tag_credits \n  ) \n SELECT \n    tc . tag , \n    tc . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM tag_credits tc , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\n\nCopy\n\n```\n+-------------------------+--------------------+ \n | TAG                     | ATTRIBUTED_CREDITS | \n +-------------------------+--------------------| \n | untagged                |        9.020031304 | \n | COST_CENTER=finance     |        1.027742521 | \n | COST_CENTER=engineering |        1.018755812 | \n | COST_CENTER=marketing   |       0.4801370376 | \n +-------------------------+--------------------+\n```\n\n## Viewing cost by tag in Snowsight \u00b6\n\nYou can attribute costs by reporting on the use of resources that have the `cost_center` tag. You can access this data in Snowsight .\n\n1. Switch to a role that has access to the ACCOUNT\\_USAGE schema .\n2. In the navigation menu, select Admin \u00bb Cost management .\n3. Select Consumption .\n4. From the Tags drop-down, select the `cost_center` tag.\n5. To focus on a specific cost center, select a value from the list of the tag\u2019s values.\n6. Select Apply .\n\nFor more details about filtering in Snowsight, see Filter by tag .\n\n## About the QUERY\\_ ATTRIBUTION\\_ HISTORY view \u00b6\n\nYou can use the QUERY\\_ATTRIBUTION\\_HISTORY view to attribute cost based on queries. The cost per\nquery is the warehouse credit usage for executing the query. This cost does not include any other credit usage that is incurred\nas a result of query execution. For example, the following are not included in the query cost:\n\n* Data transfer costs\n* Storage costs\n* Cloud services costs\n* Costs for serverless features\n* Costs for tokens processed by AI services\n\nFor queries that are executed concurrently, the cost of the warehouse is attributed to individual queries based on the weighted\naverage of their resource consumption during a given time interval.\n\nThe cost per query does not include warehouse _idle time_ . Idle time is a period of time in which no queries are running in the\nwarehouse and can be measured at the warehouse level.\n\n## Additional examples of queries \u00b6\n\nThe next sections provide additional queries that you can use for cost attribution:\n\n* Grouping similar queries\n* Attributing costs of hierarchical queries\n\n### Grouping similar queries \u00b6\n\nFor recurrent or similar queries, use the `query_hash` or `query_parameterized_hash` to group costs\nby query.\n\nTo find the most expensive recurrent queries for the current month, execute the following statement:\n\n```\nSELECT query_parameterized_hash , \n       COUNT (*) AS query_count , \n       SUM ( credits_attributed_compute ) AS total_credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n  AND start_time < CURRENT_DATE \n  GROUP BY query_parameterized_hash \n  ORDER BY total_credits DESC \n  LIMIT 20 ;\n```\n\nCopy\n\nFor an additional query based on query ID, see Examples .\n\n### Attributing costs of hierarchical queries \u00b6\n\nFor stored procedures that issue multiple hierarchical queries, you can compute the attributed query costs for the\nprocedure by using the root query ID for the procedure.\n\n1. To find the root query ID for a stored procedure, use the ACCESS\\_HISTORY view . For example,\n   to find the root query ID for a stored procedure, set the `query_id` and execute the following statements:\n   \n   ```\n   SET query_id = '<query_id>' ; \n   \n    SELECT query_id , \n          parent_query_id , \n          root_query_id , \n          direct_objects_accessed \n     FROM SNOWFLAKE . ACCOUNT_USAGE . ACCESS_HISTORY \n     WHERE query_id = $ query_id ;\n   ```\n   \n   Copy\n   \n   For more information, see Ancestor queries with stored procedures .\n2. To sum the query cost for the entire procedure, replace `<root_query_id>` and execute the following statements:\n   \n   ```\n   SET query_id = '<root_query_id>' ; \n   \n    SELECT SUM ( credits_attributed_compute ) AS total_attributed_credits \n     FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n     WHERE ( root_query_id = $ query_id OR query_id = $ query_id );\n   ```\n   \n   Copy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)"
      ],
      "full_content": null
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/access-history",
      "title": "Access History | Snowflake Documentation",
      "publish_date": "2023-07-07",
      "excerpts": [
        "Guides Data Governance Access history\n\n# Access History \u00b6\n\n Enterprise Edition Feature\n\nAccess History requires Enterprise Edition (or higher). To inquire about upgrading, please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\n\nThis topic provides concepts on the user access history in Snowflake.\n\n## Overview \u00b6\n\nAccess History in Snowflake refers to when the user query reads data and when the SQL statement performs a data write\noperation, such as INSERT, UPDATE, and DELETE along with variations of the COPY command, from the source data object to the target data\nobject. The user access history can be found by querying the ACCESS\\_HISTORY view in the ACCOUNT\\_USAGE and ORGANIZATION\\_USAGE schemas. The\nrecords in these views facilitate regulatory compliance auditing and provide insights on popular and frequently accessed tables and columns\nbecause there is a direct link between the user (i.e. query operator), the query, the table or view, the column, and the data.\n\nEach row in the ACCESS\\_HISTORY view contains a single record per SQL statement. The record contains the following kinds of information:\n\n* The _source columns_ the query accessed directly and indirectly, such as the underlying tables that the data for the query comes from.\n* The _projected columns_ the user sees in the query result, such as the columns specified in a SELECT statement.\n* The columns that are used to determine the query result but are not projected, such as columns in a WHERE clause to filter the result.\n\nFor example:\n\n```\nCREATE OR REPLACE VIEW v1 ( vc1 , vc2 ) AS \n SELECT c1 as vc1 , \n       c2 as vc2 \n FROM t \n WHERE t . c3 > 0 \n ;\n```\n\nCopy\n\n* Columns C1 and C2 are source columns that the view accesses directly, which are recorded in the `base_objects_accessed` column of\n  the ACCESS\\_HISTORY view.\n* Column C3 is used to filter the rows the view includes, which is recorded in the `base_objects_accessed` column of\n  the ACCESS\\_HISTORY view.\n* Columns VC1 and VC2 are projected columns the user sees when querying the view, `SELECT * FROM v1;` , which are recorded in the `direct_objects_accessed` column of the ACCESS\\_HISTORY view.\n\nThe same behavior applies to a key column in a WHERE clause. For example:\n\n```\nCREATE OR REPLACE VIEW join_v ( vc1 , vc2 , c1 ) AS \n  SELECT \n      bt . c1 AS vc1 , \n      bt . c2 AS vc2 , \n      jt . c1 \n  FROM bt , jt \n  WHERE bt . c3 = jt . c1 ;\n```\n\nCopy\n\n* Two different tables are required to create the view: `bt` (base table) and `jt` (join table.).\n* Columns C1, C2, and C3 from the base table and column C1 from the join table are all recorded in the `base_objects_accessed` column\n  of the ACCESS\\_HISTORY view.\n* Columns VC1, VC2, and C1 are projected columns the user sees when querying the view, `SELECT * FROM join_v;` , which are\n  recorded in the `direct_objects_accessed` column of the ACCESS\\_HISTORY view.\n\nNote\n\nRecords in the Account Usage QUERY\\_HISTORY view do not always get recorded in the\nACCESS\\_HISTORY view. The structure of the SQL statement determines whether Snowflake records an entry in the ACCESS\\_HISTORY view.\n\nFor details on the read and write operations Snowflake supports in the ACCESS\\_HISTORY view, refer to the view Usage notes .\n\n## Tracking read and write operations \u00b6\n\nThe ACCESS\\_HISTORY view in both the ACCOUNT\\_USAGE and the ORGANIZATION\\_USAGE schemas includes the following columns:\n\n```\nquery_id | query_start_time | user_name | direct_objects_accessed | base_objects_accessed | objects_modified | object_modified_by_ddl | policies_referenced | parent_query_id | root_query_id\n```\n\nCopy\n\nRead operations are tracked through the first five columns, while the last column, `objects_modified` , specifies the data write information that involved Snowflake columns, tables, and stages.\n\nThe query in Snowflake and how the database objects were created determines the information Snowflake returns in the `direct_objects_accessed` , `base_objects_accessed` , and `objects_modified` columns.\n\nSimilarly, if the query references an object protected by a row access policy or a column protected by a masking policy, Snowflake records\nthe policy information in the `policies_referenced` column.\n\nThe `object_modified_by_ddl` column records the DDL operation on a database, schema, table, view, and column. These operations also\ninclude statements that specify a row access policy on a table or view, a masking policy on a column, and tag updates\n(e.g. set a tag, change a tag value) on the object or column.\n\nThe `parent_query_id` and `root_query_id` columns record query IDs that correspond to:\n\n* A query that performs a read or write operation on another object.\n* A query that performs a read or write operation on an object that calls a stored procedure, including nested stored procedure calls. For\n  details, see ancestor queries (in this topic).\n\nFor column details, see the Columns section in the ACCESS\\_HISTORY view.\n\n### Read \u00b6\n\nConsider the following scenario to understand a read query and how the ACCESS\\_HISTORY view records this information:\n\n* A series of objects: `base_table` \u00bb `view_1` \u00bb `view_2` \u00bb `view_3` .\n* A read query on `view_2` , such as:\n  \n  ```\n  select * from view_2 ;\n  ```\n  \n  Copy\n\nIn this example, Snowflake returns:\n\n* `view_2` in the `direct_objects_accessed` column because the query specifies `view_2` .\n* `base_table` in the `base_objects_accessed` column because that is the original source of the data in `view_2` .\n\nNote that `view_1` and `view_3` are not included in the `direct_objects_accessed` and `base_objects_accessed` columns\nbecause neither of those views were included in the query and they are not the base object that serves as the source for the data in `view_2` .\n\n### Write \u00b6\n\nConsider the following scenario to understand a write operation and how the ACCESS\\_HISTORY view records this information:\n\n* A data source: `base_table`\n* Create a table from the data source (i.e. CTAS):\n  \n  ```\n  create table table_1 as select * from base_table ;\n  ```\n  \n  Copy\n\nIn this example, Snowflake returns:\n\n* `base_table` in the `base_objects_accessed` and `direct_objects_accessed` columns because the table was accessed directly\n  and is the source of the data.\n* `table_1` in the `objects_modified` column with the columns that were written to when creating the table.\n\n### Supported operations \u00b6\n\nFor a complete description of the read and write operations the ACCESS\\_HISTORY view supports, see the usage notes sections in the ACCESS\\_HISTORY view .\n\n### Multiple statements in a single request \u00b6\n\nSnowflake supports executing multiple statements simultaneously as a single request. How you track the request in the access history depends\non whether it was executed in Snowsight or programmatically.\n\n* When you use Snowsight to execute multiple statements, it runs the queries one at a time and returns the `query_id` of the last executed query. You can find all executed statements and their return values in the ACCESS\\_HISTORY view.\n* Features like the Snowflake Python connector or the Snowflake SQL API combine multiple SQL statements into a single request and return a\n  single `query_id` for all of the statements. This number is actually a parent query id for all of the individual\n  statements. To return the `query_id` of each statement that comprised the request, you must query the ACCESS\\_HISTORY view using the `parent_query_id` . For example, if the request returned `query_id = 6789` , then you can return the query ids of the individual\n  statements by executing the following:\n  \n  ```\n  SELECT query_id , parent_query_id , direct_objects_accessed \n   FROM snowflake . account_usage . access_history \n   WHERE parent_query_id = 6789 ;\n  ```\n  \n  Copy\n\n### Benefits \u00b6\n\nAccess history in Snowflake provides the following benefits pertaining to read and write operations:\n\nData discovery :\n    Discover unused data to determine whether to archive or delete the data.\nTrack how sensitive data moves :\n    Track data movement from an external cloud storage location (e.g. Amazon S3 bucket) to the target Snowflake table, and vice versa.\n\nTrack internal data movement from a Snowflake table to a different Snowflake table.\n\nAfter tracing the movement of sensitive data, apply policies ( masking and row access ) to protect data, update access control settings to further regulate access to the stage and table, and set tags to ensure stages, tables, and columns with sensitive data can be tracked for compliance\nrequirements.\nData validation :\n    The accuracy and integrity of reports, dashboards, and data visualization products such as charts and graphs are validated since the\ndata can be traced to its original source.\n\nData stewards can also notify users prior to dropping or altering a given table or view.\nCompliance auditing :\n    Identify the Snowflake user who performed a write operation on a table or stage and when the write operation occurred to meet compliance\nregulations, such as [GDPR](https://gdpr-info.eu/) and [CCPA](https://oag.ca.gov/privacy/ccpa) .\nEnhance overall data governance :\n    The ACCESS\\_HISTORY view provides a unified picture of what data was accessed, when the data access took place, and how the accessed data\nmoved from the data source object to the data target object.\n\n## Column lineage \u00b6\n\nColumn lineage (i.e. access history for columns) extends the Account Usage ACCESS\\_HISTORY view to specify how data flows from the source\ncolumn to the target column in a write operation. Snowflake tracks the data from the source columns through all subsequent table objects\nthat reference data from the source columns (e.g. INSERT, MERGE, CTAS) provided that objects in the lineage chain are not dropped.\nSnowflake makes column lineage accessible by enhancing the `objects_modified` column in the ACCESS\\_HISTORY view.\n\nColumn lineage provides the following benefits:\n\nProtect Derived Objects :\n    Data stewards can easily tag sensitive source columns without having to do additional work after\ncreating derived objects (e.g. CTAS). Subsequently, the data steward can protect tables containing sensitive columns with a row access policy or protect the sensitive columns themselves with either a masking policy or a tag-based masking policy .\nSensitive Column Copy Frequency :\n    Data privacy officers can quickly determine the object count (e.g. 1 table, 2 views) of a column containing sensitive data. By knowing\nhow many times a column with sensitive data appears in a table object, data privacy officers can prove how they satisfy regulatory\ncompliance standards (e.g. to meet General Data Protection Regulation (GDPR) standards in the European Union).\nRoot Cause Analysis :\n    Column lineage provides a mechanism to trace the data to its source, which can help to pinpoint points of failure resulting from\npoor data quality and reduce the number of columns to analyze during the troubleshooting process.\n\nFor additional details about column lineage, see:\n\n* Column lineage (in this topic)\n\n## Masking and row access policy references \u00b6\n\nThe POLICY\\_REFERENCED column specifies the object that has a row access policy set on a table or a masking policy set on a column,\nincluding any intermediate objects that are protected by either a row access policy or a masking policy. Snowflake records the policy that\nis enforced on the table or column.\n\nConsider these objects:\n\n`t1` \u00bb `v1` \u00bb `v2`\n\nWhere:\n\n* `t1` is a base table.\n* `v1` is a view built from the base table.\n* `v2` is a view built from `v1` .\n\nIf the user queries `v2` , the `policies_referenced` column records either the row access policy that protects `v2` , each masking\npolicy that protects the columns in `v2` , or both kinds of policy as applicable. Additionally, this column records any masking or row\naccess policies that protect `t1` and `v1` .\n\nThese records can help data governors understand how their policy-protected objects are accessed.\n\nThe `policies_referenced` column provides additional benefits to the ACCESS\\_HISTORY view:\n\n* Identify the policy-protected objects a user accesses in a given query.\n* Simplify the policy audit process.\n  \n  Querying the ACCESS\\_HISTORY view eliminates the need for complex joins on other Account Usage views\n  (e.g. POLICY\\_REFERENCES and QUERY\\_HISTORY ), to obtain information about the protected objects and protected\n  columns a user accesses.\n\n## Account-level vs. Organization-level access history \u00b6\n\nAdministrators monitor access history at the account-level by querying the ACCESS\\_HISTORY view in the account\u2019s ACCOUNT\\_USAGE schema. There\nis no additional cost associated with the ACCOUNT\\_USAGE.ACCESS\\_HISTORY view.\n\nThe ACCESS\\_HISTORY view in the ORGANIZATION\\_USAGE schema gathers the access history of all of the accounts in an organization into a single\nview to provide an organization-level access history. This ORGANIZATION\\_USAGE.ACCESS\\_HISTORY view is only found in the organization account .\n\nOrganization-level access history in the ORGANIZATION\\_USAGE schema differs from access history in the ACCOUNT\\_USAGE schema in the\nfollowing ways:\n\nAdditional columns :\n    The ORGANIZATION\\_USAGE.ACCESS\\_HISTORY view in the organization account contains additional columns that provide insights related to organizational listings . These columns can be used to determine which of\nthe data products attached to an organization listing were accessed by a consumer\u2019s query, and whether those data products are protected\nby a policy such as a masking policy. For more information, see Organizational listing governance .\nAdditional cost :\n    The ORGANIZATION\\_USAGE.ACCESS\\_HISTORY view in the organization account is a premium view that incurs the following costs:\n\n* Compute costs associated with the serverless tasks that populate the ACCESS\\_HISTORY view.\n* Storage costs associated with storing the data in the ACCESS\\_HISTORY view.\n\nFor more information about these costs, see Costs associated with premium views .\n\n## Supported Objects \u00b6\n\nUse the following table to determine whether the ACCESS\\_HISTORY view contains a record when a SQL statement involves a specific type of object. SQL statements include the following:\n\n* Data Manipulation Language (DML) statements. For example, statements used to insert data into a table.\n* Data Query Language (DQL) statements. For example, statements that use a SELECT statement to project data.\n* Data Definition Language (DDL) statements. For example, statements that create or alter a Snowflake object.\n\n|Object |DML |DQL |DDL |Notes |\n| --- | --- | --- | --- | --- |\n|DATABASE |n/a |n/a |\u2714 | |\n|DYNAMIC TABLE |Partial |\u2714 |\u2714 |Support for DML is only for the `ALTER DYNAMIC TABLE ... REFRESH` command. |\n|EXTERNAL TABLE |\u2714 |\u2714 |\u2714 | |\n|FUNCTION |n/a |\u2714 |\u2714 |Support for DQL is limited to a function that appears in a SELECT statement. |\n|ICEBERG TABLE |Partial |\u2714 |\u2714 |Full support (DML, DQL, DDL) for Snowflake-managed Apache Iceberg\u2122 tables. Support for DQL and DDL only for externally managed Apache Iceberg\u2122 tables. |\n|LISTING |n/a |n/a |\u2714 | |\n|MATERIALIZED VIEW |n/a |\u2714 |\u2714 | |\n|POLICY |n/a |\u2714 |\u2714 |Support for DDL shows when a policy is applied to an object. Support for DQL shows the policies under enforcement when a query is run. |\n|PROCEDURE |n/a |\u2714 |\u2714 |A procedure can have multiple SQL statements with each statement generating a separate record. |\n|ROLE |n/a |n/a |\u2714 | |\n|SCHEMA |n/a |n/a |\u2714 | |\n|SEQUENCE | |n/a |\u2714 |Non-support for DML is intentional. |\n|SESSION |n/a |n/a |\u2714 | |\n|SHARE |n/a |n/a |\u2714 | |\n|STAGE |Partial | |\u2714 |Support for DML is limited to using the stage as the source for a table. For DQL, there is no support for queries against a stage. |\n|STREAM |n/a |Partial |\u2714 |Support for DQL is limited to using a stream as the source for a table. |\n|TABLE |\u2714 |\u2714 |\u2714 | |\n|TAG |n/a |n/a |\u2714 | |\n|VIEW |n/a |\u2714 |\u2714 | |\n\n## Querying the ACCESS\\_ HISTORY View \u00b6\n\nThe following sections provide example queries for the ACCESS\\_HISTORY view.\n\nNote that some of the example queries filter on the `query_start_time` column to increase query performance. Another option to\nincrease performance is to query over narrower time ranges.\n\n## Access history examples \u00b6\n\n### Read queries \u00b6\n\nThe subsections below detail how to query the ACCESS\\_HISTORY view for read operations for the following use cases:\n\n* Obtain the access history for a specific user.\n* Facilitate compliance audits for sensitive data access in the last 30 days, based on `object_id` (e.g. a table id), to answer the\n  following questions:\n  \n    + Who accessed the data?\n    + When was the data accessed?\n    + What columns were accessed?\n\n#### Return the user access history \u00b6\n\nReturn the user access history, ordered by user and query start time, starting from the most recent access.\n\n> ```\n> SELECT user_name \n>        , query_id \n>        , query_start_time \n>        , direct_objects_accessed \n>        , base_objects_accessed \n>  FROM access_history \n>  ORDER BY 1 , 3 desc \n>  ;\n> ```\n> \n> Copy\n> \n>\n\n#### Facilitate compliance audits \u00b6\n\nThe following examples help to facilitate compliance audits:\n\n* Add the `object_id` value to determine who accessed a sensitive table in the last 30 days:\n  \n  ```\n  SELECT distinct user_name \n   FROM access_history \n       , lateral flatten ( base_objects_accessed ) f1 \n   WHERE f1 . value : \"objectId\" ::int =< fill_in_object_id > \n   AND f1 . value : \"objectDomain\" ::string = 'Table' \n   AND query_start_time >= dateadd ( 'day' , - 30 , current_timestamp ()) \n   ;\n  ```\n  \n  Copy\n* Using the `object_id` value of `32998411400350` , determine when the access occurred in the last 30 days:\n  \n  ```\n  SELECT query_id \n         , query_start_time \n   FROM access_history \n       , lateral flatten ( base_objects_accessed ) f1 \n   WHERE f1 . value : \"objectId\" ::int = 32998411400350 \n   AND f1 . value : \"objectDomain\" ::string = 'Table' \n   AND query_start_time >= dateadd ( 'day' , - 30 , current_timestamp ()) \n   ;\n  ```\n  \n  Copy\n* Using the `object_id` value of `32998411400350` , determine which columns were accessed in the last 30 days:\n  \n  ```\n  SELECT distinct f4 . value AS column_name \n   FROM access_history \n       , lateral flatten ( base_objects_accessed ) f1 \n       , lateral flatten ( f1 . value ) f2 \n       , lateral flatten ( f2 . value ) f3 \n       , lateral flatten ( f3 . value ) f4 \n   WHERE f1 . value : \"objectId\" ::int = 32998411400350 \n   AND f1 . value : \"objectDomain\" ::string = 'Table' \n   AND f4 . key = 'columnName' \n   ;\n  ```\n  \n  Copy\n\n### Write operations \u00b6\n\nThe subsections below detail how to query the ACCESS\\_HISTORY view for write operations for the following use cases:\n\n* Load data from a stage to a table.\n* Unload data from a table to a stage.\n* Use the PUT command to upload a local file to a stage.\n* Use the GET command to retrieve data files from a stage to a local directory.\n* Tracking sensitive stage data movement.\n\n#### Load data from a stage to a table \u00b6\n\nLoad a set of values from a data file in external cloud storage into columns in a target table.\n\n> ```\n> copy into table1 ( col1 , col2 ) \n>  from ( select t .$ 1 , t .$ 2 from @ mystage1 / data1 . csv . gz );\n> ```\n> \n> Copy\n> \n> \n\nThe `direct_objects_accessed` and `base_objects_accessed` column specify that an external named stage was accessed:\n\n> ```\n> { \n>   \"objectDomain\": STAGE\n>   \"objectName\": \"mystage1\" ,\n>   \"objectId\": 1,\n>   \"stageKind\": \"External Named\" \n>  }\n> ```\n> \n> Copy\n> \n> \n\nThe `objects_modified` column specifies that data was written to two columns of the table:\n\n> ```\n> { \n>   \"columns\": [ \n>      { \n>        \"columnName\": \"col1\" ,\n>        \"columnId\": 1\n>      } ,\n>      { \n>        \"columnName\": \"col2\" ,\n>        \"columnId\": 2\n>      } \n>   ] ,\n>   \"objectId\": 1,\n>   \"objectName\": \"TEST_DB.TEST_SCHEMA.TABLE1\" ,\n>   \"objectDomain\": TABLE\n>  }\n> ```\n> \n> Copy\n> \n>\n\n#### Unload data from a table to a stage \u00b6\n\nUnload a set of values from a Snowflake table into cloud storage.\n\n> ```\n> copy into @ mystage1 / data1 . csv \n>  from table1 ;\n> ```\n> \n> Copy\n> \n> \n\nThe `direct_objects_accessed` and `base_objects_accessed` columns specify the table columns that were\naccessed:\n\n> ```\n> { \n>   \"objectDomain\": TABLE\n>   \"objectName\": \"TEST_DB.TEST_SCHEMA.TABLE1\" ,\n>   \"objectId\": 123,\n>   \"columns\": [ \n>      { \n>        \"columnName\": \"col1\" ,\n>        \"columnId\": 1\n>      } ,\n>      { \n>        \"columnName\": \"col2\" ,\n>        \"columnId\": 2\n>      } \n>   ] \n>  }\n> ```\n> \n> Copy\n> \n> \n\nThe `objects_modified` column specifies the stage to which the accessed data was written:\n\n> ```\n> { \n>   \"objectId\": 1,\n>   \"objectName\": \"mystage1\" ,\n>   \"objectDomain\": STAGE,\n>   \"stageKind\": \"External Named\" \n>  }\n> ```\n> \n> Copy\n> \n>\n\n#### Use the PUT Command to upload a local file to a stage \u00b6\n\nCopy a data file to an internal (i.e. Snowflake) stage.\n\n> ```\n> put file :/// tmp / data / mydata . csv @ my_int_stage ;\n> ```\n> \n> Copy\n> \n> \n\nThe `direct_objects_accessed` and `base_objects_accessed` columns specify the local path to the file that was\naccessed:\n\n> ```\n> { \n>   \"location\": \"file:///tmp/data/mydata.csv\"\n>  }\n> ```\n> \n> Copy\n> \n> \n\nThe `objects_modified` column specifies the stage where the accessed data was written:\n\n> ```\n> { \n>   \"objectId\": 1,\n>   \"objectName\": \"my_int_stage\" ,\n>   \"objectDomain\": STAGE,\n>   \"stageKind\": \"Internal Named\" \n>  }\n> ```\n> \n> Copy\n> \n>\n\n#### Use the GET command to retrieve data files from a stage to a local directory \u00b6\n\nRetrieve a data file from an internal stage to a directory on the local machine.\n\n> ```\n> get @% mytable file :/// tmp / data /;\n> ```\n> \n> Copy\n> \n> \n\nThe `direct_objects_accessed` and `base_objects_accessed` columns specify the stage and local directory that were\naccessed:\n\n> ```\n> { \n>   \"objectDomain\": Stage\n>   \"objectName\": \"mytable\" ,\n>   \"objectId\": 1,\n>   \"stageKind\": \"Table\" \n>  }\n> ```\n> \n> Copy\n> \n> \n\nThe `objects_modified` column specifies the directory to which the accessed data was written:\n\n> ```\n> { \n>   \"location\": \"file:///tmp/data/\"\n>  }\n> ```\n> \n> Copy\n> \n>\n\n#### Tracking Sensitive stage data movement \u00b6\n\nTrack sensitive stage data as it moves through a series of queries executed in chronological order.\n\nExecute the following queries. Note that five of the statements access stage data. Therefore, when you query the ACCESS\\_HISTORY view for\nstage access, the result set should include five rows.\n\n> ```\n> use test_db . test_schema ; \n>  create or replace table T1 ( content variant ); \n>  insert into T1 ( content ) select parse_json ( '{\"name\": \"A\", \"id\":1}' ); \n> \n>  -- T1 -> T6 \n>  insert into T6 select * from T1 ; \n> \n>  -- S1 -> T1 \n>  copy into T1 from @ S1 ; \n> \n>  -- T1 -> T2 \n>  create table T2 as select content : \"name\" as name , content : \"id\" as id from T1 ; \n> \n>  -- T1 -> S2 \n>  copy into @ S2 from T1 ; \n> \n>  -- S1 -> T3 \n>  create or replace table T3 ( customer_info variant ); \n>  copy into T3 from @ S1 ; \n> \n>  -- T1 -> T4 \n>  create or replace table T4 ( name string , id string , address string ); \n>  insert into T4 ( name , id ) select content : \"name\" , content : \"id\" from T1 ; \n> \n>  -- T6 -> T7 \n>  create table T7 as select * from T6 ;\n> ```\n> \n> Copy\n> \n> Where:\n> \n> * `T1` , `T2` \u2026 `T7` specify the names of tables.\n> * `S1` and `S2` specify the names of stages.\n> \n> \n\nQuery the access history to determine the access to stage `S1` .\n\n> The data for the `direct_objects_accessed` , `base_objects_accessed` , and `objects_modified` columns are shown in the\n> following table.\n> \n> |`direct_objects_accessed` |`base_objects_accessed` |`objects_modified` |\n> | --- | --- | --- |\n> |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68610,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66564,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T1\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68610,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66564,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T1\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68611,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66566,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T6\" \n>   } \n>  ]\n> ```\n> \n> Copy |\n> |```\n> [ \n>   { \n>     \"objectDomain\": \"Stage\" ,\n>     \"objectId\": 117,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.S1\" ,\n>     \"stageKind\": \"External Named\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"objectDomain\": \"Stage\" ,\n>     \"objectId\": 117,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.S1\" ,\n>     \"stageKind\": \"External Named\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68610,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66564,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T1\" \n>   } \n>  ]\n> ```\n> \n> Copy |\n> |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68610,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66564,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T1\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68610,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66564,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T1\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68613,\n>         \"columnName\": \"ID\" \n>       } ,\n>       { \n>         \"columnId\": 68612,\n>         \"columnName\": \"NAME\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66568,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T2\" \n>   } \n>  ]\n> ```\n> \n> Copy |\n> |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68610,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66564,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T1\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68610,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66564,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T1\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"objectDomain\": \"Stage\" ,\n>     \"objectId\": 118,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.S2\" ,\n>     \"stageKind\": \"External Named\" \n>   } \n>  ]\n> ```\n> \n> Copy |\n> |```\n> [ \n>   { \n>     \"objectDomain\": \"Stage\" ,\n>     \"objectId\": 117,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.S1\" ,\n>     \"stageKind\": \"External Named\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"objectDomain\": \"Stage\" ,\n>     \"objectId\": 117,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.S1\" ,\n>     \"stageKind\": \"External Named\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68614,\n>         \"columnName\": \"CUSTOMER_INFO\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66570,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T3\" \n>   } \n>  ]\n> ```\n> \n> Copy |\n> |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68610,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66564,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T1\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68610,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66564,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T1\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68615,\n>         \"columnName\": \"NAME\" \n>       } ,\n>       { \n>         \"columnId\": 68616,\n>         \"columnName\": \"ID\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66572,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T4\" \n>   } \n>  ]\n> ```\n> \n> Copy |\n> |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68611,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66566,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T6\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68611,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66566,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T6\" \n>   } \n>  ]\n> ```\n> \n> Copy |```\n> [ \n>   { \n>     \"columns\": [ \n>       { \n>         \"columnId\": 68618,\n>         \"columnName\": \"CONTENT\" \n>       } \n>     ] ,\n>     \"objectDomain\": \"Table\" ,\n>     \"objectId\": 66574,\n>     \"objectName\": \"TEST_DB.TEST_SCHEMA.T7\" \n>   } \n>  ]\n> ```\n> \n> Copy |\n> \n> \n> Note the following about the query example:\n> \n> * Uses a recursive common table expression .\n> * Uses a JOIN construct rather than a USING clause .\n>   \n>   ```\n>   with access_history_flatten as ( \n>       select \n>           r . value : \"objectId\" as source_id , \n>           r . value : \"objectName\" as source_name , \n>           r . value : \"objectDomain\" as source_domain , \n>           w . value : \"objectId\" as target_id , \n>           w . value : \"objectName\" as target_name , \n>           w . value : \"objectDomain\" as target_domain , \n>           c . value : \"columnName\" as target_column , \n>           t . query_start_time as query_start_time \n>       from \n>           ( select * from TEST_DB . ACCOUNT_USAGE . ACCESS_HISTORY ) t , \n>           lateral flatten ( input => t . BASE_OBJECTS_ACCESSED ) r , \n>           lateral flatten ( input => t . OBJECTS_MODIFIED ) w , \n>           lateral flatten ( input => w . value : \"columns\" , outer => true ) c \n>           ), \n>       sensitive_data_movements ( path , target_id , target_name , target_domain , target_column , query_start_time ) \n>       as \n>         -- Common Table Expression \n>         ( \n>           -- Anchor Clause: Get the objects that access S1 directly \n>           select \n>               f . source_name || '-->' || f . target_name as path , \n>               f . target_id , \n>               f . target_name , \n>               f . target_domain , \n>               f . target_column , \n>               f . query_start_time \n>           from \n>               access_history_flatten f \n>           where \n>           f . source_domain = 'Stage' \n>           and f . source_name = 'TEST_DB.TEST_SCHEMA.S1' \n>           and f . query_start_time >= dateadd ( day , - 30 , date_trunc ( day , current_date )) \n>           union all \n>           -- Recursive Clause: Recursively get all the objects that access S1 indirectly \n>           select sensitive_data_movements . path || '-->' || f . target_name as path , f . target_id , f . target_name , f . target_domain , f . target_column , f . query_start_time \n>             from \n>                access_history_flatten f \n>               join sensitive_data_movements \n>               on f . source_id = sensitive_data_movements . target_id \n>                   and f . source_domain = sensitive_data_movements . target_domain \n>                   and f . query_start_time >= sensitive_data_movements . query_start_time \n>         ) \n>    select path , target_name , target_id , target_domain , array_agg ( distinct target_column ) as target_columns \n>    from sensitive_data_movements \n>    group by path , target_id , target_name , target_domain ;\n>   ```\n>   \n>   Copy\n> \n> The query produces the following result set related to stage `S1` data movement:\n> \n> > |PATH |TARGET\\_NAME |TARGET\\_ID |TARGET\\_DOMAIN |TARGET\\_COLUMNS |\n> > | --- | --- | --- | --- | --- |\n> > |TEST\\_DB.TEST\\_SCHEMA.S1\u2013>TEST\\_DB.TEST\\_SCHEMA.T1 |TEST\\_DB.TEST\\_SCHEMA.T1 |66564 |Table |[\u201cCONTENT\u201d] |\n> > |TEST\\_DB.TEST\\_SCHEMA.S1\u2013>TEST\\_DB.TEST\\_SCHEMA.T1\u2013>TEST\\_DB.TEST\\_SCHEMA.S2 |TEST\\_DB.TEST\\_SCHEMA.S2 |118 |Stage |[] |\n> > |TEST\\_DB.TEST\\_SCHEMA.S1\u2013>TEST\\_DB.TEST\\_SCHEMA.T1\u2013>TEST\\_DB.TEST\\_SCHEMA.T2 |TEST\\_DB.TEST\\_SCHEMA.T2 |66568 |Table |[\u201cNAME\u201d,\u201dID\u201d] |\n> > |TEST\\_DB.TEST\\_SCHEMA.S1\u2013>TEST\\_DB.TEST\\_SCHEMA.T1\u2013>TEST\\_DB.TEST\\_SCHEMA.T4 |TEST\\_DB.TEST\\_SCHEMA.T4 |66572 |Table |[\u201cID\u201d,\u201dNAME\u201d] |\n> > |TEST\\_DB.TEST\\_SCHEMA.S1\u2013>TEST\\_DB.TEST\\_SCHEMA.T3 |TEST\\_DB.TEST\\_SCHEMA.T3 |66570 |Table |[\u201cCUSTOMER\\_INFO\u201d] |\n> > \n> >\n> \n>\n\n### Column lineage \u00b6\n\nThe following example queries the ACCESS\\_HISTORY view and uses the FLATTEN function to flatten the `objects_modified` column.\n\nAs a representative example, execute the following SQL query in your Snowflake account to produce the table below, where the numbered\ncomments indicate the following:\n\n* `// 1` : Get the mapping between the `directSources` field and the target column.\n* `// 2` : Get the mapping between the `baseSources` field and the target column.\n\n```\n// 1 \n\n select \n  directSources . value : \"objectId\" as source_object_id , \n  directSources . value : \"objectName\" as source_object_name , \n  directSources . value : \"columnName\" as source_column_name , \n  'DIRECT' as source_column_type , \n  om . value : \"objectName\" as target_object_name , \n  columns_modified . value : \"columnName\" as target_column_name \n from \n  ( \n    select \n      * \n    from \n      snowflake . account_usage . access_history \n  ) t , \n  lateral flatten ( input => t . OBJECTS_MODIFIED ) om , \n  lateral flatten ( input => om . value : \"columns\" , outer => true ) columns_modified , \n  lateral flatten ( \n    input => columns_modified . value : \"directSources\" , \n    outer => true \n  ) directSources \n\n union \n\n // 2 \n\n select \n  baseSources . value : \"objectId\" as source_object_id , \n  baseSources . value : \"objectName\" as source_object_name , \n  baseSources . value : \"columnName\" as source_column_name , \n  'BASE' as source_column_type , \n  om . value : \"objectName\" as target_object_name , \n  columns_modified . value : \"columnName\" as target_column_name \n from \n  ( \n    select \n      * \n    from \n      snowflake . account_usage . access_history \n  ) t , \n  lateral flatten ( input => t . OBJECTS_MODIFIED ) om , \n  lateral flatten ( input => om . value : \"columns\" , outer => true ) columns_modified , \n  lateral flatten ( \n    input => columns_modified . value : \"baseSources\" , \n    outer => true \n  ) baseSources \n ;\n```\n\nCopy\n\nReturns:\n\n> |SOURCE\\_OBJECT\\_ID |SOURCE\\_OBJECT\\_NAME |SOURCE\\_COLUMN\\_NAME |SOURCE\\_COLUMN\\_TYPE |TARGET\\_OBJECT\\_NAME |TARGET\\_COLUMN\\_NAME |\n> | --- | --- | --- | --- | --- | --- |\n> |1 |D.S.T0 |NAME |BASE |D.S.T1 |NAME |\n> |2 |D.S.V1 |NAME |DIRECT |D.S.T1 |NAME |\n> \n>\n\n### Track row access policy references \u00b6\n\nReturn a row for each instance when a row access policy is set on a table, view, or materialized view without duplicates:\n\n> ```\n> use role accountadmin ; \n>  select distinct \n>     obj_policy . value : \"policyName\" ::VARCHAR as policy_name \n>  from snowflake . account_usage . access_history as ah \n>     , lateral flatten ( ah . policies_referenced ) as obj \n>     , lateral flatten ( obj . value : \"policies\" ) as obj_policy \n>  ;\n> ```\n> \n> Copy\n> \n>\n\n### Track masking policy references \u00b6\n\nReturn a row for each instance when a masking policy protects a column without duplicates. Note that additional flattening is necessary\nbecause the `policies_referenced` column specifies the masking policy on a column one level deeper than the row access policy on a\ntable:\n\n> ```\n> use role accountadmin ; \n>  select distinct \n>     policies . value : \"policyName\" ::VARCHAR as policy_name \n>  from snowflake . account_usage . access_history as ah \n>     , lateral flatten ( ah . policies_referenced ) as obj \n>     , lateral flatten ( obj . value : \"columns\" ) as columns \n>     , lateral flatten ( columns . value : \"policies\" ) as policies \n>  ;\n> ```\n> \n> Copy\n> \n>\n\n### Track the enforced policy in a query \u00b6\n\nReturn the time when the policy was updated (POLICY\\_CHANGED\\_TIME) and the policy conditions (POLICY\\_BODY) for a given query in a given time\nframe.\n\nPrior to using this query, update the WHERE clause input values:\n\n```\nwhere query_start_time > '2023-07-07' and \n   query_start_time < '2023-07-08' and \n   query_id = '01ad7987-0606-6e2c-0001-dd20f12a9777' )\n```\n\nCopy\n\nWhere:\n\n`query_start_time > '2023-07-07'`\n    Specifies the beginning timestamp. `query_start_time < '2023-07-08'`\n    Specifies the end timestamp. `query_id = '01ad7987-0606-6e2c-0001-dd20f12a9777'`\n    Specifies the query identifier in the Account Usage ACCESS\\_HISTORY view.\n\nRun the query:\n\n```\nSELECT * \n from ( \n  select j1 .*, j2 . QUERY_START_TIME as POLICY_CHANGED_TIME , POLICY_BODY \n from \n ( \n  select distinct t1 .*, \n      t4 . value : \"policyId\" ::number as PID \n  from ( select * \n      from SNOWFLAKE . ACCOUNT_USAGE . ACCESS_HISTORY \n      where query_start_time > '2023-07-07' and \n         query_start_time < '2023-07-08' and \n         query_id = '01ad7987-0606-6e2c-0001-dd20f12a9777' ) as t1 , // \n  lateral flatten ( input => t1 . POLICIES_REFERENCED , OUTER => TRUE ) t2 , \n  lateral flatten ( input => t2 . value : \"columns\" , OUTER => TRUE ) t3 , \n  lateral flatten ( input => t3 . value : \"policies\" , OUTER => TRUE ) t4 \n ) as j1 \n left join \n ( \n  select OBJECT_MODIFIED_BY_DDL : \"objectId\" ::number as PID , \n      QUERY_START_TIME , \n      OBJECT_MODIFIED_BY_DDL : \"properties\" . \"policyBody\" . \"value\" as POLICY_BODY \n      from SNOWFLAKE . ACCOUNT_USAGE . ACCESS_HISTORY \n      where OBJECT_MODIFIED_BY_DDL is not null and \n      ( OBJECT_MODIFIED_BY_DDL : \"objectDomain\" ilike '%masking%' or OBJECT_MODIFIED_BY_DDL : \"objectDomain\" ilike '%row%' ) \n ) as j2 \n On j1 . POLICIES_REFERENCED is not null and j1 . pid = j2 . pid and j1 . QUERY_START_TIME > j2 . QUERY_START_TIME ) as j3 \n QUALIFY ROW_NUMBER () OVER ( PARTITION BY query_id , pid ORDER BY policy_changed_time DESC ) = 1 ;\n```\n\nCopy\n\n### UDFs \u00b6\n\nThese UDF examples show how the Account Usage ACCESS\\_HISTORY view records:\n\n* Calling a UDF named `get_product` .\n* Insert the product of calling the `get_product` function into a table named `mydb.tables.t1` .\n* Shared UDFs.\n\n#### Call a UDF \u00b6\n\nConsider the following SQL UDF that calculates the product of two numbers and assume it is stored in the schema named `mydb.udfs` :\n\n> ```\n> CREATE FUNCTION MYDB . UDFS . GET_PRODUCT ( num1 number , num2 number ) \n>  RETURNS number \n>  AS \n>  $$ \n>     NUM1 * NUM2 \n>  $$ \n>  ;\n> ```\n> \n> Copy\n> \n> \n\nCalling `get_product` directly results in recording the UDF details in the `direct_objects_accessed` column:\n\n> ```\n> [ \n>   { \n>     \"objectDomain\": \"FUNCTION\" ,\n>     \"objectName\": \"MYDB.UDFS.GET_PRODUCT\" ,\n>     \"objectId\": \"2\" ,\n>     \"argumentSignature\": \"(NUM1 NUMBER, NUM2 NUMBER)\",\n>     \"dataType\": \"NUMBER(38,0)\"\n>   } \n>  ]\n> ```\n> \n> Copy\n> \n> \n\nThis example is analogous to calling a stored procedure (in this topic).\n\n#### UDF with INSERT DML \u00b6\n\nConsider the following INSERT statement to update the columns named 1 and 2 in the table named `mydb.tables.t1` :\n\n> ```\n> insert into t1 ( product ) \n>  select get_product ( c1 , c2 ) from mydb . tables . t1 ;\n> ```\n> \n> Copy\n> \n> \n\nThe ACCESS\\_HISTORY view records the `get_product` function in the:\n\n* `direct_objects_accessed` column because the function is explicitly named in the SQL statement, and\n* `objects_modified` column in the `directSources` array because the function is the source of the values that are inserted into\n  the columns.\n\nSimilarly, the table `t1` is recorded in these same columns:\n\n> |`direct_objects_accessed` |`objects_modified` |\n> | --- | --- |\n> |```\n> [ \n>   { \n>     \"objectDomain\": \"FUNCTION\" ,\n>     \"objectName\": \"MYDB.UDFS.GET_PRODUCT\" ,\n>     \"objectId\": \"2\" ,\n>     \"argumentSignature\": \"(NUM1 NUMBER, NUM2 NUMBER)\",\n>     \"dataType\": \"NUMBER(38,0)\"\n>   } ,\n>   { \n>     \"objectDomain\": \"TABLE\" ,\n>     \"objectName\": \"MYDB.TABLES.T1\" ,\n>     \"objectId\": 1,\n>     \"columns\": \n>     [ \n>       { \n>         \"columnName\": \"c1\" ,\n>         \"columnId\": 1\n>       } ,\n>       { \n>         \"columnName\": \"c2\" ,\n>         \"columnId\": 2\n>       } \n>     ] \n>   } \n>  ]\n> ```\n> \n> Copy |```\n>  [ \n>    { \n>      \"objectDomain\": \"TABLE\" ,\n>      \"objectName\": \"MYDB.TABLES.T1\" ,\n>      \"objectId\": 2,\n>      \"columns\": \n>      [ \n>        { \n>          \"columnId\": \"product\" ,\n>          \"columnName\": \"201\" ,\n>          \"directSourceColumns\": \n>          [ \n>            { \n>              \"objectDomain\": \"Table\" ,\n>              \"objectName\": \"MYDB.TABLES.T1\" ,\n>              \"objectId\": \"1\" ,\n>              \"columnName\": \"c1\" \n>            } ,\n>            { \n>              \"objectDomain\": \"Table\" ,\n>              \"objectName\": \"MYDB.TABLES.T1\" ,\n>              \"objectId\": \"1\" ,\n>              \"columnName\": \"c2\" \n>            } ,\n>            { \n>              \"objectDomain\": \"FUNCTION\" ,\n>              \"objectName\": \"MYDB.UDFS.GET_PRODUCT\" ,\n>              \"objectId\": \"2\" ,\n>              \"argumentSignature\": \"(NUM1 NUMBER, NUM2 NUMBER)\",\n>              \"dataType\": \"NUMBER(38,0)\"\n>            } \n>          ] ,\n>          \"baseSourceColumns\": [] \n>        } \n>      ] \n>    } \n>  ]\n> ```\n> \n> Copy |\n> \n>\n\n#### Shared UDFs \u00b6\n\nShared UDFs can be referenced directly or indirectly:\n\n* A direct reference is the same as calling the UDF explicitly (in this topic) but results\n  in the UDF being recorded in both the `base_objects_accessed` and `direct_objects_accessed` columns.\n* An example of an indirect reference is calling the UDF to create a view:\n  \n  > ```\n  > create view v as \n  >  select get_product ( c1 , c2 ) as vc from t ;\n  > ```\n  > \n  > Copy\n  > \n  > \n  \n  The `base_objects_accessed` column records the UDF and the table.\n  \n  The `direct_objects_accessed` column records the view.\n\n### Tracking objects modified by a DDL operation \u00b6\n\n#### Create a tag with ALLOWED\\_ VALUES \u00b6\n\nCreate the tag:\n\n> ```\n> create tag governance . tags . pii allowed_values 'sensitive' , 'public' ;\n> ```\n> \n> Copy\n> \n> \n\nColumn value:\n\n> ```\n> { \n>   \"objectDomain\": \"TAG\" ,\n>   \"objectName\": \"governance.tags.pii\" ,\n>   \"objectId\": \"1\" ,\n>   \"operationType\": \"CREATE\" ,\n>   \"properties\": { \n>     \"allowedValues\": { \n>       \"sensitive\": { \n>         \"subOperationType\": \"ADD\" \n>       } ,\n>       \"public\": { \n>         \"subOperationType\": \"ADD\" \n>       } \n>     } \n>   } \n>  }\n> ```\n> \n> Copy\n> \n> \n\nNote\n\nIf you do not specify allowed values when creating the tag, the `properties` field is an empty array (i.e. `{}` ).\n\n#### Create a table with a tag and masking policy \u00b6\n\nCreate the table with a masking policy on the column, a tag on the column, and a tag on the table:\n\n> ```\n> create or replace table hr . data . user_info ( \n>   email string \n>     with masking policy governance . policies . email_mask \n>     with tag ( governance . tags . pii = 'sensitive' ) \n>   ) \n>  with tag ( governance . tags . pii = 'sensitive' );\n> ```\n> \n> Copy\n> \n> \n\nColumn value:\n\n> ```\n> { \n>   \"objectDomain\": \"TABLE\" ,\n>   \"objectName\": \"hr.data.user_info\" ,\n>   \"objectId\": \"1\" ,\n>   \"operationType\": \"CREATE\" ,\n>   \"properties\": { \n>     \"tags\": { \n>       \"governance.tags.pii\": { \n>         \"subOperationType\": \"ADD\" ,\n>         \"objectId\": { \n>           \"value\": \"1\" \n>         } ,\n>         \"tagValue\": { \n>           \"value\": \"sensitive\" \n>         } \n>       } \n>     } ,\n>     \"columns\": { \n>       \"email\": { \n>         objectId: { \n>           \"value\": 1\n>         } ,\n>         \"subOperationType\": \"ADD\" ,\n>         \"tags\": { \n>           \"governance.tags.pii\": { \n>             \"subOperationType\": \"ADD\" ,\n>             \"objectId\": { \n>               \"value\": \"1\" \n>             } ,\n>             \"tagValue\": { \n>               \"value\": \"sensitive\" \n>             } \n>           } \n>         } ,\n>         \"maskingPolicies\": { \n>           \"governance.policies.email_mask\": { \n>             \"subOperationType\": \"ADD\" ,\n>             \"objectId\": { \n>               \"value\": 2\n>             } \n>           } \n>         } \n>       } \n>     } \n>   } \n>  }\n> ```\n> \n> Copy\n> \n>\n\n#### Set a masking policy on a tag \u00b6\n\nSet a masking policy on the tag (i.e. tag-based masking ):\n\n> ```\n> alter tag governance . tags . pii set masking policy governance . policies . email_mask ;\n> ```\n> \n> Copy\n> \n> \n\nColumn value:\n\n> ```\n> { \n>   \"objectDomain\": \"TAG\" ,\n>   \"objectName\": \"governance.tags.pii\" ,\n>   \"objectId\": \"1\" ,\n>   \"operationType\": \"ALTER\" ,\n>   \"properties\": { \n>     \"maskingPolicies\": { \n>       \"governance.policies.email_mask\": { \n>         \"subOperationType\": \"ADD\" ,\n>         \"objectId\": { \n>           \"value\": 2\n>         } \n>       } \n>     } \n>   } \n>  }\n> ```\n> \n> Copy\n> \n>\n\n#### Swap a table \u00b6\n\nSwap the table named `t2` with the table named `t3` :\n\n> ```\n> alter table governance . tables . t2 swap with governance . tables . t3 ;\n> ```\n> \n> Copy\n> \n> \n\nNote the two different records in the view.\n\nRecord 1:\n\n> ```\n> { \n>   \"objectDomain\": \"Table\" ,\n>   \"objectId\": 0,\n>   \"objectName\": \"GOVERNANCE.TABLES.T2\" ,\n>   \"operationType\": \"ALTER\" ,\n>   \"properties\": { \n>     \"swapTargetDomain\": { \n>       \"value\": \"Table\" \n>     } ,\n>     \"swapTargetId\": { \n>       \"value\": 0\n>     } ,\n>     \"swapTargetName\": { \n>       \"value\": \"GOVERNANCE.TABLES.T3\" \n>     } \n>   } \n>  }\n> ```\n> \n> Copy\n> \n> \n\nRecord 2:\n\n> ```\n> { \n>   \"objectDomain\": \"Table\" ,\n>   \"objectId\": 0,\n>   \"objectName\": \"GOVERNANCE.TABLES.T3\" ,\n>   \"operationType\": \"ALTER\" ,\n>   \"properties\": { \n>     \"swapTargetDomain\": { \n>       \"value\": \"Table\" \n>     } ,\n>     \"swapTargetId\": { \n>       \"value\": 0\n>     } ,\n>     \"swapTargetName\": { \n>       \"value\": \"GOVERNANCE.TABLES.T2\" \n>     } \n>   } \n>  }\n> ```\n> \n> Copy\n> \n>\n\n#### Drop a masking policy \u00b6\n\nDrop the masking policy:\n\n> ```\n> drop masking policy governance . policies . email_mask ;\n> ```\n> \n> Copy\n> \n> \n\nColumn value:\n\n> ```\n> { \n>   \"objectDomain\" : \"MASKING_POLICY\" ,\n>   \"objectName\": \"governance.policies.email_mask\" ,\n>   \"objectId\" : \"1\" ,\n>   \"operationType\": \"DROP\" ,\n>   \"properties\" : {} \n>  }\n> ```\n> \n> Copy\n> \n> Note\n> \n> The column value is representative and applies to a DROP operation on a tag and row access policy.\n> \n> The `properties` field is an empty array and does not provide any information on the policy prior to the DROP operation.\n> \n>\n\n#### Track tag references on a column \u00b6\n\nQuery the `object_modified_by_ddl` column to monitor how a tag is set on a column.\n\nAs the table administrator, set a tag on a column, unset the tag, and update the tag with a different string value:\n\n> ```\n> alter table hr . tables . empl_info \n>   alter column email set tag governance . tags . test_tag = 'test' ; \n> \n>  alter table hr . tables . empl_info \n>   alter column email unset tag governance . tags . test_tag ; \n> \n>  alter table hr . tables . empl_info \n>   alter column email set tag governance . tags . data_category = 'sensitive' ;\n> ```\n> \n> Copy\n> \n> \n\nAs the data engineer, change the tag value:\n\n> ```\n> alter table hr . tables . empl_info \n>   alter column email set tag governance . tags . data_category = 'public' ;\n> ```\n> \n> Copy\n> \n> \n\nQuery the ACCESS\\_HISTORY view to monitor the changes:\n\n> ```\n> select \n>   query_start_time , \n>   user_name , \n>   object_modified_by_ddl : \"objectName\" ::string as table_name , \n>   'EMAIL' as column_name , \n>   tag_history . value : \"subOperationType\" ::string as operation , \n>   tag_history . key as tag_name , \n>   nvl (( tag_history . value : \"tagValue\" . \"value\" ) ::string , '' ) as value \n>  from \n>   TEST_DB . ACCOUNT_USAGE . access_history ah , \n>   lateral flatten ( input => ah . OBJECT_MODIFIED_BY_DDL : \"properties\" . \"columns\" . \"EMAIL\" . \"tags\" ) tag_history \n>  where true \n>   and object_modified_by_ddl : \"objectDomain\" = 'Table' \n>   and object_modified_by_ddl : \"objectName\" = 'TEST_DB.TEST_SH.T' \n>  order by query_start_time asc ;\n> ```\n> \n> Copy\n> \n> \n\nReturns:\n\n> ```\n> +-----------------------------------+---------------+---------------------+-------------+-----------+-------------------------------+-----------+ \n>  | QUERY_START_TIME                  | USER_NAME     | TABLE_NAME          | COLUMN_NAME | OPERATION | TAG_NAME                      | VALUE     | \n>  +-----------------------------------+---------------+---------------------+-------------+-----------+-------------------------------+-----------+ \n>  | Mon, Feb. 14, 2023 12:01:01 -0600 | TABLE_ADMIN   | HR.TABLES.EMPL_INFO | EMAIL       | ADD       | GOVERNANCE.TAGS.TEST_TAG      | test      | \n>  | Mon, Feb. 14, 2023 12:02:01 -0600 | TABLE_ADMIN   | HR.TABLES.EMPL_INFO | EMAIL       | DROP      | GOVERNANCE.TAGS.TEST_TAG      |           | \n>  | Mon, Feb. 14, 2023 12:03:01 -0600 | TABLE_ADMIN   | HR.TABLES.EMPL_INFO | EMAIL       | ADD       | GOVERNANCE.TAGS.DATA_CATEGORY | sensitive | \n>  | Mon, Feb. 14, 2023 12:04:01 -0600 | DATA_ENGINEER | HR.TABLES.EMPL_INFO | EMAIL       | ADD       | GOVERNANCE.TAGS.DATA_CATEGORY | public    | \n>  +-----------------------------------+---------------+---------------------+-------------+-----------+-------------------------------+-----------+\n> ```\n> \n>\n\n### Call a stored procedure \u00b6\n\nConsider the following stored procedure and assume it is stored in the schema named `mydb.procedures` :\n\n> ```\n> create or replace procedure get_id_value ( name string ) \n>  returns string not null \n>  language javascript \n>  as \n>  $$ \n>   var my_sql_command = \"select id from A where name = '\" + NAME + \"'\" ; \n>   var statement = snowflake . createStatement ( { sqlText : my_sql_command } ); \n>   var result = statement . execute (); \n>   result . next (); \n>   return result . getColumnValue ( 1 ); \n>  $$ \n>  ;\n> ```\n> \n> Copy\n> \n> \n\nCalling `my_procedure` directly results in recording the procedure details in both the `direct_objects_accessed` and `base_objects_accessed` columns as follows:\n\n> ```\n> [ \n>   { \n>     \"objectDomain\": \"PROCEDURE\" ,\n>     \"objectName\": \"MYDB.PROCEDURES.GET_ID_VALUE\" ,\n>     \"argumentSignature\": \"(NAME STRING)\",\n>     \"dataType\": \"STRING\" \n>   } \n>  ]\n> ```\n> \n> Copy\n> \n> \n\nThis example is analogous to calling a UDF (in this topic).\n\n### Ancestor queries with stored procedures \u00b6\n\nYou can use the `parent_query_id` and `root_query_id` columns to understand how stored procedure calls relate to each other.\n\nSuppose that you have three different stored procedure statements and you run them in the following order:\n\n> ```\n> CREATE OR REPLACE PROCEDURE myproc_child () \n>  RETURNS INTEGER \n>  LANGUAGE SQL \n>  AS \n>  $$ \n>   BEGIN \n>   SELECT * FROM mydb . mysch . mytable ; \n>   RETURN 1 ; \n>   END \n>  $$; \n> \n>  CREATE OR REPLACE PROCEDURE myproc_parent () \n>  RETURNS INTEGER \n>  LANGUAGE SQL \n>  AS \n>  $$ \n>   BEGIN \n>   CALL myproc_child (); \n>   RETURN 1 ; \n>   END \n>  $$; \n> \n>  CALL myproc_parent ();\n> ```\n> \n> Copy\n> \n> \n\nA query on the ACCESS\\_HISTORY view records the information as follows:\n\n> ```\n> SELECT \n>   query_id , \n>   parent_query_id , \n>   root_query_id , \n>   direct_objects_accessed \n>  FROM \n>   SNOWFLAKE . ACCOUNT_USAGE . ACCESS_HISTORY ;\n> ```\n> \n> Copy\n> \n> ```\n> +----------+-----------------+---------------+-----------------------------------+ \n>  | QUERY_ID | PARENT_QUERY_ID | ROOT_QUERY_ID | DIRECT_OBJECTS_ACCESSED           | \n>  +----------+-----------------+---------------+-----------------------------------+ \n>  |  1       | NULL            | NULL          | [{\"objectName\": \"myproc_parent\"}] | \n>  |  2       | 1               | 1             | [{\"objectName\": \"myproc_child\"}]  | \n>  |  3       | 2               | 1             | [{\"objectName\": \"mytable\"}]       | \n>  +----------+-----------------+---------------+-----------------------------------+\n> ```\n> \n> \n\n* The first row corresponds to calling the second procedure named `myproc_parent` as shown in the `direct_objects_accessed` column.\n  \n  The `parent_query_id` and `root_query_id` columns return NULL because you called this stored procedure directly.\n* The second row corresponds to the query that calls the first procedure named `myproc_child` as shown in the `direct_objects_accessed column` .\n  \n  The `parent_query_id` and `root_query_id` columns return the same query ID because the query calling `myproc_child` was\n  initiated by the query calling `myproc_parent` , which you called directly.\n* The third row corresponds to the query that accessed the table named `mytable` in the `myproc_child` procedure as shown in\n  the `direct_objects_accessed` column.\n  \n  The `parent_query_id` column returns the query ID of the query that accessed `mytable` , which corresponds to calling `myproc_child` . That stored procedure was initiated by the query calling `myproc_parent` , which is shown in the `root_query_id` column.\n\n### Sequence \u00b6\n\nConsider the following SQL statement that creates a sequence:\n\n```\nCREATE SEQUENCE SEQ \n  START = 2 \n  INCREMENT = 7 \n  COMMENT = 'Comment on sequence' ;\n```\n\nCopy\n\nCreating this sequence results in the following entry in the access history:\n\n```\n{ \n \"objectDomain\" : \"Sequence\" , \n \"objectId\" : 1 , \n \"objectName\" : \"TEST_DB.TEST_SCHEMA.SEQ\" , \n \"operationType\" : \"CREATE\" , \n \"properties\" : { \n \"start\" : { \n \"value\" : \"2\" \n }, \n \"increment\" : { \n \"value\" : \"7\" \n }, \n \"comment\" : { \n \"value\" : \"Comment on Sequence\" \n } \n } \n }\n```\n\nCopy\n\n### Join \u00b6\n\nA join in a query shows up in the access history as a `joinObject` in the `direct_accessed_objects` column. The `joinObject` does\nnot appear in other columns because access history only tracks joins that are explicitly mentioned in the query.\n\nFor example, consider the following query that joins table `t1` with table `t2` :\n\n```\nCREATE OR REPLACE VIEW v1 ( vc1 , vc2 ) AS \n  SELECT \n    t1 . c1 AS vc1 , \n    t2 . c2 AS vc2 \n FROM t1 LEFT OUTER JOIN t2    ON t1 . c2 = t2 . c1 ;\n```\n\nCopy\n\nExecuting this query results in the following appearing for the `t1` object in the `direct_accessed_objects` column:\n\n```\n{ \n \"columns\" : [ \n { \n \"columnId\" : 0 , \n \"columnName\" : \"C1\" \n }, \n { \n \"columnId\" : 0 , \n \"columnName\" : \"C2\" \n } \n ], \n \"joinObjects\" : [ { \"joinType\" : \"LEFT_OUTER_JOIN\" , \"node\" : { \"objectDomain\" : \"Table\" , \"objectId\" : 0 , \"objectName\" : \"DB1.SCH.T2\" } } ], \"objectDomain\" : \"Table\" , \n \"objectId\" : 0 , \n \"objectName\" : \"DB1.SCH.T1\" \n }\n```\n\nCopy\n\nNote\n\nIn this example, access history wouldn\u2019t contain a `joinObject` for the `t2` object because it would be redundant to the information\nprovided by the `joinObject` for table `t1` .\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)"
      ],
      "full_content": null
    },
    {
      "url": "https://blog.greybeam.ai/snowflake-cost-per-query/",
      "title": "Deep Dive: Snowflake's Query Cost and Idle Time Attribution",
      "publish_date": "2024-10-22",
      "excerpts": [
        "[](https://www.greybeam.ai/)\n\n* [Blog](https://blog.greybeam.ai/)\n* [Waitlist](https://greybeam.ai)\n* [Customer Stories](https://blog.greybeam.ai/tag/customer-story/)\n\n Subscribe\n\nSep 9, 2024 13 min read How-To\n\n# A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query\n\nSnowflake's new QUERY\\_ATTRIBUTION\\_HISTORY view\n\nSnowflake recently released a new feature for granular cost attribution down to individual queries through the `QUERY_ATTRIBUTION_HISTORY` view in `ACCOUNT_USAGE` . As a company focused on SQL optimization, we at Greybeam were eager to dive in and see how this new capability compares to our own custom cost attribution logic. What we found was surprising - and it led us down a rabbit hole of query cost analysis.\n\n### The Promise and Limitations of QUERY\\_ATTRIBUTION\\_HISTORY\n\nThe new view aims to provide visibility into the compute costs associated with each query. Some key things to note:\n\n* Data is only available from July 1, 2024 onwards\n* Short queries (<100ms) are excluded\n* Idle time is not included in the attributed costs\n* There can be up to a 6 hour delay in data appearing\n\nThere's also a `WAREHOUSE_UTILIZATION` view that displays cost of idle time. At the time of writing, this must be enabled by your Snowflake support team.\n\n### Our Initial Findings\n\nWe set up a test with an X-Small warehouse and 600 second auto-suspend to dramatically illustrate idle time. Running a series of short queries (mostly <500ms) over an hour, we expected to see a very small fraction of the total credits in that hour attributed to our queries, but we were very wrong.\n\nOn September 4th at the 14th hour, ~40 seconds of queries were executed and some how in the `QUERY_ATTRIBUTION_HISTORY` view it showed that nearly half of the total credits (0.43 of 0.88) attributed to query execution. This seemed impossibly high given the short query runtimes, yet the pattern continues.\n\nQUERY\\_ATTRIBUTION\\_HISTORY aggregated by the hour.\n\nThis may just be an anomaly in our Snowflake account, so try it yourself.\n\n```\nWITH query_execution AS (\n    SELECT\n        qa.query_id\n        , TIMEADD(\n                'millisecond',\n                qh.queued_overload_time + qh.compilation_time +\n                qh.queued_provisioning_time + qh.queued_repair_time +\n                qh.list_external_files_time,\n                qh.start_time\n            ) AS execution_start_time\n        , qh.end_time::timestamp AS end_time\n        , DATEDIFF('MILLISECOND', execution_start_time, qh.end_time)*0.001 as execution_time_secs\n        , qa.credits_attributed_compute\n        , DATE_TRUNC('HOUR', execution_start_time) as execution_start_hour\n        , w.credits_used_compute\n    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY AS qa\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS qh\n        ON qa.query_id = qh.query_id\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS w\n        ON execution_start_hour = w.start_time\n        AND qh.warehouse_id = w.warehouse_id\n    WHERE\n        qh.warehouse_id = 4\n)\n\nSELECT\n    DATE_TRUNC('HOUR', execution_start_time) AS hour\n    , SUM(execution_time_secs) AS total_execution_time_secs\n    , COUNT(*) AS num_queries\n    , SUM(credits_attributed_compute) AS query_credits\n    , ANY_VALUE(credits_used_compute) AS metered_credits\nFROM query_execution\nGROUP BY ALL\nORDER BY 1 ASC\n;\n```\n\n### Digging Deeper\n\nTo investigate further, we compared the results to our own custom cost attribution logic that accounts for idle time. Here\u2019s a snippet of what we found for the same hour:\n\nGreybeam\u2019s internal query cost attribution results\n\nAs you can see, our calculations show much smaller fractions of credits attributed to the actual query runtimes for the first hour, with the bulk going to idle periods. This aligns much more closely with our expectations given the warehouse configuration, and it works historically!\n\n### Potential Issues\n\nAt the time of writing, we\u2019ve identified a few potential problems with the new view:\n\n1. Warehouse ID mismatch\u200a\u2014\u200aThe `warehouse_id` in `QUERY_ATTRIBUTION_HISTORY` doesn't match the actual `warehouse_id` from `QUERY_HISTORY` .\n2. Inflated query costs\u200a\u2014\u200aThe credits attributed to short queries seem disproportionately high in some cases.\n3. Idle time accounting\u200a\u2014\u200aIt\u2019s unclear how idle time factors into the attribution, if at all.\n\nWe\u2019ve raised these concerns with Snowflake, and they\u2019ve recommended filing a support ticket for further investigation. In the meantime, we\u2019ll continue to rely on our custom attribution logic for accuracy.\n\n## Our Approach to Query Cost Attribution\n\nGiven the discrepancies we\u2019ve found, we wanted to share our methodology for calculating per-query costs, including idle time. Here\u2019s an overview of our process:\n\n1. Gather warehouse suspend events\n2. Enrich query data with execution times and idle periods\n3. Create a timeline of all events (queries and idle periods)\n4. Join with `WAREHOUSE_METERING_HISTORY` to attribute costs\n\nBefore we dive in, let\u2019s cover a few basics:\n\nSnippet of WAREHOUSE\\_METERING\\_HISTORY\n\n* We use `WAREHOUSE_METERING_HISTORY` as our source of truth for warehouse compute credits. The credits billed here will reconcile with Snowflake\u2019s cost management dashboards.\n* Credits here are represented on an hourly grain. We like to refer to this as _credits metered_ , analogous to how most homes in North America are metered for their electricity. In our solution, we\u2019ll need to allocate queries and idle times into their metered hours.\n* We use a weighted time-based approach to attribute costs within the metered hour. In reality, Snowflake\u2019s credit attribution is likely much more complex, especially in situations with more clusters or warehouse scaling.\n\nHow we need to break down our queries and idle times.\n\nThe full SQL query will be available at the end of this blog.\n\n### Step 1: Gather Warehouse Suspend Events\n\n\u2757\n\nWe've updated this article with an optimization using `ASOF JOIN` . Check out how to use ASOF JOINs [here](https://blog.greybeam.ai/snowflake-asof-join/) .\n\nFirst, we need to know when warehouses are suspended, this is pulled from [`WAREHOUSE_EVENTS_HISTORY`](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_events_history?ref=blog.greybeam.ai) .\n\n```\nWITH warehouse_events AS (\n    SELECT\n        warehouse_id\n        , timestamp\n        , LAG(timestamp) OVER (PARTITION BY warehouse_id ORDER BY timestamp) as lag_timestamp\n    FROM snowflake.account_usage.warehouse_events_history\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'\n        AND DATEADD('DAY', 15, timestamp) >= current_date\n)\n```\n\nIt\u2019s worth mentioning that the `WAREHOUSE_EVENTS_HISTORY` view has had a reputation for being somewhat unreliable. In fact, Ian from Select considered using this table in his [cost-per-query analysis](https://select.dev/posts/cost-per-query?ref=blog.greybeam.ai) but ultimately decided against it due to these reliability concerns.\n\nHowever, we\u2019ve been in touch with a Snowflake engineer who informed us that recent updates have significantly improved the reliability of this table. While it may not be perfect, we're only using it for the suspended timestamp and not the cluster events, so it\u2019s \u201cclose enough\u201d for our purposes\n\nIn addition, warehouse suspension doesn\u2019t actually occur during the `SUSPEND_WAREHOUSE` event. Technically, it happens when the `WAREHOUSE_CONSISTENT` event is logged. The `WAREHOUSE_CONSISTENT` event indicates that all compute resources associated with the warehouse have been fully released. You can find more information about this event in the [Snowflake documentation](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_events_history?ref=blog.greybeam.ai) .\n\nFor the sake of simplicity (and because the time difference is usually negligible), we\u2019re sticking with the `SUSPEND_WAREHOUSE` event in our analysis. This approach gives us a good balance between accuracy and complexity in our cost attribution model.\n\nBefore moving onto enriching query data, we want to apply filters to reduce the load from table scans. Feel free to adjust the dates as you see fit.\n\n```\nWITH warehouse_list AS (\n    SELECT \n        DISTINCT warehouse_name,\n        warehouse_id\n    FROM warehouse_metering_history\n    WHERE \n        warehouse_name IS NOT NULL\n        AND start_time >= $startDate\n),\n\nwarehouse_events AS (\n    SELECT\n        weh.warehouse_id\n        , weh.timestamp\n    FROM warehouse_events_history as weh\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'        \n),\n\nqueries_filtered AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.warehouse_name\n        , q.warehouse_size\n        , q.role_name\n        , q.user_name\n        , q.query_text\n        , q.query_hash\n        , q.queued_overload_time\n        , q.compilation_time\n        , q.queued_provisioning_time\n        , q.queued_repair_time\n        , q.list_external_files_time\n        , q.start_time\n        , TIMEADD(\n                'millisecond',\n                q.queued_overload_time + q.compilation_time +\n                q.queued_provisioning_time + q.queued_repair_time +\n                q.list_external_files_time,\n                q.start_time\n            ) AS execution_start_time\n        , q.end_time::timestamp AS end_time\n        , w.timestamp AS suspended_at\n        , MAX(q.end_time) OVER (PARTITION BY q.warehouse_id, w.timestamp ORDER BY execution_start_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as end_time_max\n        , LEAD(execution_start_time) OVER (PARTITION BY q.warehouse_id ORDER BY execution_start_time ASC) as next_query_at\n    FROM query_history AS q\n    ASOF JOIN warehouse_events AS w\n        MATCH_CONDITION (q.end_time::timestamp <= w.timestamp)\n        ON q.warehouse_id = w.warehouse_id\n    WHERE\n        q.warehouse_size IS NOT NULL\n        AND q.execution_status = 'SUCCESS'\n        AND start_time >= $startDate\n        AND EXISTS (\n            SELECT 1\n            FROM warehouse_list AS wl\n            WHERE\n                q.warehouse_id = wl.warehouse_id\n            )\n)\n```\n\n### Step 2: Enrich Query Data\n\nIn this step, we take the raw query data and enrich it with additional information that will allow us to breakdown query and idle times into their hourly components. We choose hourly slots because the source of truth for credits comes from `WAREHOUSE_METERING_HISTORY` , which is on an hourly grain.\n\n```\nqueries_enriched AS (\n      SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.execution_start_time\n        , q.end_time::timestamp AS end_time\n        , q.end_time_max AS end_time_running\n        , q.next_query_at\n        , q.suspended_at\n        , (CASE\n            WHEN q.next_query_at > q.suspended_at THEN q.end_time_max\n            WHEN q.next_query_at > q.end_time_max THEN q.end_time_max\n            WHEN q.next_query_at < q.end_time_max THEN NULL\n            WHEN q.next_query_at IS NULL THEN q.end_time\n            END)::timestamp AS idle_start_at\n        , IFF(idle_start_at IS NOT NULL, LEAST(COALESCE(next_query_at, '3000-01-01'), q.suspended_at), NULL)::timestamp AS idle_end_at\n        , HOUR(execution_start_time::timestamp) = HOUR(q.end_time::timestamp) AS is_same_hour_query\n        , HOUR(idle_start_at) = HOUR(idle_end_at) AS is_same_hour_idle\n        , DATE_TRUNC('HOUR', execution_start_time) AS query_start_hour\n        , DATE_TRUNC('HOUR', idle_start_at) as idle_start_hour\n        , DATEDIFF('HOUR', execution_start_time, q.end_time) AS hours_span_query\n        , DATEDIFF('HOUR', idle_start_at, idle_end_at) AS hours_span_idle\n    FROM queries_filtered AS q\n)\n```\n\nKey points to highlight:\n\n1. **Execution Start Time** : We calculate the actual execution start time that the query begins running on the warehouse (thanks [Ian](https://select.dev/posts/cost-per-query?ref=blog.greybeam.ai) !).\n2. **Idle Time Calculation** : We determine idle periods by looking at the gap between our running query end time and the next query\u2019s start time (or warehouse suspension time). This is because it's possible a prior query is still running, so we need to keep track of the running end time and compare it against the start time of the next query. If the next query starts after the end of our current query, then there\u2019s idle time.\n3. **Hour Boundaries** : We identify queries and idle periods that span hour boundaries. This is important because Snowflake bills by the hour, so we need to properly attribute costs that cross these boundaries.\n4. **Warehouse Suspension** : We join with the warehouse\\_events table to identify when warehouses were suspended, which helps us accurately determine the end of idle periods. If the next query starts after the warehouse suspends, then the end of the idle period is the suspension time.\n\n### Step 3: Create Timeline of All Events\n\nWe now need to create an hourly timeline of all events so that we can reconcile our credits with `WAREHOUSE_METERING_HISTORY` . The timeline of all events can be broken down into 4 components:\n\n1. A query executed and ended in the same hour\n2. Idle time started and ended in the same hour\n3. A query executed and ended in a different hour\n4. Idle time started and ended in a different hour\n\n1 and 2 are straight forward since they don\u2019t cross any hourly boundaries we can simply select from the dataset and join directly to `WAREHOUSE_METERING_HISTORY` .\n\n```\nSELECT\n        q.query_id\n        , q.warehouse_id\n        , 'query' AS type\n        , q.execution_start_time AS event_start_at\n        , q.end_time AS event_end_at\n        , q.query_start_hour AS meter_start_hour\n        , NULL AS meter_end_hour\n        , q.execution_start_time AS meter_start_at\n        , q.end_time AS meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    WHERE\n        q.is_same_hour_query = TRUE\n    \n    UNION ALL\n    \n    SELECT\n        'idle_' || q.query_id\n        , q.warehouse_id\n        , 'idle' AS type\n        , q.idle_start_at AS event_start_at\n        , q.idle_end_at AS event_end_at\n        , q.idle_start_hour AS meter_start_hour\n        , NULL AS meter_end_hour\n        , q.idle_start_at AS meter_start_at\n        , q.idle_end_at AS meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    WHERE\n        q.is_same_hour_idle = TRUE\n```\n\nFor 3 and 4, we need a record for each hour that the queries and idle times ran within. For example, if a query ran from 7:55PM to 10:40PM, we\u2019d need a record for 7, 8, 9, and 10PM.\n\nA query that executed across 4 hourly slots (including 0).\n\nOriginally we used a slightly more complicated join:\n\n```\nFROM queries_enriched AS q\nJOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS m\n    ON q.warehouse_id = m.warehouse_id\n    AND m.start_time >= q.meter_start_time\n    AND m.start_time < q.end_time\n```\n\nThis took forever to run on a large account. Instead, we first create records for each hour so that the join to `WAREHOUSE_METERING_HISTORY` is a direct join in the next step.\n\n```\nnumgen AS (\n    SELECT\n        0 AS num\n    UNION ALL \n    \n    SELECT\n        ROW_NUMBER() OVER (ORDER BY NULL)\n    FROM table(generator(ROWCOUNT=>24)) -- assuming no one has idle or queries running more than 24 hours\n),\n\nmega_timeline AS (\n    -- parts 1 and 2 here\n    \n    SELECT\n        'idle_' || q.query_id\n        , q.warehouse_id\n        , 'idle'\n        , q.idle_start_at AS event_start_at\n        , q.idle_end_at AS event_end_at\n        , DATEADD('HOUR', n.num, DATE_TRUNC('HOUR', q.idle_start_at)) AS meter_start_hour\n        , DATEADD('HOUR', n.num + 1, DATE_TRUNC('HOUR', q.idle_start_at)) AS meter_end_hour\n        , GREATEST(meter_start_hour, q.idle_start_at) as meter_start_at\n        , LEAST(meter_end_hour, q.idle_end_at) as meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    LEFT JOIN numgen AS n\n        ON q.hours_span_idle >= n.num\n    WHERE\n        q.is_same_hour_idle = FALSE\n    \n    UNION ALL\n    \n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , 'query'\n        , q.execution_start_time AS event_start_at\n        , q.end_time AS event_end_at\n        , DATEADD('HOUR', n.num, DATE_TRUNC('HOUR', q.execution_start_time)) AS meter_start_hour\n        , DATEADD('HOUR', n.num + 1, DATE_TRUNC('HOUR', q.execution_start_time)) AS meter_end_hour\n        , GREATEST(meter_start_hour, q.execution_start_time) as meter_start_at\n        , LEAST(meter_end_hour, q.end_time) as meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    LEFT JOIN numgen AS n\n        ON q.hours_span_query >= n.num\n    WHERE\n        q.is_same_hour_query = FALSE\n)\n```\n\n### Step 4: Attribute Costs\n\nFinally, with each query and idle period properly allocated to their hourly slots, we can directly join to `WAREHOUSE_METERING_HISTORY` and calculate our credits used.\n\n```\nmetered AS (\n    SELECT\n        m.query_id\n        , m.warehouse_id\n        , m.type\n        , m.event_start_at\n        , m.event_end_at\n        , m.meter_start_hour\n        , m.meter_start_at\n        , m.meter_end_at\n        , m.meter_time_secs\n        , SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour) AS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN snowflake.account_usage.warehouse_metering_history AS w -- inner join because both tables have different delays\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time -- we can directly join now since we used our numgen method\n)\n```\n\nIn this approach we allocate credits based on the proportion of the total execution time in that hour:\n\n1. **Time-based Weighting** : We use the duration of each event (query or idle period) as the basis for our weighting. This is represented by `m.meter_time_secs` .\n2. **Hourly Totals** : We calculate the total time for all events within each hour for each warehouse `SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour)` .\n3. **Credit Allocation** : We then allocate credits to each event based on its proportion of the total time in that hour `(m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute` .\n\nOne important note: This approach assumes that all time within an hour is equally valuable in terms of credit consumption. In reality, Snowflake may have more complex internal algorithms for credit attribution, especially for multi-cluster warehouses or warehouses that change size within an hour. However, this weighted time-based approach provides a reasonable and transparent method for cost attribution that aligns well with Snowflake\u2019s consumption-based billing model.\n\n## Conclusion\n\nWhile Snowflake\u2019s new `QUERY_ATTRIBUTION_HISTORY` view is a promising step towards easier cost attribution, our initial testing reveals some potential issues that need to be addressed. For now, we recommend carefully validating the results against your own calculations and metering history.\n\nWe\u2019re excited to see how this feature evolves and will continue to monitor its accuracy. In the meantime, implementing your own cost attribution logic can provide valuable insights into query performance and resource utilization.\n\nBy accounting for idle time and carefully tracking query execution across hour boundaries, we\u2019re able to get a more complete and accurate picture of costs. This level of detail is crucial for optimizing Snowflake usage and controlling costs effectively.\n\n* * *\n\n## Struggling with Snowflake costs?\n\nAll usage-based cloud platforms can get expensive when not used carefully. There are a ton of controls teams can fiddle with to get a handle on their Snowflake costs. At Greybeam, we\u2019ve built a query performance and observability platform that automagically optimizes SQL queries sent to Snowflake, saving you thousands in compute costs. Reach out to [[email protected]](/cdn-cgi/l/email-protection) to learn more about how we can optimize your Snowflake environment.\n\n* * *\n\n## Full SQL Cost Attribution\n\n```\nSET startDate = DATEADD('DAY', -15, current_date);\nWITH warehouse_list AS (\n    SELECT \n        DISTINCT warehouse_name,\n        warehouse_id\n    FROM warehouse_metering_history\n    WHERE \n        warehouse_name IS NOT NULL\n        AND start_time >= $startDate\n),\n\nwarehouse_events AS (\n    SELECT\n        weh.warehouse_id\n        , weh.timestamp\n    FROM warehouse_events_history as weh\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'        \n),\n\nqueries_filtered AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.warehouse_name\n        , q.warehouse_size\n        , q.role_name\n        , q.user_name\n        , q.query_text\n        , q.query_hash\n        , q.queued_overload_time\n        , q.compilation_time\n        , q.queued_provisioning_time\n        , q.queued_repair_time\n        , q.list_external_files_time\n        , q.start_time\n        , TIMEADD(\n                'millisecond',\n                q.queued_overload_time + q.compilation_time +\n                q.queued_provisioning_time + q.queued_repair_time +\n                q.list_external_files_time,\n                q.start_time\n            ) AS execution_start_time\n        , q.end_time::timestamp AS end_time\n        , w.timestamp AS suspended_at\n        , MAX(q.end_time) OVER (PARTITION BY q.warehouse_id, w.timestamp ORDER BY execution_start_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as end_time_max\n        , LEAD(execution_start_time) OVER (PARTITION BY q.warehouse_id ORDER BY execution_start_time ASC) as next_query_at\n    FROM query_history AS q\n    ASOF JOIN warehouse_events AS w\n        MATCH_CONDITION (q.end_time::timestamp <= w.timestamp)\n        ON q.warehouse_id = w.warehouse_id\n    WHERE\n        q.warehouse_size IS NOT NULL\n        AND q.execution_status = 'SUCCESS'\n        AND start_time >= $startDate\n        AND EXISTS (\n            SELECT 1\n            FROM warehouse_list AS wl\n            WHERE\n                q.warehouse_id = wl.warehouse_id\n            )\n),\n\nqueries_enriched AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.execution_start_time\n        , q.end_time::timestamp AS end_time\n        , q.end_time_max AS end_time_running\n        , q.next_query_at\n        , q.suspended_at\n        , (CASE\n            WHEN q.next_query_at > q.suspended_at THEN q.end_time_max\n            WHEN q.next_query_at > q.end_time_max THEN q.end_time_max\n            WHEN q.next_query_at < q.end_time_max THEN NULL\n            WHEN q.next_query_at IS NULL THEN q.end_time\n            END)::timestamp AS idle_start_at\n        , IFF(idle_start_at IS NOT NULL, LEAST(COALESCE(next_query_at, '3000-01-01'), q.suspended_at), NULL)::timestamp AS idle_end_at\n        , HOUR(execution_start_time::timestamp) = HOUR(q.end_time::timestamp) AS is_same_hour_query\n        , HOUR(idle_start_at) = HOUR(idle_end_at) AS is_same_hour_idle\n        , DATE_TRUNC('HOUR', execution_start_time) AS query_start_hour\n        , DATE_TRUNC('HOUR', idle_start_at) as idle_start_hour\n        , DATEDIFF('HOUR', execution_start_time, q.end_time) AS hours_span_query\n        , DATEDIFF('HOUR', idle_start_at, idle_end_at) AS hours_span_idle\n    FROM queries_filtered AS q\n),\n\nnumgen AS (\n    SELECT\n        0 AS num\n    UNION ALL \n    \n    SELECT\n        ROW_NUMBER() OVER (ORDER BY NULL)\n    FROM table(generator(ROWCOUNT=>24)) -- assuming no one has idle or queries running more than 24 hours\n),\n\nmega_timeline AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , 'query' AS type\n        , q.execution_start_time AS event_start_at\n        , q.end_time AS event_end_at\n        , DATEDIFF('MILLISECOND', event_start_at, event_end_at)*0.001 AS event_time_secs\n        , q.query_start_hour AS meter_start_hour\n        , NULL AS meter_end_hour\n        , q.execution_start_time AS meter_start_at\n        , q.end_time AS meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    WHERE\n        q.is_same_hour_query = TRUE\n    \n    UNION ALL\n    \n    SELECT\n        'idle_' || q.query_id\n        , q.warehouse_id\n        , 'idle' AS type\n        , q.idle_start_at AS event_start_at\n        , q.idle_end_at AS event_end_at\n        , DATEDIFF('MILLISECOND', event_start_at, event_end_at)*0.001 AS event_time_secs\n        , q.idle_start_hour AS meter_start_hour\n        , NULL AS meter_end_hour\n        , q.idle_start_at AS meter_start_at\n        , q.idle_end_at AS meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    WHERE\n        q.is_same_hour_idle = TRUE\n\n    UNION ALL\n\n    SELECT\n        'idle_' || q.query_id\n        , q.warehouse_id\n        , 'idle'\n        , q.idle_start_at AS event_start_at\n        , q.idle_end_at AS event_end_at\n        , DATEDIFF('MILLISECOND', event_start_at, event_end_at)*0.001 AS event_time_secs\n        , DATEADD('HOUR', n.num, DATE_TRUNC('HOUR', q.idle_start_at)) AS meter_start_hour\n        , DATEADD('HOUR', n.num + 1, DATE_TRUNC('HOUR', q.idle_start_at)) AS meter_end_hour\n        , GREATEST(meter_start_hour, q.idle_start_at) as meter_start_at\n        , LEAST(meter_end_hour, q.idle_end_at) as meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    LEFT JOIN numgen AS n\n        ON q.hours_span_idle >= n.num\n    WHERE\n        q.is_same_hour_idle = FALSE\n    \n    UNION ALL\n    \n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , 'query'\n        , q.execution_start_time AS event_start_at\n        , q.end_time AS event_end_at\n        , DATEDIFF('MILLISECOND', event_start_at, event_end_at)*0.001 AS event_time_secs\n        , DATEADD('HOUR', n.num, DATE_TRUNC('HOUR', q.execution_start_time)) AS meter_start_hour\n        , DATEADD('HOUR', n.num + 1, DATE_TRUNC('HOUR', q.execution_start_time)) AS meter_end_hour\n        , GREATEST(meter_start_hour, q.execution_start_time) as meter_start_at\n        , LEAST(meter_end_hour, q.end_time) as meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    LEFT JOIN numgen AS n\n        ON q.hours_span_query >= n.num\n    WHERE\n        q.is_same_hour_query = FALSE\n    ),\n\nmetered AS (\n    SELECT\n        m.query_id\n        , REPLACE(m.query_id, 'idle_', '') as original_query_id\n        , m.warehouse_id\n        , m.type\n        , m.event_start_at\n        , m.event_end_at\n        , m.event_time_secs\n        , m.meter_start_hour\n        , m.meter_start_at\n        , m.meter_end_at\n        , m.meter_time_secs\n        , SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour) AS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN warehouse_metering_history AS w\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time\n),\n\nfinal AS (\n    SELECT\n        m.* EXCLUDE total_meter_time_secs, meter_end_at, original_query_id\n        , q.query_text\n        , q.query_hash\n        , q.warehouse_size\n        , q.warehouse_name\n        , q.role_name\n        , q.user_name\n    FROM metered AS m\n    JOIN queries_filtered AS q\n        ON m.original_query_id = q.query_id\n)\nSELECT\n    *\nFROM final\n;\n```\n\n#### Kyle Cheung\n\noptimization for you, you, you, you, and you\n\n### Comments ()\n\n### You might also like...\n\n## Cut Costs by Querying Snowflake Tables in DuckDB with Apache Arrow\n\nJan 30, 2025\n\n## Querying Snowflake Managed Iceberg Tables with DuckDB\n\nDec 12, 2024\n\n## Getting Started with pyIceberg and AWS Glue\n\nDec 6, 2024\n\n[Powered by Ghost](https://ghost.org/)"
      ],
      "full_content": null
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/query_attribution_history",
      "title": "QUERY_ATTRIBUTION_HISTORY view | Snowflake Documentation",
      "publish_date": null,
      "excerpts": [
        "Reference General reference SNOWFLAKE database Account Usage QUERY\\_ATTRIBUTION\\_HISTORY\n\nSchemas:\n    ACCOUNT\\_USAGE\n\n# QUERY\\_ ATTRIBUTION\\_ HISTORY view \u00b6\n\nThis Account Usage view can be used to determine the compute cost of a given query run on warehouses in your account\nin the last 365 days (1 year).\n\nFor more information, see Viewing cost by tag in SQL .\n\n## Columns \u00b6\n\n|Column name |Data type |Description |\n| --- | --- | --- |\n|QUERY\\_ID |VARCHAR |Internal/system-generated identifier for the SQL statement. |\n|PARENT\\_QUERY\\_ID |VARCHAR |Query ID of the parent query or NULL if the query does not have a parent. |\n|ROOT\\_QUERY\\_ID |VARCHAR |Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n|WAREHOUSE\\_ID |NUMBER |Internal/system-generated identifier for the warehouse that the query was executed on. |\n|WAREHOUSE\\_NAME |VARCHAR |Name of the warehouse that the query executed on. |\n|QUERY\\_HASH |VARCHAR |The hash value computed based on the canonicalized SQL text. |\n|QUERY\\_PARAMETERIZED\\_HASH |VARCHAR |The hash value computed based on the parameterized query. |\n|QUERY\\_TAG |VARCHAR |Query tag set for this statement through the QUERY\\_TAG session parameter. |\n|USER\\_NAME |VARCHAR |User who issued the query. |\n|START\\_TIME |TIMESTAMP\\_LTZ |Time when query execution started (in the local time zone). |\n|END\\_TIME |TIMESTAMP\\_LTZ |Time when query execution ended (in the local time zone). |\n|CREDITS\\_ATTRIBUTED\\_COMPUTE |NUMBER |Number of credits attributed to this query. Includes only the credit usage for the query execution and doesn\u2019t include any warehouse idle time. |\n|CREDITS\\_USED\\_QUERY\\_ACCELERATION |NUMBER |Number of credits consumed by the Query Acceleration Service to accelerate the query. NULL if the query is not accelerated. . . The total cost for an accelerated query is the sum of this column and the CREDITS\\_ATTRIBUTED\\_COMPUTE column. |\n\n## Usage notes \u00b6\n\n* Latency for this view can be up to eight hours.\n* This view displays results for any role granted the USAGE\\_VIEWER or GOVERNANCE\\_VIEWER database role .\n\n* The value in the `credits_attributed_compute` column contains the warehouse credit usage for executing the query,\n  inclusive of any resizing and/or autoscaling of multi-cluster warehouse(s). This cost is attributed based on\n  the weighted average of the resource consumption.\n  \n  The value doesn\u2019t include any credit usage for warehouse idle time. Idle time is a period\n  of time in which no queries are running in the warehouse and can be measured at the warehouse level.\n  \n  The value doesn\u2019t include any other credit usage that is incurred as a result of query execution.\n  For example, the following are not included in the query cost:\n  \n    + Data transfer costs\n    + Storage costs\n    + Cloud services costs\n    + Costs for serverless features\n    + Costs for tokens processed by AI services\n* For queries that are executed concurrently, the cost of the warehouse is attributed to individual queries based on the\n  weighted average of their resource consumption during a given time interval.\n* Short-running queries (<= ~100ms) are currently too short for per query cost attribution and are not included in the view.\n* Data for all columns is available starting from mid-August, 2024. Some data prior to this date might be available in the view, but\n  might be incomplete.\n\n## Examples \u00b6\n\n### Query costs for related queries \u00b6\n\nTo determine the costs of a specific query and similar queries using the query parameterized hash, replace `<query_id>` and execute the following statements:\n\n```\nSET query_id = '<query_id>' ; \n\n WITH query_hash_of_query AS ( \n  SELECT query_parameterized_hash \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_id = $ query_id \n  LIMIT 1 \n ) \n SELECT \n  query_parameterized_hash , \n  COUNT (*) AS query_count , \n  SUM ( credits_attributed_compute ) AS recurrent_query_attributed_credits \n FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n  AND start_time < CURRENT_DATE \n  AND query_parameterized_hash = ( SELECT query_parameterized_hash FROM query_hash_of_query ) \n GROUP BY ALL ;\n```\n\nCopy\n\n### Query costs for the current user \u00b6\n\nTo determine the costs of queries executed by the current user for the current month, execute the following statement:\n\n```\nSELECT user_name , SUM ( credits_attributed_compute ) AS credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE user_name = CURRENT_USER () \n    AND start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY user_name ;\n```\n\nCopy\n\nFor an example of attributing warehouse costs to users, see Resources shared by users from different departments .\n\n### Query costs for stored procedures \u00b6\n\nFor stored procedures that issue multiple hierarchical queries, you can compute the attributed query costs for the\nprocedure by using the root query ID for the procedure.\n\n1. To find the root query ID for a stored procedure, use the ACCESS\\_HISTORY view . For example,\n   to find the root query ID for a stored procedure, set the `query_id` and execute the following statements:\n   \n   ```\n   SET query_id = '<query_id>' ; \n   \n    SELECT query_id , \n          parent_query_id , \n          root_query_id , \n          direct_objects_accessed \n     FROM SNOWFLAKE . ACCOUNT_USAGE . ACCESS_HISTORY \n     WHERE query_id = $ query_id ;\n   ```\n   \n   Copy\n   \n   For more information, see Ancestor queries with stored procedures .\n2. To sum the query cost for the entire procedure, replace `<root_query_id>` and execute the following statements:\n   \n   ```\n   SET query_id = '<root_query_id>' ; \n   \n    SELECT SUM ( credits_attributed_compute ) AS total_attributed_credits \n     FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n     WHERE ( root_query_id = $ query_id OR query_id = $ query_id );\n   ```\n   \n   Copy\n\n### Additional examples \u00b6\n\nFor more examples, see Resources shared by users from different departments .\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Columns\n2. Usage notes\n3. Examples\n4. Query costs for related queries\n5. Query costs for the current user\n6. Query costs for stored procedures\n7. Additional examples\n\nRelated content\n\n1. Overview of warehouses\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 4
    }
  ]
}

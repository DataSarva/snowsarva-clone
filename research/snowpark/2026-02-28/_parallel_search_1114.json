{
  "search_id": "search_8702390ec065411584db62e8e6699cc5",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-attributing",
      "title": "Attributing cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Visibility Attributing cost\nSection Title: Attributing cost \u00b6\nContent:\nAn organization can apportion the cost of using Snowflake to logical units within the organization (for example, to different\ndepartments, environments, or projects). This chargeback or showback model is useful for accounting purposes and pinpoints\nareas of the organization that could benefit from controls and optimizations that can reduce costs.\nTo attribute costs to different groups like departments or projects, use the following recommended approach:\nUse object tags to associate resources and users with departments or projects.\nUse query tags to associate individual queries with departments or projects when the queries are\nmade by the same application on behalf of users belonging to multiple departments.\nSection Title: Attributing cost \u00b6 > Types of cost attribution scenarios \u00b6\nContent:\nThe following cost attribution scenarios are the most commonly encountered. In these scenarios, warehouses are used as an\nexample of a resource that incurs costs.\nSection Title: Attributing cost \u00b6 > Types of cost attribution scenarios \u00b6\nContent:\n**Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate\nwarehouses with a department. You can use these object tags to attribute the costs incurred by those warehouses to that\ndepartment entirely. **Resources that are shared by users from multiple departments:** An example of this is a warehouse shared by users from\ndifferent departments. In this case, you use object tags to associate each user with a department. The costs of queries are\nattributed to the users. Using the object tags assigned to users, you can break down the costs by department. **Applications or workflows shared by users from different departments:** An example of this is an application that issues\nqueries on behalf of its users.\n ... \nSection Title: Attributing cost \u00b6 > ... > Tagging the resources and users \u00b6\nContent:\nAfter creating and replicating the tags, you can use these tags to identify the warehouses and users belonging to each\ndepartment. For example, because the sales department uses both `warehouse1` and `warehouse2` , you can set the `cost_center` tag to `'SALES'` for both warehouses.\nTip\nIdeally, you should have workflows that automate the process of applying these tags when you create resources and users.\n```\nUSE ROLE tag_admin ; \n\n ALTER WAREHOUSE warehouse1 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse2 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse3 SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n\n ALTER USER finance_user SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n ALTER USER sales_user SET TAG cost_management . tags . cost_center = 'SALES' ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6\nContent:\n**Attributing costs within an account**You can attribute costs within an account by querying the following views in the ACCOUNT_USAGE schema:\nTAG_REFERENCES view : Identifies objects (for example, warehouses and users) that have tags. WAREHOUSE_METERING_HISTORY view : Provides credit usage for warehouses. QUERY_ATTRIBUTION_HISTORY view : Provides the compute costs for queries. The cost per query is\nthe warehouse credit usage for executing the query.For more information on using this view, see About the QUERY_ATTRIBUTION_HISTORY view . **Attributing costs across accounts in an organization**Within an organization, you can also attribute costs for resources that are used **exclusively by a single department** by\nquerying views in the ORGANIZATION_USAGE schema from the organization account .Note\nIn the ORGANIZATION_USAGE schema, the TAG_REFERENCES view is only available in the organization account.\nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6\nContent:\nThe QUERY_ATTRIBUTION_HISTORY view is only available in the ACCOUNT_USAGE schema for an account. There is no\norganization-wide equivalent of the view.\n ... \nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6 > Resources not shared by departments \u00b6\nContent:\nSuppose that you want to attribute costs by department and that each department uses a set of dedicated warehouses.\nIf you tag warehouses with a `cost_center` tag to identify the department that owns the warehouse, you can join the\nACCOUNT_USAGE TAG_REFERENCES view with the WAREHOUSE_METERING_HISTORY view on the `object_id` and `warehouse_id` columns to get usage\ninformation by warehouse, and you can use the `tag_value` column to identify the departments that own those warehouses.\nThe following SQL statement performs this join:\nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6 > Resources not shared by departments \u00b6\nContent:\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6 > Resources not shared by departments \u00b6\nContent:\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ORGANIZATION_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ORGANIZATION_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n          AND tag_database = 'COST_MANAGEMENT' AND tag_schema = 'TAGS' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\nCopy\nSection Title: Attributing cost \u00b6 > ... > Resources shared by users from different departments \u00b6\nContent:\nSuppose that users in different departments share the same warehouses and you want to break down the credits used by each\ndepartment. You can tag the users with a `cost_center` tag to identify the department that they belong to, and you can join\nthe TAG_REFERENCES view with the QUERY_ATTRIBUTION_HISTORY view .\nNote\nYou can only get this data for a single account at a time. You cannot execute a query that retrieves this data across\naccounts in an organization.\nThe next sections provide examples of SQL statements for attributing costs for shared resources.\nCalculating the cost of user queries for the last month\nCalculating the cost of user queries by department without idle time\nCalculating the cost of queries by users without idle time\nCalculating the cost of queries by users without tags\n ... \nSection Title: ... > Resources used by applications that need to attribute costs to different departments \u00b6\nContent:\nThe examples in this section calculate the costs for one or more applications that are powered by Snowflake.\nThe examples assume that these applications set query tags that identify the application for all queries executed. To set the\nquery tag for queries in a session, execute the ALTER SESSION command. For example:\n```\nALTER SESSION SET QUERY_TAG = 'COST_CENTER=finance' ;\n```\nCopy\nThis associates the `COST_CENTER=finance` tag with all subsequent queries executed during the session.\nYou can then use the query tag to trace back the cost incurred by these queries to the appropriate departments.\nThe next sections provide examples of using this approach.\nCalculating the cost of queries by department\nCalculating the cost of queries (excluding idle time) by query tag\nCalculating the cost of queries (including idle time) by query tag\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries by department \u00b6\nContent:\nThe following example calculates the compute credits and the credits used for the query acceleration service for the finance department. This depends on the `COST_CENTER=finance` query tag being applied to the original queries that were executed.\nNote that the costs exclude idle time.\n```\nSELECT \n    query_tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_tag = 'COST_CENTER=finance' \n  GROUP BY query_tag ;\n```\nCopy\n```\n+---------------------+-----------------+------+ \n | QUERY_TAG           | COMPUTE_CREDITS | QAS  | \n |---------------------+-----------------|------| \n | COST_CENTER=finance |      0.00576115 | null | \n +---------------------+-----------------+------+\n```\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (excluding idle time) by query tag \u00b6\nContent:\nThe following example calculates the cost of queries by query tag and includes queries without tags (identified as \u201cuntagged\u201d).\n```\nSELECT \n    COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n  GROUP BY tag \n  ORDER BY compute_credits DESC ;\n```\nCopy\n```\n+-------------------------+-----------------+------+ \n | TAG                     | COMPUTE_CREDITS | QAS  | \n |-------------------------+-----------------+------+ \n | untagged                | 3.623173449     | null | \n | COST_CENTER=engineering | 0.531431948     | null | \n |-------------------------+-----------------+------+\n```\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (including idle time) by query tag \u00b6\nContent:\nThe following example distributes the idle time that is not captured in the per-query cost across departments in proportion\nto their usage of the warehouse.\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (including idle time) by query tag \u00b6\nContent:\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n      AND start_time < CURRENT_DATE \n  ), \n  tag_credits AS ( \n    SELECT \n        COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n        SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n      GROUP BY tag \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n      FROM tag_credits \n  ) \n SELECT \n    tc . tag , \n    tc . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM tag_credits tc , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > About the QUERY_ATTRIBUTION_HISTORY view \u00b6\nContent:\nYou can use the QUERY_ATTRIBUTION_HISTORY view to attribute cost based on queries. The cost per\nquery is the warehouse credit usage for executing the query. This cost does not include any other credit usage that is incurred\nas a result of query execution. For example, the following are not included in the query cost:\nData transfer costs\nStorage costs\nCloud services costs\nCosts for serverless features\nCosts for tokens processed by AI services\nFor queries that are executed concurrently, the cost of the warehouse is attributed to individual queries based on the weighted\naverage of their resource consumption during a given time interval.\nThe cost per query does not include warehouse *idle time* . Idle time is a period of time in which no queries are running in the\nwarehouse and can be measured at the warehouse level.\n ... \nSection Title: Attributing cost \u00b6 > Additional examples of queries \u00b6 > Attributing costs of hierarchical queries \u00b6\nContent:\nFor stored procedures that issue multiple hierarchical queries, you can compute the attributed query costs for the\nprocedure by using the root query ID for the procedure.\nTo find the root query ID for a stored procedure, use the ACCESS_HISTORY view . For example,\nto find the root query ID for a stored procedure, set the `query_id` and execute the following statements:CopyFor more information, see Example: Ancestor queries with stored procedures .\nTo sum the query cost for the entire procedure, replace `<root_query_id>` and execute the following statements:Copy\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/query_attribution_history",
      "title": "QUERY_ATTRIBUTION_HISTORY view | Snowflake Documentation",
      "excerpts": [
        "Reference General reference SNOWFLAKE database Account Usage QUERY_ATTRIBUTION_HISTORY\nSchemas:\nACCOUNT_USAGE\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6\nContent:\nThis Account Usage view can be used to determine the compute cost of a given query run on warehouses in your account\nin the last 365 days (1 year).\nFor more information, see Viewing cost by tag in SQL .\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column name | Data type | Description |\n| QUERY_ID | VARCHAR | Internal/system-generated identifier for the SQL statement. |\n| PARENT_QUERY_ID | VARCHAR | Query ID of the parent query or NULL if the query does not have a parent. |\n| ROOT_QUERY_ID | VARCHAR | Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse that the query was executed on. |\n| WAREHOUSE_NAME | VARCHAR | Name of the warehouse that the query executed on. |\n| QUERY_HASH | VARCHAR | The hash value computed based on the canonicalized SQL text. |\n| QUERY_PARAMETERIZED_HASH | VARCHAR | The hash value computed based on the parameterized query. |\n| QUERY_TAG | VARCHAR | Query tag set for this statement through the QUERY_TAG session parameter. |\n| USER_NAME | VARCHAR | User who issued the query. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column name | Data type | Description |\n| QUERY_ID | VARCHAR | Internal/system-generated identifier for the SQL statement. |\n| PARENT_QUERY_ID | VARCHAR | Query ID of the parent query or NULL if the query does not have a parent. |\n| ROOT_QUERY_ID | VARCHAR | Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse that the query was executed on. |\n| START_TIME | TIMESTAMP_LTZ | Time when query execution started (in the local time zone). |\n| END_TIME | TIMESTAMP_LTZ | Time when query execution ended (in the local time zone). |\n| CREDITS_ATTRIBUTED_COMPUTE | NUMBER | Number of credits attributed to this query. Includes only the credit usage for the query execution and doesn\u2019t include any warehouse idle time. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column name | Data type | Description |\n| QUERY_ID | VARCHAR | Internal/system-generated identifier for the SQL statement. |\n| PARENT_QUERY_ID | VARCHAR | Query ID of the parent query or NULL if the query does not have a parent. |\n| ROOT_QUERY_ID | VARCHAR | Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse that the query was executed on. |\n| CREDITS_USED_QUERY_ACCELERATION | NUMBER | Number of credits consumed by the Query Acceleration Service to accelerate the query. NULL if the query is not accelerated. . . The total cost for an accelerated query is the sum of this column and the CREDITS_ATTRIBUTED_COMPUTE column. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Usage notes \u00b6\nContent:\nLatency for this view can be up to eight hours. This view displays results for any role granted the USAGE_VIEWER or GOVERNANCE_VIEWER database role . The value in the `credits_attributed_compute` column contains the warehouse credit usage for executing the query,\ninclusive of any resizing and/or autoscaling of multi-cluster warehouse(s). This cost is attributed based on\nthe weighted average of the resource consumption.The value doesn\u2019t include any credit usage for warehouse idle time. Idle time is a period\nof time in which no queries are running in the warehouse and can be measured at the warehouse level.The value doesn\u2019t include any other credit usage that is incurred as a result of query execution.\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Usage notes \u00b6\nContent:\nFor example, the following are not included in the query cost:\nData transfer costs\nStorage costs\nCloud services costs\nCosts for serverless features\nCosts for tokens processed by AI services\nFor queries that are executed concurrently, the cost of the warehouse is attributed to individual queries based on the\nweighted average of their resource consumption during a given time interval. Short-running queries (<= ~100ms) are currently too short for per query cost attribution and are not included in the view. Data for all columns is available starting from mid-August, 2024. Some data prior to this date might be available in the view, but\nmight be incomplete.\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Examples \u00b6 > Query costs for related queries \u00b6\nContent:\nTo determine the costs of a specific query and similar queries using the query parameterized hash, replace `<query_id>` and execute the following statements:\n```\nSET query_id = '<query_id>' ; \n\n WITH query_hash_of_query AS ( \n  SELECT query_parameterized_hash \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_id = $ query_id \n  LIMIT 1 \n ) \n SELECT \n  query_parameterized_hash , \n  COUNT (*) AS query_count , \n  SUM ( credits_attributed_compute ) AS recurrent_query_attributed_credits \n FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n  AND start_time < CURRENT_DATE \n  AND query_parameterized_hash = ( SELECT query_parameterized_hash FROM query_hash_of_query ) \n GROUP BY ALL ;\n```\nCopy\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Examples \u00b6 > Query costs for the current user \u00b6\nContent:\nTo determine the costs of queries executed by the current user for the current month, execute the following statement:\n```\nSELECT user_name , SUM ( credits_attributed_compute ) AS credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE user_name = CURRENT_USER () \n    AND start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY user_name ;\n```\nCopy\nFor an example of attributing warehouse costs to users, see Resources shared by users from different departments .\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Examples \u00b6 > Query costs for stored procedures \u00b6\nContent:\nFor stored procedures that issue multiple hierarchical queries, you can compute the attributed query costs for the\nprocedure by using the root query ID for the procedure.\nTo find the root query ID for a stored procedure, use the ACCESS_HISTORY view . For example,\nto find the root query ID for a stored procedure, set the `query_id` and execute the following statements:CopyFor more information, see Ancestor queries with stored procedures .\nTo sum the query cost for the entire procedure, replace `<root_query_id>` and execute the following statements:Copy\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Examples \u00b6 > Additional examples \u00b6\nContent:\nFor more examples, see Resources shared by users from different departments .\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nColumns\nUsage notes\nExamples\nQuery costs for related queries\nQuery costs for the current user\nQuery costs for stored procedures\nAdditional examples\nRelated content\nOverview of warehouses\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy\nContent:\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > ... > Your Privacy > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Targeting Cookies\nContent:\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://blog.greybeam.ai/snowflake-cost-per-query/",
      "title": "Deep Dive: Snowflake's Query Cost and Idle Time Attribution",
      "publish_date": "2024-10-22",
      "excerpts": [
        "[](https://www.greybeam.ai/)\n[Blog](https://blog.greybeam.ai/)\n[Waitlist](https://greybeam.ai)\n[Customer Stories](https://blog.greybeam.ai/tag/customer-story/)\nSubscribe\nSep 9, 2024 13 min read How-To\nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query\nContent:\nSnowflake's new QUERY_ATTRIBUTION_HISTORY view\nSnowflake recently released a new feature for granular cost attribution down to individual queries through the `QUERY_ATTRIBUTION_HISTORY` view in `ACCOUNT_USAGE` . As a company focused on SQL optimization, we at Greybeam were eager to dive in and see how this new capability compares to our own custom cost attribution logic. What we found was surprising - and it led us down a rabbit hole of query cost analysis.\nSection Title: ... > The Promise and Limitations of QUERY_ATTRIBUTION_HISTORY\nContent:\nThe new view aims to provide visibility into the compute costs associated with each query. Some key things to note:\nData is only available from July 1, 2024 onwards\nShort queries (<100ms) are excluded\nIdle time is not included in the attributed costs\nThere can be up to a 6 hour delay in data appearing\nThere's also a `WAREHOUSE_UTILIZATION` view that displays cost of idle time. At the time of writing, this must be enabled by your Snowflake support team.\nSection Title: ... > Our Initial Findings\nContent:\nWe set up a test with an X-Small warehouse and 600 second auto-suspend to dramatically illustrate idle time. Running a series of short queries (mostly <500ms) over an hour, we expected to see a very small fraction of the total credits in that hour attributed to our queries, but we were very wrong.\nOn September 4th at the 14th hour, ~40 seconds of queries were executed and some how in the `QUERY_ATTRIBUTION_HISTORY` view it showed that nearly half of the total credits (0.43 of 0.88) attributed to query execution. This seemed impossibly high given the short query runtimes, yet the pattern continues.\nQUERY_ATTRIBUTION_HISTORY aggregated by the hour.\nThis may just be an anomaly in our Snowflake account, so try it yourself.\nSection Title: ... > Our Initial Findings\nContent:\n```\nWITH query_execution AS (\n    SELECT\n        qa.query_id\n        , TIMEADD(\n                'millisecond',\n                qh.queued_overload_time + qh.compilation_time +\n                qh.queued_provisioning_time + qh.queued_repair_time +\n                qh.list_external_files_time,\n                qh.start_time\n            ) AS execution_start_time\n        , qh.end_time::timestamp AS end_time\n        , DATEDIFF('MILLISECOND', execution_start_time, qh.end_time)*0.001 as execution_time_secs\n        , qa.credits_attributed_compute\n        , DATE_TRUNC('HOUR', execution_start_time) as execution_start_hour\n        , w.credits_used_compute\n    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY AS qa\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS qh\n        ON qa.query_id = qh.query_id\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS w\n        ON execution_start_hour = w.start_time\n        AND qh.warehouse_id = w.warehouse_id\n    WHERE\n ... \nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > ... > Digging Deeper\nContent:\nTo investigate further, we compared the results to our own custom cost attribution logic that accounts for idle time. Here\u2019s a snippet of what we found for the same hour:\nGreybeam\u2019s internal query cost attribution results\nAs you can see, our calculations show much smaller fractions of credits attributed to the actual query runtimes for the first hour, with the bulk going to idle periods. This aligns much more closely with our expectations given the warehouse configuration, and it works historically!\nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > ... > Potential Issues\nContent:\nAt the time of writing, we\u2019ve identified a few potential problems with the new view:\nWarehouse ID mismatch\u200a\u2014\u200aThe `warehouse_id` in `QUERY_ATTRIBUTION_HISTORY` doesn't match the actual `warehouse_id` from `QUERY_HISTORY` .\nInflated query costs\u200a\u2014\u200aThe credits attributed to short queries seem disproportionately high in some cases.\nIdle time accounting\u200a\u2014\u200aIt\u2019s unclear how idle time factors into the attribution, if at all.\nWe\u2019ve raised these concerns with Snowflake, and they\u2019ve recommended filing a support ticket for further investigation. In the meantime, we\u2019ll continue to rely on our custom attribution logic for accuracy.\nSection Title: ... > Our Approach to Query Cost Attribution\nContent:\nGiven the discrepancies we\u2019ve found, we wanted to share our methodology for calculating per-query costs, including idle time. Here\u2019s an overview of our process:\nGather warehouse suspend events\nEnrich query data with execution times and idle periods\nCreate a timeline of all events (queries and idle periods)\nJoin with `WAREHOUSE_METERING_HISTORY` to attribute costs\nBefore we dive in, let\u2019s cover a few basics:\nSnippet of WAREHOUSE_METERING_HISTORY\nSection Title: ... > Our Approach to Query Cost Attribution\nContent:\nWe use `WAREHOUSE_METERING_HISTORY` as our source of truth for warehouse compute credits. The credits billed here will reconcile with Snowflake\u2019s cost management dashboards.\nCredits here are represented on an hourly grain. We like to refer to this as *credits metered* , analogous to how most homes in North America are metered for their electricity. In our solution, we\u2019ll need to allocate queries and idle times into their metered hours.\nWe use a weighted time-based approach to attribute costs within the metered hour. In reality, Snowflake\u2019s credit attribution is likely much more complex, especially in situations with more clusters or warehouse scaling.\nHow we need to break down our queries and idle times.\nThe full SQL query will be available at the end of this blog.\n ... \nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\nIn addition, warehouse suspension doesn\u2019t actually occur during the `SUSPEND_WAREHOUSE` event. Technically, it happens when the `WAREHOUSE_CONSISTENT` event is logged. The `WAREHOUSE_CONSISTENT` event indicates that all compute resources associated with the warehouse have been fully released. You can find more information about this event in the [Snowflake documentation](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_events_history?ref=blog.greybeam.ai) .\nFor the sake of simplicity (and because the time difference is usually negligible), we\u2019re sticking with the `SUSPEND_WAREHOUSE` event in our analysis. This approach gives us a good balance between accuracy and complexity in our cost attribution model.\nBefore moving onto enriching query data, we want to apply filters to reduce the load from table scans. Feel free to adjust the dates as you see fit.\n ... \nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\nq.queued_provisioning_time + q.queued_repair_time +\n                q.list_external_files_time,\n                q.start_time\n            ) AS execution_start_time\n        , q.end_time::timestamp AS end_time\n        , w.timestamp AS suspended_at\n        , MAX(q.end_time) OVER (PARTITION BY q.warehouse_id, w.timestamp ORDER BY execution_start_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as end_time_max\n        , LEAD(execution_start_time) OVER (PARTITION BY q.warehouse_id ORDER BY execution_start_time ASC) as next_query_at\n    FROM query_history AS q\n    ASOF JOIN warehouse_events AS w\n        MATCH_CONDITION (q.end_time::timestamp <= w.timestamp)\n        ON q.warehouse_id = w.warehouse_id\n    WHERE\n        q.warehouse_size IS NOT NULL\n        AND q.execution_status = 'SUCCESS'\n        AND start_time >= $startDate\n        AND EXISTS (\n            SELECT 1\n            FROM warehouse_list AS wl\n            WHERE\nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\nq.warehouse_id = wl.warehouse_id\n            )\n)\n```\n ... \nSection Title: ... > Step 2: Enrich Query Data\nContent:\n**Execution Start Time** : We calculate the actual execution start time that the query begins running on the warehouse (thanks [Ian](https://select.dev/posts/cost-per-query?ref=blog.greybeam.ai) ! ). **Idle Time Calculation** : We determine idle periods by looking at the gap between our running query end time and the next query\u2019s start time (or warehouse suspension time). This is because it's possible a prior query is still running, so we need to keep track of the running end time and compare it against the start time of the next query. If the next query starts after the end of our current query, then there\u2019s idle time. **Hour Boundaries** : We identify queries and idle periods that span hour boundaries. This is important because Snowflake bills by the hour, so we need to properly attribute costs that cross these boundaries.\nSection Title: ... > Step 2: Enrich Query Data\nContent:\n**Warehouse Suspension** : We join with the warehouse_events table to identify when warehouses were suspended, which helps us accurately determine the end of idle periods. If the next query starts after the warehouse suspends, then the end of the idle period is the suspension time.\n ... \nSection Title: ... > Step 4: Attribute Costs\nContent:\nFinally, with each query and idle period properly allocated to their hourly slots, we can directly join to `WAREHOUSE_METERING_HISTORY` and calculate our credits used.\n```\nmetered AS (\n    SELECT\n        m.query_id\n        , m.warehouse_id\n        , m.type\n        , m.event_start_at\n        , m.event_end_at\n        , m.meter_start_hour\n        , m.meter_start_at\n        , m.meter_end_at\n        , m.meter_time_secs\n        , SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour) AS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN snowflake.account_usage.warehouse_metering_history AS w -- inner join because both tables have different delays\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time -- we can directly join now since we used our numgen method\n)\n```\nSection Title: ... > Step 4: Attribute Costs\nContent:\nIn this approach we allocate credits based on the proportion of the total execution time in that hour:\n**Time-based Weighting** : We use the duration of each event (query or idle period) as the basis for our weighting. This is represented by `m.meter_time_secs` .\n**Hourly Totals** : We calculate the total time for all events within each hour for each warehouse `SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour)` .\n**Credit Allocation** : We then allocate credits to each event based on its proportion of the total time in that hour `(m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute` .\nSection Title: ... > Step 4: Attribute Costs\nContent:\nOne important note: This approach assumes that all time within an hour is equally valuable in terms of credit consumption. In reality, Snowflake may have more complex internal algorithms for credit attribution, especially for multi-cluster warehouses or warehouses that change size within an hour. However, this weighted time-based approach provides a reasonable and transparent method for cost attribution that aligns well with Snowflake\u2019s consumption-based billing model.\nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > Conclusion\nContent:\nWhile Snowflake\u2019s new `QUERY_ATTRIBUTION_HISTORY` view is a promising step towards easier cost attribution, our initial testing reveals some potential issues that need to be addressed. For now, we recommend carefully validating the results against your own calculations and metering history.\nWe\u2019re excited to see how this feature evolves and will continue to monitor its accuracy. In the meantime, implementing your own cost attribution logic can provide valuable insights into query performance and resource utilization.\nBy accounting for idle time and carefully tracking query execution across hour boundaries, we\u2019re able to get a more complete and accurate picture of costs. This level of detail is crucial for optimizing Snowflake usage and controlling costs effectively.\n ... \nSection Title: ... > Full SQL Cost Attribution\nContent:\nq.compilation_time +\n                q.queued_provisioning_time + q.queued_repair_time +\n                q.list_external_files_time,\n                q.start_time\n            ) AS execution_start_time\n        , q.end_time::timestamp AS end_time\n        , w.timestamp AS suspended_at\n        , MAX(q.end_time) OVER (PARTITION BY q.warehouse_id, w.timestamp ORDER BY execution_start_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as end_time_max\n        , LEAD(execution_start_time) OVER (PARTITION BY q.warehouse_id ORDER BY execution_start_time ASC) as next_query_at\n    FROM query_history AS q\n    ASOF JOIN warehouse_events AS w\n        MATCH_CONDITION (q.end_time::timestamp <= w.timestamp)\n        ON q.warehouse_id = w.warehouse_id\n    WHERE\n        q.warehouse_size IS NOT NULL\n        AND q.execution_status = 'SUCCESS'\n        AND start_time >= $startDate\n        AND EXISTS (\n            SELECT 1\n            FROM warehouse_list AS wl\n ... \nSection Title: ... > Full SQL Cost Attribution\nContent:\nDATE_TRUNC('HOUR', q.execution_start_time)) AS meter_start_hour\n        , DATEADD('HOUR', n.num + 1, DATE_TRUNC('HOUR', q.execution_start_time)) AS meter_end_hour\n        , GREATEST(meter_start_hour, q.execution_start_time) as meter_start_at\n        , LEAST(meter_end_hour, q.end_time) as meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    LEFT JOIN numgen AS n\n        ON q.hours_span_query >= n.num\n    WHERE\n        q.is_same_hour_query = FALSE\n    ),\n\nmetered AS (\n    SELECT\n        m.query_id\n        , REPLACE(m.query_id, 'idle_', '') as original_query_id\n        , m.warehouse_id\n        , m.type\n        , m.event_start_at\n        , m.event_end_at\n        , m.event_time_secs\n        , m.meter_start_hour\n        , m.meter_start_at\n        , m.meter_end_at\n        , m.meter_time_secs\n        , SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour)\nSection Title: ... > Full SQL Cost Attribution\nContent:\nAS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN warehouse_metering_history AS w\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time\n),\n\nfinal AS (\n    SELECT\n        m.* EXCLUDE total_meter_time_secs, meter_end_at, original_query_id\n        , q.query_text\n        , q.query_hash\n        , q.warehouse_size\n        , q.warehouse_name\n        , q.role_name\n        , q.user_name\n    FROM metered AS m\n    JOIN queries_filtered AS q\n        ON m.original_query_id = q.query_id\n)\nSELECT\n    *\nFROM final\n;\n```"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/organization-usage/query_attribution_history",
      "title": "QUERY_ATTRIBUTION_HISTORY view - Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nReference General reference SNOWFLAKE database Organization Usage QUERY_ATTRIBUTION_HISTORY\nSchema:\nORGANIZATION_USAGE\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6\nContent:\nEnterprise Edition Feature\nAvailable in the organization account, which requires Enterprise Edition or higher. To inquire about upgrading, please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\nImportant\nThis view is only available in the organization account. For more information, see Premium views in the organization account .\nThis Organization Usage view can be used to determine the compute cost of a given query run on warehouses in your organization.\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n**Organization-level columns**\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description |\n| ORGANIZATION_NAME | VARCHAR | Name of the organization. |\n| ACCOUNT_LOCATOR | VARCHAR | System-generated identifier for the account. |\n| ACCOUNT_NAME | VARCHAR | User-defined identifier for the account. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n**Additional columns**\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column name | Data type | Description |\n| QUERY_ID | VARCHAR | Internal/system-generated identifier for the SQL statement. |\n| PARENT_QUERY_ID | VARCHAR | Query ID of the parent query or NULL if the query does not have a parent. |\n| ROOT_QUERY_ID | VARCHAR | Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse that the query was executed on. |\n| WAREHOUSE_NAME | VARCHAR | Name of the warehouse that the query executed on. |\n| QUERY_HASH | VARCHAR | The hash value computed based on the canonicalized SQL text. |\n| QUERY_PARAMETERIZED_HASH | VARCHAR | The hash value computed based on the parameterized query. |\n| QUERY_TAG | VARCHAR | Query tag set for this statement through the QUERY_TAG session parameter. |\n| USER_NAME | VARCHAR | User who issued the query. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column name | Data type | Description |\n| QUERY_ID | VARCHAR | Internal/system-generated identifier for the SQL statement. |\n| PARENT_QUERY_ID | VARCHAR | Query ID of the parent query or NULL if the query does not have a parent. |\n| ROOT_QUERY_ID | VARCHAR | Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse that the query was executed on. |\n| START_TIME | TIMESTAMP_LTZ | Time when query execution started (in the local time zone). |\n| END_TIME | TIMESTAMP_LTZ | Time when query execution ended (in the local time zone). |\n| CREDITS_ATTRIBUTED_COMPUTE | NUMBER | Number of credits attributed to this query. Includes only the credit usage for the query execution and doesn\u2019t include any warehouse idle time. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column name | Data type | Description |\n| QUERY_ID | VARCHAR | Internal/system-generated identifier for the SQL statement. |\n| PARENT_QUERY_ID | VARCHAR | Query ID of the parent query or NULL if the query does not have a parent. |\n| ROOT_QUERY_ID | VARCHAR | Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse that the query was executed on. |\n| CREDITS_USED_QUERY_ACCELERATION | NUMBER | Number of credits consumed by the Query Acceleration Service to accelerate the query. NULL if the query is not accelerated. . . The total cost for an accelerated query is the sum of this column and the CREDITS_ATTRIBUTED_COMPUTE column. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Usage notes \u00b6\nContent:\nLatency for the view may be up to 24 hours.\nThe QUERY_ATTRIBUTE_HISTORY view in the ACCOUNT_USAGE schema contains most of the same columns as the QUERY_ATTRIBUTE_HISTORY view in the ORGANIZATION_USAGE schema. For sample queries against the ACCOUNT_USAGE view, see Examples . Simply replace SNOWFLAKE.ACCOUNT_USAGE with SNOWFLAKE.ORGANIZATION_USAGE in the queries to find organization-level information.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nColumns\nUsage notes\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy\nContent:\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > ... > Your Privacy > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Targeting Cookies\nContent:\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/",
      "title": "Cost Optimization - Snowflake",
      "excerpts": [
        "Section Title: Cost Optimization > ... > Visibility: Understand & contextualize your consumption footprint\nContent:\nYou can neither control nor optimize what you can't see. Gain deep and granular insight into all aspects of your cloud spending, fostering transparency, and attributing costs effectively.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake\u2019s resource billing models\nContent:\nIt is essential to review Snowflake's billing models to align technical\nand non-technical resources on financial drivers and consumption\nterminology. Snowflake's elastic, credit-based consumption model charges\nseparately for compute (Virtual Warehouses, Compute Pools, etc),\nstorage, data transfer, and various serverless features (e.g., Snowpipe,\nAutomatic Clustering, Search Optimization, Replication/Failover, AI\nServices). Understanding the interplay of these billing types ensures\nyou can attribute costs associated with each category\u2019s unique usage\nparameters. High-level categories are below.\nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake\u2019s resource billing models\nContent:\n**Compute (Virtual Warehouses, Snowpark Container Services, Openflow):** This is often the most dynamic and largest portion of\nSnowflake spend. Virtual Warehouses are billed per-second after an\ninitial 60-second minimum when active, with credit consumption\ndirectly proportional to warehouse size (e.g., an \u201cX-Small\u201d Gen1\nwarehouse consumes one credit per hour, a 'Small' consumes two credits\nper hour, doubling with each size increase) while SPCS (via [compute pools](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) )\nare billed for all uptime with a minimum of five minutes. [Openflow](https://docs.snowflake.com/en/user-guide/data-integration/openflow/cost) is billed per second of runtime with a 60-second minimum.\n ... \nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nImplementing robust and organizationally consistent tagging and labeling\nstrategies across all resources (e.g. storage objects, warehouses,\naccounts, queries) is crucial to accurately allocate costs to specific\nteams, products, or initiatives and linking actions to outcomes.\n**Tagging in Snowflake**\nTagging can be done at several levels:\n ... \nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nThis\nsignificantly reduces the manual effort required for tagging and\nensures that new objects created within a tagged schema or propagated\nworkflow automatically inherit the correct cost attribution. Snowflake\nstrongly recommends tags for warehouses, databases, tables, and users\nto enable granular cost breakdowns. You can use the [TAG_REFERENCES view](https://docs.snowflake.com/sql-reference/account-usage/tag_references) in SNOWFLAKE.ACCOUNT_USAGE to combine with common usage views like\nWAREHOUSE_METERING_HISTORY and TABLE_STORAGE_METRICS to allocate usage\nto relevant business groups. Object Tags are best utilized when\nSnowflake objects are not shared across cost owners.\nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\n**Query tags for granular workload attribution:** [Query tags](https://docs.snowflake.com/en/user-guide/cost-attributing) can be set via session parameters (e.g., ALTER SESSION SET QUERY_TAG =\n'your_tag';) or directly within SQL clients or ETL tools. This\nassociates individual queries with specific departments, projects, or\napplications, even when using shared warehouses. This is extremely\nvaluable for shared warehouses where multiple teams or applications\nuse the same compute resource, allowing for granular showback. It is\nalso easy to programmatically make changes to query tags within\nscripts or processes to allocate costs appropriately. Query tags can\nbe found in the QUERY_HISTORY view of the SNOWFLAKE.ACCOUNT_USAGE\nschema.\nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\n**Tagging models**\nIn the initial setup of a business unit or use case, it is important to\nconsider the [model for tagging](https://docs.snowflake.com/en/user-guide/cost-attributing) costs within the platform via shared or dedicated resources. These fall\ninto three large buckets:\nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\n**Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate warehouses with a\ndepartment. You can use object tags to attribute the costs incurred by\nthose warehouses to that department entirely. **Resources shared by users from multiple departments:** An example of\nthis is a warehouse shared by users from different departments. In\nthis case, you use object tags to associate each user with a\ndepartment. The costs of queries are attributed to the users. Using\nthe object tags assigned to users, you can break down the costs by\ndepartment. **Applications or workflows shared by users from different departments:** An example of this is an application that issues\nqueries on behalf of its users.\nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nIn this case, each query executed by\nthe application is [assigned a query tag](https://docs.snowflake.com/en/sql-reference/sql/alter-session) that identifies the team or cost center of the user for whom the query\nis being made.\nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nEach model has its pros and cons, including how to handle concepts such\nas idle time or whether to show/charge back attributed or billed\ncredits. Review each model before deploying resources. If an\norganization is caught between models, a common approach is to start in\na shared resource environment and graduate to dedicated resources as the\nworkload increases.\n**Tag enforcement**\nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nClear and consistent naming conventions for accounts, warehouses,\ndatabases, schemas, and tables facilitate immediate cost understanding. Enforcing robust tagging policies (e.g., requiring specific tags for new\nresource creation and using automated scripts to identify untagged\nresources) is crucial for accurate data interpretation and effective\ncost management. Without tag enforcement, it is difficult to accurately\nallocate all costs and can require manual effort, like extensive\ntag-mapping tables. Tag values are enforced within an account, but if a\nmulti-account strategy is needed for your organization, a tag [database can be replicated](https://docs.snowflake.com/en/user-guide/cost-attributing) and leveraged across all accounts to ensure consistent values are used.\nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nFor best-in-class visibility, it is recommended to have a tagging\nstrategy and tag all resources in an organization to allocate costs to\nrelevant owners.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Embed cost accountability into your organization's DNA\nContent:\nIf cost accountability models have not been implemented previously,\nconsider a showback model. This involves transparently reporting\nSnowflake costs to different departments or projects to raise awareness\nof their costs. By showing each team their monthly consumption (broken\ndown by warehouse usage, query costs, and storage, etc.), it encourages\na cost-conscious culture. This initial step helps teams understand the\nfinancial impact of their actions without the immediate pressure of\nbudget cuts. Tools like Snowflake's built-in [Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) & [budget](https://docs.snowflake.com/en/user-guide/budgets) views, third-party cost management platforms, or custom dashboards can\nbe used to provide these reports.\n**Chargeback**\nSection Title: Cost Optimization > Visibility > Overview > Embed cost accountability into your organization's DNA\nContent:\nFor more financially mature organizations, a chargeback model can be\nvery effective for managing costs. This system directly bills\ndepartments for their Snowflake usage. This creates a powerful financial\nincentive for teams to optimize their workloads. To make this transition\nsmooth and fair, you need to define clear rules for cost allocation. By\nimplementing chargeback, you turn each department into a financial\nstakeholder, encouraging them to right-size their warehouses, suspend\nthem during idle periods, and write more efficient queries. This shift\nin accountability leads to a more disciplined and cost-effective use of\nyour Snowflake environment.\nIn either case, having a centralized dashboard or visual for all\norganizations to review intra-period is critical for financial\naccountability and next-step actions.\nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\nThe most mature FinOps customers are those who programmatically and\nstrategically drive consumption insights across the business. This\ninvolves three core elements:\n ... \nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\n**Identify demand drivers and unit economics:** To understand what\ndrives Snowflake spend, correlate historical credit, storage, and data\ntransfer usage with key business metrics like cost per customer or per\ntransaction. Use Snowflake's [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) schema, including the WAREHOUSE_METERING_HISTORY and QUERY_HISTORY\nviews, as the primary data sources for this analysis.\n**Granular cost attribution:** Accurately tie costs back to business\nteams or workloads by implementing a mandatory tagging strategy for\nall warehouses and queries. Align these tags with your organization's\nfinancial structure to provide clear cost segmentation.\n**Build the predictive model**\nThis phase integrates historical trends with strategic business inputs\nto create forward-looking projections.\n ... \nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\nAn example is using stored procedures that leverage the [SYSTEM$CANCEL_QUERY](https://docs.snowflake.com/en/sql-reference/functions/system_cancel_query) function to terminate statements that exceed predefined runtime\nthresholds or contain ill-advised logic, such as exploding joins. This approach allows you to more finely customize the types of\nqueries you want to cancel, as you have full control over defining\nthe stored procedure logic. **Auto-suspend policies** : Auto-suspend policies are a foundational\ncost control for virtual warehouses, automatically suspending a\nwarehouse after a defined period of inactivity.\n ... \nSection Title: Cost Optimization > Control > Overview > Govern resource creation and administration\nContent:\nThis ensures every credit spent can be\naccurately attributed to the correct department or project, enabling\nrobust chargeback and accountability. **Automate deactivation:** To prevent object sprawl, implement\npolicies that identify and deactivate stale resources after a\npredetermined period of disuse.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nSeparate warehouses by workload (e.g., ELT versus analytics versus\ndata science)\nWorkload size in bytes should match the t-shirt size of the warehouse\nin the majority of the workloads\u2013larger warehouse size doesn\u2019t always\nmean faster\nAlign warehouse size for optimal cost-performance settings\n[Utilize Multi-Cluster Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-considerations) configuration to solve for high concurrency\nUtilize [Query Acceleration Services](https://docs.snowflake.com/en/user-guide/query-acceleration-service) to help with infrequent, large data scans\nFor memory-intensive workloads, use a warehouse type of Snowpark\nOptimized or higher memory resource constraint configurations as\nappropriate\nSet appropriate auto-suspend settings - longer for high cache use,\nlower for no cache reuse\nSet appropriate warehouse query timeout settings for the workload and\nthe use cases it supports.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nFollow the principles outlined above and understand that this is a\ncontinuous improvement process.\nChoose a size based on the estimated or actual workload size and\nmonitor.\nUtilize Snowflake's extensive telemetry data, such as [QUERY_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) and [WAREHOUSE_METERING_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history) ,\nto validate that the warehouse size is impacting the metrics you care\nabout in the direction you intend.\n**Optimal warehouse settings**\nWhile Snowflake strives for minimal knobs and self-managed tuning, there\nare situations where selecting the right settings for warehouses can\nhelp with optimal cost and/or performance. Some of the key [warehouse settings](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse) include\n ... \nSection Title: Cost Optimization > Optimize > Overview > Leverage Managed Services\nContent:\n**Serverless tasks:** Serverless tasks enable the execution of SQL\nstatements or stored procedures on a user-defined schedule, eliminating\nthe need for a user-managed virtual warehouse. This is a cost-effective\nsolution for infrequent workloads where a warm cache offers minimal\nvalue, or for unpredictable workloads that don't utilize a minimum\n60-second usage."
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-exploring-compute",
      "title": "Exploring compute cost - Source: Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Visibility Exploring cost Exploring compute cost\nSection Title: Exploring compute cost \u00b6\nContent:\nTotal compute cost consists of the overall use of:\nVirtual warehouses (user-managed compute resources)\nServerless features such as Automatic Clustering and Snowpipe that use Snowflake-managed compute resources\nCloud services layer of the Snowflake architecture\nvCPU usage for Openflow BYOC cost and scaling considerations and Openflow Snowflake Deployment cost and scaling considerations .\nSee Openflow components for more information about Openflow components including runtimes.\nThis topic describes how to gain insight into historical compute costs using Snowsight , or by writing queries against views in\nthe ACCOUNT_USAGE and ORGANIZATION_USAGE schemas.\nSnowsight allows you to quickly and easily obtain information about cost from a visual dashboard. Queries against the usage views\nallow you to drill down into cost data and can help generate custom reports and dashboards.\nSection Title: Exploring compute cost \u00b6\nContent:\nIf you need more information about how compute costs are incurred, refer to Understanding compute cost .\nNote\nThe cloud services layer consumes credits, but not all of those credits are actually billed. Usage for cloud services is charged only if\nthe daily consumption of cloud services exceeds 10% of the daily usage of virtual warehouses. Snowsight and a majority of views\nshow the total number of credits consumed by warehouses, serverless features, and cloud services without accounting for this daily\nadjustment to cloud services.\nTo determine how many credits were actually billed for compute costs, run queries against the METERING_DAILY_HISTORY view .\nSection Title: Exploring compute cost \u00b6 > Viewing credit usage \u00b6\nContent:\nAll compute resources (virtual warehouses, serverless, cloud services) consume Snowflake credits. Users can use Snowsight to\nview the overall cost of compute usage for any given day, week, or month.\nTo explore compute cost:\nSwitch to a role with access to cost and usage data .\nIn the navigation menu, select Admin \u00bb Cost management .\nSelect a warehouse to use to view the usage data. Snowflake recommends using an XS warehouse for this purpose.\nSelect Consumption .\nSelect Compute from the Usage Type drop-down.\nFor usage notes related to the Consumption page, see Usage notes .\nSection Title: Exploring compute cost \u00b6 > Viewing credit usage \u00b6 > Filter by tag \u00b6\nContent:\nYou can use tags to attribute the cost of using resources to a logical\nunit within your organization. A tag is a Snowflake object that can have one or more values associated with it. A user with the\nappropriate privileges applies a tag/value pair to each resource that is used by a cost center or other logical unit (e.g. the Development\nenvironment, a business unit, or business line). Once resources have been tagged, you can isolate costs based on a\nspecific tag/value pair, allowing you to attribute this cost to a specific logical unit.\nTo filter the Consumption dashboard to show costs associated with a specific tag/value combination:\n ... \nSection Title: Exploring compute cost \u00b6 > Viewing credit usage \u00b6 > View consumption by type, service, or resource \u00b6\nContent:\nWhen viewing the bar graph that displays compute history, you can filter the data By Type , By Service or By Resource .\nBy Type :\nSeparates resource consumption into compute (virtual warehouses and serverless resources) and cloud services. For the purpose\nof this filter, cloud services is separated out from the other types of compute resources.\nBy Service :\nSeparates resource consumption into warehouse consumption and consumption by each serverless feature. For example,\nWAREHOUSE_METERING represents credits consumed by warehouses while PIPE represents credits consumed by the serverless Snowpipe feature.\nCloud services compute is included in warehouse consumption.\nBy Resource :\nSeparates resource consumption by the Snowflake object that consumed credits. For example, each warehouse is represented,\nas is every table that incurred serverless costs.\n ... \nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > General cost views \u00b6\nContent:\n| View | Compute resource | Description | Schema |\n| METERING_DAILY_HISTORY | Warehouses |  |  |\nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > General cost views \u00b6\nContent:\nServerless\nCloud Services\nOpenflow runtimes |Credits consumed by all compute resources (warehouses, serverless, cloud services and Openflow) in a given day.\nCan be used to determine whether cloud services compute costs were actually billed for a specific day (that is, cloud services credit\nconsumption exceeded 10% of warehouse consumption). |ORGANIZATION_USAGE ACCOUNT_USAGE |\n|METERING_HISTORY |Warehouses\nServerless\nCloud Services\nOpenflow runtimes |Credits consumed by warehouses, cloud services, serverless, and Openflow features on an hourly basis. To see how many credits an individual\nwarehouse is consuming, query the WAREHOUSE_METERING_HISTORY view. |ACCOUNT_USAGE |\n|USAGE_IN_CURRENCY_DAILY |Warehouses\nServerless\nCloud Services |Daily credit consumption by all compute resources along with the cost of that usage in the organization\u2019s currency. |ORGANIZATION_USAGE |\n ... \nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > Feature-specific cost views \u00b6\nContent:\n|\n|LISTING_AUTO_FULFILLMENT_\nREFRESH_DAILY |Warehouses |Credits used to refresh data fulfilled to other regions by Cross-Cloud Auto-Fulfillment. |\n|LISTING_AUTO_FULFILLMENT_\nUSAGE_HISTORY |Warehouses |Estimated usage associated with fulfilling data products to other regions by using Cross-Cloud Auto-Fulfillment. Refer to the SERVICE_TYPE of REPLICATION. |\n|MATERIALIZED_VIEW_REFRESH_\nHISTORY |Serverless |Credits consumed the refreshing of materialized views. |\n|OPENFLOW_USAGE_HISTORY |Openflow |Credits consumed by Openflow runtimes. This view is available in the ACCOUNT_USAGE schema only. |\n|PIPE_USAGE_HISTORY |Serverless |Credits consumed by Snowpipe. |\n|QUERY_ACCELERATION_HISTORY |Serverless |Credits consumed by the query acceleration service. |\n|QUERY_ATTRIBUTION_HISTORY |Warehouses |Credits consumed per query for warehouse usage.\n ... \nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > Feature-specific cost views \u00b6\nContent:\nCloud Services |Hourly credit usage of each warehouse, including the cloud services cost associated with using the warehouse. |\nNote\nThe views and table functions of the Snowflake Information Schema also provide usage data related to cost. Though\nthe ACCOUNT_USAGE schema is preferred, the Information Schema can be faster in some circumstances.\nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > Example queries \u00b6\nContent:\nThe following queries drill-down into data in ACCOUNT_USAGE views to gain insight into compute costs.\nNote\nQueries executed against views in the Account Usage schema can be modified to gain insight into cost for the entire organization by\nusing the corresponding view in the Organization Usage schema. For example, both schemas include a WAREHOUSE_METERING_HISTORY view.\nClick the name of a query below to see the full SQL example.\nCompute for Warehouses :\n* Query: Average hour-by-hour Snowflake spend (across all warehouses) over the past m days\nQuery: Credit consumption by warehouse over specific time period\nQuery: Warehouse usage over m-day average\nQuery: Warehouse cost attribution by query tag .\nQuery: Warehouse cost attribution by user .\nCompute for Cloud Services :\n* Query: Billed cloud services\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for warehouses \u00b6\nContent:\nQuery: Average hour-by-hour Snowflake spend (across all warehouses) over the past m days\nThis query shows the total credit consumption on an hourly basis to help understand consumption trends (peaks, valleys) over the past m\ndays. This helps identify times of day when there are spikes in consumption.\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for warehouses \u00b6\nContent:\nQuery: Credit consumption by warehouse over specific time period\nThis query shows the total credit consumption for each warehouse over a specific time period. This helps identify warehouses that are\nconsuming more credits than others and specific warehouses that are consuming more credits than anticipated.\n```\n-- Credits used (all time = past year) \n SELECT warehouse_name , \n  SUM ( credits_used_compute ) AS credits_used_compute_sum \n FROM snowflake . account_usage . warehouse_metering_history \n GROUP BY 1 \n ORDER BY 2 DESC ; \n\n -- Credits used (past N days/weeks/months) \n SELECT warehouse_name , \n  SUM ( credits_used_compute ) AS credits_used_compute_sum \n FROM snowflake . account_usage . warehouse_metering_history \n WHERE start_time >= DATEADD ( day , - m , CURRENT_TIMESTAMP ()) \n GROUP BY 1 \n ORDER BY 2 DESC ;\n```\nCopy\nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for warehouses \u00b6\nContent:\nQuery: Warehouse usage over m-day average\nThis query returns the daily average credit consumption grouped by week and warehouse. It can be used to identify anomalies in credit\nconsumption for warehouses across weeks from the past year.\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for cloud services \u00b6\nContent:\n```\nSELECT query_type , \n  SUM ( credits_used_cloud_services ) AS cs_credits , \n  COUNT ( 1 ) num_queries \n FROM snowflake . account_usage . query_history \n WHERE true \n  AND start_time >= TIMESTAMPADD ( day , - 1 , CURRENT_TIMESTAMP ) \n GROUP BY 1 \n ORDER BY 2 DESC \n LIMIT 10 ;\n```\nCopy\nQuery: Cloud services cost for queries of a given type\nThis query returns the total credits consumed for cloud services by all queries of a specific type. Replace `'COPY'` if you want to focus on a different type of query and `day` if you want to explore a longer or shorter period of time.\n```\nSELECT * \n FROM snowflake . account_usage . query_history \n WHERE true \n  AND start_time >= TIMESTAMPADD ( day , - 1 , CURRENT_TIMESTAMP ) \n  AND query_type = 'COPY' \n ORDER BY credits_used_cloud_services DESC \n LIMIT 10 ;\n```\nCopy\nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for cloud services \u00b6\nContent:\nQuery: Warehouses with high cloud services usage\nThis query shows the warehouses that are not using enough warehouse time to cover the cloud services portion of compute. This provides a\nlaunching point for additional investigation by isolating warehouses with a high ratio of cloud service use (>10% of overall credits).\nInvestigation candidates include issues around cloning, listing files in S3, partner tools, setting session parameters, etc.\n```\nSELECT \n  warehouse_name , \n  SUM ( credits_used ) AS credits_used , \n  SUM ( credits_used_cloud_services ) AS credits_used_cloud_services , \n  SUM ( credits_used_cloud_services )/ SUM ( credits_used ) AS percent_cloud_services \n FROM snowflake . account_usage . warehouse_metering_history \n WHERE TO_DATE ( start_time ) >= DATEADD ( month ,- 1 , CURRENT_TIMESTAMP ()) \n    AND credits_used_cloud_services > 0 \n GROUP BY 1 \n ORDER BY 4 DESC ;\n```\nCopy\nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for cloud services \u00b6\nContent:\nQuery: Cloud services usage sorted by portion of query time\nThis query returns all queries run within the last minute and sorts them by parts of total query execution time (e.g. compilation time vs. queue time).\n```\nSELECT * \n FROM snowflake . account_usage . query_history \n WHERE true \n  AND start_time >= TIMESTAMPADD ( minute , - 60 , CURRENT_TIMESTAMP ) \n ORDER BY compilation_time DESC , \n  execution_time DESC , \n  list_external_files_time DESC , \n  queued_overload_time DESC , \n  credits_used_cloud_services DESC \n LIMIT 10 ;\n```\nCopy\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Query Acceleration Service \u00b6\nContent:\nQuery: Query Acceleration Service cost by warehouse\nThis query returns the total number of credits used by each warehouse in your account for the query acceleration service\n(month-to-date):\n```\nSELECT warehouse_name , \n       SUM ( credits_used ) AS total_credits_used \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_HISTORY \n  WHERE start_time >= DATE_TRUNC ( month , CURRENT_DATE ) \n  GROUP BY 1 \n  ORDER BY 2 DESC ;\n```\nCopy\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for partner tools \u00b6\nContent:\nquery_history qh \n      JOIN snowflake . account_usage . sessions se \n        ON se . session_id = qh . session_id \n    WHERE warehouse_name IS NOT NULL \n      AND execution_time > 0 \n      AND start_time > DATEADD ( month ,- 1 , CURRENT_TIMESTAMP ()) \n    GROUP BY 1 , 2 , 3 \n  ), \n  hour_execution_cte AS ( \n    SELECT start_time_hour , \n      warehouse_name , \n      SUM ( client_hour_execution_time ) AS hour_execution_time \n    FROM client_hour_execution_cte \n    GROUP BY 1 , 2 \n  ), \n  approximate_credits AS ( \n    SELECT A . client_application_name , \n      C . warehouse_name , \n      ( A . client_hour_execution_time / B . hour_execution_time )* C . credits_used AS approximate_credits_used \n    FROM client_hour_execution_cte A \n      JOIN hour_execution_cte B \n        ON A . start_time_hour = B . start_time_hour and B . warehouse_name = A . warehouse_name \n      JOIN snowflake . account_usage .\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Snowflake Notebooks \u00b6\nContent:\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nViewing credit usage\nQuerying data for compute cost\nRelated content\nUnderstanding compute cost\nExploring overall cost\nAttributing cost\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/release-notes/2024/other/2024-08-30-per-query-cost-attribution",
      "title": "August 30, 2024 \u2014 Query attribution costs | Snowflake Documentation",
      "publish_date": "2024-08-30",
      "excerpts": [
        "The new QUERY_ATTRIBUTION_HISTORY view in the ACCOUNT_USAGE schema provides information about the warehouse cost for queries and enables the attribution of ... Aug 30, 2024 \u00b7 Sep 25, 2024 - Snowflake Feature Store GA \u00b7 Sep 25, 2024 - New Cortex ... Sep 04, 2024 - Call stored procedures in the FROM clause \u00b7 Sep 01\u00a0..."
      ]
    },
    {
      "url": "https://articles.analytics.today/best-practices-for-query-tagging-in-snowflake",
      "title": "Snowflake Query Cost Attribution: Attribute Credits to QUERY TAGS",
      "excerpts": [
        "Step-by-step guide to attributing Snowflake compute credits to queries by EXECUTION_TIME, parsing QUERY_TAG JSON, and estimating dollar cost per tag. 8 Oct 2025 \u2014 Step-by-step guide to attributing Snowflake compute credits to queries by EXECUTION_TIME, parsing QUERY_TAG JSON, and estimating dollar cost\u00a0..."
      ]
    },
    {
      "url": "https://www.reddit.com/r/snowflake/comments/1j9tsmb/how_to_join_attribution_history_with_query_history/",
      "title": "How to join attribution history with query history - snowflake - Reddit",
      "publish_date": "2025-03-12",
      "excerpts": [
        "So came across another view query_attribution_history which gives the compute for each query readily available and it is snowflake populated ..."
      ]
    },
    {
      "url": "https://www.reddit.com/r/snowflake/comments/1og1ot7/snowflake_stored_procedures_and_data_pipelines/",
      "title": "Snowflake Stored Procedures and Data Pipelines and ETL/ELT and ...",
      "excerpts": [
        "Skip to main content Open menu Open navigation  Go to Reddit Home\nr/snowflake\nExpand user menu Open settings menu\nGo to snowflake r/snowflake \u2022\nPeacencalm9 [Portugu\u00eas (Brasil)](https://www.reddit.com/r/snowflake/comments/1og1ot7/snowflake_stored_procedures_and_data_pipelines/?tl=pt-br) [\u0939\u093f\u0928\u094d\u0926\u0940](https://www.reddit.com/r/snowflake/comments/1og1ot7/snowflake_stored_procedures_and_data_pipelines/?tl=hi) [Fran\u00e7ais](https://www.reddit.com/r/snowflake/comments/1og1ot7/snowflake_stored_procedures_and_data_pipelines/?tl=fr) [Deutsch](https://www.reddit.com/r/snowflake/comments/1og1ot7/snowflake_stored_procedures_and_data_pipelines/?tl=de) [\u0e44\u0e17\u0e22](https://www.reddit.com/r/snowflake/comments/1og1ot7/snowflake_stored_procedures_and_data_pipelines/?tl=th)\nSection Title: Snowflake Stored Procedures and Data Pipelines and ETL/ELT and data warehouse\nContent:\nIn what case scenarios, are you guys using stored procedures in snowflake. How complete ETL process with SCD Type1 implemented in snowflake. Staging and then warehouse load\nAny one using heavy stored procedures and Data pipeline ELT/ETL in snowflake without any other tools\nRead more Share\nSection Title: Related Answers Section\nContent:\nRelated Answers\nUse cases for stored procedures in Snowflake\nConnecting Snowflake to Oracle and SQL Server\nBest tools for Snowflake data pipelines\nTop ELT software for data pipelines\nDBT usage in Snowflake\nSuggested Posts\nPublic\nAnyone can view, post, and comment to this community\n0 0\nExpand Navigation Collapse Navigation"
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}

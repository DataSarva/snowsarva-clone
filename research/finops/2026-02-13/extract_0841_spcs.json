{
  "extract_id": "extract_8605587c097b44438abe440207516aa8",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views",
      "title": "Snowpark Container Services costs | Snowflake Documentation",
      "publish_date": "2026-01-01",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nDeveloper Snowpark Container Services Snowpark Container Services Costs\n\n# Snowpark Container Services costs \u00b6\n\n Feature \u2014 Generally Available\n\nSnowpark Container Services is available to accounts in AWS, Microsoft Azure, and Google Cloud Platform commercial regions , with some exceptions. For more information, see Available regions and considerations .\n\nThe costs associated with using Snowpark Container Services can be categorized into storage cost, compute pool cost, and data\ntransfer cost.\n\n## Storage cost \u00b6\n\nWhen you use Snowpark Container Services, storage costs associated with Snowflake, including the cost of Snowflake stage usage\nor database table storage, apply. For more information, see Exploring storage cost . In addition, the\nfollowing cost considerations apply:\n\n* **Image repository storage cost:** The implementation of the image repository uses\n  a Snowflake stage. Therefore, the associated cost for using the Snowflake stage applies.\n* **Log storage cost:** When you store local container logs in event tables , event table storage\n  costs apply.\n* **Mounting volumes cost:**\n  \n    + When you mount a Snowflake stage as a volume, the cost of using the Snowflake stage applies.\n    + When you mount storage from the compute pool node as a volume, it appears as local storage in the container. But there is no\n        additional cost because the local storage cost is covered by the cost of the compute pool node.\n* **Block storage cost:** When you create a service that uses block storage , you are billed for block storage and snapshot storage. For more information about storage pricing, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) . The SPCS Block Storage Pricing table in this document provides the information.\n\n## Compute pool cost \u00b6\n\nA compute pool is a collection of one or more virtual machine (VM) nodes on which Snowflake\nruns your Snowpark Container Services jobs and services. The number and type (instance family) of the nodes in the compute pool\n(see CREATE COMPUTE POOL ) determine the credits it consumes and thus the cost you pay. For more information, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\n\nYou incur charges for a compute pool in the IDLE, ACTIVE, STOPPING, or RESIZING state, but not when it is in a STARTING or\nSUSPENDED state. To optimize compute pool expenses, you should leverage the AUTO\\_SUSPEND feature (see CREATE COMPUTE POOL).\n\nThe following views provide usage information:\n\n* **ACCOUNT\\_USAGE views**\n  \n  The following ACCOUNT\\_USAGE views contain Snowpark Container Services credit usage information:\n  \n    + The SNOWPARK\\_CONTAINER\\_SERVICES\\_HISTORY view offers\n        credit usage information (hourly consumption) exclusively for Snowpark Container Services.\n    + In the METERING\\_DAILY\\_HISTORY view , query for rows in which the `service_type` column contains the value `SNOWPARK_CONTAINER_SERVICES` .\n    + In the METERING\\_HISTORY view , query for rows in which the `service_type` column contains the value `SNOWPARK_CONTAINER_SERVICES` .\n* **ORGANIZATION\\_USAGE views**\n  \n    + In the METERING\\_DAILY\\_HISTORY view , use the `SERVICE_TYPE = SNOWPARK_CONTAINER_SERVICES` query filter.\n\n## Data transfer cost \u00b6\n\nData transfer is the process of moving data into (ingress) and out of (egress) Snowflake. For more information, see Understanding data transfer cost . When you use Snowpark Container Services, the following additional cost\nconsiderations apply:\n\n* **Outbound data transfer:** Snowflake applies the same data transfer rate for outbound data transfers from services and jobs\n  to other cloud regions and to the internet, consistent with the rate for all Snowflake outbound data transfers. For more\n  information, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) (table 4a).\n  \n  You can query the DATA\\_TRANSFER\\_HISTORY ACCOUNT\\_USAGE view for\n  usage information. The `transfer_type` column identifies this cost as the `SNOWPARK_CONTAINER_SERVICES` type.\n* **Internal data transfer:** This class of data transfer refers to data movements across compute entities within Snowflake, such as\n  between two compute pools or a compute pool and a warehouse, that resulted from executing a service function .\n  For more information, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) (tables 4(a) for AWS, 4(b) for Azure, and the column titled \u201cSPCS Data Transfer to Same Cloud Provider, Same Region\u201d).\n  \n  To view the costs associated with internal data transfer, you can do the following:\n  \n    + Query the INTERNAL\\_DATA\\_TRANSFER\\_HISTORY view in the ACCOUNT\\_USAGE schema.\n    + Query the DATA\\_TRANSFER\\_HISTORY view in the ACCOUNT\\_USAGE schema. The `transfer_type` column identifies this cost as the `INTERNAL` type.\n    + Query the DATA\\_TRANSFER\\_HISTORY view in the ORGANIZATION\\_USAGE schema.\n        The `transfer_type` column identifies this cost as the `INTERNAL` type.\n    + Query the DATA\\_TRANSFER\\_DAILY\\_HISTORY view in the ORGANIZATION\\_USAGE schema. The `service_type` column identifies this cost as the `INTERNAL_DATA_TRANSFER` type.\n    + Query the RATE\\_SHEET\\_DAILY view in the ORGANIZATION USAGE\n        schema. The `service_type` column identifies this cost as the `INTERNAL_DATA_TRANSFER` type.\n    + Query the USAGE\\_IN\\_CURRENCY\\_DAILY view in the ORGANIZATION USAGE\n        schema. The `service_type` column identifies this cost as the `INTERNAL_DATA_TRANSFER` type.\n\nNote\n\nData transfer costs are currently not billed for Snowflake accounts on Google Cloud.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Storage cost\n2. Compute pool cost\n3. Data transfer cost\n\nRelated content\n\n1. Snowpark Container Services\n2. Snowpark Container Services: Working with compute pools\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    },
    {
      "url": "https://select.dev/posts/snowpark-container-services",
      "title": "A Beginner's Guide to Snowpark Container Services: Understanding the Building Blocks and Pricing",
      "publish_date": "2025-11-15",
      "excerpts": [
        "* Features\n* Pricing\n* Resources\n* Customers\n* Careers\n Book a demo\n\nBook a demo\n\n\u2190 Back\n\n# A Beginner's Guide to Snowpark Container Services: Understanding the Building Blocks and Pricing\n\nJeff Skoldberg . Saturday, November 15, 2025\n\nCopy link\n\n## Table of Contents\n\n1. What is Snowpark Container Services?\n\nIf you're coming to Snowpark Container Services (SPCS) from a background in traditional cloud platforms like AWS, GCP, or Azure, Snowpark Container Services might feel familiar yet confusingly different. You can run containers, but the terminology (compute pool, service, etc.) and behavior are just foreign enough to be confusing. Let's break it down.\n\n## What is Snowpark Container Services?\n\nSnowpark Container Services lets you run containerized applications directly inside Snowflake. It is Snowflake's version of AWS ECS, Google Cloud Run, or Azure Container Instances. But instead of running in your cloud account, your containers run in Snowflake's compute environment and have native access to your Snowflake data.\n\nThe best part is your containers can directly query Snowflake tables, call Snowflake functions, and access your data without the need to create a service account, managing credentials for data access, or moving data outside the platform. Also the users of your app will be granted access using a simple grant statement, so you don\u2019t need to build authentication software or manage user / passwords, etc. I recently deployed a container app for an AWS / Snowflake customer, and they chose SPCS instead of ECS because the user management and security is much simpler.\n\nThere are lots of articles covering examples of how to deploy apps in Snowpark Container Services. In this article, we\u2019ll focus mostly on the building blocks (terminology), pricing, and some of the nuances or difference vs traditional container services.\n\n## The Core Building Blocks\n\nLet's understand the four main objects you'll work with:\n\n### 1\\. Image Repository\n\nThis is Snowflake's container registry, similar to Docker Hub or Amazon ECR. You push your Docker images here, and Snowflake pulls from it when starting your services.\n\n```\n1 CREATE  IMAGE REPOSITORY my_app_repo ;\n```\n\nSnowpark Container Services does not pull container images directly from external registries like Docker Hub. You can use public base images when building locally, but before deploying to Snowflake, you must push the final image to a **Snowflake image repository** within your account. Services can only run images stored in Snowflake\u2019s own repository system.\n\n### 2\\. Compute Pool\n\nHere's where things diverge from what you might know. A compute pool is a collection of virtual machine instances that run one or many containers. Think of it as your cluster of nodes that can run many services or apps.\n\n```\n1 CREATE COMPUTE  POOL my_pool 2  MIN_NODES  = 1 3  MAX_NODES  = 3 4  INSTANCE_FAMILY  =  CPU_X64_XS 5  AUTO_RESUME  = TRUE 6  AUTO_SUSPEND_SECS  = 60 ;\n```\n\n**Key parameters:**\n\n* `MIN_NODES` / `MAX_NODES` : How many VM instances can run (this is your capacity, not your containers). Min nodes must be greater than zero. It will auto-suspend if no services are running.\n* `INSTANCE_FAMILY` : The size/type of VM (CPU\\_X64\\_S, CPU\\_X64\\_M, GPU\\_NV\\_S, etc.)\n* `AUTO_RESUME` : Whether the pool automatically starts when needed\n* `AUTO_SUSPEND_SECS` : How long to wait before suspending an _idle_ pool with no services running.\n\n### 3\\. Service\n\nA service is the running containerized application. It defines which image to run, how many instances (containers) to start, and how to route traffic to them.\n\n```\n1 -- minimum required code: 2 CREATE  SERVICE my_web_app 3 IN COMPUTE  POOL my_pool 4 FROM @specs 5  SPECIFICATION_FILE  = 'service.yaml' ; 6 7 -- more common properties... 8 CREATE  SERVICE my_api_service 9 IN COMPUTE  POOL my_pool 10 FROM @specs 11  SPECIFICATION_FILE  = 'service.yaml' 12  MIN_INSTANCES  = 1 13  MAX_INSTANCES  = 5 -- Scale up to 5 based on CPU 14  MIN_READY_INSTANCES  = 1 -- Need at least 1 for service to be ready 15  EXTERNAL_ACCESS_INTEGRATIONS  = ( api_eai ,  cdn_eai )\n```\n\nExpand Code\n\nThe service reads a YAML specification file (stored in a stage) that describes your containers, similar to a Kubernetes deployment or Docker Compose file. Here\u2019s an example SPECIFICATION\\_FILE.yml that the service will read on startup:\n\n```\n1 spec: 2  containers: 3 -  name: my_app_container 4    image:  / dbt_dev / my_app / repo / my_app_container:latest 5    env: 6 # your app env vars here 7 ## This app reads data from Snowflake using these vars 8      SNOWFLAKE_WAREHOUSE: COMPUTE_WH 9      SNOWFLAKE_DATABASE: FIVETRAN 10      SNOWFLAKE_SCHEMA: SAP_ECC 11      SNOWFLAKE_ROLE: DBT_ROLE 12      APP_HOST:  0.0 .0 .0 13      APP_PORT:  \"8080\" 14    readinessProbe: 15      port:  8080\n```\n\nExpand Code\n\n### 4\\. External Access Integration (Optional)\n\nIf your container needs to call external APIs or services outside Snowflake, you need an external access integration. This is Snowflake's way of controlling outbound network access.\n\n```\n1 CREATE  EXTERNAL ACCESS INTEGRATION my_api_access 2  ALLOWED_NETWORK_RULES  = ( my_network_rule ) 3  ENABLED  = TRUE ;\n```\n\nAs you can see, there are some new concepts here that are very specific to this service. General Snowflake knowledge or cloud knowledge will help jump start you, but you need to get these new terms under your belt.\n\n## How much does Snowpark Container Services cost?\n\nThe pricing on SPCS is quite reasonable in my opinion. It will be more expensive compared to traditional container services (ECS, Cloud Run, ACI), but the added benefits of Snowflake governance and data proximity may often be worth it. Here are the various charges associated with SPCS.\n\n### Compute Pools\n\nThe main billing component of SPCS is the compute pools. On the surface, SPCS seems to be very affordable. An Extra Small costs only 0.06 Credits per hour. At $3 per credit, an XS would cost $4.32 per day or $1576 per year. Similar hardware on Cloud Run would cost just under $3 / day. So SPCS is more expensive, but has the added benefits we discussed.\n\nWhen a compute pool resumes, you are billed for a minimum of 5 minutes.\n\n### Comparison to Snowflake Virtual Warehouses costs\n\nCompared to Snowflake\u2019s virtual warehouses, the smallest size warehouse costs 1 credit per hour. SPCS provides a much cheaper alternative for lightweight tasks like running Python scripts or hosting notebooks. As shown above, you can run a compute node for 6% the total cost of an XS virtual warehouse!\n\n### Storage Costs\n\nStorage is cheap, and SPCS won\u2019t burn a ton of storage, but let\u2019s be aware of this aspect. Here are the various ways you could be billed for storage:\n\n* Image Repository: this is implemented as a stage, so standard storage rates apply.\n* Block volumes for containers: these store your application state, similar to AWS EBS or Google Persistent Disk. This is most expensive aspect when it comes to storage. Ranging from about $82 to $100 per TB per month.\n* Logging to Snowflake\u2019s event table (standard storage)\n* Any other stage / table used by your app (standard storage)\n\n## SPCS Cost Management Gotcha: Public Services Don't Auto-Suspend\n\nHere's where many newcomers (myself included) get surprised. If you're used to Cloud Run, AWS Lambda, ECS, or similar \"serverless\" container platforms, you might expect your service to automatically shut down when no one is using it. **It doesn't.**\n\n### The Reality of SPCS Costs\n\nBen Franklin said \"Experience is the best teacher, but a fool will learn from no other.\" Unfortunately I learned from the following the hard way, so hopefully you can learn from my mistakes!\n\nCompute pools have AUTO\\_SUSPEND\\_SECS, but it only applies when the pool is empty. If you have a service running on the compute pool, the pool stays active. Setting `AUTO_SUSPEND_SECS = 60` on your compute pool doesn't suspend the pool if a service is running and **services will not suspend themselves.**\n\nServices run 24/7 by default. When you create a service with `MIN_INSTANCES = 1` , (0 is not allowed), Snowflake keeps one container running constantly. That means you're paying for compute 24/7 until you explicitly suspend the service. Unlike cloud run, it won\u2019t scale to zero when no one is using it. The `auto_suspend_secs` property of the service is a preview feature; it will work for internal apps that do not have public endpoints such as [Service Functions](https://docs.snowflake.com/en/sql-reference/sql/create-function-spcs) , and [Service to Service](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/working-with-services) communications. But for web apps running in containers, they will not auto-suspend when idle. Snowflake only tracks internal service function calls for determining inactivity, not **ingress HTTP traffic** .\n\nAs mentioned above, it is still affordable due to the low credit consumption rate, even if you let things run 24x7. ($4.32 / day assuming $3 / credit and XS). This is just something to look at for as it is came as a surprise to me.\n\n### The Two-Step Suspension Dance\n\nFor the compute pool to auto-suspend, you must first suspend the service:\n\n```\n1 -- Step 1: Suspend the service 2 ALTER  SERVICE my_app SUSPEND ; 3 4 -- Step 2: Now AUTO_SUSPEND_SECS will kick in if you wait for it 5 -- OR you can force it immediately: 6 ALTER COMPUTE  POOL my_pool SUSPEND ; 7 -- you can suspend compute pool by itself which will suspend the service.\n```\n\n### Auto-Resume Works (But Only for Services)\n\nHere's the silver lining: While services don't auto-suspend, they _do_ auto-resume when someone accesses their endpoint - but only if you enable it:\n\n```\n1 ALTER  SERVICE my_app  SET  AUTO_RESUME  = TRUE ; 2 3 -- Now: 4 -- 1. You manually suspend the service 5 -- 2. Someone hits your service endpoint 6 -- 3. The service automatically wakes up 7 -- 4. The compute pool automatically resumes (because AUTO_RESUME = TRUE on pool)\n```\n\n### Snowpark Container Services Cost Management Strategies\n\nGiven these limitations, here are practical approaches if you don\u2019t want your app to run 24x7:\n\n**1\\. Manual Suspend/Resume (Best for Dev/Test)**\n\n```\n1 -- When you're done working: 2 ALTER  SERVICE my_app SUSPEND ; 3 4 -- The service will auto-resume when someone accesses it 5 -- This is manual but reliable\n```\n\n**2\\. Scheduled Tasks (Good for Predictable Usage)**\n\n```\n1 -- Suspend every night at 6 PM 2 CREATE  TASK suspend_service_nightly 3  SCHEDULE  = 'USING CRON 0 18 * * * America/New_York' 4 AS 5 ALTER  SERVICE my_app SUSPEND ; 6 7 -- it will auto resume when someone uses next.\n```\n\n**3\\. Monitor and Alert (Essential for Production)**\n\n```\n1 -- show commands don't run an a warehouse 2 SHOW  SERVICES ; 3 4 -- If you want to use sql for calcs and filtering... 5 SELECT 6  service_name , 7 status , 8  compute_pool_name , 9  current_instances , 10  DATEDIFF ( 'hour' ,  last_resumed , CURRENT_TIMESTAMP ( ) ) as  hours_running 11 FROM  INFORMATION_SCHEMA . SERVICES 12 WHERE status = 'RUNNING' ;\n```\n\n### Accessing suspended SPCS URLs\n\nWhen the service is suspended, the user will reach this error when visiting the URL:\n\n```\n1 { 2 \"responseType\" :  \"ERROR\" , 3 \"requestId\" :  \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" , 4 \"detail\" :  \"Service EXAMPLE_SERVICE not reachable: no service hosts found.\" 5 }\n```\n\nAs long as the service and compute pool have auto-resume enabled, they will begin to spin up when the user tries the URL. It will take about 39-60 seconds, then they can refresh the page. This experience is a little sub-optimal, but not terrible if the users know to expect it.\n\n## Running Batch Jobs using Compute Pools\n\nSince SPCS Compute Pools have a much cheaper cost per credit, we can leverage this as a much cheaper way to run Python (or any language) batch jobs in Snowflake. Let\u2019s say you have a Python job that is containerized and pushed to a Snowflake registry, and a YAML file that describes the service pushed to a Stage. Then you can run an `execute job service ...` command to run the code in that container. These services will turn off when they are finished, allowing the Compute Pool to auto-suspend normally. This is a nice trick to harness cheaper compute in Snowflake!\n\n```\n1 EXECUTE  JOB SERVICE 2 IN COMPUTE  POOL my_compute_pool 3  NAME  =  my_batch_job 4 FROM @my_stage 5  SPEC  = 'job_spec.yaml' ;\n```\n\n## Monitoring Snowpark Container Services cost in Snowsight\n\nThe Snowsight UI provides a convenient way to monitor SPCS spend. Using accountadmin role or a role with access to monitor usage, navigate to Admin \u2192 Cost Management \u2192 Consumption then change the Service Type filter to SPCS.\n\n## Key Takeaways\n\n1. **Compute pools don't idle if services are running** \\- The `AUTO_SUSPEND_SECS` setting only applies when the pool has no active services\n2. **You must explicitly manage service lifecycle** \\- Use `ALTER SERVICE ... SUSPEND` when not in use, and enable `AUTO_RESUME = TRUE` for automatic wake-up (for http apps)\n3. **Auto-suspend doesn't work for public endpoints** \\- Services with public endpoints can auto-resume but won't auto-suspend based on inactivity\n4. **Plan your cost management strategy upfront** \\- Decide whether you'll use manual suspension, scheduled tasks, or just keep services running and budget accordingly\n5. **Every object needs permissions** \\- Image repositories, stages, compute pools, external access integrations, and services all require specific grants\n\n## Getting Started Checklist\n\n\u2705 Create dedicated role for container management\n\n\u2705 Set up database, schema, image repository, and stage\n\n\u2705 Configure external access integration if calling external APIs\n\n\u2705 Create compute pool with appropriate size and auto-suspend settings\n\n\u2705 Upload service specification to stage\n\n\u2705 Create service with `AUTO_RESUME = TRUE`\n\n\u2705 Document manual suspend procedures or create scheduled tasks\n\n\u2705 Set up monitoring queries to track running services and costs\n\n\u2705 Test the suspend/resume cycle before deploying to production\n\n## Wrap Up\n\nSnowpark Container Services brings the power of containerized applications directly to your data in Snowflake. But there is a learning curve; you need to understand image repositories, compute pools, services, and their interconnected behavior. Once you grasp these building blocks and the cost management quirks, you can build powerful data applications that run right next to your data without ever leaving the Snowflake platform.\n\nJeff Skoldberg \u00b7 Sales Engineer @ SELECT\n\nJeff Skoldberg is a Sales Engineer at SELECT, helping customers get maximum value out of the SELECT app to reduce their Snowflake spend. Prior to joining SELECT, Jeff was a Data and Analytics Consultant with 15+ years experience in automating insights and using data to control business processes. From a technology standpoint, he specializes in Snowflake + dbt + Tableau. From a business topic standpoint, he has experience in Public Utility, Clinical Trials, Publishing, CPG, and Manufacturing.\n\nWant to hear about our latest Snowflake learnings? Subscribe to get notified.\n\nSubscribe\n\n### Get up and running in less than 15 minutes\n\nConnect your Snowflake account and instantly understand your savings potential.\n\nBook a demo Start free trial"
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 2
    }
  ]
}

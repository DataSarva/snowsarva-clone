{
  "search_id": "search_75b6c3b172f54f6391fbbd2ee96fca9e",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/query_attribution_history",
      "title": "QUERY_ATTRIBUTION_HISTORY view | Snowflake Documentation",
      "excerpts": [
        "Reference General reference SNOWFLAKE database Account Usage QUERY_ATTRIBUTION_HISTORY\nSchemas:\nACCOUNT_USAGE\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6\nContent:\nThis Account Usage view can be used to determine the compute cost of a given query run on warehouses in your account\nin the last 365 days (1 year).\nFor more information, see Viewing cost by tag in SQL .\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column name | Data type | Description |\n| QUERY_ID | VARCHAR | Internal/system-generated identifier for the SQL statement. |\n| PARENT_QUERY_ID | VARCHAR | Query ID of the parent query or NULL if the query does not have a parent. |\n| ROOT_QUERY_ID | VARCHAR | Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse that the query was executed on. |\n| WAREHOUSE_NAME | VARCHAR | Name of the warehouse that the query executed on. |\n| QUERY_HASH | VARCHAR | The hash value computed based on the canonicalized SQL text. |\n| QUERY_PARAMETERIZED_HASH | VARCHAR | The hash value computed based on the parameterized query. |\n| QUERY_TAG | VARCHAR | Query tag set for this statement through the QUERY_TAG session parameter. |\n| USER_NAME | VARCHAR | User who issued the query. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column name | Data type | Description |\n| QUERY_ID | VARCHAR | Internal/system-generated identifier for the SQL statement. |\n| PARENT_QUERY_ID | VARCHAR | Query ID of the parent query or NULL if the query does not have a parent. |\n| ROOT_QUERY_ID | VARCHAR | Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse that the query was executed on. |\n| START_TIME | TIMESTAMP_LTZ | Time when query execution started (in the local time zone). |\n| END_TIME | TIMESTAMP_LTZ | Time when query execution ended (in the local time zone). |\n| CREDITS_ATTRIBUTED_COMPUTE | NUMBER | Number of credits attributed to this query. Includes only the credit usage for the query execution and doesn\u2019t include any warehouse idle time. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column name | Data type | Description |\n| QUERY_ID | VARCHAR | Internal/system-generated identifier for the SQL statement. |\n| PARENT_QUERY_ID | VARCHAR | Query ID of the parent query or NULL if the query does not have a parent. |\n| ROOT_QUERY_ID | VARCHAR | Query ID of the topmost query in the chain or NULL if the query does not have a parent. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse that the query was executed on. |\n| CREDITS_USED_QUERY_ACCELERATION | NUMBER | Number of credits consumed by the Query Acceleration Service to accelerate the query. NULL if the query is not accelerated. . . The total cost for an accelerated query is the sum of this column and the CREDITS_ATTRIBUTED_COMPUTE column. |\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Usage notes \u00b6\nContent:\nLatency for this view can be up to eight hours. This view displays results for any role granted the USAGE_VIEWER or GOVERNANCE_VIEWER database role . The value in the `credits_attributed_compute` column contains the warehouse credit usage for executing the query,\ninclusive of any resizing and/or autoscaling of multi-cluster warehouse(s). This cost is attributed based on\nthe weighted average of the resource consumption.The value doesn\u2019t include any credit usage for warehouse idle time. Idle time is a period\nof time in which no queries are running in the warehouse and can be measured at the warehouse level.The value doesn\u2019t include any other credit usage that is incurred as a result of query execution.\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Usage notes \u00b6\nContent:\nFor example, the following are not included in the query cost:\nData transfer costs\nStorage costs\nCloud services costs\nCosts for serverless features\nCosts for tokens processed by AI services\nFor queries that are executed concurrently, the cost of the warehouse is attributed to individual queries based on the\nweighted average of their resource consumption during a given time interval. Short-running queries (<= ~100ms) are currently too short for per query cost attribution and are not included in the view. Data for all columns is available starting from mid-August, 2024. Some data prior to this date might be available in the view, but\nmight be incomplete.\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Examples \u00b6 > Query costs for related queries \u00b6\nContent:\nTo determine the costs of a specific query and similar queries using the query parameterized hash, replace `<query_id>` and execute the following statements:\n```\nSET query_id = '<query_id>' ; \n\n WITH query_hash_of_query AS ( \n  SELECT query_parameterized_hash \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_id = $ query_id \n  LIMIT 1 \n ) \n SELECT \n  query_parameterized_hash , \n  COUNT (*) AS query_count , \n  SUM ( credits_attributed_compute ) AS recurrent_query_attributed_credits \n FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n  AND start_time < CURRENT_DATE \n  AND query_parameterized_hash = ( SELECT query_parameterized_hash FROM query_hash_of_query ) \n GROUP BY ALL ;\n```\nCopy\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Examples \u00b6 > Query costs for the current user \u00b6\nContent:\nTo determine the costs of queries executed by the current user for the current month, execute the following statement:\n```\nSELECT user_name , SUM ( credits_attributed_compute ) AS credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE user_name = CURRENT_USER () \n    AND start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY user_name ;\n```\nCopy\nFor an example of attributing warehouse costs to users, see Resources shared by users from different departments .\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Examples \u00b6 > Query costs for stored procedures \u00b6\nContent:\nFor stored procedures that issue multiple hierarchical queries, you can compute the attributed query costs for the\nprocedure by using the root query ID for the procedure.\nTo find the root query ID for a stored procedure, use the ACCESS_HISTORY view . For example,\nto find the root query ID for a stored procedure, set the `query_id` and execute the following statements:CopyFor more information, see Ancestor queries with stored procedures .\nTo sum the query cost for the entire procedure, replace `<root_query_id>` and execute the following statements:Copy\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Examples \u00b6 > Additional examples \u00b6\nContent:\nFor more examples, see Resources shared by users from different departments .\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nColumns\nUsage notes\nExamples\nQuery costs for related queries\nQuery costs for the current user\nQuery costs for stored procedures\nAdditional examples\nRelated content\nOverview of warehouses\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy\nContent:\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > ... > Your Privacy > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Targeting Cookies\nContent:\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nCookies Details\u200e\nSection Title: QUERY_ ATTRIBUTION_ HISTORY view \u00b6 > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/query_history",
      "title": "QUERY_HISTORY view - Source: Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nReference General reference SNOWFLAKE database Account Usage QUERY_HISTORY\nSchemas:\nACCOUNT_USAGE , READER_ACCOUNT_USAGE\nSection Title: QUERY_HISTORY view \u00b6\nContent:\nThis Account Usage view can be used to query Snowflake query history by various dimensions (time range, session, user, warehouse, and so on) within the last 365 days (1 year).\nThe view is available in both the ACCOUNT_USAGE and READER_ACCOUNT_USAGE schemas with the following differences:\nThe following columns are available *only* in the reader account view:\n`reader_account_name`\n`reader_account_deleted_on`\nAlternatively, you can call the Information Schema table function, also named QUERY_HISTORY; however, note that the table function restricts\nthe results to activity over the past 7 days, versus 365 days for the Account Usage view. See the description of the QUERY_HISTORY function .\nSee also:\nQUERY_HISTORY , QUERY_HISTORY_BY_* (Information Schema table function)\nMonitor query activity with Query History (Snowsight dashboard)\nUse the Grouped Query History view in Snowsight\nSection Title: QUERY_HISTORY view \u00b6 > Columns \u00b6\nContent:\nThe *Available only in reader account usage views* column in the following table indicates whether the QUERY_HISTORY column is available in the READER_ACCOUNT_USAGE schema.\n ... \nSection Title: QUERY_HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description | Available only in reader account usage views |\n| `reader_account_name` | VARCHAR | Name of the reader account in which the SQL statement was executed. | \u2714 |\n| `query_id` | VARCHAR | Internal/system-generated identifier for the SQL statement. | \u2714 |\n| `query_text` | VARCHAR | Text of the SQL statement. The limit is 100K characters. Longer SQL statements are truncated. |  |\n| `database_id` | NUMBER | internal/system-generated identifier for the database that was in use. | \u2714 |\n| `bytes_read_from_result` | NUMBER | Number of bytes read from a result object. |  |\nSection Title: QUERY_HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description | Available only in reader account usage views |\n| `reader_account_name` | VARCHAR | Name of the reader account in which the SQL statement was executed. | \u2714 |\n| `query_id` | VARCHAR | Internal/system-generated identifier for the SQL statement. | \u2714 |\n| `query_text` | VARCHAR | Text of the SQL statement. The limit is 100K characters. Longer SQL statements are truncated. |  |\n| `database_id` | NUMBER | internal/system-generated identifier for the database that was in use. | \u2714 |\n| `rows_produced` | NUMBER | The number of rows produced by this statement. The `rows_produced` column will be deprecated in a future release. The value in the `rows_produced` column doesn\u2019t always reflect the logical number of rows affected by a query. Snowflake recommends using the `rows_inserted` , `rows_updated` , `rows_written_to_result` , or `rows_deleted` columns instead. | \u2714 |\n ... \nSection Title: QUERY_HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description | Available only in reader account usage views |\n| `reader_account_name` | VARCHAR | Name of the reader account in which the SQL statement was executed. | \u2714 |\n| `query_id` | VARCHAR | Internal/system-generated identifier for the SQL statement. | \u2714 |\n| `query_text` | VARCHAR | Text of the SQL statement. The limit is 100K characters. Longer SQL statements are truncated. |  |\n| `database_id` | NUMBER | internal/system-generated identifier for the database that was in use. | \u2714 |\n| `outbound_data_transfer_region` | VARCHAR | Target region for statements that unload data to another region and/or cloud. | \u2714 |\n| `outbound_data_transfer_bytes` | NUMBER | Number of bytes transferred in statements that unload data from Snowflake tables. | \u2714 |\n| `inbound_data_transfer_cloud` | VARCHAR | Source cloud provider for statements that load data from another region and/or cloud. | \u2714 |\n ... \nSection Title: QUERY_HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description | Available only in reader account usage views |\n| `reader_account_name` | VARCHAR | Name of the reader account in which the SQL statement was executed. | \u2714 |\n| `query_id` | VARCHAR | Internal/system-generated identifier for the SQL statement. | \u2714 |\n| `query_text` | VARCHAR | Text of the SQL statement. The limit is 100K characters. Longer SQL statements are truncated. |  |\n| `database_id` | NUMBER | internal/system-generated identifier for the database that was in use. | \u2714 |\n| `credits_used_cloud_services` | NUMBER | Number of credits used for cloud services. This value does not take into account the adjustment for cloud services , and may therefore be greater than the credits that are billed. To determine how many credits were actually billed, run queries against the METERING_DAILY_HISTORY view . | \u2714 |\n ... \nSection Title: QUERY_HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description | Available only in reader account usage views |\n| `reader_account_name` | VARCHAR | Name of the reader account in which the SQL statement was executed. | \u2714 |\n| `query_id` | VARCHAR | Internal/system-generated identifier for the SQL statement. | \u2714 |\n| `query_text` | VARCHAR | Text of the SQL statement. The limit is 100K characters. Longer SQL statements are truncated. |  |\n| `database_id` | NUMBER | internal/system-generated identifier for the database that was in use. | \u2714 |\n| `query_acceleration_bytes_scanned` | NUMBER | Number of bytes scanned by the query acceleration service . |  |\n| `query_acceleration_partitions_scanned` | NUMBER | Number of partitions scanned by the query acceleration service. |  |\n| `query_acceleration_upper_limit_scale_factor` | NUMBER | Upper limit scale factor that a query would have benefited from . |  |\n ... \nSection Title: QUERY_HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description | Available only in reader account usage views |\n| `reader_account_name` | VARCHAR | Name of the reader account in which the SQL statement was executed. | \u2714 |\n| `query_id` | VARCHAR | Internal/system-generated identifier for the SQL statement. | \u2714 |\n| `query_text` | VARCHAR | Text of the SQL statement. The limit is 100K characters. Longer SQL statements are truncated. |  |\n| `database_id` | NUMBER | internal/system-generated identifier for the database that was in use. | \u2714 |\n| `user_database_id` | VARCHAR | When the value in the `user_type` column is SNOWFLAKE_SERVICE, it specifies the internal, Snowflake-generated identifier for the service\u2019s database; otherwise, it\u2019s NULL |  |\n| `user_schema_name` | VARCHAR | When the value in the `user_type` column is SNOWFLAKE_SERVICE, it specifies the service\u2019s schema name; otherwise, it\u2019s NULL. |  |\nSection Title: QUERY_HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description | Available only in reader account usage views |\n| `reader_account_name` | VARCHAR | Name of the reader account in which the SQL statement was executed. | \u2714 |\n| `query_id` | VARCHAR | Internal/system-generated identifier for the SQL statement. | \u2714 |\n| `query_text` | VARCHAR | Text of the SQL statement. The limit is 100K characters. Longer SQL statements are truncated. |  |\n| `database_id` | NUMBER | internal/system-generated identifier for the database that was in use. | \u2714 |\n| `user_schema_id` | VARCHAR | When the value in the `user_type` column is SNOWFLAKE_SERVICE, it specifies the internal, Snowflake-generated identifier for the service\u2019s schema; otherwise, it\u2019s NULL. |  |\n ... \nSection Title: QUERY_HISTORY view \u00b6 > Usage notes \u00b6 > General \u00b6\nContent:\nLatency for the view may be up to 45 minutes.\nThe values for the columns `external_function_total_invocations` , `external_function_total_sent_rows` , `external_function_total_received_rows` , `external_function_total_sent_bytes` , and `external_function_total_received_bytes` are affected by many factors, including:\nThe number of external functions in the SQL statement.\nThe number of rows per batch sent to each remote service.\nThe number of retries due to transient errors (for example, because a response was not received within the expected time).\nIf you want to filter on client-generated query statements, use QUERY_HISTORY (an Information Schema table function).\nCanceled queries are identified by their `error_message` text ( `SQL execution canceled` ), not by their `execution_status` value.\n ... \nSection Title: QUERY_HISTORY view \u00b6 > Usage notes \u00b6 > Query history for hybrid tables \u00b6\nContent:\nThe following notes explain when records are logged in the QUERY_HISTORY view for queries against hybrid tables:\nSection Title: QUERY_HISTORY view \u00b6 > Usage notes \u00b6 > Query history for hybrid tables \u00b6\nContent:\nShort-running queries that operate exclusively against hybrid tables do not generate a record in this\nview or QUERY_HISTORY (Information\nSchema table function). To monitor such queries, use the AGGREGATE_QUERY_HISTORY view. This view allows you to more easily monitor high-throughput operational\nworkloads for trends and issues. Short-running queries that operate exclusively against hybrid tables do not provide a query profile\nthat you can inspect in Snowsight. Queries against hybrid tables do generate both a record in the QUERY_HISTORY view and a query profile if any of the\nfollowing conditions are met:\nA query is executed against any table type other than the hybrid table type. This\ncondition ensures that there is no behavior change for any existing\nnon-Unistore workloads. A query fails with an EXECUTION_STATUS of `failed_with_incident` (see QUERY_HISTORY ).\nSection Title: QUERY_HISTORY view \u00b6 > Usage notes \u00b6 > Query history for hybrid tables \u00b6\nContent:\nThis\ncondition ensures that you can investigate and report the specific failed\nquery to receive assistance. A query is running longer than approximately 500 milliseconds. This\ncondition ensures that you can investigate performance issues for slow queries. Query result size is too large. A query is associated with a Snowflake transaction. A query contains a system function with side effects. A query is not one of the following statement types: SELECT, INSERT,\nDELETE, UPDATE, MERGE. A query is executed from SnowSQL, Snowsight, or Classic Console. This\ncondition ensures that you can manually generate a full query profile to\ninvestigate performance issues for any specific query even if it is not\ncategorized as long-running. Even if a query does not meet any of these criteria, queries can be\nperiodically sampled to generate a record in the QUERY_HISTORY view and a\nquery profile to help your investigation.\nSection Title: QUERY_HISTORY view \u00b6 > Usage notes \u00b6 > PUT and GET commands \u00b6\nContent:\nFor the PUT and GET commands,\nan EXECUTION_STATUS of `success` in the QUERY_HISTORY does *not* mean that data files were successfully uploaded or downloaded.\nInstead, the status indicates that Snowflake received authorization to proceed with the file transfer.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nColumns\nUsage notes\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas\nSection Title: QUERY_HISTORY view \u00b6 > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\n ... \nSection Title: QUERY_HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Details\u200e\nSection Title: QUERY_HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details\u200e\nSection Title: QUERY_HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details\u200e\nSection Title: QUERY_HISTORY view \u00b6 > Privacy Preference Center > Your Privacy > Targeting Cookies\nContent:\nTargeting Cookies\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\nCookies Details\u200e\nSection Title: QUERY_HISTORY view \u00b6 > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/query_acceleration_history",
      "title": "QUERY_ACCELERATION_HISTORY view | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSQL data types reference\nSQL command reference\nFunction and stored procedure reference\nClass reference\nScripting reference\nGeneral reference\nAPI reference\nReference General reference SNOWFLAKE database Account Usage QUERY_ACCELERATION_HISTORY\nSchema:\nACCOUNT_USAGE\nSection Title: QUERY_ACCELERATION_HISTORY view \u00b6\nContent:\nEnterprise Edition Feature\nThe query acceleration service requires Enterprise Edition (or higher).\nTo inquire about upgrading, please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\nThis Account Usage view can be used to query the history of queries accelerated by the query acceleration service . The information returned by the view includes the warehouse name\nand the credits consumed by the query acceleration service.\nSection Title: QUERY_ACCELERATION_HISTORY view \u00b6 > Columns \u00b6\nContent:\n| Column Name | Data Type | Description |\n| START_TIME | TIMESTAMP_LTZ | Start of the specified time range. |\n| END_TIME | TIMESTAMP_LTZ | End of the specified time range. |\n| CREDITS_USED | NUMBER | Number of credits billed for the query acceleration service during the START_TIME and END_TIME window. |\n| WAREHOUSE_ID | NUMBER | Internal/system-generated identifier for the warehouse. |\n| WAREHOUSE_NAME | VARCHAR | Name of the warehouse. |\nSection Title: QUERY_ACCELERATION_HISTORY view \u00b6 > Usage notes \u00b6\nContent:\nBilling history is not necessarily updated immediately. Latency for the view may be up to 180 minutes (3 hours).\nIf you want to reconcile the data in this view with a corresponding view in the ORGANIZATION USAGE schema , you must first set the timezone of the session to UTC. Before querying the Account Usage view, execute:Copy\nSection Title: QUERY_ACCELERATION_HISTORY view \u00b6 > Examples \u00b6\nContent:\nThis query returns the total number of credits used by each warehouse in your account for the query acceleration service\n(month-to-date):\n```\nSELECT warehouse_name , \n       SUM ( credits_used ) AS total_credits_used \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_HISTORY \n  WHERE start_time >= DATE_TRUNC ( month , CURRENT_DATE ) \n  GROUP BY 1 \n  ORDER BY 2 DESC ;\n```\nCopy\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nColumns\nUsage notes\nExamples\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/query-acceleration-service",
      "title": "Using the Query Acceleration Service (QAS) | Snowflake Documentation",
      "excerpts": [
        "Section Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Examples \u00b6\nContent:\nNote\nThese examples assume the ACCOUNTADMIN role (or a role granted IMPORTED PRIVILEGES on the\nshared SNOWFLAKE database) is in use. If it is not in use, execute the following command before running the queries in the examples:\n```\nUSE ROLE ACCOUNTADMIN ;\n```\nCopy\nIdentify the queries in the past week that might benefit the most from the service by the longest amount of query execution time that is\neligible for acceleration:\n```\nSELECT query_id , eligible_query_acceleration_time \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_ELIGIBLE \n  WHERE start_time > DATEADD ( 'day' , - 7 , CURRENT_TIMESTAMP ()) \n  ORDER BY eligible_query_acceleration_time DESC ;\n```\nCopy\nIdentify the queries in the past week that might benefit the most from the service in a specific warehouse `mywh` :\nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Examples \u00b6\nContent:\n```\nSELECT query_id , eligible_query_acceleration_time \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_ELIGIBLE \n  WHERE warehouse_name = 'MYWH' \n  AND start_time > DATEADD ( 'day' , - 7 , CURRENT_TIMESTAMP ()) \n  ORDER BY eligible_query_acceleration_time DESC ;\n```\nCopy\nIdentify the warehouses with the most queries, in the past week, eligible for the query acceleration service:\n```\nSELECT warehouse_name , COUNT ( query_id ) AS num_eligible_queries \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_ELIGIBLE \n  WHERE start_time > DATEADD ( 'day' , - 7 , CURRENT_TIMESTAMP ()) \n  GROUP BY warehouse_name \n  ORDER BY num_eligible_queries DESC ;\n```\nCopy\nIdentify the warehouses with the most eligible time for the query acceleration service in the past week:\nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Examples \u00b6\nContent:\n```\nSELECT warehouse_name , SUM ( eligible_query_acceleration_time ) AS total_eligible_time \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_ELIGIBLE \n  WHERE start_time > DATEADD ( 'day' , - 7 , CURRENT_TIMESTAMP ()) \n  GROUP BY warehouse_name \n  ORDER BY total_eligible_time DESC ;\n```\nCopy\nIdentify the upper limit scale factor in the past week for the query acceleration\nservice for warehouse `mywh` :\n```\nSELECT MAX ( upper_limit_scale_factor ) \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_ELIGIBLE \n  WHERE warehouse_name = 'MYWH' \n  AND start_time > DATEADD ( 'day' , - 7 , CURRENT_TIMESTAMP ());\n```\nCopy\nIdentify the distribution of scale factors in the past week for the query acceleration service for warehouse `mywh` :\nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Examples \u00b6\nContent:\n```\nSELECT upper_limit_scale_factor , COUNT ( upper_limit_scale_factor ) \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_ELIGIBLE \n  WHERE warehouse_name = 'MYWH' \n  AND start_time > DATEADD ( 'day' , - 7 , CURRENT_TIMESTAMP ()) \n  GROUP BY 1 ORDER BY 1 ;\n```\nCopy\nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > Adjusting the scale factor \u00b6\nContent:\nThe scale factor is a cost control mechanism that allows you to set an upper bound on the amount of compute resources a warehouse can\nlease for query acceleration. This value is used as a multiplier based on warehouse size and cost.\nFor example, suppose that you set the scale factor to 5 for a medium warehouse. This means that:\nThe warehouse can lease compute resources up to 5 times the size of a medium warehouse.\nBecause a medium warehouse costs 4 credits per hour , leasing these resources can cost up\nto an additional 20 credits per hour (4 credits per warehouse x 5 times its size).\nTip\nThe scale factor applies to the entire warehouse, whether it\u2019s a single-cluster or multi-cluster warehouse.\nIf you use QAS for a multi-cluster warehouse, consider increasing the scale factor.\nThat way, all the warehouse clusters can take advantage of the QAS optimizations.\n ... \nSection Title: ... > Using the Account Usage QUERY_ HISTORY view to monitor query acceleration usage \u00b6\nContent:\nTo see the effects of query acceleration on a query, you can use the following columns in the QUERY_HISTORY view .\nQUERY_ACCELERATION_BYTES_SCANNED\nQUERY_ACCELERATION_PARTITIONS_SCANNED\nQUERY_ACCELERATION_UPPER_LIMIT_SCALE_FACTOR\nYou can use these columns to identify the queries that benefited from the query acceleration service. For each query, you can also\ndetermine the total number of partitions and bytes scanned by the query acceleration service.\nFor descriptions of each of these columns, see QUERY_HISTORY view .\nNote\nFor a given query, the sum of the QUERY_ACCELERATION_BYTES_SCANNED and BYTES_SCANNED columns might be greater when the query\nacceleration service is used than when the service is not used. The same is true for the sum of the columns\nQUERY_ACCELERATION_PARTITIONS_SCANNED and PARTITIONS_SCANNED.\nSection Title: ... > Using the Account Usage QUERY_ HISTORY view to monitor query acceleration usage \u00b6\nContent:\nThe increase in the number of bytes and partitions is due to the intermediary results that are generated by the service to\nfacilitate query acceleration.\nFor example, to find the queries with the most bytes scanned by the query acceleration service in the past 24 hours:\n```\nSELECT query_id , \n       query_text , \n       warehouse_name , \n       start_time , \n       end_time , \n       query_acceleration_bytes_scanned , \n       query_acceleration_partitions_scanned , \n       query_acceleration_upper_limit_scale_factor \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_HISTORY \n  WHERE query_acceleration_partitions_scanned > 0 \n  AND start_time >= DATEADD ( hour , - 24 , CURRENT_TIMESTAMP ()) \n  ORDER BY query_acceleration_bytes_scanned DESC ;\n```\nCopy\nTo find the queries with the largest number of partitions scanned by the query acceleration service in the past 24 hours:\nSection Title: ... > Using the Account Usage QUERY_ HISTORY view to monitor query acceleration usage \u00b6\nContent:\n```\nSELECT query_id , \n       query_text , \n       warehouse_name , \n       start_time , \n       end_time , \n       query_acceleration_bytes_scanned , \n       query_acceleration_partitions_scanned , \n       query_acceleration_upper_limit_scale_factor \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_HISTORY \n  WHERE query_acceleration_partitions_scanned > 0 \n  AND start_time >= DATEADD ( hour , - 24 , CURRENT_TIMESTAMP ()) \n  ORDER BY query_acceleration_partitions_scanned DESC ;\n```\nCopy\nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > Query acceleration service cost \u00b6\nContent:\nQuery Acceleration consumes credits as it uses serverless compute resources to execute portions of\neligible queries.\nQuery Acceleration is billed like other serverless features in Snowflake in that you pay by the second for the compute resources used. To\nlearn how many credits per compute-hour are consumed by the Query Acceleration Service, refer to the \u201cServerless\nFeature Credit Table\u201d in the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Viewing billing information in Snowsight \u00b6\nContent:\nIf you have Query Acceleration enabled for your account, use the Cost Management page in Snowsight to view billing\ninformation for the Query Acceleration Service.\nTo see Query Acceleration Service spending, complete the following steps:\nIn the navigation menu, select Admin \u00bb Cost management .\nSelect the Consumption tab.\nSelect Query Acceleration from the Service Type drop-down.Snowsight displays the Query Acceleration Service spending for your account.\n ... \nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Example \u00b6\nContent:\nThis query returns the total number of credits used by each warehouse in your account for the query acceleration service\n(month-to-date):\n```\nSELECT warehouse_name , \n       SUM ( credits_used ) AS total_credits_used \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_HISTORY \n  WHERE start_time >= DATE_TRUNC ( month , CURRENT_DATE ) \n  GROUP BY 1 \n  ORDER BY 2 DESC ;\n```\nCopy\n ... \nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Example \u00b6\nContent:\nThis query returns the total number of credits used by each warehouse in each account for the query acceleration service (month-to-date):\n```\nSELECT account_name , \n       warehouse_name , \n       SUM ( credits_used ) AS total_credits_used \n  FROM SNOWFLAKE . ORGANIZATION_USAGE . QUERY_ACCELERATION_HISTORY \n  WHERE usage_date >= DATE_TRUNC ( month , CURRENT_DATE ) \n  GROUP BY 1 , 2 \n  ORDER BY 3 DESC ;\n```\nCopy\n ... \nSection Title: ... > Viewing warehouse and query acceleration service costs \u00b6\nContent:\nThe following query computes the costs of the warehouse and the query acceleration service for a specific warehouse. You can execute\nthis query after enabling the query acceleration service for a warehouse to compare costs before and after enabling query acceleration.\nThe date range for the query begins 8 weeks prior to the first credit usage for the query acceleration service to 8 weeks after the last\nincurred cost for query acceleration service (or up to the current date).\nNote\nThis query is most useful for evaluating the cost of the service if the warehouse properties and workload remain the same\nbefore and after enabling the query acceleration service.\nThis query returns results only if there has been credit usage for accelerated queries in the warehouse.\nThis example query returns the warehouse and query acceleration service costs for `my_warehouse` :\nSection Title: ... > Viewing warehouse and query acceleration service costs \u00b6\nContent:\n```\nWITH credits AS ( \n  SELECT 'QC' AS credit_type , \n         TO_DATE ( end_time ) AS credit_date , \n         SUM ( credits_used ) AS num_credits \n    FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_HISTORY \n    WHERE warehouse_name = 'my_warehouse' \n    AND credit_date BETWEEN \n           DATEADD ( WEEK , - 8 , ( \n             SELECT TO_DATE ( MIN ( end_time )) \n               FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_HISTORY \n               WHERE warehouse_name = 'my_warehouse' \n           )) \n           AND \n           DATEADD ( WEEK , + 8 , ( \n             SELECT TO_DATE ( MAX ( end_time )) \n               FROM SNOWFLAKE . ACCOUNT_USAGE .\nSection Title: ... > Viewing warehouse and query acceleration service costs \u00b6\nContent:\nQUERY_ACCELERATION_HISTORY \n               WHERE warehouse_name = 'my_warehouse' \n           )) \n  GROUP BY credit_date \n  UNION ALL \n  SELECT 'WC' AS credit_type , \n         TO_DATE ( end_time ) AS credit_date , \n         SUM ( credits_used ) AS num_credits \n    FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n    WHERE warehouse_name = 'my_warehouse' \n    AND credit_date BETWEEN \n           DATEADD ( WEEK , - 8 , ( \n             SELECT TO_DATE ( MIN ( end_time )) \n               FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_HISTORY \n               WHERE warehouse_name = 'my_warehouse' \n           )) \n           AND \n           DATEADD ( WEEK , + 8 , ( \n             SELECT TO_DATE ( MAX ( end_time )) \n               FROM SNOWFLAKE . ACCOUNT_USAGE .\nSection Title: ... > Viewing warehouse and query acceleration service costs \u00b6\nContent:\nQUERY_ACCELERATION_HISTORY \n               WHERE warehouse_name = 'my_warehouse' \n           )) \n  GROUP BY credit_date \n ) \n SELECT credit_date , \n       SUM ( IFF ( credit_type = 'QC' , num_credits , 0 )) AS qas_credits , \n       SUM ( IFF ( credit_type = 'WC' , num_credits , 0 )) AS compute_credits , \n       compute_credits + qas_credits AS total_credits , \n       AVG ( total_credits ) OVER ( \n         PARTITION BY NULL ORDER BY credit_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW ) \n         AS avg_total_credits_7days \n  FROM credits \n  GROUP BY credit_date \n  ORDER BY credit_date ;\n```\n ... \nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Viewing query performance \u00b6\nContent:\n```\nWITH qas_eligible_or_accelerated AS ( \n  SELECT TO_DATE ( qh . end_time ) AS exec_date , \n        COUNT (*) AS num_execs , \n        SUM ( qh . execution_time ) AS exec_time , \n        MAX ( IFF ( qh . query_acceleration_bytes_scanned > 0 , 1 , NULL )) AS qas_accel_flag \n    FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_HISTORY AS qh \n    WHERE qh . warehouse_name = 'my_warehouse' \n    AND TO_DATE ( qh . end_time ) BETWEEN \n           DATEADD ( WEEK , - 8 , ( \n             SELECT TO_DATE ( MIN ( end_time )) \n               FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_HISTORY \n              WHERE warehouse_name = 'my_warehouse' \n           )) \n           AND \n           DATEADD ( WEEK , + 8 , ( \n             SELECT TO_DATE ( MAX ( end_time )) \n               FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_HISTORY \n              WHERE warehouse_name = 'my_warehouse' \n           )) \n    AND ( qh .\nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Viewing query performance \u00b6\nContent:\nquery_acceleration_bytes_scanned > 0 \n\n          EXISTS ( \n            SELECT 1 \n              FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ACCELERATION_ELIGIBLE AS qae \n               WHERE qae . query_id = qh . query_id \n               AND qae . warehouse_name = qh . warehouse_name \n          ) \n         ) \n    GROUP BY exec_date \n ) \n SELECT exec_date , \n       SUM ( exec_time ) OVER ( \n         PARTITION BY NULL ORDER BY exec_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW \n       ) / \n       NULLIFZERO ( SUM ( num_execs ) OVER ( \n         PARTITION BY NULL ORDER BY exec_date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW ) \n       ) AS avg_exec_time_7days , \n      exec_time / NULLIFZERO ( num_execs ) AS avg_exec_time , \n      qas_accel_flag , \n      num_execs , \n      exec_time \n  FROM qas_eligible_or_accelerated ;\n```\n ... \nSection Title: Using the Query Acceleration Service (QAS) \u00b6 > ... > Viewing query performance \u00b6\nContent:\nTip\nWhen the query acceleration service (QAS) is enabled, Snowflake writes a small amount of data to remote storage\nfor each eligible query, even if QAS isn\u2019t used for that query. Therefore, don\u2019t be concerned by a nonzero\nvalue for `bytes_spilled_to_remote_storage` in the QUERY_HISTORY view when QAS is enabled."
      ]
    },
    {
      "url": "https://blog.greybeam.ai/snowflake-cost-per-query/",
      "title": "Deep Dive: Snowflake's Query Cost and Idle Time Attribution",
      "publish_date": "2024-10-22",
      "excerpts": [
        "[](https://www.greybeam.ai/)\n[Blog](https://blog.greybeam.ai/)\n[Waitlist](https://greybeam.ai)\n[Customer Stories](https://blog.greybeam.ai/tag/customer-story/)\nSubscribe\nSep 9, 2024 13 min read How-To\nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query\nContent:\nSnowflake's new QUERY_ATTRIBUTION_HISTORY view\nSnowflake recently released a new feature for granular cost attribution down to individual queries through the `QUERY_ATTRIBUTION_HISTORY` view in `ACCOUNT_USAGE` . As a company focused on SQL optimization, we at Greybeam were eager to dive in and see how this new capability compares to our own custom cost attribution logic. What we found was surprising - and it led us down a rabbit hole of query cost analysis.\nSection Title: ... > The Promise and Limitations of QUERY_ATTRIBUTION_HISTORY\nContent:\nThe new view aims to provide visibility into the compute costs associated with each query. Some key things to note:\nData is only available from July 1, 2024 onwards\nShort queries (<100ms) are excluded\nIdle time is not included in the attributed costs\nThere can be up to a 6 hour delay in data appearing\nThere's also a `WAREHOUSE_UTILIZATION` view that displays cost of idle time. At the time of writing, this must be enabled by your Snowflake support team.\nSection Title: ... > Our Initial Findings\nContent:\nWe set up a test with an X-Small warehouse and 600 second auto-suspend to dramatically illustrate idle time. Running a series of short queries (mostly <500ms) over an hour, we expected to see a very small fraction of the total credits in that hour attributed to our queries, but we were very wrong.\nOn September 4th at the 14th hour, ~40 seconds of queries were executed and some how in the `QUERY_ATTRIBUTION_HISTORY` view it showed that nearly half of the total credits (0.43 of 0.88) attributed to query execution. This seemed impossibly high given the short query runtimes, yet the pattern continues.\nQUERY_ATTRIBUTION_HISTORY aggregated by the hour.\nThis may just be an anomaly in our Snowflake account, so try it yourself.\nSection Title: ... > Our Initial Findings\nContent:\n```\nWITH query_execution AS (\n    SELECT\n        qa.query_id\n        , TIMEADD(\n                'millisecond',\n                qh.queued_overload_time + qh.compilation_time +\n                qh.queued_provisioning_time + qh.queued_repair_time +\n                qh.list_external_files_time,\n                qh.start_time\n            ) AS execution_start_time\n        , qh.end_time::timestamp AS end_time\n        , DATEDIFF('MILLISECOND', execution_start_time, qh.end_time)*0.001 as execution_time_secs\n        , qa.credits_attributed_compute\n        , DATE_TRUNC('HOUR', execution_start_time) as execution_start_hour\n        , w.credits_used_compute\n    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY AS qa\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS qh\n        ON qa.query_id = qh.query_id\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS w\n        ON execution_start_hour = w.start_time\n        AND qh.warehouse_id = w.warehouse_id\n    WHERE\n ... \nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > ... > Digging Deeper\nContent:\nTo investigate further, we compared the results to our own custom cost attribution logic that accounts for idle time. Here\u2019s a snippet of what we found for the same hour:\nGreybeam\u2019s internal query cost attribution results\nAs you can see, our calculations show much smaller fractions of credits attributed to the actual query runtimes for the first hour, with the bulk going to idle periods. This aligns much more closely with our expectations given the warehouse configuration, and it works historically!\nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > ... > Potential Issues\nContent:\nAt the time of writing, we\u2019ve identified a few potential problems with the new view:\nWarehouse ID mismatch\u200a\u2014\u200aThe `warehouse_id` in `QUERY_ATTRIBUTION_HISTORY` doesn't match the actual `warehouse_id` from `QUERY_HISTORY` .\nInflated query costs\u200a\u2014\u200aThe credits attributed to short queries seem disproportionately high in some cases.\nIdle time accounting\u200a\u2014\u200aIt\u2019s unclear how idle time factors into the attribution, if at all.\nWe\u2019ve raised these concerns with Snowflake, and they\u2019ve recommended filing a support ticket for further investigation. In the meantime, we\u2019ll continue to rely on our custom attribution logic for accuracy.\n ... \nSection Title: ... > Our Approach to Query Cost Attribution\nContent:\nWe use `WAREHOUSE_METERING_HISTORY` as our source of truth for warehouse compute credits. The credits billed here will reconcile with Snowflake\u2019s cost management dashboards.\nCredits here are represented on an hourly grain. We like to refer to this as *credits metered* , analogous to how most homes in North America are metered for their electricity. In our solution, we\u2019ll need to allocate queries and idle times into their metered hours.\nWe use a weighted time-based approach to attribute costs within the metered hour. In reality, Snowflake\u2019s credit attribution is likely much more complex, especially in situations with more clusters or warehouse scaling.\nHow we need to break down our queries and idle times.\nThe full SQL query will be available at the end of this blog.\nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\n\u2757\nWe've updated this article with an optimization using `ASOF JOIN` . Check out how to use ASOF JOINs [here](https://blog.greybeam.ai/snowflake-asof-join/) .\nFirst, we need to know when warehouses are suspended, this is pulled from [`WAREHOUSE_EVENTS_HISTORY`](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_events_history?ref=blog.greybeam.ai) .\n```\nWITH warehouse_events AS (\n    SELECT\n        warehouse_id\n        , timestamp\n        , LAG(timestamp) OVER (PARTITION BY warehouse_id ORDER BY timestamp) as lag_timestamp\n    FROM snowflake.account_usage.warehouse_events_history\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'\n        AND DATEADD('DAY', 15, timestamp) >= current_date\n)\n```\n ... \nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\nq.queued_provisioning_time + q.queued_repair_time +\n                q.list_external_files_time,\n                q.start_time\n            ) AS execution_start_time\n        , q.end_time::timestamp AS end_time\n        , w.timestamp AS suspended_at\n        , MAX(q.end_time) OVER (PARTITION BY q.warehouse_id, w.timestamp ORDER BY execution_start_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as end_time_max\n        , LEAD(execution_start_time) OVER (PARTITION BY q.warehouse_id ORDER BY execution_start_time ASC) as next_query_at\n    FROM query_history AS q\n    ASOF JOIN warehouse_events AS w\n        MATCH_CONDITION (q.end_time::timestamp <= w.timestamp)\n        ON q.warehouse_id = w.warehouse_id\n    WHERE\n        q.warehouse_size IS NOT NULL\n        AND q.execution_status = 'SUCCESS'\n        AND start_time >= $startDate\n        AND EXISTS (\n            SELECT 1\n            FROM warehouse_list AS wl\n            WHERE\nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\nq.warehouse_id = wl.warehouse_id\n            )\n)\n```\nSection Title: ... > Step 2: Enrich Query Data\nContent:\nIn this step, we take the raw query data and enrich it with additional information that will allow us to breakdown query and idle times into their hourly components. We choose hourly slots because the source of truth for credits comes from `WAREHOUSE_METERING_HISTORY` , which is on an hourly grain.\n ... \nSection Title: ... > Step 2: Enrich Query Data\nContent:\nKey points to highlight:\n ... \nSection Title: ... > Step 3: Create Timeline of All Events\nContent:\nWe now need to create an hourly timeline of all events so that we can reconcile our credits with `WAREHOUSE_METERING_HISTORY` . The timeline of all events can be broken down into 4 components:\nA query executed and ended in the same hour\nIdle time started and ended in the same hour\nA query executed and ended in a different hour\nIdle time started and ended in a different hour\n1 and 2 are straight forward since they don\u2019t cross any hourly boundaries we can simply select from the dataset and join directly to `WAREHOUSE_METERING_HISTORY` .\n ... \nSection Title: ... > Step 3: Create Timeline of All Events\nContent:\nmeter_time_secs\n    FROM queries_enriched AS q\n    WHERE\n        q.is_same_hour_idle = TRUE\n```\nSection Title: ... > Step 3: Create Timeline of All Events\nContent:\nFor 3 and 4, we need a record for each hour that the queries and idle times ran within. For example, if a query ran from 7:55PM to 10:40PM, we\u2019d need a record for 7, 8, 9, and 10PM.\nA query that executed across 4 hourly slots (including 0).\nOriginally we used a slightly more complicated join:\n```\nFROM queries_enriched AS q\nJOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS m\n    ON q.warehouse_id = m.warehouse_id\n    AND m.start_time >= q.meter_start_time\n    AND m.start_time < q.end_time\n```\nThis took forever to run on a large account. Instead, we first create records for each hour so that the join to `WAREHOUSE_METERING_HISTORY` is a direct join in the next step.\n ... \nSection Title: ... > Step 4: Attribute Costs\nContent:\nFinally, with each query and idle period properly allocated to their hourly slots, we can directly join to `WAREHOUSE_METERING_HISTORY` and calculate our credits used.\n```\nmetered AS (\n    SELECT\n        m.query_id\n        , m.warehouse_id\n        , m.type\n        , m.event_start_at\n        , m.event_end_at\n        , m.meter_start_hour\n        , m.meter_start_at\n        , m.meter_end_at\n        , m.meter_time_secs\n        , SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour) AS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN snowflake.account_usage.warehouse_metering_history AS w -- inner join because both tables have different delays\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time -- we can directly join now since we used our numgen method\n)\n```\nSection Title: ... > Step 4: Attribute Costs\nContent:\nIn this approach we allocate credits based on the proportion of the total execution time in that hour:\n**Time-based Weighting** : We use the duration of each event (query or idle period) as the basis for our weighting. This is represented by `m.meter_time_secs` .\n**Hourly Totals** : We calculate the total time for all events within each hour for each warehouse `SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour)` .\n**Credit Allocation** : We then allocate credits to each event based on its proportion of the total time in that hour `(m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute` .\nSection Title: ... > Step 4: Attribute Costs\nContent:\nOne important note: This approach assumes that all time within an hour is equally valuable in terms of credit consumption. In reality, Snowflake may have more complex internal algorithms for credit attribution, especially for multi-cluster warehouses or warehouses that change size within an hour. However, this weighted time-based approach provides a reasonable and transparent method for cost attribution that aligns well with Snowflake\u2019s consumption-based billing model.\nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > Conclusion\nContent:\nWhile Snowflake\u2019s new `QUERY_ATTRIBUTION_HISTORY` view is a promising step towards easier cost attribution, our initial testing reveals some potential issues that need to be addressed. For now, we recommend carefully validating the results against your own calculations and metering history.\nWe\u2019re excited to see how this feature evolves and will continue to monitor its accuracy. In the meantime, implementing your own cost attribution logic can provide valuable insights into query performance and resource utilization.\nBy accounting for idle time and carefully tracking query execution across hour boundaries, we\u2019re able to get a more complete and accurate picture of costs. This level of detail is crucial for optimizing Snowflake usage and controlling costs effectively.\nSection Title: ... > Struggling with Snowflake costs?\nContent:\nAll usage-based cloud platforms can get expensive when not used carefully. There are a ton of controls teams can fiddle with to get a handle on their Snowflake costs. At Greybeam, we\u2019ve built a query performance and observability platform that automagically optimizes SQL queries sent to Snowflake, saving you thousands in compute costs. Reach out to [[email protected]](/cdn-cgi/l/email-protection) to learn more about how we can optimize your Snowflake environment.\n ... \nSection Title: ... > Full SQL Cost Attribution\nContent:\nq.compilation_time +\n                q.queued_provisioning_time + q.queued_repair_time +\n                q.list_external_files_time,\n                q.start_time\n            ) AS execution_start_time\n        , q.end_time::timestamp AS end_time\n        , w.timestamp AS suspended_at\n        , MAX(q.end_time) OVER (PARTITION BY q.warehouse_id, w.timestamp ORDER BY execution_start_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as end_time_max\n        , LEAD(execution_start_time) OVER (PARTITION BY q.warehouse_id ORDER BY execution_start_time ASC) as next_query_at\n    FROM query_history AS q\n    ASOF JOIN warehouse_events AS w\n        MATCH_CONDITION (q.end_time::timestamp <= w.timestamp)\n        ON q.warehouse_id = w.warehouse_id\n    WHERE\n        q.warehouse_size IS NOT NULL\n        AND q.execution_status = 'SUCCESS'\n        AND start_time >= $startDate\n        AND EXISTS (\n            SELECT 1\n            FROM warehouse_list AS wl\n ... \nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > ... > Kyle Cheung\nContent:\noptimization for you, you, you, you, and you\nSection Title: ... > Cut Costs by Querying Snowflake Tables in DuckDB with Apache Arrow\nContent:\nJan 30, 2025\nSection Title: ... > Querying Snowflake Managed Iceberg Tables with DuckDB\nContent:\nDec 12, 2024\nSection Title: ... > Getting Started with pyIceberg and AWS Glue\nContent:\nDec 6, 2024\n[Powered by Ghost](https://ghost.org/)"
      ]
    },
    {
      "url": "https://yukidata.com/snowflake-cost-per-query/",
      "title": "Snowflake Cost Per Query: Query-Level Cost Attribution Guide | Yuki",
      "publish_date": "2025-07-30",
      "excerpts": [
        "[](https://yukidata.com)\nSolutionsClose Solutions Open Solutions\nResourcesClose Resources Open ResourcesResources[Blog](https://yukidata.com/blog/) [Customers](https://yukidata.com/customers/) [News](https://yukidata.com/news/) [Documentation](https://docs.yukidata.com/)[](https://yukidata.com/dbt-cloud-snowflake-oauth/)[How to Set Up Snowflake OAuth with dbt](https://yukidata.com/dbt-cloud-snowflake-oauth/)[Learn More >](https://yukidata.com/dbt-cloud-snowflake-oauth/)[](https://yukidata.com/qwilt/)[How Qwilt Cut 63% of Snowflake Costs in Days](https://yukidata.com/qwilt/)[Learn More >](https://yukidata.com/qwilt/)\n[About Us](https://yukidata.com/about/)\n[Contact Us](https://yukidata.com/contact/)\n[Get Started](https://yukidata.com/request-demo/)\n[Free Trial](https://yukidata.com/free-trial)\n[](https://yukidata.com)\nSolutionsClose Solutions Open Solutions\nResourcesClose Resources Open Resources[Blog](https://yukidata.com/blog/) [Customers](https://yukidata.com/customers/) [News](https://yukidata.com/news/) [Documentation](https://yukidata.com/customers/)\n[About Us](https://yukidata.com/about/)\n[Contact Us](https://yukidata.com/contact/)\nSection Title: Snowflake Cost Per Query: Complete Guide to Query-Level Cost Attribution\nContent:\nBy Ido Arieli Noga\nJune 10, 2025 | 5 min read\nPicture this: Your Snowflake bill has just hit a $50,000 all time high for the month. You have no idea what queries keep hiking up the [price](https://yukidata.com/blog/snowflake-costs-pricing-guide/) . You\u2019re flying blind and optimizing in the dark.\nIf this is you, you\u2019re not alone. Many Snowflake customers understand what drives their warehouse level costs, but can\u2019t dig deep to query-level cost attribution level. That gap is the difference between an out of budget and within budget month.\nThe good news? Calculating Snowflake cost per query is doable. And it\u2019s vital to truly understand [Snowflake cost optimization](https://yukidata.com/blog/snowflake-cost-optimization-guide/) if you\u2019re not investing in a third-party tool or a [consultant](https://yukidata.com/blog/snowflake-optimization-consultants/) .\nIn this article, we\u2019ll break down:\nSection Title: Snowflake Cost Per Query: Complete Guide to Query-Level Cost Attribution\nContent:\nWhy query-level cost attribution is so important\nEasier, but less in-depth calculation strategy\nA more advanced calculation approach\nSnowflake\u2019s native query attribution\nHow to streamline costs with third-party tools\nSection Title: ... > Why Query-Level Cost Attribution Matters\nContent:\nRemember: Snowflake\u2019s billing model means you\u2019re paying for every second your virtual warehouses are running \u2013 and you have a minimum 60-second charge every time a warehouse resumes.\nSnowflake will give you a warehouse level cost breakdown, but what it doesn\u2019t show you are the queries behind your bill.\n*This* is why you need to understand the cost per query. Query-level cost attribution reveals situations like:\nThat single poorly optimized dashboard query costing you $2,000 every month\nRedundant data transformations running multiple times and churning out identical results\nThose developmental queries running on production-sized warehouses\n[Idle time queries](https://yukidata.com/blog/the-silent-credit-killer-snowflakes-idle-time-drain/) eating up 40% of your compute budget\nNow that you know why keeping an eye on Snowflake cost per query is so important, let\u2019s take a look at two approaches to calculate costs.\nSection Title: ... > The Easier Approach: Execution Time x Billing Rate\nContent:\nThere\u2019s an easy way and a harder way to calculate your Snowflake cost per query. The more straightforward path is easier to do, but comes with a few problems.\nAll you have to do is take your query\u2019s execution time and multiply it by your warehouse billing rate.\n**Example calculation:**\nSay you have a:\nQuery that runs for 10 minutes on a Medium warehouse\nThat medium warehouse costs you four credits per hour\nAt $3 per credit, you\u2019re looking at: 10/60 hours x 4 credits/hour x $3/credit = $2\nSection Title: Snowflake Cost Per Query: Complete Guide to Query-Level Cost Attribution > ... > SQL Implementation\nContent:\n```\nWITH warehouse_sizes AS (\n\u00a0 \u00a0 SELECT 'X-Small' AS warehouse_size, 1 AS credits_per_hour UNION ALL\n\u00a0 \u00a0 SELECT 'Small' AS warehouse_size, 2 AS credits_per_hour UNION ALL\n\u00a0 \u00a0 SELECT 'Medium' AS warehouse_size, 4 AS credits_per_hour UNION ALL\n\u00a0 \u00a0 SELECT 'Large' AS warehouse_size, 8 AS credits_per_hour UNION ALL\n\u00a0 \u00a0 SELECT 'X-Large' AS warehouse_size, 16 AS credits_per_hour UNION ALL\n\u00a0 \u00a0 SELECT '2X-Large' AS warehouse_size, 32 AS credits_per_hour UNION ALL\n\u00a0 \u00a0 SELECT '3X-Large' AS warehouse_size, 64 AS credits_per_hour UNION ALL\n\u00a0 \u00a0 SELECT '4X-Large' AS warehouse_size, 128 AS credits_per_hour\n)\nSELECT\n\u00a0 \u00a0 qh.query_id,\n\u00a0 \u00a0 qh.query_text,\n\u00a0 \u00a0 qh.execution_time/1000/3600) * wh.credits_per_hour * 3 AS query_cost\nFROM snowflake.account_usage.query_history AS qh\nINNER JOIN warehouse_sizes AS wh\n\u00a0 \u00a0 ON qh.warehouse_size=wh.warehouse_size\nWHERE\n\u00a0 \u00a0 start_time >= CURRENT_DATE - 30\n```\nSection Title: ... > The Problem With the Easier Approach\nContent:\nThis approach is simple for a reason. It gives you an idea of your spend, but not the full picture.\n**Idle time excluded:** You\u2019re paying for the entire active time of a warehouse, including idle periods. If a query runs for six seconds but causes a warehouse to idle for four minutes before auto-suspension, you\u2019re paying for a full four minutes and six seconds.\n**Concurrency distorts costs:** When multiple queries run simultaneously, you\u2019re charge for total warehouse time, not individual query time. Two 20 minute queries running concurrently would cost you 20 minutes, not 40.\n**Query metadata makes group difficult:** If you\u2019re using Looker, dbt, or Tableau, you should know each injects unique metadata into every query execution, making identical queries look different:\nSection Title: ... > The Problem With the Easier Approach\nContent:\n```\n-- First execution\nSELECT id, created_at FROM orders\n-- {\"user_id\":181,\"history_slug\":\"9dcf35a\"}\n\n-- Second execution\u00a0\nSELECT id, created_at FROM orders\n-- {\"user_id\":181,\"history_slug\":\"1kal99e\"}\n```\nWithout proper text processing, these identical queries won\u2019t be grouped together for cost analysis.\nThere is a way to remove comments and metadata like this:\n```\nSELECT\n\u00a0 \u00a0 -- Remove /* comment */ blocks\n\u00a0 \u00a0 REGEXP_REPLACE(query_text, '(/\\*.*\\*/)') AS _cleaned_query_text,\n\u00a0 \u00a0 -- Remove -- single line comments\u00a0\n\u00a0 \u00a0 REGEXP_REPLACE(_cleaned_query_text, '(--.*$)|(--.*\\n)') AS cleaned_query_text\nFROM snowflake.account_usage.query_history\n```\nUse this code to group functionally identical queries no matter the injected metadata.\nNote: Test this regex pattern with your specific metadata format as syntax may vary.\nSection Title: ... > The Advanced Approach: Proportional Credit Allocation\nContent:\nThe more difficult, but accurate, method starts with actual warehouse charges from warehouse_metering_history and allocates credits according to execution time within every hour.\nFor example, if a warehouse consumes 100 credits in an hour and three queries ran (two for 10 minutes, one for 20), credits are allocated proportionally with this method:\nQuery 1 (10 minutes) 25 credits\nQuery 2 (20 minutes): 50 credits\nQuery 3 (10 minutes): 25 credits\nThis approach automatically accounts for idle time because it distributes queries that ran during that period.\n[](https://hubspot-cta-redirect-eu1-prod.s3.amazonaws.com/cta/redirect/145054770/db29918a-4bc4-46fa-ae3d-5d61199e83f7)\nSection Title: ... > Implementation How-To\nContent:\nDon\u2019t be intimidated by this being called an \u201cadvanced\u201d approach. Here\u2019s how to implement these calculations into your regular workflow:\n**Filter queries** running on warehouses (excluding cloud service-only queries)\n**Calculate execution start time** by adding queue and compilation time to start_time\n**Generate hourly time slots** for the analysis period\n**Join queries to hours** they were executed in\n**Calculate proportional time** for each query used within every hour\n**Allocate actual credits** from warehouse_metering_history proportionally\nSection Title: ... > Snowflake\u2019s Native Query Attribution\nContent:\nSnowflake actually does have a query attribution history for you to look at native cost attribution. It was released in July 2024. But it has a few limitations:\nData only available from July 1, 2024 onwards\nShort queries (under 100ms) are excluded\nIdle time isn\u2019t included in attribution costs\nData appears with up to 6-hour delays\nSome users report inflated costs for short queries\nSnowflake\u2019s native query attribution is promising, but it\u2019s not there yet. Many organizations still turn to a custom logic for more accurate readings.\nSection Title: ... > How to Identify Your Most Expensive Queries\nContent:\nNow that you know how to find your Snowflake cost per query, here\u2019s how to quickly spot your most expensive queries:\n```\nSELECT\n\u00a0 \u00a0 md5(cleaned_query_text) as query_signature,\n\u00a0 \u00a0 sum(query_cost) as total_cost_last_30d,\n\u00a0 \u00a0 total_cost_last_30d * 12 as estimated_annual_cost,\n\u00a0 \u00a0 max_by(query_text, start_time) as latest_query_text,\n\u00a0 \u00a0 avg(execution_time/1000) as avg_execution_time_s,\n\u00a0 \u00a0 count(*) as num_executions\nFROM query_history_enriched\nWHERE start_time >= dateadd('day', -30, current_date())\nGROUP BY 1\nORDER BY total_cost_last_30d DESC\nLIMIT 100\n```\nUse this to find those queries driving thousands of costs annually \u2013 and monthly. Those are prime candidates for optimization.\nSection Title: ... > Streamline Cost Optimization With Purpose-Built Tools\nContent:\nYou know how to build your DIY solution for figuring out costs per query, but this method \u2013 and other cost measurement solutions \u2013 are often unsustainable long-term. You\u2019re facing an uphill battle against:\nConstant updates for new Snowflake releases\nHandling the complexity of multi-cluster and auto-scaling warehouses\nLong-term cost analyses require robust data retention strategies\nTechnical solutions may not be best for business stakeholders\nYou don\u2019t have to spend entire engineering cycles building and maintaining cost attribution systems. Many organizations are turning to Snowflake cost optimization platforms like Yuki for help. These solutions provide:\nSection Title: ... > Streamline Cost Optimization With Purpose-Built Tools\nContent:\nAutomated cost attribution with real-time accuracy\nQuery optimization recommendations based on execution patterns\nWarehouse right-sizing and automated scaling\nBudget alerts and governance to prevent cost overruns\nEasy to use and understand executive dashboards perfect for stakeholders\nPlug-and-play tools like Yuki mean calculating \u2013 and optimizing \u2013 your queries with ease, no additional dev lift needed.\nReady to gain complete visibility into your Snowflake costs? Yuki\u2019s intelligent cost optimization platform provides automated query-level attribution, optimization recommendations, and governance controls \u2013 all without engineering overhead.\nContact us to get your [free demo today.](https://yukidata.com/request-demo/)\nBy Ido Arieli Noga\nSection Title: ... > Streamline Cost Optimization With Purpose-Built Tools\nContent:\nIdo Arieli Noga is the CEO and Co-Founder of Yuki, where he helps businesses cut Snowflake spend through smart warehouse scaling and DevOps-driven optimization. He brings over 12 years of experience across data storage, BI, and FinOps, including nearly four years as Head of Data at Lightico and five years managing large-scale virtual environments in the government sector. Ido holds a degree in Computer Science and is passionate about building scalable, cost-efficient data infrastructures. Since founding Yuki in 2023, he\u2019s focused on helping teams reduce costs without changing queries or code. Find more of his insights on Medium or LinkedIn.\nSection Title: Snowflake Cost Per Query: Complete Guide to Query-Level Cost Attribution > ... > Free cost analysis\nContent:\nTake 5 minutes to learn how much money you can save on your Snowflake account.\nBy clicking Submit you\u2019re confirming that you agree with our Terms and Conditions.\n[### Follow us on LinkedIn](https://www.linkedin.com/company/yukidata)\nSection Title: Snowflake Cost Per Query: Complete Guide to Query-Level Cost Attribution > ... > Related posts\nContent:\n[](https://yukidata.com/dbt-cloud-snowflake-oauth/)\nSection Title: ... > [How to Set Up Snowflake OAuth with dbt](https://yukidata.com/dbt-cloud-snowflake-oauth/)\nContent:\n[Learn More >](https://yukidata.com/dbt-cloud-snowflake-oauth/)\nFebruary 7, 2026\n[](https://yukidata.com/nowflake-cybersecurity-guide/)\nSection Title: ... > [Snowflake Cybersecurity: What Changed After 2024 & How to Protect Your Data](https://yukid...\nContent:\n[Learn More >](https://yukidata.com/nowflake-cybersecurity-guide/)\nFebruary 5, 2026\n[](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\nSection Title: ... > [How to Fix Snowflake Error 251005: \u201cUser is Empty\u201d (And Why It Keeps Happening)](https://y...\nContent:\n[Learn More >](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\nFebruary 2, 2026\nSection Title: Snowflake Cost Per Query: Complete Guide to Query-Level Cost Attribution > Related posts\nContent:\n[](https://yukidata.com/dbt-cloud-snowflake-oauth/)\nSection Title: ... > [How to Set Up Snowflake OAuth with dbt](https://yukidata.com/dbt-cloud-snowflake-oauth/)\nContent:\n[Learn More >](https://yukidata.com/dbt-cloud-snowflake-oauth/)\nFebruary 7, 2026\n[](https://yukidata.com/nowflake-cybersecurity-guide/)\nSection Title: ... > [Snowflake Cybersecurity: What Changed After 2024 & How to Protect Your Data](https://yukid...\nContent:\n[Learn More >](https://yukidata.com/nowflake-cybersecurity-guide/)\nFebruary 5, 2026\n[](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\nSection Title: ... > [How to Fix Snowflake Error 251005: \u201cUser is Empty\u201d (And Why It Keeps Happening)](https://y...\nContent:\n[Learn More >](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\nFebruary 2, 2026\n[Back to Blog](https://yukidata.com/blog/)\nSection Title: Snowflake Cost Per Query: Complete Guide to Query-Level Cost Attribution > ... > Free cost analysis\nContent:\nTake 5 minutes to learn how much money you can save on your Snowflake account.\nBy clicking Submit you\u2019re confirming that you agree with our Terms and Conditions.\nSkip to content"
      ]
    },
    {
      "url": "https://blog.dataengineerthings.org/snowflake-compute-cost-per-query-c84105b7e897",
      "title": "Snowflake Compute Cost per Query. Unveiling Snowflake\u2019s\u2026 | by Nikhil Suthar | Data Engineer Things",
      "publish_date": "2025-02-17",
      "excerpts": [
        "Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n[](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n[## Data Engineer Things](https://blog.dataengineerthings.org/?source=post_page---publication_nav-f2ba5b8f6eb3-c84105b7e897---------------------------------------)\n\u00b7\nFollow publication\n[](https://blog.dataengineerthings.org/?source=post_page---post_publication_sidebar-f2ba5b8f6eb3-c84105b7e897---------------------------------------)\nThings learned in our data engineering journey and ideas on data and engineering.\nFollow publication\nMember-only story\nSection Title: Snowflake Compute Cost per Query > Unveiling Snowflake\u2019s **QUERY_ATTRIBUTION_HISTORY** View.\nContent:\n[](https://medium.com/@nikhil-suthar?source=post_page---byline--c84105b7e897---------------------------------------)\n[Nikhil Suthar](https://medium.com/@nikhil-suthar?source=post_page---byline--c84105b7e897---------------------------------------)\n5 min read\n\u00b7\nSep 7, 2024\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-engineer-things%2Fc84105b7e897&operation=register&redirect=https%3A%2F%2Fblog.dataengineerthings.org%2Fsnowflake-compute-cost-per-query-c84105b7e897&user=Nikhil+Suthar&userId=e85a2b7008cc&source=---header_actions--c84105b7e897---------------------clap_footer------------------)\n--\nSection Title: Snowflake Compute Cost per Query > Unveiling Snowflake\u2019s **QUERY_ATTRIBUTION_HISTORY** View.\nContent:\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc84105b7e897&operation=register&redirect=https%3A%2F%2Fblog.dataengineerthings.org%2Fsnowflake-compute-cost-per-query-c84105b7e897&source=---header_actions--c84105b7e897---------------------bookmark_footer------------------)\nShare\n**O** ne of the most anticipated updates from Snowflake has arrived \u2014 the `**_QUERY_ATTRIBUTION_HISTORY_**` feature. As a data professional, I often encountered the challenge of lacking granular insights into credit consumption per query in Snowflake. While Snowflake has always excelled in providing high-level details on credit usage, there was a gap in understanding which specific queries were driving costs \u2014 especially when multiple queries were running in the same Snowflake warehouse. With the release of `_QUERY_ATTRIBUTION_HISTORY_` , this gap has been filled.\nSection Title: Snowflake Compute Cost per Query > Unveiling Snowflake\u2019s **QUERY_ATTRIBUTION_HISTORY** View.\nContent:\nPress enter or click to view image in full size\nSnowflake Query Attribution History View by Nikhil Suthar\nIn this post, I\u2019ll explore how this new view works, why it\u2019s crucial, and the significant benefits it brings to users and organizations alike.\nSection Title: Snowflake Compute Cost per Query > Before the Release\nContent:\nBefore the introduction of `_QUERY_ATTRIBUTION_HISTORY_` , compute credit costs were only aggregated at the warehouse level, presenting several challenges:\n**Inability to segregate costs** for multiple queries running simultaneously in the same warehouse.\n**Lack of cost differentiation** based on query tags within the same warehouse.\n**Difficulty in allocating costs** for different use cases or teams using the same Snowflake warehouse.\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-engineer-things%2Fc84105b7e897&operation=register&redirect=https%3A%2F%2Fblog.dataengineerthings.org%2Fsnowflake-compute-cost-per-query-c84105b7e897&user=Nikhil+Suthar&userId=e85a2b7008cc&source=---footer_actions--c84105b7e897---------------------clap_footer------------------)\nSection Title: Snowflake Compute Cost per Query > Before the Release\nContent:\n-- [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdata-engineer-things%2Fc84105b7e897&operation=register&redirect=https%3A%2F%2Fblog.dataengineerthings.org%2Fsnowflake-compute-cost-per-query-c84105b7e897&user=Nikhil+Suthar&userId=e85a2b7008cc&source=---footer_actions--c84105b7e897---------------------clap_footer------------------)\n--\n[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc84105b7e897&operation=register&redirect=https%3A%2F%2Fblog.dataengineerthings.org%2Fsnowflake-compute-cost-per-query-c84105b7e897&source=---footer_actions--c84105b7e897---------------------bookmark_footer------------------)\n[](https://blog.dataengineerthings.org/?source=post_page---post_publication_info--c84105b7e897---------------------------------------)\nSection Title: Snowflake Compute Cost per Query > Before the Release\nContent:\n[](https://blog.dataengineerthings.org/?source=post_page---post_publication_info--c84105b7e897---------------------------------------)\nFollow\n[## Published in Data Engineer Things](https://blog.dataengineerthings.org/?source=post_page---post_publication_info--c84105b7e897---------------------------------------)\n35K followers\n\u00b7 Last published 3 hours ago\nThings learned in our data engineering journey and ideas on data and engineering.\nFollow\n[](https://medium.com/@nikhil-suthar?source=post_page---post_author_info--c84105b7e897---------------------------------------)\n[](https://medium.com/@nikhil-suthar?source=post_page---post_author_info--c84105b7e897---------------------------------------)\n[## Written by Nikhil Suthar](https://medium.com/@nikhil-suthar?source=post_page---post_author_info--c84105b7e897---------------------------------------)\nSection Title: Snowflake Compute Cost per Query > Before the Release\nContent:\n[126 followers](https://medium.com/@nikhil-suthar/followers?source=post_page---post_author_info--c84105b7e897---------------------------------------)\n\u00b7 [40 following](https://medium.com/@nikhil-suthar/following?source=post_page---post_author_info--c84105b7e897---------------------------------------)\nBuilder of Intelligent Data Ecosystems\nSection Title: Snowflake Compute Cost per Query > No responses yet\nContent:\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--c84105b7e897---------------------------------------)\n[Help](https://help.medium.com/hc/en-us?source=post_page-----c84105b7e897---------------------------------------)\n[Status](https://status.medium.com/?source=post_page-----c84105b7e897---------------------------------------)\n[About](https://medium.com/about?autoplay=1&source=post_page-----c84105b7e897---------------------------------------)\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----c84105b7e897---------------------------------------)\nPress\n[Blog](https://blog.medium.com/?source=post_page-----c84105b7e897---------------------------------------)\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c84105b7e897---------------------------------------)\nSection Title: Snowflake Compute Cost per Query > No responses yet\nContent:\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----c84105b7e897---------------------------------------)\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c84105b7e897---------------------------------------)\n[Text to speech](https://speechify.com/medium?source=post_page-----c84105b7e897---------------------------------------)"
      ]
    },
    {
      "url": "https://www.reddit.com/r/snowflake/comments/1j9tsmb/how_to_join_attribution_history_with_query_history/",
      "title": "How to join attribution history with query history - snowflake",
      "publish_date": "2025-04-03",
      "excerpts": [
        "So came across another view query_attribution_history which gives the compute for each query readily available and it is snowflake populated ..."
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}

{"extract_id":"extract_4fa44d7d79ed4072af2dff907a21465e","results":[{"url":"https://www.snowflake.com/en/blog/10-best-practices-every-snowflake-admin-can-do-to-optimize-resources/","title":"10 Best Practices for Optimizing Resources %%sep%% %%sitename%% Blog","publish_date":"2024-08-12","excerpts":["blog\n\n##### Category\n\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\n\nProduct and Technology\n\nOct 30, 2020 | 10 min read\n\n# 10 Best Practices Every Snowflake Admin Can Do to Optimize Resources\n\nAs we covered in part 1 of this blog series, Snowflake’s platform is architecturally different from almost every traditional database system and cloud data warehouse . Snowflake has completely separate compute and storage, and both tiers of the platform are near instantly elastic. The need to do advanced resource planning, agonize over workload schedules, and prevent new workloads on the system due to the fear of disk and CPU limitations just go away with Snowflake. As a cloud data platform, Snowflake can near instantly scale to meet planned, ad hoc, or surprise growth. This means instead of paying for a fixed, limited amount of storage and compute, the amount of storage and compute grows and shrinks as your needs change over time. By taking advantage of a core tenet of the cloud, elasticity and compute can be dynamically scaled to workloads throughout the day as concurrency needs or raw compute power fluctuate to meet demand. Storage will grow and shrink over time for databases, tables, and meta-data. There are a few optimizations every Snowflake account administrator should make and some more-advanced methods they should consider as their Snowflake compute footprint grows. Because compute and storage are separated and they are elastic, these resources should be monitored for consumption, surprise growth, and resource efficiency. Snowflake is virtually unlimited by default, and account administrators can put in place minor account-level and resource-level restrictions to defend against rogue users or suboptimal use of resources and credits. For example, they can proactively control compute at the individual virtual warehouse level, at the user level, or at the account and organization level through resource monitors . Users, databases, tables, queries, and workloads can be monitored through the ACCOUNT\\_USAGE schema shared with all Snowflake accounts. The following figure shows the status and basic configuration of virtual warehouses in an account:\n\nGiven all this, here are 10 best practices Snowflake account administrators should be doing.\n\n#### Best Practice #1: Enable Auto-Suspend\n\nMake sure all virtual warehouses are set to auto-suspend. This way, when they are done processing queries, auto-suspend will turn off your virtual warehouses when they are done processing queries, and thus stop credit consumption. Run the following query to identify all the virtual warehouses that do not have auto-suspend enabled:\n\n#### Best Practice #2: Enable Auto-Resume\n\nMake sure all virtual warehouses are set to auto-resume. If you are going to implement auto-suspend and set appropriate timeout limits, enabling auto-resume is a must; otherwise, users will not be able to query the system. Run the following to identify all the virtual warehouses that will not auto-resume when they are queried:\n\n#### Best Practice #3: Set Timeouts Appropriately for Workloads\n\nAll virtual warehouses should have an appropriate timeout for their particular workload: * For task, data loading, and ETL/ELT warehouses, set the timeout for suspension immediately upon completion.\n* For BI and SELECT query warehouses, set the suspension timeout to 10 minutes in most situations to keep data caches warm for frequent access by end users.\n* For DevOps, DataOps, and data science warehouses, set the suspension timeout to 5 minutes because having a warm cache is not as important for ad hoc and highly unique queries.\n **Here’s an example configuration:**\n\n#### Best Practice #4: Set Account Statement Timeouts\n\nUse the STATEMENT\\_QUEUED\\_TIMEOUT\\_IN\\_SECONDS and STATEMENT\\_TIMEOUT\\_IN\\_SECONDS parameters to automatically stop queries that are taking too long to execute, either due to a user error or a frozen cluster. Customize warehouse, account, session, and user timeout-level statements according to your data strategy for long-running queries. **Here’s an example:**\n\n```\nALTER  WAREHOUSE LOAD_WH  SET  STATEMENT_TIMEOUT_IN_SECONDS = 3600 ; rnSHOW PARAMETERS  IN  WAREHOUSE LOAD_WH ;\n```\n\nCopy\n\n#### Best Practice #5: Identify Warehouses Deviating from the Seven-Day Average\n\nHere’s a handy tip that came from a direct interaction I had with a customer who set a warehouse to a larger size to do a task but did not put it back the way he found it. I made the following query for him to run every morning to identify warehouse credit usage that deviates from the seven-day average. The following figure shows the results of running the query.\n\n#### Best Practice #6: Monitor Warehouses That Are Approaching the Cloud Service Billing Threshold\n\nThe following query looks at warehouses where cloud services costs comprise a high percentage of the workload. Overall for an account (and outside of serverless features), Snowflake will charge for cloud services only if they exceed 10% of the daily virtual warehouse credit consumption. Cloud services tasks are useful for meta-data operations such as BI tool discovery queries, heartbeat queries, SHOW commands, cache usage, and several other service optimizing features. So if you use 100 compute credits in a day, but you use 15 additional credits for cloud services (unlikely), you will be charged an additional 5 credits for that day for the 5 cloud service credits that were over the 10% allowance. This means 105 credits total would be billed for the day with Snowflake providing 10 free credits of cloud services usage. This query helps you figure out which warehouses are nearing or exceed that 10% threshold so that you can investigate.\n\n#### Best Practice #7: Drop Unused Tables\n\nYou might have unused tables that are candidates to be dropped. Just make sure no one is querying these tables. In fact, you might want to make it mandatory to check all tables before deletion. (If you have Time Travel set up, you can undrop a table if you make an error.) This is specific to the database context, so be sure to look at tables across your databases. Also, be mindful of tables used only in view DDLs. Here’s an example query:\n\n#### Best Practice #8: Purge Dormant Users\n\nIt’s a good idea to purge from your account dormant users or users that never logged in to Snowflake. Here’s an example that generates a list of both types of users:\n\n#### Guardrails for Automatic Scaling\n\nReal-life usage of a data platform varies significantly from hour to hour, day to day, and month to month. By default, Snowflake is designed to scale automatically and provide maximum performance and efficiency. However, some workloads are better served by static and highly predictable resources, and Snowflake can easily be configured to provide that consistent consumption model for every day of the year. By implementing a few account-level and resource-level restrictions, account admins can prevent unexpected usage by careless users or suboptimal scaling profiles: * Through resource monitors , admins can receive proactive alerts and control compute at the account level, virtual warehouse level, and even the user level.\n* On a reactive basis, admins can monitor users, databases, tables, queries, and workloads through the ACCOUNT\\_USAGE schema shared with all Snowflake accounts. This data is commonly used to forecast usage trends and provide showback and chargeback billing for departments, teams, and workloads. Daily usage metrics are built into the platform for both individual users, account administrators, and organization administrators. This figure shows the built-in dashboard providing an hourly breakdown of credits for both compute and cloud services:\n\n#### Best Practice #9: Find Warehouses That Don’t Have Resource Monitors\n\nResource monitors are a great way to proactively control workload budgets and prevent unexpected resource spikes. Resource monitors can help monitor both user usage and service account usage in Snowflake. First, you should have dedicated virtual warehouses for each of your loading, ELT, BI, reporting, and data science workloads as well as for other workloads. Accounts and warehouses can have total, yearly, monthly, weekly, and daily credit quotas. The following query will identify all warehouses that do not have a resource monitor:\n\n#### Best Practice #10: Apply Resource Monitors\n\nYou can use the UI or SQL to apply your resource monitor policy. Based on account preference settings, resource monitors can notify you when consumption reaches a lower threshold, and then suspend the warehouse or account at a higher threshold.\n\n#### Considerations for Resource Monitoring\n\n* We recommend setting monitors to notify you when a certain threshold of consumption is reached.\n* When consumption approaches the maximum budgeted level, set the resource monitor to auto-suspend the warehouse or the entire account, allowing queries to complete but preventing future requests.\n* Resource monitors can also be used to terminate all currently running queries and immediately suspend the resource or account. This setting is usually reserved for situations where a hard quota is exceeded.\n* For customers that do not want to set hard limits, it’s still always a good idea to have notification monitors set on all warehouses in case usage unexpectedly spikes. That way, all admins within the account will get an email or on-screen notification when thresholds are reached.\n The following figure shows the resource monitor configuration screen:\n\n**The SQL below can be used to programmatically create resource monitors:**\n\n#### Bonus Best Practice: Use BI Partner Dashboards\n\nAs a bonus eleventh best practice, use the dashboards created by the Snowflake enthusiasts at some of our BI and analytics partners to help you monitor Snowflake usage. Because Snowflake’s usage is shared back to every account with a standard schema, these are plug-and-play dashboards. One of the best parts of Snowflake is that it uses data sharing at petabyte scale to share pipelines of usage history back to all Snowflake accounts. Please see the documentation for more details. Tableau has put together a variety of dashboards showing credit usage, performance, and user adoption for the platform. The Compute Cost Overview dashboard, shown below, can be used to understand credit burn, plan budget allocation, and identify peak outliers to reduce the impact of “buy times” spend. Tableau’s Enterprise Analytics team uses these dashboards to uncover emerging usage patterns and optimize warehouse cost efficiency.\n\nThe team at Sigma put together a series of dashboards looking at compute costs, user uptick, and performance. Sigma Analytics offers easy-to-use templates to create presentation-ready dashboards displaying your Snowflake account usage data. These dashboards can be used to quickly dive into understanding and sharing your organization’s Snowflake usage. The figure below shows the pre-built usage dashboard available in Sigma:\n\nFollow the links below to see the various usage dashboards pre-built by our BI partners: * [Tableau](https://www.tableau.com/about/blog/2019/5/monitor-understand-snowflake-account-usage)\n* [Qlik/Attunity](https://community.qlik.com/t5/Technology-Partners-Ecosystem-Documents/Qlik-amp-Snowflake-Usage-Dashboard/ta-p/1646629)\n* [Sigma](https://www.sigmacomputing.com/blog/set-up-snowflake-monitoring-quickly-with-sigma-templates/)\n* [Looker](https://looker.com/platform/blocks/source/cost-and-usage-analysis-by-snowflake)\n* [Microsoft Power BI](https://medium.com/analytics-vidhya/snowflake-power-bi-snowflake-usage-report-f628dadbdc85)\n* [ThoughtSpot](https://docs.thoughtspot.com/cloud/latest/spotapps-snowflake)\n\n#### Conclusion\n\nWith Snowflake’s highly elastic compute and per-second billing model, account administrators should monitor usage, growth, and resource efficiency on an ongoing basis to make sure they match performance requirements and budgets. Even though Snowflake can help to optimize resources automatically, there are opportunities for account administrators to further tune their deployment, especially as their compute footprint grows. We recommend these basic best practices for monitoring and optimizing resources to avoid common pitfalls that are easy to overlook.\n\n###### Authors\n\nDavid A. Spezia\n\nMike Klaczynski\n\n###### Share Article\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2F10-best-practices-every-snowflake-admin-can-do-to-optimize-resources&title=10+Best+Practices+Every+Snowflake+Admin+Can+Do+to+Optimize+Resources)\n\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2F10-best-practices-every-snowflake-admin-can-do-to-optimize-resources&text=10+Best+Practices+Every+Snowflake+Admin+Can+Do+to+Optimize+Resources)\n\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2F10-best-practices-every-snowflake-admin-can-do-to-optimize-resources)\n\nSubscribe to our blog newsletter\n\nGet the best, coolest and latest delivered to your inbox each week\n\n\\*\n\nSubscribe Now\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\nstart for free\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\n\\*\n\n\\*\n\n\\* Country United States Canada United Kingdom Germany France Australia Japan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo The Democratic Republic of The Cook Islands Costa Rica Cote D'Ivoire (Ivory Coast) Croatia (Hrvatska) Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Heard and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jordan Kazakhstan Kenya Kiribati Korea Republic of (South) Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia Federated States of Moldova Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Occupied Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Reunion Romania Russian Federation Saint Helena Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and The South Sandwich Island Spain Sri Lanka Suriname Svalbard and Jan Mayen Islands Swaziland Sweden Switzerland Taiwan Tajikistan Tanzania United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Venezuela Viet Nam Virgin Islands (British) Virgin Islands (U.S.) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe\n\n\\*\n\nAdd me to the list to receive dedicated product updates and general availability emails.\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\n\nSubscribe Now\n\nIndustries * Advertising, Media & Entertainment\n* Financial Services\n* Healthcare & Life Sciences\n* Manufacturing\n* Public Sector\n* Retail & Consumer Goods\n* Technology\n\nLearn * Resource Library\n* Live Demos\n* Fundamentals\n* Training\n* Certifications\n* Snowflake University\n* Developer Guides\n* Documentation\n\n* Privacy Policy\n* Site Terms\n* Communication Preferences\n* Cookie Settings\n* Do Not Share My Personal Information\n* Legal\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"],"full_content":null},{"url":"https://www.snowflake.com/en/blog/managing-snowflakes-compute-resources/","title":"Managing Snowflake’s Compute Resources","publish_date":"2024-08-12","excerpts":["blog\n\n##### Category\n\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\n\nData Engineering\n\nDEC 16, 2020 | 10 min read\n\n# Managing Snowflake’s Compute Resources\n\n_This is the 3rd blog in our series on Snowflake Resource Optimization._ In parts 1 and 2 of this blog series, we showed you how Snowflake’s unique architecture allows for a virtually unlimited number of compute resources to be accessed near-instantaneously. We also provided best practices for administering these compute resources to optimize performance and reduce credit consumption. In this blog post, I’ll show you the best practices our internal Snowflake team uses to manage our own usage of Snowflake. My name is Tamir Rozenberg and I support one of the world's largest Snowflake deployments - it just so happens to be the one we have here internally at Snowflake. Prior to Snowflake, I managed Instacart's Snowflake platform. In the 4 years I’ve been overseeing Snowflake resources, I’ve developed several strategies that have helped me implement highly efficient and performant deployments. I'll share those strategies with you in this post. When we talk about the multi-cluster compute layer of Snowflake, we’re referring to the virtual warehouses that execute queries on the centralized storage layer. Below is the architectural diagram that shows the 3 components of the Snowflake platform.\n\n## Snowflake Virtual Warehouses\n\nA [virtual warehouse](https://docs.snowflake.com/en/user-guide/warehouses.html) , often referred to simply as a “warehouse,” is a cluster of compute resources in Snowflake. A warehouse provides the required resources, such as CPU, memory, and temporary storage, to perform SELECT, UPDATE, DELETE, and INSERT commands.\n\n## Snowflake’s Management Flexibility\n\nSnowflake provides powerful features and control capabilities that can help you determine the best approach for warehouse management based on your organization’s use cases. Each organization can enforce its own strategy, and the strategy can be completely different for each organization. The information below provides a few recommendations for how to set up your warehouses based on your use cases.\n\n## Understanding Workload Management\n\nSnowflake's unique architecture makes it possible to run multiple workloads concurrently, without performance impact, by using more than one concurrently running warehouse. Each workload, for example, an ETL workload and an analytics workload, can have its own isolated resources even while each is operating on the same databases and tables. That means the ETL workload does not impact the performance of the analytics workload and vice versa. To get most of the value out of Snowflake virtual warehouses, it is important to use the correct warehouse for the work you intend to do, and it’s also important to not mix multiple workloads in a single warehouse. For example, a heavy batch operation workload should not use the warehouses designated for ad hoc queries and analytics.\n\n## Analyzing Your Workloads\n\nIt is good practice to classify your workloads before you choose a warehouse size and configuration. **Here are a few common types of workloads:** * **Ad-hoc Analytics:** Self-service Analytics or ad-hoc heavy queries\n* **Data Loading:** A COPY command that constantly loads data from an external data source\n* **Data transformations:** A series of commands to transform your raw data into a more consumable format\n* **Reporting:** Dashboards and other reports refreshed on a schedule, or on-demand by executives\n* **Applications:** End-user applications that are displaying data based on query results\n* **Batch:** A massive batch transformation that runs frequently\n I used the following table to help me map various workloads to an appropriate [warehouse size](https://docs.snowflake.com/en/user-guide/warehouses-overview.html) (XL, L, M, or S) based on the following criteria: * **Frequency** : How often the workload runs\n* **Concurrency** : How many processes will run simultaneously\n* **Scan Size** : What's the average size of data scan the query will perform\n* **Copy Files:** How many files will be copied simultaneously\n* **SLA Minutes** : The number of minutes stipulated by the SLA\n **Here is an example of how I would assign various workloads to warehouses based on these categories.**\n\n|Workload Description |Workload Type |Frequency |Concurrency |Scan Size |Copy Files |SLA Minutes |Warehouse Name |Warehouse Size |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|Data Transformation |Batch |Hourly |1 |1 TB | |10 |**transform\\_wh** |XL |\n|Third-Party Data Ingestion |Loader |Hourly |2 | |2 |5 |**load\\_wh** |XS (see this [tip](https://docs.snowflake.com/en/user-guide/warehouses-overview.html) ) |\n|Tableau |Ad-Hoc Analytics |Ad hoc |8 |2 TB | |1 |**analytics\\_wh** |XL (see this [tip](https://docs.snowflake.com/en/user-guide/warehouses-overview.html) ) |\n|Alerts, Monitoring... |Batch |Hourly |2 |500 GB | |1 |app\\_wh |M |\n|Console |Applications |On demand |16 |1 TB | |0\\.1 |application\\_wh |XL |\n|Snowflake UI |Ad-Hoc Analytics |Ad hoc |8 |3 TB | |1 |**analytics\\_wh** |XL |\n|Amazon S3 Loader |Loader |Hourly |5 | |5 |5 |**load\\_wh** |XS |\n|Database, Machine Learning, or Python Connector |Batch |Hourly |2 |5 TB | |5 |**transform\\_wh** |XL |\n\nFrom the table above, we can consolidate multiple workloads under one warehouse. When using this strategy, try to categorize your workloads to warehouses and combine all identical workloads to the same warehouse until you maximize warehouse utilization. The “Warehouse Utilization” section below provides more information.\n\n## Centralize Requests for Resources\n\nAs a best practice, I recommend delegating the responsibility of scaling warehouses up and down to just a few members of your team. Create a dedicated role with permissions to operate on all warehouses and grant that role to a limited number of engineers. This will allow you to control your policy and strategy for warehouse utilization.\n\n## Getting Started\n\nBelow are some specific things my team and I do when we setup new warehouses. These are useful if you are migrating from another data platform and need to configure your Snowflake resources but also for adding new workloads. Remember to focus on the business problem you’re trying to tackle, identify the workloads, then decide how to implement that into action with Snowflake resources.\n\n### Configuring a Warehouse\n\nConfiguring a warehouse is done by the warehouse administrators. * [**Size**](https://docs.snowflake.com/en/user-guide/warehouses-overview.html) **:** If you are not sure what is the best size for the warehouse, start with a smaller size and slowly increase it based on the workload’s performance and its SLA.\n* [**Maximum clusters**](https://docs.snowflake.com/en/user-guide/warehouses-multicluster.html) **:** Similar to size, start with a small number and increase it based on workload activities (available only in enterprise edition or above).\n* [**Scaling policy**](https://docs.snowflake.com/en/user-guide/warehouses-multicluster.html) **:** Usually this is set to Standard but if queuing is not an issue for the workload, set it to Economy, which will conserve credits by trying to keep running clusters fully loaded (available only in enterprise edition or above).\n* [**Auto-suspend**](https://docs.snowflake.com/en/user-guide/warehouses-overview.html) **:** This specifies the number of seconds of inactivity after which a warehouse is automatically suspended. Note that the default value is 600 seconds, meaning the warehouse suspends automatically after 10 minutes of inactivity. If your workload runs infrequently, to reduce your cost, set the value to 60 seconds (one minute), as shown in the following example. Setting the value to NULL is not recommended unless your query workloads require a continually running warehouse. Setting the value to NULL can result in a significant consumption of credits (and corresponding charges), particularly for larger warehouses.\n\n```\nAlter warehouse warehouse_name set AUTO_SUSPEND=60\n```\n\n[**Auto-resume**](https://docs.snowflake.com/en/user-guide/warehouses-overview.html) **:** Rarely disable this option; if you do, the warehouse will resume only when you explicitly run the command alter warehouse warehouse\\_name resume. If the option is enabled, the warehouse will resume automatically when a new query is submitted.\n\n### Setting the Warehouses’ Timeout\n\nFor each warehouse, set the applicable timeout based on the workload that is expected. For example, set the parameter STATEMENT\\_TIMEOUT\\_IN\\_SECONDS to 1,000 seconds if the expected time for the warehouse to complete the workload is less than 1,000 seconds. Depending on your use case consider also setting up the parameter STATEMENT\\_QUEUED\\_TIMEOUT\\_IN\\_SECONDS for warehouses that process ad-hoc queries workload. Setting the correct timeout will prevent mixing different workloads into one warehouse. The following screenshot shows the output of warehouse parameters.\n\n### Warehouse Naming Conventions\n\nTry to provide a relatively short but descriptive name for each warehouse. For example loader\\_wh or tableau\\_wh are great examples of good warehouse names that easily can be mapped to a workload. A poor approach would be using the size of the warehouse as the name, for example, XSMALL **\\_** wh. Such a name can be very confusing because the size of the warehouse is configurable and can be changed over time.\n\n### Setting Permissions on a Warehouse\n\nSetting the right permissions on warehouses can help with centralized management and can provide better control and governance. Using permissions correctly can reduce the administration tasks. Snowflake provides a set of permissions that can be assigned to roles to simplify warehouse management in a multi-warehouse environment: * USAGE: Grants the role the ability to use the warehouse and to run commands using the warehouse.\n* MONITOR: Grants the role the ability to see details within an object (for example, queries and usage within a warehouse). It is best practice to provide this ability to the role so the workload using the warehouse can monitor the activities in the warehouse and the role will be able to see the warehouse load over time using the Snowflake UI.\n* OPERATE: Grants the role the ability to start, stop, suspend, or resume a virtual warehouse. This is a sensitive permission. It is highly recommended to grant this permission to **ONLY** the warehouse administration role.\n\n### Setting a Default Warehouse for users\n\nTo provide better control and governance of warehouse usage, a best practice is to define a default warehouse for each user. This can prevent users from using the wrong warehouse for their workloads. Here is an example:\n\n```\ncreate user looker password='abc123' default_role = myrole must_change_password = true default_warehouse = analytics_wh;\n```\n\nFor more information, see these [tips](https://docs.snowflake.com/en/user-guide/warehouses-overview.html) .\n\n## Monitoring Best Practices\n\n* Now that you have virtual warehouse resources running on Snowflake, let’s explore the various ways to monitor and manage them.\n* We provided 11 tips for optimizing your resources in our 2nd blog post .\n* Let me show you three more ways to manage your resource consumption.\n\n### Warehouse Utilization\n\nQueuing queries in a warehouse is the first indication that a warehouse is overutilized. In some cases, this can be due to a design decision and it is OK to have queued processes. However, for workloads with tight SLAs, for example, console queries or BI tools, it’s probably not desirable to have a long queue. You can monitor queue size by using the ACCOUNT\\_USAGE schema’s [warehouse\\_load\\_history](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_load_history.html) view, or you can look at real-time queue size by running a command such as the following, which will show all the current warehouse parameters and the queue size:\n\n```\nSHOW WAREHOUSES LIKE '%TEST'\n```\n\nSnowflake also provides detailed graphs showing warehouse load over time. [Our documentation has more details on how to use these charts](https://docs.snowflake.com/en/user-guide/warehouses-load-monitoring.html) .\n\n### Identifying Costly Queries\n\nIf you followed the best practices above and you still want to find the query on a single warehouse that consumes most of your credits, you can query account\\_usage to get more insight about costly queries or queries that require tuning. Below you can find a few examples for queries you can use: **Here’s an example that breaks down by day and username of long-running queries that run frequently:**\n\n```\nSELECT Date_trunc('day', start_time),rn      user_name,rn      Substr(query_text, 1, 100) AS query,rn      Count(*),rn      Sum(total_elapsed_time),rn      Max(query_id)rnFROM   snowflake.account_usage.query_historyrnWHERE  warehouse_name = 'WAREHOUSE_NAME'rn      AND Date_trunc('day', start_time) >= CURRENT_DATE() - 7rnGROUP  BY Date_trunc('day', start_time),rn          user_name,rn          Substr(query_text, 1, 100)rnORDER  BY Date_trunc('day', start_time) DESC,rn          Sum(total_elapsed_time) DESC\n```\n\n**Here’s an example to identify a query that consumed lots of credits:**\n\n```\nSELECT Query_ID,USER_NAME,warehouse_name,warehouse_type,start_time,end_timern    ,ROUND(total_elapsed_sec,2)   total_elapsed_secrn,ROUND(Credit_Unit *total_elapsed_sec / 60/60, 2)  total_creditrnFROM (rnselect Query_ID,USER_NAME,warehouse_name ,warehouse_typern    ,start_time,end_timern    ,total_elapsed_time/1000   total_elapsed_secrn    ,CASE WHEN warehouse_size = 'X-Small'    THEN 1rn            WHEN warehouse_size = 'Small'      THEN 2rn            WHEN warehouse_size = 'Medium'     THEN 4rn            WHEN warehouse_size = 'Large'      THEN 8rn            WHEN warehouse_size = 'X-Large'    THEN 16rn            WHEN warehouse_size = '2X-Large'   THEN 32rn            WHEN warehouse_size = '3X-Large'   THEN 64rn            WHEN warehouse_size = '4X-Large'   THEN 128rn      ELSE 1  rn      END Credit_Unitrn  from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORYrn  where ( error_message IS NULLrn        or error_code = 604 )rn        and warehouse_size IS NOT NULL rn        and START_TIME >= DATEADD(DAY, -30, CURRENT_TIMESTAMP()rn        ) rn                                ) rnORDER BY total_credit desc\n```\n\n### Using the QUERY\\_TAG Session Parameter\n\n[QUERY\\_TAG](https://docs.snowflake.com/en/sql-reference/parameters.html) is an optional string that can be used to tag queries and other SQL statements executed within a session. Using it is helpful when multiple applications are sharing the same user ID and warehouse. If you use it wisely and consistently, you can identify workload trends such as performance issues and cost issues. As a best practice, tag your session with a unique identifier for your workload. Then, all the workload activities will be recorded with the tag you specified. This can help you identify issues with certain elements of the data pipeline. **Here’s an example of using this parameter:**\n\n```\nALTER SESSION SET QUERY_TAG='My workload tag'\n```\n\nAs shown in the following figure, you can search for specific tags using the Snowflake History tab. This can help you identify the slowest part of your pipeline.\n\n## Conclusion\n\nIn the 4 years I've been managing Snowflake deployments, I’ve used these strategies to great success and have seen significant results, such as up to a ⅓ reduction in credit consumption and an improvement in user experience and better SLAs. While any specific situation can differ, following the guidance above can result in better performance and response time for ad hoc queries, minimize queuing, and reduce costs. Snowflake provides lots of flexibility to control and manage performance and cost for many use cases such as data lakes, data warehouses, machine learning, and data science. Use this flexibility to match your needs and you’ll have a highly efficient and cost-effective solution for your organization.\n\n###### Authors\n\nTamir Rozenberg\n\nMike Klaczynski\n\n###### Share Article\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fmanaging-snowflakes-compute-resources&title=Managing+Snowflake%E2%80%99s+Compute+Resources)\n\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fmanaging-snowflakes-compute-resources&text=Managing+Snowflake%E2%80%99s+Compute+Resources)\n\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fmanaging-snowflakes-compute-resources)\n\nSubscribe to our blog newsletter\n\nGet the best, coolest and latest delivered to your inbox each week\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\nstart for free\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n* [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n* [Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n* [Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n* [Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n* [Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n* [Technology](https://www.snowflake.com/en/solutions/industries/technology/)\n\nLearn * [Resource Library](https://snowflake.com/en/resources/)\n* Live Demos\n* [Fundamentals](https://www.snowflake.com/en/fundamentals/)\n* [Training](https://www.snowflake.com/en/resources/learn/training/)\n* [Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n* [Snowflake University](https://learn.snowflake.com/en/)\n* [Developer Guides](https://www.snowflake.com/en/developers/guides)\n* [Documentation](https://docs.snowflake.com/)\n\n* [Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n* [Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\n* Cookie Settings\n* [Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Legal](https://www.snowflake.com/en/legal/)\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"],"full_content":null},{"url":"https://www.snowflake.com/en/developers/guides/resource-optimization-usage-monitoring/","title":"Resource Optimization: Usage Monitoring","publish_date":"2023-02-25","excerpts":["Data for Breakfast Around the World\n\nDrive impact across your organization with data and agentic intelligence.\n\nregister now\n\nSnowflake for Developers Guides Resource Optimization: Usage Monitoring\n\n## Resource Optimization: Usage Monitoring\n\nMatt Meredith\n\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/resource-optimization-usage-monitoring)\n\n## Overview\n\nThis resource optimization guide represents one module of the four contained in the series. These guides are meant to help customers better monitor and manage their credit consumption. Helping our customers build confidence that their credits are being used efficiently is key to an ongoing successful partnership. In addition to this set of Snowflake Quickstarts for Resource Optimization, Snowflake also offers community support as well as Training and Professional Services offerings. To learn more about the paid offerings, take a look at upcoming education and training .\n\nThis blog post can provide you with a better understanding of Snowflake's Resource Optimization capabilities.\n\nContact our team at marketing@snowflake.com , we appreciate your feedback.\n\n### Usage Monitoring\n\nUsage Monitoring queries are designed to identify the warehouses, queries, tools, and users that are responsible for consuming the most credits over a specified period of time. These queries can be used to determine which of those resources are consuming more credits than anticipated and take the necessary steps to reduce their consumption.\n\n### What You’ll Learn\n\n* how to analyze consumption trends and patterns\n* how to identify consumption anomolies\n* how to analyze partner tool consumption metrics\n\n### What You’ll Need\n\n* A Snowflake Account\n* Access to view [Account Usage Data Share](https://docs.snowflake.com/en/sql-reference/account-usage.html)\n\n### Related Materials\n\n* Resource Optimization: Setup & Configuration\n* Resource Optimization: Billing Metrics\n* Resource Optimization: Performance\n\n## Query Tiers\n\nEach query within the Resource Optimization Snowflake Quickstarts will have a tier designation just to the right of its name as \"(T\\*)\". The following tier descriptions should help to better understand those designations.\n\n### Tier 1 Queries\n\nAt its core, Tier 1 queries are essential to Resource Optimization at Snowflake and should be used by each customer to help with their consumption monitoring - regardless of size, industry, location, etc.\n\n### Tier 2 Queries\n\nTier 2 queries, while still playing a vital role in the process, offer an extra level of depth around Resource Optimization and while they may not be essential to all customers and their workloads, it can offer further explanation as to any additional areas in which over-consumption may be identified.\n\n### Tier 3 Queries\n\nFinally, Tier 3 queries are designed to be used by customers that are looking to leave no stone unturned when it comes to optimizing their consumption of Snowflake. While these queries are still very helpful in this process, they are not as critical as the queries in Tier 1 & 2.\n\n## Credit Consumption by Warehouse (T1)\n\n###### Tier 1\n\n#### Description:\n\nShows the total credit consumption for each warehouse over a specific time period.\n\n#### How to Interpret Results:\n\nAre there specific warehouses that are consuming more credits than the others? Should they be? Are there specific warehouses that are consuming more credits than anticipated for that warehouse?\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\n// Credits used (all time = past year)\nSELECT WAREHOUSE_NAME\n      ,SUM(CREDITS_USED_COMPUTE) AS CREDITS_USED_COMPUTE_SUM\n  FROM ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\n GROUP BY 1\n ORDER BY 2 DESC\n;\n\n// Credits used (past N days/weeks/months)\nSELECT WAREHOUSE_NAME\n      ,SUM(CREDITS_USED_COMPUTE) AS CREDITS_USED_COMPUTE_SUM\n  FROM ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\n WHERE START_TIME >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())  // Past 7 days\n GROUP BY 1\n ORDER BY 2 DESC\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\n## Average Hour-by-Hour Consumption Over the Past 7 Days (T1)\n\n###### Tier 1\n\n#### Description:\n\nShows the total credit consumption on an hourly basis to help understand consumption trends (peaks, valleys) over the past 7 days.\n\n#### How to Interpret Results:\n\nAt which points of the day are we seeing spikes in our consumption? Is that expected?\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL (by hour, warehouse)\n\n```\n```\n// Credits used by [hour, warehouse] (past 7 days)\nSELECT START_TIME\n      ,WAREHOUSE_NAME\n      ,CREDITS_USED_COMPUTE\n  FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\n WHERE START_TIME >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())\n   AND WAREHOUSE_ID > 0  // Skip pseudo-VWs such as \"CLOUD_SERVICES_ONLY\"\n ORDER BY 1 DESC,2\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\n\\### (by hour)\n\n```\n```\nSELECT DATE_PART('HOUR', START_TIME) AS START_HOUR\n      ,WAREHOUSE_NAME\n      ,AVG(CREDITS_USED_COMPUTE) AS CREDITS_USED_COMPUTE_AVG\n  FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\n WHERE START_TIME >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())\n   AND WAREHOUSE_ID > 0  // Skip pseudo-VWs such as \"CLOUD_SERVICES_ONLY\"\n GROUP BY 1, 2\n ORDER BY 1, 2\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\n## Average Query Volume by Hour (Past 7 Days) (T1)\n\n###### Tier 1\n\n#### Description:\n\nShows average number of queries run on an hourly basis to help better understand typical query activity.\n\n#### How to Interpret Results:\n\nHow many queries are being run on an hourly basis? Is this more or less than we anticipated? What could be causing this?\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\nSELECT DATE_TRUNC('HOUR', START_TIME) AS QUERY_START_HOUR\n      ,WAREHOUSE_NAME\n      ,COUNT(*) AS NUM_QUERIES\n  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY\n WHERE START_TIME >= DATEADD(DAY, -7, CURRENT_TIMESTAMP())  // Past 7 days\n GROUP BY 1, 2\n ORDER BY 1 DESC, 2\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\n## Warehouse Utilization Over 7 Day Average (T1)\n\n###### Tier 1\n\n#### Description:\n\nThis query returns the daily average of credit consumption grouped by week and warehouse.\n\n#### How to Interpret Results:\n\nUse this to identify anomolies in credit consumption for warehouses across weeks from the past year.\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\nWITH CTE_DATE_WH AS(\n  SELECT TO_DATE(START_TIME) AS START_DATE\n        ,WAREHOUSE_NAME\n        ,SUM(CREDITS_USED) AS CREDITS_USED_DATE_WH\n    FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY\n   GROUP BY START_DATE\n           ,WAREHOUSE_NAME\n)\nSELECT START_DATE\n      ,WAREHOUSE_NAME\n      ,CREDITS_USED_DATE_WH\n      ,AVG(CREDITS_USED_DATE_WH) OVER (PARTITION BY WAREHOUSE_NAME ORDER BY START_DATE ROWS 7 PRECEDING) AS CREDITS_USED_7_DAY_AVG\n      ,100.0*((CREDITS_USED_DATE_WH / CREDITS_USED_7_DAY_AVG) - 1) AS PCT_OVER_TO_7_DAY_AVERAGE\n  FROM CTE_DATE_WH\nQUALIFY CREDITS_USED_DATE_WH > 100  // Minimum N=100 credits\n    AND PCT_OVER_TO_7_DAY_AVERAGE >= 0.5  // Minimum 50% increase over past 7 day average\n ORDER BY PCT_OVER_TO_7_DAY_AVERAGE DESC\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\n## Forecasting Usage/Billing (T1)\n\n###### Tier 1\n\n#### Description:\n\nThis query provides three distinct consumption metrics for each day of the contract term. (1) the contracted consumption is the dollar amount consumed if usage was flat for the entire term. (2) the actual consumption pulls from the various usage views and aggregates dollars at a day level. (3) the forecasted consumption creates a straight line regression from the actuals to project go-forward consumption.\n\n#### How to Interpret Results:\n\nThis data should be mapped as line graphs with a running total calculation to estimate future forecast against the contract amount.\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\nSET CREDIT_PRICE = 4.00; --edit this number to reflect credit price\nSET TERM_LENGTH = 12; --integer value in months\nSET TERM_START_DATE = '2020-01-01';\nSET TERM_AMOUNT = 100000.00; --number(10,2) value in dollars\nWITH CONTRACT_VALUES AS (\n     SELECT\n              $CREDIT_PRICE::decimal(10,2) as CREDIT_PRICE\n             ,$TERM_AMOUNT::decimal(38,0) as TOTAL_CONTRACT_VALUE\n             ,$TERM_START_DATE::timestamp as CONTRACT_START_DATE\n             ,DATEADD(day,-1,DATEADD(month,$TERM_LENGTH,$TERM_START_DATE))::timestamp as CONTRACT_END_DATE\n),\nPROJECTED_USAGE AS (\n     SELECT\n                CREDIT_PRICE\n               ,TOTAL_CONTRACT_VALUE\n               ,CONTRACT_START_DATE\n               ,CONTRACT_END_DATE\n               ,(TOTAL_CONTRACT_VALUE)\n                   /\n                   DATEDIFF(day,CONTRACT_START_DATE,CONTRACT_END_DATE)  AS DOLLARS_PER_DAY\n               , (TOTAL_CONTRACT_VALUE/CREDIT_PRICE)\n                   /\n               DATEDIFF(day,CONTRACT_START_DATE,CONTRACT_END_DATE) AS CREDITS_PER_DAY\n     FROM      CONTRACT_VALUES\n),\nACTUAL_USAGE AS (\n SELECT TO_DATE(START_TIME) AS CONSUMPTION_DATE\n   ,SUM(DOLLARS_USED) as ACTUAL_DOLLARS_USED\n FROM (\n   --COMPUTE FROM WAREHOUSES\n   SELECT\n            'WH Compute' as WAREHOUSE_GROUP_NAME\n           ,WMH.WAREHOUSE_NAME\n           ,NULL AS GROUP_CONTACT\n           ,NULL AS GROUP_COST_CENTER\n           ,NULL AS GROUP_COMMENT\n           ,WMH.START_TIME\n           ,WMH.END_TIME\n           ,WMH.CREDITS_USED\n           ,$CREDIT_PRICE\n           ,($CREDIT_PRICE*WMH.CREDITS_USED) AS DOLLARS_USED\n           ,'ACTUAL COMPUTE' AS MEASURE_TYPE                  \n   from    SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY WMH\n   UNION ALL\n   --COMPUTE FROM SNOWPIPE\n   SELECT\n            'Snowpipe' AS WAREHOUSE_GROUP_NAME\n           ,PUH.PIPE_NAME AS WAREHOUSE_NAME\n           ,NULL AS GROUP_CONTACT\n           ,NULL AS GROUP_COST_CENTER\n           ,NULL AS GROUP_COMMENT\n           ,PUH.START_TIME\n           ,PUH.END_TIME\n           ,PUH.CREDITS_USED\n           ,$CREDIT_PRICE\n           ,($CREDIT_PRICE*PUH.CREDITS_USED) AS DOLLARS_USED\n           ,'ACTUAL COMPUTE' AS MEASURE_TYPE\n   from    SNOWFLAKE.ACCOUNT_USAGE.PIPE_USAGE_HISTORY PUH\n   UNION ALL\n   --COMPUTE FROM CLUSTERING\n   SELECT\n            'Auto Clustering' AS WAREHOUSE_GROUP_NAME\n           ,DATABASE_NAME || '.' || SCHEMA_NAME || '.' || TABLE_NAME AS WAREHOUSE_NAME\n           ,NULL AS GROUP_CONTACT\n           ,NULL AS GROUP_COST_CENTER\n           ,NULL AS GROUP_COMMENT\n           ,ACH.START_TIME\n           ,ACH.END_TIME\n           ,ACH.CREDITS_USED\n           ,$CREDIT_PRICE\n           ,($CREDIT_PRICE*ACH.CREDITS_USED) AS DOLLARS_USED\n           ,'ACTUAL COMPUTE' AS MEASURE_TYPE\n   from    SNOWFLAKE.ACCOUNT_USAGE.AUTOMATIC_CLUSTERING_HISTORY ACH\n   UNION ALL\n   --COMPUTE FROM MATERIALIZED VIEWS\n   SELECT\n            'Materialized Views' AS WAREHOUSE_GROUP_NAME\n           ,DATABASE_NAME || '.' || SCHEMA_NAME || '.' || TABLE_NAME AS WAREHOUSE_NAME\n           ,NULL AS GROUP_CONTACT\n           ,NULL AS GROUP_COST_CENTER\n           ,NULL AS GROUP_COMMENT\n           ,MVH.START_TIME\n           ,MVH.END_TIME\n           ,MVH.CREDITS_USED\n           ,$CREDIT_PRICE\n           ,($CREDIT_PRICE*MVH.CREDITS_USED) AS DOLLARS_USED\n           ,'ACTUAL COMPUTE' AS MEASURE_TYPE\n   from    SNOWFLAKE.ACCOUNT_USAGE.MATERIALIZED_VIEW_REFRESH_HISTORY MVH\n   UNION ALL\n   --COMPUTE FROM SEARCH OPTIMIZATION\n   SELECT\n            'Search Optimization' AS WAREHOUSE_GROUP_NAME\n           ,DATABASE_NAME || '.' || SCHEMA_NAME || '.' || TABLE_NAME AS WAREHOUSE_NAME\n           ,NULL AS GROUP_CONTACT\n           ,NULL AS GROUP_COST_CENTER\n           ,NULL AS GROUP_COMMENT\n           ,SOH.START_TIME\n           ,SOH.END_TIME\n           ,SOH.CREDITS_USED\n           ,$CREDIT_PRICE\n           ,($CREDIT_PRICE*SOH.CREDITS_USED) AS DOLLARS_USED\n           ,'ACTUAL COMPUTE' AS MEASURE_TYPE\n   from    SNOWFLAKE.ACCOUNT_USAGE.SEARCH_OPTIMIZATION_HISTORY SOH\n   UNION ALL\n   --COMPUTE FROM REPLICATION\n   SELECT\n            'Replication' AS WAREHOUSE_GROUP_NAME\n           ,DATABASE_NAME AS WAREHOUSE_NAME\n           ,NULL AS GROUP_CONTACT\n           ,NULL AS GROUP_COST_CENTER\n           ,NULL AS GROUP_COMMENT\n           ,RUH.START_TIME\n           ,RUH.END_TIME\n           ,RUH.CREDITS_USED\n           ,$CREDIT_PRICE\n           ,($CREDIT_PRICE*RUH.CREDITS_USED) AS DOLLARS_USED\n           ,'ACTUAL COMPUTE' AS MEASURE_TYPE\n   from    SNOWFLAKE.ACCOUNT_USAGE.REPLICATION_USAGE_HISTORY RUH\n   UNION ALL\n\n   --STORAGE COSTS\n   SELECT\n            'Storage' AS WAREHOUSE_GROUP_NAME\n           ,'Storage' AS WAREHOUSE_NAME\n           ,NULL AS GROUP_CONTACT\n           ,NULL AS GROUP_COST_CENTER\n           ,NULL AS GROUP_COMMENT\n           ,SU.USAGE_DATE\n           ,SU.USAGE_DATE\n           ,NULL AS CREDITS_USED\n           ,$CREDIT_PRICE\n           ,((STORAGE_BYTES + STAGE_BYTES + FAILSAFE_BYTES)/(1024*1024*1024*1024)*23)/DA.DAYS_IN_MONTH AS DOLLARS_USED\n           ,'ACTUAL COMPUTE' AS MEASURE_TYPE\n   from    SNOWFLAKE.ACCOUNT_USAGE.STORAGE_USAGE SU\n   JOIN    (SELECT COUNT(*) AS DAYS_IN_MONTH,TO_DATE(DATE_PART('year',D_DATE)||'-'||DATE_PART('month',D_DATE)||'-01') as DATE_MONTH FROM SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.DATE_DIM GROUP BY TO_DATE(DATE_PART('year',D_DATE)||'-'||DATE_PART('month',D_DATE)||'-01')) DA ON DA.DATE_MONTH = TO_DATE(DATE_PART('year',USAGE_DATE)||'-'||DATE_PART('month',USAGE_DATE)||'-01')\n) A\n group by 1\n),\nFORECASTED_USAGE_SLOPE_INTERCEPT as (\n SELECT\n          REGR_SLOPE(AU.ACTUAL_DOLLARS_USED,DATEDIFF(day,CONTRACT_START_DATE,AU.CONSUMPTION_DATE)) as SLOPE\n          ,REGR_INTERCEPT(AU.ACTUAL_DOLLARS_USED,DATEDIFF(day,CONTRACT_START_DATE,AU.CONSUMPTION_DATE)) as INTERCEPT\n FROM        PROJECTED_USAGE PU\n JOIN        SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.DATE_DIM DA ON DA.D_DATE BETWEEN PU.CONTRACT_START_DATE AND PU.CONTRACT_END_DATE\n LEFT JOIN   ACTUAL_USAGE AU ON AU.CONSUMPTION_DATE = TO_DATE(DA.D_DATE)\n)\nSELECT\n        DA.D_DATE::date as CONSUMPTION_DATE\n       ,PU.DOLLARS_PER_DAY AS CONTRACTED_DOLLARS_USED\n       ,AU.ACTUAL_DOLLARS_USED\n       --the below is the mx+b equation to get the forecasted linear slope\n       ,DATEDIFF(day,CONTRACT_START_DATE,DA.D_DATE)*FU.SLOPE + FU.INTERCEPT AS FORECASTED_DOLLARS_USED\nFROM        PROJECTED_USAGE PU\nJOIN        SNOWFLAKE_SAMPLE_DATA.TPCDS_SF10TCL.DATE_DIM    DA ON DA.D_DATE BETWEEN PU.CONTRACT_START_DATE AND PU.CONTRACT_END_DATE\nLEFT JOIN   ACTUAL_USAGE                                    AU ON AU.CONSUMPTION_DATE = TO_DATE(DA.D_DATE)\nJOIN        FORECASTED_USAGE_SLOPE_INTERCEPT                FU ON 1 = 1\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\n## Partner Tools Consuming Credits (T1)\n\n###### Tier 1\n\n#### Description:\n\nIdentifies which of Snowflake's partner tools/solutions (BI, ETL, etc.) are consuming the most credits.\n\n#### How to Interpret Results:\n\nAre there certain partner solutions that are consuming more credits than anticipated? What is the reasoning for this?\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\n--THIS IS APPROXIMATE CREDIT CONSUMPTION BY CLIENT APPLICATION\nWITH CLIENT_HOUR_EXECUTION_CTE AS (\n    SELECT  CASE\n         WHEN CLIENT_APPLICATION_ID LIKE 'Go %' THEN 'Go'\n         WHEN CLIENT_APPLICATION_ID LIKE 'Snowflake UI %' THEN 'Snowflake UI'\n         WHEN CLIENT_APPLICATION_ID LIKE 'SnowSQL %' THEN 'SnowSQL'\n         WHEN CLIENT_APPLICATION_ID LIKE 'JDBC %' THEN 'JDBC'\n         WHEN CLIENT_APPLICATION_ID LIKE 'PythonConnector %' THEN 'Python'\n         WHEN CLIENT_APPLICATION_ID LIKE 'ODBC %' THEN 'ODBC'\n         ELSE 'NOT YET MAPPED: ' || CLIENT_APPLICATION_ID\n       END AS CLIENT_APPLICATION_NAME\n    ,WAREHOUSE_NAME\n    ,DATE_TRUNC('hour',START_TIME) as START_TIME_HOUR\n    ,SUM(EXECUTION_TIME)  as CLIENT_HOUR_EXECUTION_TIME\n    FROM \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"QUERY_HISTORY\" QH\n    JOIN \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"SESSIONS\" SE ON SE.SESSION_ID = QH.SESSION_ID\n    WHERE WAREHOUSE_NAME IS NOT NULL\n    AND EXECUTION_TIME > 0\n  \n --Change the below filter if you want to look at a longer range than the last 1 month \n    AND START_TIME > DATEADD(Month,-1,CURRENT_TIMESTAMP())\n    group by 1,2,3\n    )\n, HOUR_EXECUTION_CTE AS (\n    SELECT  START_TIME_HOUR\n    ,WAREHOUSE_NAME\n    ,SUM(CLIENT_HOUR_EXECUTION_TIME) AS HOUR_EXECUTION_TIME\n    FROM CLIENT_HOUR_EXECUTION_CTE\n    group by 1,2\n)\n, APPROXIMATE_CREDITS AS (\n    SELECT \n    A.CLIENT_APPLICATION_NAME\n    ,C.WAREHOUSE_NAME\n    ,(A.CLIENT_HOUR_EXECUTION_TIME/B.HOUR_EXECUTION_TIME)*C.CREDITS_USED AS APPROXIMATE_CREDITS_USED\n\n    FROM CLIENT_HOUR_EXECUTION_CTE A\n    JOIN HOUR_EXECUTION_CTE B  ON A.START_TIME_HOUR = B.START_TIME_HOUR and B.WAREHOUSE_NAME = A.WAREHOUSE_NAME\n    JOIN \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"WAREHOUSE_METERING_HISTORY\" C ON C.WAREHOUSE_NAME = A.WAREHOUSE_NAME AND C.START_TIME = A.START_TIME_HOUR\n)\n\nSELECT \n CLIENT_APPLICATION_NAME\n,WAREHOUSE_NAME\n,SUM(APPROXIMATE_CREDITS_USED) AS APPROXIMATE_CREDITS_USED\nFROM APPROXIMATE_CREDITS\nGROUP BY 1,2\nORDER BY 3 DESC\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\n## Credit Consumption by User (T1)\n\n###### Tier 1\n\n#### Description:\n\nIdentifies which users are consuming the most credits within your Snowflake environment.\n\n#### How to Interpret Results:\n\nAre there certain users that are consuming more credits than they should? What is the purpose behind this additional usage?\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\n--THIS IS APPROXIMATE CREDIT CONSUMPTION BY USER\nWITH USER_HOUR_EXECUTION_CTE AS (\n    SELECT  USER_NAME\n    ,WAREHOUSE_NAME\n    ,DATE_TRUNC('hour',START_TIME) as START_TIME_HOUR\n    ,SUM(EXECUTION_TIME)  as USER_HOUR_EXECUTION_TIME\n    FROM \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"QUERY_HISTORY\" \n    WHERE WAREHOUSE_NAME IS NOT NULL\n    AND EXECUTION_TIME > 0\n  \n --Change the below filter if you want to look at a longer range than the last 1 month \n    AND START_TIME > DATEADD(Month,-1,CURRENT_TIMESTAMP())\n    group by 1,2,3\n    )\n, HOUR_EXECUTION_CTE AS (\n    SELECT  START_TIME_HOUR\n    ,WAREHOUSE_NAME\n    ,SUM(USER_HOUR_EXECUTION_TIME) AS HOUR_EXECUTION_TIME\n    FROM USER_HOUR_EXECUTION_CTE\n    group by 1,2\n)\n, APPROXIMATE_CREDITS AS (\n    SELECT \n    A.USER_NAME\n    ,C.WAREHOUSE_NAME\n    ,(A.USER_HOUR_EXECUTION_TIME/B.HOUR_EXECUTION_TIME)*C.CREDITS_USED AS APPROXIMATE_CREDITS_USED\n\n    FROM USER_HOUR_EXECUTION_CTE A\n    JOIN HOUR_EXECUTION_CTE B  ON A.START_TIME_HOUR = B.START_TIME_HOUR and B.WAREHOUSE_NAME = A.WAREHOUSE_NAME\n    JOIN \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"WAREHOUSE_METERING_HISTORY\" C ON C.WAREHOUSE_NAME = A.WAREHOUSE_NAME AND C.START_TIME = A.START_TIME_HOUR\n)\n\nSELECT \n USER_NAME\n,WAREHOUSE_NAME\n,SUM(APPROXIMATE_CREDITS_USED) AS APPROXIMATE_CREDITS_USED\nFROM APPROXIMATE_CREDITS\nGROUP BY 1,2\nORDER BY 3 DESC\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\n## Queries by # of Times Executed and Execution Time (T2)\n\n###### Tier 2\n\n#### Description:\n\nAre there any queries that get executed a ton?? how much execution time do they take up?\n\n#### How to Interpret Results:\n\nOpportunity to materialize the result set as a table?\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\nSELECT \nQUERY_TEXT\n,count(*) as number_of_queries\n,sum(TOTAL_ELAPSED_TIME)/1000 as execution_seconds\n,sum(TOTAL_ELAPSED_TIME)/(1000*60) as execution_minutes\n,sum(TOTAL_ELAPSED_TIME)/(1000*60*60) as execution_hours\n\n  from SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY Q\n  where 1=1\n  and TO_DATE(Q.START_TIME) >     DATEADD(month,-1,TO_DATE(CURRENT_TIMESTAMP())) \n and TOTAL_ELAPSED_TIME > 0 --only get queries that actually used compute\n  group by 1\n  having count(*) >= 10 --configurable/minimal threshold\n  order by 2 desc\n  limit 100 --configurable upper bound threshold\n  ;\n```\n\nCopy\n```\n\n## Top 50 Longest Running Queries (T2)\n\n###### Tier 2\n\n#### Description:\n\nLooks at the top 50 longest running queries to see if there are patterns\n\n#### How to Interpret Results:\n\nIs there an opportunity to optimize with clustering or upsize the warehouse?\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\nselect\n          \n          QUERY_ID\n         --reconfigure the url if your account is not in AWS US-West\n         ,'https://'||CURRENT_ACCOUNT()||'.snowflakecomputing.com/console#/monitoring/queries/detail?queryId='||Q.QUERY_ID as QUERY_PROFILE_URL\n         ,ROW_NUMBER() OVER(ORDER BY PARTITIONS_SCANNED DESC) as QUERY_ID_INT\n         ,QUERY_TEXT\n         ,TOTAL_ELAPSED_TIME/1000 AS QUERY_EXECUTION_TIME_SECONDS\n         ,PARTITIONS_SCANNED\n         ,PARTITIONS_TOTAL\n\nfrom SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY Q\n where 1=1\n  and TO_DATE(Q.START_TIME) >     DATEADD(month,-1,TO_DATE(CURRENT_TIMESTAMP())) \n    and TOTAL_ELAPSED_TIME > 0 --only get queries that actually used compute\n    and ERROR_CODE iS NULL\n    and PARTITIONS_SCANNED is not null\n   \n  order by  TOTAL_ELAPSED_TIME desc\n   \n   LIMIT 50\n   ;\n```\n\nCopy\n```\n\n## Top 50 Queries that Scanned the Most Data (T2)\n\n###### Tier 2\n\n#### Description:\n\nLooks at the top 50 queries that scan the largest number of micro partitions\n\n#### How to Interpret Results:\n\nIs there an opportunity to optimize with clustering or upsize the warehouse?\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\nselect\n          \n          QUERY_ID\n          --reconfigure the url if your account is not in AWS US-West\n         ,'https://'||CURRENT_ACCOUNT()||'.snowflakecomputing.com/console#/monitoring/queries/detail?queryId='||Q.QUERY_ID as QUERY_PROFILE_URL\n         ,ROW_NUMBER() OVER(ORDER BY PARTITIONS_SCANNED DESC) as QUERY_ID_INT\n         ,QUERY_TEXT\n         ,TOTAL_ELAPSED_TIME/1000 AS QUERY_EXECUTION_TIME_SECONDS\n         ,PARTITIONS_SCANNED\n         ,PARTITIONS_TOTAL\n\nfrom SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY Q\n where 1=1\n  and TO_DATE(Q.START_TIME) >     DATEADD(month,-1,TO_DATE(CURRENT_TIMESTAMP())) \n    and TOTAL_ELAPSED_TIME > 0 --only get queries that actually used compute\n    and ERROR_CODE iS NULL\n    and PARTITIONS_SCANNED is not null\n   \n  order by  PARTITIONS_SCANNED desc\n   \n   LIMIT 50\n   ;\n```\n\nCopy\n```\n\n## Queries by Execution Buckets over the Past 7 Days (T2)\n\n###### Tier 2\n\n#### Description:\n\nGroup the queries for a given warehouse by execution time buckets\n\n#### How to Interpret Results:\n\nThis is an opportunity to identify query SLA trends and make a decision to downsize a warehouse, upsize a warehouse, or separate out some queries to another warehouse\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\nWITH BUCKETS AS (\n\nSELECT 'Less than 1 second' as execution_time_bucket, 0 as execution_time_lower_bound, 1000 as execution_time_upper_bound\nUNION ALL\nSELECT '1-5 seconds' as execution_time_bucket, 1000 as execution_time_lower_bound, 5000 as execution_time_upper_bound\nUNION ALL\nSELECT '5-10 seconds' as execution_time_bucket, 5000 as execution_time_lower_bound, 10000 as execution_time_upper_bound\nUNION ALL\nSELECT '10-20 seconds' as execution_time_bucket, 10000 as execution_time_lower_bound, 20000 as execution_time_upper_bound\nUNION ALL\nSELECT '20-30 seconds' as execution_time_bucket, 20000 as execution_time_lower_bound, 30000 as execution_time_upper_bound\nUNION ALL\nSELECT '30-60 seconds' as execution_time_bucket, 30000 as execution_time_lower_bound, 60000 as execution_time_upper_bound\nUNION ALL\nSELECT '1-2 minutes' as execution_time_bucket, 60000 as execution_time_lower_bound, 120000 as execution_time_upper_bound\nUNION ALL\nSELECT 'more than 2 minutes' as execution_time_bucket, 120000 as execution_time_lower_bound, NULL as execution_time_upper_bound\n)\n\nSELECT \n COALESCE(execution_time_bucket,'more than 2 minutes')\n,count(Query_ID) as number_of_queries\n\nfrom SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY Q\nFULL OUTER JOIN BUCKETS B ON (Q.TOTAL_ELAPSED_TIME) >= B.execution_time_lower_bound and (Q.TOTAL_ELAPSED_TIME) < B.execution_time_upper_bound\nwhere Q.Query_ID is null\nOR (\nTO_DATE(Q.START_TIME) >= DATEADD(week,-1,TO_DATE(CURRENT_TIMESTAMP())) \nand warehouse_name = <WAREHOUSE_NAME>\nand TOTAL_ELAPSED_TIME > 0 \n  )\ngroup by 1,COALESCE(b.execution_time_lower_bound,120000)\norder by COALESCE(b.execution_time_lower_bound,120000)\n  ;\n```\n\nCopy\n```\n\n#### Screenshot\n\n## Warehouses with High Cloud Services Usage (T2)\n\n###### Tier 2\n\n#### Description:\n\nShows the warehouses that are not using enough compute to cover the cloud services portion of compute, ordered by the ratio of cloud services to total compute\n\n#### How to Interpret Results:\n\nFocus on Warehouses that are using a high volume and ratio of cloud services compute. Investigate why this is the case to reduce overall cost (might be cloning, listing files in S3, partner tools setting session parameters, etc.). The goal to reduce cloud services credit consumption is to aim for cloud services credit to be less than 10% of overall credits.\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\nselect \n    WAREHOUSE_NAME\n    ,SUM(CREDITS_USED) as CREDITS_USED\n    ,SUM(CREDITS_USED_CLOUD_SERVICES) as CREDITS_USED_CLOUD_SERVICES\n    ,SUM(CREDITS_USED_CLOUD_SERVICES)/SUM(CREDITS_USED) as PERCENT_CLOUD_SERVICES\nfrom \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"WAREHOUSE_METERING_HISTORY\"\nwhere TO_DATE(START_TIME) >= DATEADD(month,-1,CURRENT_TIMESTAMP())\nand CREDITS_USED_CLOUD_SERVICES > 0\ngroup by 1\norder by 4 desc\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\n## Warehouse Utilization (T2)\n\n###### Tier 2\n\n#### Description:\n\nThis query is designed to give a rough idea of how busy Warehouses are compared to the credit consumption per hour. It will show the end user the number of credits consumed, the number of queries executed and the total execution time of those queries in each hour window.\n\n#### How to Interpret Results:\n\nThis data can be used to draw correlations between credit consumption and the #/duration of query executions. The more queries or higher query duration for the fewest number of credits may help drive more value per credit.\n\n#### Primary Schema:\n\nAccount\\_Usage\n\n#### SQL\n\n```\n```\nSELECT\n       WMH.WAREHOUSE_NAME\n      ,WMH.START_TIME\n      ,WMH.CREDITS_USED\n      ,SUM(COALESCE(B.EXECUTION_TIME_SECONDS,0)) as TOTAL_EXECUTION_TIME_SECONDS\n      ,SUM(COALESCE(QUERY_COUNT,0)) AS QUERY_COUNT\n\nFROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY WMH\nLEFT JOIN (\n\n      --QUERIES FULLY EXECUTED WITHIN THE HOUR\n      SELECT\n         WMH.WAREHOUSE_NAME\n        ,WMH.START_TIME\n        ,SUM(COALESCE(QH.EXECUTION_TIME,0))/(1000) AS EXECUTION_TIME_SECONDS\n        ,COUNT(DISTINCT QH.QUERY_ID) AS QUERY_COUNT\n      FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY     WMH\n      JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY             QH ON QH.WAREHOUSE_NAME = WMH.WAREHOUSE_NAME\n                                                                          AND QH.START_TIME BETWEEN WMH.START_TIME AND WMH.END_TIME\n                                                                          AND QH.END_TIME BETWEEN WMH.START_TIME AND WMH.END_TIME\n      WHERE TO_DATE(WMH.START_TIME) >= DATEADD(week,-1,CURRENT_TIMESTAMP())\n      AND TO_DATE(QH.START_TIME) >= DATEADD(week,-1,CURRENT_TIMESTAMP())\n      GROUP BY\n      WMH.WAREHOUSE_NAME\n      ,WMH.START_TIME\n\n      UNION ALL\n\n      --FRONT part OF QUERIES Executed longer than 1 Hour\n      SELECT\n         WMH.WAREHOUSE_NAME\n        ,WMH.START_TIME\n        ,SUM(COALESCE(DATEDIFF(seconds,QH.START_TIME,WMH.END_TIME),0)) AS EXECUTION_TIME_SECONDS\n        ,COUNT(DISTINCT QUERY_ID) AS QUERY_COUNT\n      FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY     WMH\n      JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY             QH ON QH.WAREHOUSE_NAME = WMH.WAREHOUSE_NAME\n                                                                          AND QH.START_TIME BETWEEN WMH.START_TIME AND WMH.END_TIME\n                                                                          AND QH.END_TIME > WMH.END_TIME\n      WHERE TO_DATE(WMH.START_TIME) >= DATEADD(week,-1,CURRENT_TIMESTAMP())\n      AND TO_DATE(QH.START_TIME) >= DATEADD(week,-1,CURRENT_TIMESTAMP())\n      GROUP BY\n      WMH.WAREHOUSE_NAME\n      ,WMH.START_TIME\n\n      UNION ALL\n\n      --Back part OF QUERIES Executed longer than 1 Hour\n      SELECT\n         WMH.WAREHOUSE_NAME\n        ,WMH.START_TIME\n        ,SUM(COALESCE(DATEDIFF(seconds,WMH.START_TIME,QH.END_TIME),0)) AS EXECUTION_TIME_SECONDS\n        ,COUNT(DISTINCT QUERY_ID) AS QUERY_COUNT\n      FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY     WMH\n      JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY             QH ON QH.WAREHOUSE_NAME = WMH.WAREHOUSE_NAME\n                                                                          AND QH.END_TIME BETWEEN WMH.START_TIME AND WMH.END_TIME\n                                                                          AND QH.START_TIME < WMH.START_TIME\n      WHERE TO_DATE(WMH.START_TIME) >= DATEADD(week,-1,CURRENT_TIMESTAMP())\n      AND TO_DATE(QH.START_TIME) >= DATEADD(week,-1,CURRENT_TIMESTAMP())\n      GROUP BY\n      WMH.WAREHOUSE_NAME\n      ,WMH.START_TIME\n\n      UNION ALL\n\n      --Middle part OF QUERIES Executed longer than 1 Hour\n      SELECT\n         WMH.WAREHOUSE_NAME\n        ,WMH.START_TIME\n        ,SUM(COALESCE(DATEDIFF(seconds,WMH.START_TIME,WMH.END_TIME),0)) AS EXECUTION_TIME_SECONDS\n        ,COUNT(DISTINCT QUERY_ID) AS QUERY_COUNT\n      FROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY     WMH\n      JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY             QH ON QH.WAREHOUSE_NAME = WMH.WAREHOUSE_NAME\n                                                                          AND WMH.START_TIME > QH.START_TIME\n                                                                          AND WMH.END_TIME < QH.END_TIME\n      WHERE TO_DATE(WMH.START_TIME) >= DATEADD(week,-1,CURRENT_TIMESTAMP())\n      AND TO_DATE(QH.START_TIME) >= DATEADD(week,-1,CURRENT_TIMESTAMP())\n      GROUP BY\n      WMH.WAREHOUSE_NAME\n      ,WMH.START_TIME\n\n) B ON B.WAREHOUSE_NAME = WMH.WAREHOUSE_NAME AND B.START_TIME = WMH.START_TIME\n\nWHERE TO_DATE(WMH.START_TIME) >= DATEADD(week,-1,CURRENT_TIMESTAMP())\nGROUP BY\n\n      WMH.WAREHOUSE_NAME\n      ,WMH.START_TIME\n      ,WMH.CREDITS_USED\n;\n```\n\nCopy\n```\n\n#### Screenshot\n\nUpdated Feb 25, 2023\n\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\n\\*\n\n\\*\n\n\\* Country United States Canada United Kingdom Germany France Australia Japan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo The Democratic Republic of The Cook Islands Costa Rica Cote D'Ivoire (Ivory Coast) Croatia (Hrvatska) Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Heard and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jordan Kazakhstan Kenya Kiribati Korea Republic of (South) Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia Federated States of Moldova Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Occupied Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Reunion Romania Russian Federation Saint Helena Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and The South Sandwich Island Spain Sri Lanka Suriname Svalbard and Jan Mayen Islands Swaziland Sweden Switzerland Taiwan Tajikistan Tanzania United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Venezuela Viet Nam Virgin Islands (British) Virgin Islands (U.S.) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe\n\n\\*\n\nAdd me to the list to receive dedicated product updates and general availability emails.\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\n\nSubscribe Now\n\nIndustries * Advertising, Media & Entertainment\n* Financial Services\n* Healthcare & Life Sciences\n* Manufacturing\n* Public Sector\n* Retail & Consumer Goods\n* Technology\n\nLearn * Resource Library\n* Live Demos\n* Fundamentals\n* Training\n* Certifications\n* Snowflake University\n* Developer Guides\n* Documentation\n\n* Privacy Policy\n* Site Terms\n* Communication Preferences\n* Cookie Settings\n* Do Not Share My Personal Information\n* Legal\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"],"full_content":null},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-performance/","title":"Performance","publish_date":null,"excerpts":["Data for Breakfast Around the World\n\nDrive impact across your organization with data and agentic intelligence.\n\n[register now](https://www.snowflake.com/en/data-for-breakfast/)\n\n[Snowflake for Developers](https://www.snowflake.com/content/snowflake-site/global/en/developers) [Guides](https://www.snowflake.com/content/snowflake-site/global/en/developers/guides) Performance\n\n## Performance\n\nWell Architected Framework Team\n\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-performance)\n\n## Overview\n\nOptimizing performance on Snowflake is crucial for efficient data\nanalysis. This guidance, for data architects, developers, and\nadministrators, outlines best practices for designing, implementing, and\nmaintaining data workloads.\n\nApplying these recommendations streamlines operations, enhances user\nexperience, and improves business value through increased revenue and\nreduced costs. For instance, faster query execution translates to\nquicker insights and greater adoption.\n\nPerformance tuning often balances performance and cost, with\nimprovements frequently benefiting both. Snowflake's autoscaling\ncapabilities, for example, ensure consistent performance by dynamically\nallocating resources as concurrency increases, while also providing cost\nefficiency by scaling down during lower demand.\n\n## Principles and recommendations\n\n### Principles\n\n#### Establish and validate performance objectives\n\n> Define clear, measurable, and achievable performance targets within\n> technical and budgetary limits before application design. Consider key\n> workload characteristics.\n> \n> \n\n#### Optimize data architecture and access\n\n> Design efficient data models and access patterns to minimize data\n> scanned. Leverage appropriate data types, clustering, indexing, and\n> performance features.\n> \n> \n\n#### Architect for scalability and workload partitioning\n\n> Utilize Snowflake’s elastic warehouse features and application design\n> to strategically partition workloads, optimizing resource utilization,\n> concurrency, and latency.\n> \n> \n\n#### Implement continuous performance monitoring and optimization\n\n> Establish comprehensive monitoring and logging to identify performance\n> bottlenecks. Proactively optimize the system over time, adapting to\n> evolving requirements.\n> \n> \n\n### Recommendations\n\nThe following key recommendations are covered within the principles of\nPerformance:\n\n* **Leveraging Elasticity** Dynamically adjust resources based on workload. Utilize horizontal\n  scaling (multi-cluster warehouses) and vertical sizing to match query\n  complexity. Employ manual scaling for predictable peaks and\n  auto-scaling for real-time fluctuations.\n* **Peak Load Planning** Proactively plan for high-demand periods. Use manual scaling for\n  predictable surges; enable auto-scaling and predictive scaling for\n  less predictable demand.\n* **Continuous Performance Improvement** Monitor key performance indicators like latency, throughput, and\n  utilization. Foster continuous tuning through training, feedback, and\n  metric review.\n* **Test-First Design** Integrate performance testing into the design phase. Define KPIs,\n  establish baselines, and simulate workloads early. Use load,\n  scalability, and stress testing throughout development to prevent\n  bottlenecks.\n* **Effective Data Shaping** Leverage Snowflake micro-partition clustering. Define clustering keys\n  for frequently filtered/joined columns to enable efficient pruning and\n  reduce I/O. Regularly monitor clustering depth.\n* **Well-Designed SQL** Write efficient queries by avoiding `SELECT \\*` , specifying needed\n  columns, and using efficient joins. Use query profiling tools to\n  refine performance and minimize resource consumption.\n* **Optimizing Warehouses** Reduce queues and spillage, scale appropriately, and use query\n  acceleration. Right-size warehouses and manage concurrency to reduce\n  contention and optimize resource use.\n* **Optimizing Storage** Use automatic clustering, Search Optimization Service, and\n  materialized views strategically. Match the technique to your\n  workload.\n* **High-Level Practices** To optimize query performance, select fewer columns, leverage query\n  pruning with clustered columns, and use pre-aggregated tables.\n  Simplify SQL by reducing unnecessary sorts, preferring window\n  functions, and avoiding OR conditions in joins. Minimize view\n  complexity, maximize cache usage, scale warehouses, and tune cluster\n  scaling policies for balanced performance and cost.\n\n## Establish and validate performance objectives\n\n### Overview\n\nSnowflake optimizes performance through its Performance by Design\nmethodology, integrating performance as a core architectural feature.\nThis approach links business objectives with efficient resource\nmanagement, leveraging key principles, careful configurations, and\nSnowflake's elasticity for scalability and cost-efficiency.\n\n#### Desired outcome\n\nBy implementing these best practices, you can achieve a highly\nresponsive and cost-optimized Snowflake environment. Expect predictable\nquery performance that consistently meets Service Level Objectives\n(SLOs), eliminate costly architectural debt, and effectively manage\noperational expenses by aligning compute resources with workload\ndemands. Ultimately, businesses can claim a well-architected data\nplatform that supports their needs without unnecessary expenditure.\n\n#### Recommendations\n\nPerformance is a core, cost-driven feature. Prioritize \"as fast as\nnecessary\" over \"as fast as possible\" to prevent over-provisioning.\nDefine performance (latency, throughput, concurrency) as a negotiable\nbusiness metric requiring cost-benefit analysis, as it directly impacts\nSnowflake's operational expenses through credit consumption.\nUnderstanding cloud architecture's continuous financial implications is\ncrucial. Performance planning from the start is essential to avoid\ninefficiencies, poor user experience, and costly re-engineering.\n\n### Performance by Design\n\nAchieve a high-performing Snowflake environment by implementing a\ndeliberate strategy before coding. The Performance by Design methodology\nensures performance is a core architectural feature from a project's\nstart, aligning business objectives, user experience, and financial\ngovernance. Proactive decisions establish an efficiently scalable\nfoundation, preventing expensive architectural debt.\n\n**Performance as a feature**\n\nPrior to solution design, it is imperative to internalize a key\nprinciple of cloud computing: every performance requirement entails a\ndirect and quantifiable cost. The paradigm shift from \"maximize speed\"\nto \"achieve necessary speed\" represents the initial stride toward a\nmeticulously architected system.\n\n**Performance as a negotiable business metric**\n\nPerformance, latency, and concurrency are more than fixed technical\nspecs; they're business requirements that need to be defined and\njustified like any other product feature. If someone asks for a\n\"five-second query response time,\" that should invite a discussion about\nvalue and cost, not be treated as an absolute must-have. Different\nworkloads have different performance needs, and a one-size-fits-all\napproach leads to overspending and wasted resources.\n\n**The Cloud Paradigm: From capital to operational expense**\n\nIn a traditional on-premises setup, performance often hinges on\nsignificant upfront spending for hardware. Once purchased, the cost to\nrun each query is very low. The cloud, however, uses a completely\ndifferent financial model. With Snowflake, performance is an ongoing\noperational cost. Every active virtual warehouse uses credits, meaning\nthat your architectural and query design decisions have immediate and\ncontinuous financial consequences.\n\n**Avoiding architectural debt: The cost of unplanned implementation**\n\nSnowflake's inherent simplicity can lead to solutions implemented\nwithout adequate upfront planning, fostering architectural debt. This\ndebt manifests as excessive credit consumption from inefficient queries,\nsuboptimal user experience due to unpredictable performance, and\nexpensive re-engineering projects. To avoid these issues, establish\nclear project plans and formalize performance metrics from the outset,\nensuring optimal system health and cost efficiency.\n\n#### The Core Process: The Design Document\n\nTo translate principle into practice, any significant data project\nshould begin with a design document. This is not just a technical\ndocument, but a formal agreement that forces critical trade-off\nconversations to happen at the project's inception.\n\n**Assembling the Stakeholders: A Cross-Functional Responsibility**\n\nCreating a design document is a collaborative effort. It ensures that\nbusiness goals are accurately translated into technical and financial\nrealities. The key participants include:\n\n* Business Stakeholders: To define the goals and the value of the\n  desired outcomes.\n* Product Managers: To translate business needs into specific features\n  and user stories.\n* Engineering Leads: To evaluate technical feasibility and design the\n  architecture.\n* FinOps/Finance: To provide budget oversight and model the cost\n  implications of different service levels.\n\n**Service Level Objectives (SLOs)**\n\nService Level Objectives (SLOs) are essential for optimizing\nperformance. They establish measurable and acceptable performance\nbenchmarks for a system or service. By continuously monitoring and\nevaluating against these concrete targets, organizations can ensure\ntheir services consistently fulfill both user expectations and business\nneeds.\n\nEffective SLOs should be:\n\n* **Specific:** Avoid vague goals. Define precise targets, such as \"95%\n  of interactive dashboard queries under 10 seconds,\" to allow for\n  accurate measurement.\n* **Measurable:** Establish clear metrics and the tools for data\n  collection and analysis. For example, to measure \"nightly ETL of 1TB\n  in under 60 minutes,\" systems must track data volume and processing\n  time.\n* **Achievable:** Set realistic and attainable targets given current\n  resources and technology, to avoid excessive costs without business\n  value.\n* **Relevant:** Align SLOs with business needs and end-user\n  expectations, focusing on critical user experience or business\n  processes.\n* **Time-bound:** Many SLOs benefit from a defined timeframe, aiding in\n  performance evaluation over a specific period.\n\nDefining and monitoring SLOs helps organizations proactively identify\nand address performance bottlenecks, ensuring consistent service\nquality, enhanced user satisfaction, and better business outcomes.\nRegular review and adjustment of SLOs are crucial to adapt to evolving\nbusiness needs and technological advancements.\n\n#### Defining the four pillars of workload requirements\n\nDefine a workload’s performance profile using these four key pillars:\n\n* **Workload Categorization** : Not all work is the same. The first step\n  is to classify each workload based on its usage pattern and\n  performance characteristics. Common categories include:\n  \n    + **Interactive:** User-facing queries, typically from dashboards\n        (e.g., Tableau, Power BI, Looker), where low latency and high\n        concurrency are critical for a good user experience.\n    + **Ad-Hoc/Exploratory:** Queries from data scientists or analysts\n        that are often complex, unpredictable, and long-running. Latency is\n        less critical than preventing impact on other workloads.\n    + **Batch:** Scheduled, large-scale data processing jobs like ELT/ETL\n        pipelines. Throughput and predictability are the primary goals.\n    + **Programmatic:** Automated queries from applications or services\n        that often have highly predictable patterns and strict performance\n        requirements.\n* **Workload Performance Targets** : For each category, establish\n  specific, measurable, and realistic performance targets. Avoid overly\n  general goals, such as \"all queries must return in 10 seconds,\" in\n  favor of concrete objectives.\n  \n    + **Latency Example:** \"95% of all interactive dashboard filter\n        queries must complete in under 10 seconds.\"\n    + **Throughput Example:** \"The nightly ETL process must load and\n        transform 1 TB of data in under 60 minutes.\"\n* **Data Freshness Tiers:** The demand for real-time data must be\n  balanced against its often higher cost. Define explicit data freshness\n  tiers to align business value with architectural complexity and credit\n  consumption.\n  \n    + **Near Real-Time (Seconds to Minutes):** Highest cost; requires a\n        streaming architecture (e.g., Snowpipe, Dynamic Tables). Justified\n        for use cases like fraud detection, real-time ad selection, etc.\n    + **Hourly:** Medium cost; suitable for operational reporting.\n    + **Daily:** Lowest cost; standard for most strategic business\n        intelligence and analytics.\n* **Concurrency and Scale Projections:** Plan for the expected load and\n  future growth to inform warehouse sizing and scaling strategies.\n  Examples of key questions to answer include:\n  \n    + How many users are expected to use this system simultaneously during\n        peak hours?\n    + What is the anticipated data volume growth over the next 6-12\n        months?\n    + Are there predictable spikes in usage (e.g., end-of-quarter\n        reporting, Black Friday)?\n\n**Integrating performance into the development lifecycle**\n\nValidate agreements from the design phase throughout development.\nProactive performance testing, a \"shift-left\" methodology, mitigates\nproject risks and prevents launch-time performance issues.\n\nDo not await large-scale data for performance analysis. Early on, review\nquery profiles and EXPLAIN plans to identify potential problems like\nexploding joins (join with a missing or incorrect condition, causing a\nmassive, unintended multiplication of rows) or full table scans, which\nindicate ineffective query pruning.\n\n#### Mandatory pre-release benchmarking\n\nBefore deploying a new workload to production, benchmark it as a formal\nquality gate. This involves running it on a production-sized data clone\nin a staging environment to validate that it meets the Workload\nRequirements Agreement's SLOs.\n\nFor critical workloads, integrate performance benchmarks into the CI/CD\npipeline. This practice automatically detects performance regressions\nfrom code changes, allowing for immediate fixes rather than user\ndiscovery in production.\n\n### Common design shortcomings to avoid\n\nFailing to implement a Performance by Design methodology often leads to\none of the following common and costly mistakes.\n\n**Applying a single, aggressive SLA universally**\n\nApplying a single, aggressive performance target (e.g., \"all queries\nunder 5 seconds\") to every workload leads to inefficiencies. This forces\nad-hoc analysis, outlier queries, and interactive dashboards to share\nthe same expensive configuration, increasing credit usage unnecessarily.\n\n**Disregarding the cost of data freshness**\n\nAn unexamined data latency requirement, such as \"data must be 1 minute\nfresh,\" can significantly increase credit consumption. Often, data\nfreshness exceeds actual needs without fully discussing associated\ncosts. This can lead to paying a high price for a streaming architecture\nthat may not deliver proportional business value.\n\n**Performance discrepancies at deployment**\n\nDeferring performance testing until a project's end results in\nunacceptably slow or expensive processes discovered just before launch.\nThis often leads to emergency fixes, missed deadlines, and the\nbrute-force solution of running workloads on oversized warehouses. This\nobscures underlying design flaws at a significant ongoing cost.\n\n## Foundational First Steps\n\n#### Performance in the cloud\n\nIn an on-premises environment, performance is often a sunk cost. The\ncloud, particularly Snowflake, links performance and cost optimization.\nEvery query, active virtual warehouse, and background service consumes\ncredits, making performance an ongoing operational consideration.\n\nThis guide focuses on initial steps to build a performant, scalable, and\ncost-effective environment. Make conscious performance and cost\ndecisions before significant development to establish a solid\narchitectural foundation, preventing costly re-engineering.\n\n#### Establish and validate performance objectives\n\nEffective performance objectives are specific, measurable, and\nrealistic. They serve as the benchmark against which you can measure the\nsuccess of your architecture. When defining your targets, you should\nfocus on key metrics that directly impact your workloads.\n\nConsider the following core performance characteristics:\n\n* **Latency** : Ensure queries complete quickly for interactive\n  applications. For example, 95% of dashboard filter queries should\n  complete in under 10 seconds.\n* **Throughput** : Measure the work your system accomplishes over time,\n  vital for batch processing and data ingestion. For instance, the\n  nightly ETL process must load and transform 1 TB in under 60 minutes.\n* **Concurrency** : Determine how many simultaneous queries your system\n  supports without performance issues, crucial for applications with\n  numerous users. The primary BI warehouse should support 200 concurrent\n  users during peak hours without query queuing.\n\nThese objectives must be aligned with specific workload characteristics\nand balanced against the real-world business and budget constraints of\nyour project. They are established for a specific workload and rarely\napply across all workloads.\n\n### Tactical Day One configurations & concepts\n\nOnce you have a strategic plan, you can implement tactical\nconfigurations that set a baseline for good performance and cost\nmanagement.\n\n#### Proactive guardrails\n\nOptimizing warehouse performance involves strategic configuration. These\nsettings are your first line of defense against runaway queries and\nunexpected costs.\n\n* An effective **Warehouse auto-suspend strategy** balances credit\n  savings and performance. Suspended warehouses consume no credits but\n  lose their data cache, leading to slower subsequent queries. Tailor\n  the strategy to your workload:\n* **ELT/ETL warehouses:** Use an aggressive suspend time (e.g., 1-2\n  minutes). These jobs are typically bursty, making it inefficient to\n  keep the warehouse running after completion.\n* **BI/dashboard warehouses:** Employ a more lenient suspend time (e.g.,\n  10-20 minutes). The performance benefit of a warm cache for\n  latency-sensitive users often outweighs the cost of brief idle\n  periods.\n* **Setting statement timeouts** is crucial to prevent long-running,\n  costly queries. The default timeout (48 hours) is often too long.\n  Configure the [STATEMENT\\_TIMEOUT\\_IN\\_SECONDS](https://docs.snowflake.com/en/sql-reference/parameters) parameter on your warehouses to a sensible maximum, such as 15 minutes\n  for a dashboard warehouse or 8 hours for a complex ETL warehouse.\n* **Configuring Resource Monitors** provides a safety net for budget\n  control. These monitors track warehouse credit consumption and can\n  trigger actions like notifications or automatic suspension when\n  thresholds are exceeded. It's best practice to set up monitors at both\n  the account and individual warehouse levels. Refer to [Working with resource monitors in the Snowflake documentation](https://docs.snowflake.com/en/user-guide/resource-monitors) for more information.\n* **Budgets** offer a flexible way to monitor and control spending\n  across your Snowflake account. They track credit usage for\n  customizable objects, including warehouses and serverless features.\n  Budgets can alert stakeholders when projected costs approach or exceed\n  predefined limits, enabling a proactive approach to managing overall\n  spend. See [Monitor credit usage with budgets in the Snowflake documentation](https://docs.snowflake.com/en/user-guide/budgets) for details.\n\n#### Fundamental architectural concepts\n\n* **Scale up for complexity:** Increase warehouse size (e.g., from\n  Medium to Large) to improve a single, large, or complex query's\n  performance. A larger warehouse provides more resources—memory, CPU,\n  and temporary storage—to complete the work faster.\n* **Scale out for concurrency:** For a higher number of concurrent users\n  and queries, increase the number of clusters in a multi-cluster\n  warehouse. This doesn't make individual queries faster but allows more\n  to run simultaneously without performance bottlenecks.\n\n#### Using the right tool for the job (targeted acceleration)\n\nSnowflake offers several powerful features that accelerate performance\nfor specific use cases. Using them correctly is key.\n\n* **Search Optimization Service** This service accelerates point-lookup\n  queries on large tables. It creates a lightweight search access path,\n  allowing Snowflake to pinpoint data in micro-partitions and bypass\n  full table scans. It's most effective on high-cardinality columns like\n  UUIDs or email addresses. See [The Expat Guide to Search Optimization Service](https://medium.com/snowflake/search-optimization-c99b2117cb2e) .\n* **Materialized Views (MVs)** MVs are for massive-scale problems, not\n  general tuning. Consider them only when data reduction from the base\n  table is substantial, such as for pre-calculating large dashboard\n  aggregations or optimizing multi-billion row tables. MVs incur\n  continuous storage and compute costs for maintenance, which must be\n  justified by performance gains. See [Working with Materialized Views in the Snowflake documentation](https://docs.snowflake.com/en/user-guide/views-materialized) .\n* **Query Acceleration Service (QAS)** QAS acts as a safety net for your\n  warehouse, handling outlier queries with unexpectedly large scans. It\n  offloads parts of this work to shared Snowflake compute resources,\n  preventing large queries from monopolizing your warehouse and\n  impacting other users. QAS is not a primary tuning tool, but it\n  improves warehouse stability. See [Using the Query Acceleration Service (QAS) in the Snowflake documentation](https://docs.snowflake.com/en/user-guide/query-acceleration-service) .\n\n#### Modern data transformation\n\nDynamic Tables offer declarative data transformation. When designing\nthem, simplicity is key. Performance depends on the query and a hidden\nChange Data Capture (CDC) process checking source tables. Complex\nqueries with many joins create intricate CDC dependency graphs, slowing\nrefresh times. Chaining simpler Dynamic Tables is often more performant\nand manageable than a single, complex one. For more information, see [Dynamic tables in the Snowflake documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-about) .\n\n### Conclusion\n\nA high-performing Snowflake environment starts on day one. By moving\nfrom strategic planning to tactical configurations, you establish a\nrobust framework. These initial steps—defining objectives, configuring\nguardrails, and using the right tools—are essential for building a\nperformant, scalable, and cost-effective architecture in the Snowflake\nData Cloud.\n\n## Leveraging elasticity for performance\n\n#### The power of elastic compute\n\nSnowflake’s architecture separates storage and compute, enabling\nindependent and dynamic scaling. This ensures the precise processing\npower needed for your workloads. Leveraging this elasticity is crucial\nfor a well-architected Snowflake environment.\n\nThe goal is optimal price-for-performance, avoiding both\nunder-provisioning (slow queries, missed SLOs) and over-provisioning\n(unnecessary credit consumption).\n\nThis guide details three primary elasticity mechanisms. Understanding\nhow and when to use each helps build a responsive, efficient, and\ncost-effective data platform. The three mechanisms are:\n\n* **Vertical Scaling:** Adjusting the size of a warehouse (for example\n  from Medium to Large) to handle more complex queries and larger data\n  volumes.\n* **Horizontal Scaling:** Adjusting the number of clusters in a\n  warehouse to manage workload concurrency.\n* **Query Acceleration Service:** Augmenting a warehouse with serverless\n  resources to handle unpredictable, outlier queries.\n\n#### Isolate your workloads\n\nBefore applying any scaling strategy, you must first isolate your\nworkloads. A common performance anti-pattern is to direct all users and\nprocesses to a single, large virtual warehouse. This approach makes it\nimpossible to apply the correct scaling strategy, as the warehouse is\nforced to handle workloads with conflicting performance profiles.\n\nA fundamental best practice is to create separate, dedicated warehouses\nfor distinct workloads. For example, your architecture should include\ndifferent warehouses for:\n\n* **ETL/ELT:** These workloads are often characterized by complex,\n  long-running transformations that benefit from larger warehouses but\n  may not require high concurrency.\n* **BI Dashboards:** These workloads typically involve many concurrent,\n  short-running, repetitive queries. They demand low latency and benefit\n  from horizontal scaling.\n* **Data Science:** These workloads can be highly variable, often\n  involving exploratory analysis and long-running model training that\n  require specific sizing and timeouts.\n* **Ad-Hoc Analysis:** These queries from data analysts are often\n  unpredictable in both complexity and concurrency.\n\nBy isolating workloads, you can observe the specific performance profile\nof each and apply the most appropriate and cost-effective scaling\nstrategy described below.\n\n### Vertical scaling: sizing up for complexity and scale\n\nVertical scaling, or resizing, modifies a warehouse's compute power.\nSnowflake warehouses, offered in \"t-shirt sizes\" (X-Small, Small,\nMedium, etc.), double compute resources—CPU, memory, and local disk\ncache—with each size increase.\n\nConsider scaling up when a single query or complex operations demand\nmore resources than the current warehouse size can efficiently provide.\nA larger warehouse can accelerate complex queries by parallelizing work\nacross more nodes. However, increasing warehouse size is not a universal\nsolution for slow queries; diagnose the performance bottleneck first.\n\n#### When to scale up: A diagnostic checklist\n\nBefore increasing a warehouse’s size, use the following checklist to\nvalidate that it is the appropriate solution.\n\n**Symptom 1: Disk spilling**\n\nSpilling occurs when a query’s intermediate results exceed a warehouse’s\navailable memory and must be written to disk. This is a primary\nindicator of an undersized warehouse. You can identify spilling in the\nQuery Profile. There are two types of spilling, each with a different\nlevel of severity:\n\n* **Local Spilling:** This indicates a **warning** . Data is written from\n  memory to the warehouse’s local disk, impacting query performance.\n  Evaluate local spilling within your workload's SLA and the\n  spill-to-RAM ratio. For instance, if 1 TB of data is processed in\n  memory with 10 GB spilled, the impact might be minor. However, if 20\n  GB is processed with 10 GB spilled, the warehouse is likely\n  undersized.\n* **Remote Spilling:** This is a **critical alert** . Data has exhausted\n  both memory and local disk, spilling to remote cloud storage. This\n  severely degrades performance. Queries exhibiting significant remote\n  spilling are strong candidates for a larger warehouse.\n\n**Symptom 2: Slow, CPU-bound queries (non-spilling)**\n\nSometimes a query is slow even without spilling significant data. This\noften indicates that the query is CPU-bound and could benefit from the\nadditional processing power of a larger warehouse. However, before\nresizing, you must first verify that the query can actually take\nadvantage of the added resources.\n\n* **Prerequisite 1: Verify Sufficient Parallelism Potential.** A query’s\n  parallelism is limited by the number of micro-partitions it scans. If\n  there are not enough partitions, additional nodes in a larger\n  warehouse will sit idle. As a rule of thumb, a query should scan at\n  least **~40 micro-partitions per node** in the warehouse for a size\n  increase to be effective. You can find the number of partitions\n  scanned in the Table Scan operator within the Query Profile.\n* **Prerequisite 2: Verify a Lack of Execution Skew.** Performance\n  issues can also arise from data skew, where most of the processing is\n  forced onto a single node. In the [Query Profile](https://docs.google.com/document/d/1LDeFasziRlYL1Z5t9BqJ_MwhECtJHbRDKG5BNWUAuwU/edit?tab=t.lhzra61et1j6) ,\n  check the per-node execution time. If one node is active for\n  significantly longer than its peers, the problem lies in the data\n  distribution or query structure, not the warehouse size. Sizing up\n  will not solve this problem.\n\nOnly after confirming that a query has sufficient parallelism potential\nand is not suffering from execution skew should you test it on a larger\nwarehouse to address a CPU bottleneck.\n\nSee [Increasing warehouse size in the Snowflake documentation](https://docs.snowflake.com/en/user-guide/performance-query-warehouse-size) for more information.\n\n#### Reference Table: Warehouse sizing for parallelism\n\nUse the following table to determine the minimum number of\nmicro-partitions a query should scan to effectively utilize a given\nwarehouse size.\n\n|**Warehouse Size** |**Minimum Micro-partitions Scanned** |\n| --- | --- |\n|X-Small |40 |\n|Small |80 |\n|Medium |160 |\n|Large |320 |\n|X-Large |640 |\n|2X-Large |1,280 |\n|3X-Large |2,560 |\n|4X-Large |5,120 |\n|5X-Large |10,240 |\n\n### Horizontal scaling: scaling out for concurrency\n\nHorizontal scaling increases the number of compute clusters available to\nyour warehouse. This is the primary tool for managing high\nconcurrency—that is, a high volume of simultaneous queries. When you\nconfigure a multi-cluster warehouse, Snowflake automatically adds and\nremoves clusters of the same size in response to query load.\n\nThis allows your warehouse to handle fluctuating numbers of users and\nqueries without queueing. As more queries arrive, new clusters are\nstarted to run them in parallel. As the query load subsides, clusters\nare automatically suspended to save credits.\n\n#### Configuration modes\n\nYou can configure a multi-cluster warehouse to run in one of two modes:\n\n* **Auto-scale Mode:** The default and most common setting, this mode\n  automatically adjusts the number of clusters within a defined minimum\n  and maximum (e.g., 1 to 8). This is ideal for varying concurrency, as\n  Snowflake starts and stops clusters to match query load.\n* **Maximized Mode:** This mode continuously runs the maximum specified\n  number of clusters (e.g., 8 clusters if both minimum and maximum are\n  set to 8). It suits workloads with consistently high and predictable\n  concurrency, eliminating latency from new cluster starts.\n\n#### Tuning auto-scale with scaling policies\n\nWhen using Auto-scale mode, you can further refine its behavior by\nsetting a scaling policy. This allows you to define the warehouse’s\npriority: minimizing wait time or maximizing credit savings.\n\n* **Standard Policy (Performance-First):** This policy prioritizes a\n  fast user experience, minimizing queueing by launching new clusters if\n  a query is queued for a few seconds. It is ideal for time-sensitive,\n  user-facing workloads such as BI dashboards and analytical\n  applications.\n* **Economy Policy (Cost-First):** This policy prioritizes cost savings\n  by starting new clusters more conservatively. A new cluster launches\n  only if a query backlog is estimated to keep it busy for at least six\n  minutes. This makes it a good choice for non-urgent background\n  processes like ETL/ELT pipelines, where some queueing is an acceptable\n  trade-off for lower credit consumption.\n\nSee [Multi-cluster warehouses in the Snowflake documentation](https://docs.snowflake.com/en/user-guide/warehouses-multicluster) for more information.\n\n### Query Acceleration Service (QAS): Handling workload volatility\n\nThe Query Acceleration Service (QAS) enhances warehouse performance by\nproviding serverless compute resources to accelerate specific, eligible\nqueries, particularly large table scans. QAS addresses **workload volatility** , handling unpredictable, large \"outlier\" queries that\noccasionally burden a correctly sized warehouse. This service ensures\nsmooth performance without requiring a larger, more expensive warehouse\nfor infrequent, costly queries.\n\n#### The use case for QAS\n\nYou should consider enabling QAS for a warehouse when you observe\nqueries that fit the following profile:\n\n1. **The query is an outlier:** The warehouse performs well for the\n   majority of its queries but struggles with a few long-running\n   queries.\n2. **The query is dominated by table scans:** The performance\n   bottleneck is the part of the query reading large amounts of data.\n3. **The query reduces the number of rows:** After scanning a large\n   amount of data, filters or aggregates reduce the row count\n   significantly.\n4. **The estimated scan time is greater than one minute:** QAS has a\n   small startup overhead, making it ineffective for shorter queries.\n   It only provides a benefit when accelerating scan operations that\n   are estimated to take longer than 60 seconds.\n\nUsing QAS for this specific use case allows you to improve the\nperformance of a volatile workload in a highly cost-effective manner.\n\nSee [Using the Query Acceleration Service (QAS) in the Snowflake documentation](https://docs.snowflake.com/en/user-guide/query-acceleration-service) for more information on QAS\n\n### Summary: A strategic decision framework\n\nChoosing the right elasticity tool begins with understanding your\nworkload and identifying the specific performance problem you are\nfacing. Once you have isolated your workloads into dedicated warehouses,\nuse the following framework to guide your scaling strategy.\n\nStart by asking: **\"What is making my warehouse slow?\"**\n\n* **Symptom:** \"My queries are waiting in a queue because too many are\n  running at once.\"\n  \n+ **Solution:** Your problem is **concurrency** . Use **Horizontal Scaling** by configuring a multi-cluster warehouse. Tune the scaling\n        policy (Standard or Economy) based on the workload’s time\n        sensitivity.\n* **Symptom:** \"A single, complex query is taking too long to complete,\n  even when running by itself.\"\n  \n+ **Solution:** Your problem is **complexity** . Use the diagnostic\n        checklist for **Vertical Scaling** . First, check for disk spilling.\n        If there is none, verify that the query has sufficient parallelism\n        potential and is not skewed before testing it on a larger warehouse.\n* **Symptom:** \"My warehouse is usually fast, but occasionally a single\n  huge query slows everything down.\"\n  \n+ **Solution:** Your problem is **volatility** . Investigate using the **Query Acceleration Service** . Verify that the outlier queries are\n        scan-heavy and meet the one-minute scan time threshold.\n\nPerformance management is not a one-time setup. It is an iterative\nprocess. You should continuously monitor your query performance and\nwarehouse utilization, using these elasticity tools to tune your\nenvironment and maintain the optimal balance between performance and\ncost.\n\n## Test-first design: Building performance into every stage\n\nA high-performing Snowflake environment stems from a deliberate\nstrategy. Performance validation is integrated into every development\nphase. This \"Test-First Design\" approach shifts performance\nconsiderations to a proactive, continuous process, preventing\narchitectural debt and ensuring efficient resource utilization. It\nsupports the Performance by Design philosophy, ensuring that defined\nService Level Objectives (SLOs) are met, validated, and sustained with\noptimal cost efficiency.\n\n### Proactive performance validation\n\nIn Snowflake's consumption model, performance cost is tied to resource\nusage. Deferring performance testing creates technical debt, as flaws\nbecome expensive to fix. This can lead to re-engineering, missed\ndeadlines, or over-provisioning compute, masking inefficiencies at\nsignificant cost.\n\nSnowflake's ease of use and elasticity can lead to an informal approach\nto performance. Vague directives like \"queries should be fast\" are\ninsufficient. Without clear, measurable, agreed-upon performance\nmetrics, designing, validating, or guaranteeing objectives is\nimpossible.\n\nProactive performance validation addresses these challenges head-on by:\n\n* **Mitigating Technical Debt:** Identifying and correcting design flaws\n  early, when they are cheapest to fix.\n* **Ensuring Predictable Outcomes:** Translating business expectations\n  into quantifiable performance objectives that guide design and\n  development.\n* **Optimizing Cost Efficiency:** Balancing performance needs with\n  resource consumption to achieve the best price-for-performance.\n* **Reducing Risk:** Preventing performance problems before production\n  deployment.\n\n#### Formalizing performance: KPIs, SLOs, and baselines\n\nSuccessful performance validation starts with clearly defined\nexpectations. Vague expectations cannot be tested, measured, or\noptimized. Formalize performance requirements using Key Performance\nIndicators (KPIs) and Service Level Objectives (SLOs) to establish\nbaselines for testing and operational monitoring. This process extends\nthe Workload Requirements Agreements (WRA) detailed in the Performance\nby Design guidance.\n\n**Key Performance Indicators (KPIs) for Snowflake Workloads:**\n\n* **Data Freshness (Latency):** The elapsed time from when data is\n  generated in source systems to when it is available for querying in\n  Snowflake.\n  \n+ _Example SLO:_ \"99% of all point-of-sale transaction data must be\n        available in the SALES\\_ANALYTICS table within 5 minutes of the\n        transaction occurring.\"\n* **Query Latency (Response Time):** The time taken for a query to\n  execute and return results to the user or application. This is often\n  expressed using percentiles to account for variability.\n  \n+ _Example SLO:_ \"For the EXECUTIVE\\_DASHBOARD\\_WH, P90 (90th\n        percentile) of interactive queries must complete in under 3 seconds,\n        and P99 must complete in under 5 seconds.\"\n* **Throughput:** The volume of data or number of operations processed\n  within a given timeframe, crucial for batch and ingestion workloads.\n  \n+ _Example SLO:_ \"The nightly ETL\\_LOAD\\_WH must ingest and transform 50\n        GB of raw transactional data into aggregated analytical tables\n        within 30 minutes.\"\n* **Concurrency:** The number of simultaneous queries or users the\n  system can support without significant performance degradation or\n  excessive queuing.\n  \n+ _Example SLO:_ \"The primary BI\\_REPORTING\\_WH must seamlessly support\n        100 concurrent users during peak hours (9 AM - 11 AM PST) with\n        average query queue time not exceeding 5 seconds.\"\n* **Cost Efficiency:** The credit consumption relative to the business\n  value or volume of work performed. This is a critical KPI for CDOs and\n  FinOps.\n  \n+ _Example SLO:_ \"The DAILY\\_REPORTING\\_JOB must process 1 TB of raw\n        data at a cost of no more than 100 credits per run, across all\n        associated warehouses.\"\n\n### Establishing baselines:\n\nBaselines represent the expected or target performance. For new\nprojects, these are often derived from:\n\n* **Design document:** The formally agreed-upon SLOs become the initial\n  target baselines.\n* **Synthetic data & Proof-of-Concept (PoC):** Early tests with\n  representative, scaled-down data can provide initial estimates.\n* **Historical data:** For migrations, existing system performance can\n  inform baselines, with adjustments for Snowflake's capabilities.\n* **Industry benchmarks:** Comparisons with similar workloads in\n  comparable environments.\n\nThese formalized metrics transform abstract goals into concrete,\ntestable objectives, laying the groundwork for effective performance\nvalidation.\n\n#### Shifting left: Integrating performance testing into design & development\n\nThe shift-left principle for performance involves embedding testing\nactivities as early as possible in the development lifecycle. This\nensures that performance considerations are an inherent part of the\ndesign and implementation, preventing the accumulation of technical\ndebt.\n\n#### Performance in the design phase:\n\nEven before a single line of code is written, critical performance\ndecisions are made. This phase focuses on architectural review and\nproactive estimation.\n\n* **Architectural Review:** Evaluate data models, ingestion, and access\n  patterns, and transformation logic for performance. Consider the\n  impact of large JOINs on data shuffling or complex views on query\n  execution.\n* **Early Warehouse Sizing Estimates:** Based on workload categorization\n  and concurrency projections, make initial projections for warehouse\n  sizes and multi-cluster configurations. Validate these estimates\n  through subsequent testing.\n* **Query Review:** Analyze proposed high-impact SQL queries or\n  transformation logic. Use EXPLAIN on hypothetical queries to\n  understand the planned execution and identify potential anti-patterns\n  like full table scans or excessive data movement.\n\n#### Performance in the development phase:\n\nAs development progresses, unit-level and integration-level performance\ntesting becomes crucial. Developers must be empowered with the knowledge\nand tools to self-diagnose and optimize their code.\n\n* **Unit Testing for Queries:** Developers should test individual SQL\n  queries, views, or stored procedures with representative (often\n  scaled-down or synthetic) data. The focus here is on the efficiency of\n  the specific logic.\n* **Query Profile and EXPLAIN Plan Mastery:** For developers,\n  understanding the Snowflake [Query Profile](https://docs.google.com/document/d/1LDeFasziRlYL1Z5t9BqJ_MwhECtJHbRDKG5BNWUAuwU/edit?tab=t.lhzra61et1j6) is paramount. It provides a detailed breakdown of query execution,\n  identifying bottlenecks, spilling, data shuffling, and operator costs.\n  \n        + **Critical Early Warning Signs:** Beyond obvious remote spilling,\n        developers should look for:\n        \n        - **High Remote I/O:** Indicates excessive data reads from remote\n                  storage, potentially due to poor filtering or table design.\n        - **Large Data Shuffled:** Signifies inefficient joins or\n                  aggregations causing significant data movement between warehouse\n                  nodes.\n        - **Ineffective Pruning:** Scanning many more micro-partitions than\n                  necessary, suggesting missing WHERE clause filters or suboptimal\n                  clustering.\n* **Dedicated Dev/Test Environments with Data Cloning:** Leverage\n  Snowflake's Zero-Copy Cloning capability to create isolated,\n  full-scale, or representative copies of production data in development\n  and testing environments. This allows developers and testers to run\n  realistic performance tests without impacting production or incurring\n  high costs for duplicate data storage.\n\n#### Comprehensive performance validation: load, scale, and stress testing\n\nBefore promoting any significant change or new workload to production, a\nstructured approach to load, scalability, and stress testing is\nessential. These tests validate the architecture under various\nreal-world and extreme conditions, directly addressing the risks of\ndeploying untested changes and ensuring performance at scale.\n\n#### Load testing:\n\nLoad testing simulates the expected peak concurrent user and query\nvolumes defined in the SLOs. The goal is to verify that the system can\nconsistently meet its performance objectives under anticipated\nproduction loads.\n\n* **Validation:** Confirm that query latency and throughput SLOs are\n  met, and that query queuing is within acceptable limits.\n* **Multi-Cluster warehouse behavior:** Observe how multi-cluster\n  warehouses (in auto-scale mode) respond to increasing load, ensuring\n  clusters spin up and down efficiently according to the chosen scaling\n  policy (Standard for performance-first, Economy for cost-first).\n* **Tools:** Can involve custom scripting, industry-standard load\n  testing frameworks (e.g., JMeter configured for JDBC/ODBC), or\n  specialized Snowflake testing partners.\n\n#### Scalability testing:\n\nScalability testing assesses how the system performs as data volumes\nand/or user growth increase significantly beyond current expectations.\nThis helps identify limitations in the architecture that might not\nappear under typical load.\n\n* **Data volume growth:** Simulate increased data over projected periods\n  (e.g., 6-12 months). This ensures queries maintain performance or\n  degrade predictably, informing long-term warehouse sizing and\n  architectural decisions.\n* **User growth:** Test with more concurrent users than the current\n  peak. This helps understand system capacity and identify potential\n  bottlenecks.\n* **Detection:** Look for non-linear performance degradation, where\n  small load increases lead to disproportionately large performance\n  decreases, often signaling a scaling bottleneck.\n\n#### Stress testing:\n\nStress testing is crucial for identifying the true limits of a system,\nvalidating its resilience, and understanding its failure modes. This\nprocess aims to uncover the performance limitations in the system as\ndesigned, and if necessary correct early – well in advance of\ndeployment.\n\n* **Extreme Load:** Subject the system to sustained, extreme concurrency\n  or data processing volumes.\n* **Warehouse Size** : Determine if a larger warehouse size is needed to\n  handle the workload.\n* **Multi-Cluster Width** : Explore options for horizontal scale-out to\n  distribute the load across multiple clusters.\n* **Analyze query performance:** Identify slow-running queries and\n  optimize their execution plan.\n* **Monitor warehouse usage:** Regularly review warehouse credit\n  consumption and adjust size as needed.\n* **Recovery:** Evaluate how the system recovers after extreme load,\n  ensuring stability and data integrity.\n* **Query Acceleration Service (QAS) Behavior:** Observe how QAS\n  mitigates the impact of unpredictable, outlier queries under stress.\n\n#### Regression testing and CI/CD integration:\n\nTo prevent performance degradation from new features or code changes,\nintegrate performance benchmarks into the Continuous\nIntegration/Continuous Deployment (CI/CD) pipeline.\n\n* **Best practice** : Major code changes should initiate performance\n  tests using representative data.\n* **Early detection:** Identify performance regressions immediately,\n  allowing developers to fix issues before they propagate downstream or\n  reach production.\n* **Controlled environments:** Establish dedicated, isolated test\n  environments (often using [CLONE](https://docs.snowflake.com/en/sql-reference/sql/create-clone) of production data) for consistent and repeatable performance\n  measurements.\n\n#### Performance and cost tradeoffs: the continuous optimization loop\n\nIn Snowflake, every performance decision is inherently a cost decision.\nIgnoring these tradeoffs can lead to overspending, even with a\nperformant system. The Test-First Design philosophy extends into a\ncontinuous optimization loop, where cost is a primary KPI.\n\nBy integrating performance and cost considerations from design to\ncontinuous operation, organizations can build a Snowflake environment\nthat is not only robust and responsive but also financially prudent.\nThis proactive, data-driven approach ensures that the investment in\nSnowflake delivers maximum value while avoiding the common pitfalls of\ndeferred performance management.\n\n## Optimize data architecture and access\n\n### Overview\n\nOptimal performance relies on solid architectural practices. This\nentails implementing and upholding a comprehensive architectural review\nprocess, employing a layered design to efficiently handle complexity,\nand making the best possible design decisions. This process should\nactively gather input from diverse teams involved in business\napplication development, ensuring all business needs are addressed, and\nthe resulting design accommodates a broad spectrum of requirements.\n\nKey components of this process are periodic application performance\nreviews, incorporating data quality validations and data lifecycle\nmanagement. Planning physical data access paths to support peak\nperformance is also crucial. Tools for achieving this include effective\ndata modeling techniques, appropriate data type selections for storing\nvalues, and creating favorable data access pathways.\n\n### Desired outcome\n\nAdhering to sound architectural and data access recommendations helps\nSnowflake applications achieve optimal performance and scalability. This\nproactive approach ensures that the application's foundation is robust,\nenabling it to efficiently handle evolving business demands and large\nvolumes of data. The outcome is a highly responsive application that\ndelivers a superior user experience, while simultaneously minimizing\noperational overhead and resource consumption.\n\nA well-defined architectural review process, coupled with a layered\ndesign, leads to a more maintainable and adaptable Snowflake\napplication. This structured approach simplifies future enhancements,\nbug fixes, and integrations, reducing the total cost of ownership. The\nability to incorporate diverse team input throughout the design phase\nguarantees that the application effectively addresses a broad spectrum\nof business requirements, resulting in a solution that is both\ncomprehensive and future-proof.\n\nFinally, strategic planning for physical data access paths, along with\neffective data modeling and appropriate data type selections, directly\ntranslates to accelerated query execution and improved data integrity.\nRegular performance reviews, including data quality validations and\nlifecycle management, ensure that data remains accurate, accessible, and\noptimized for peak performance. This integrated approach ultimately\nempowers users with timely and reliable insights, driving better\nbusiness decisions and maximizing the value derived from the Snowflake\nplatform.\n\n### Recommendations\n\nThe following list provides actionable recommendations for a database\npractitioner to achieve strong performance in a Snowflake environment:\n\n1. Maintain high data quality\n2. Optimize data models\n3. Carefully choose data types\n4. Refine data access\n\n#### Maintain high data quality\n\nMaintaining high data quality is critical for achieving optimal query\nperformance and deriving reliable business insights within Snowflake. By\nstrategically utilizing Snowflake's inherent capabilities, such as the\ncareful selection of datatypes and the implementation of NOT NULL\nconstraints, you can establish a robust foundation for data accuracy and\nconsistency. This approach not only streamlines development efforts but\nalso significantly improves the efficiency of join operations.\n\nWhile Snowflake offers declarative constraints like PRIMARY KEY, UNIQUE,\nand FOREIGN KEY, it's crucial to understand that these are not actively\nenforced by the database. As a result, ensuring true data integrity,\nuniqueness, and accurate referential relationships requires external\nenforcement. You can achieve this by designing application logic or\nintegrated data integration processes. Such external validation is\nindispensable for optimizing query execution, guaranteeing the\ntrustworthiness of analytical results, and ultimately maximizing the\nvalue of your data in Snowflake.\n\n#### Optimize data models\n\nOptimizing data models can enhance Snowflake's performance. When\ndesigning schemas, consider the impact of very wide tables. While\nSnowflake is efficient, extensive wide table schemas can increase query\ncompilation time, particularly for complex queries or deep view/CTE\nhierarchies. For short, interactive queries, this impact can be more\nnoticeable. As an alternative, consider VARIANT for numerous scalar\nelements, or VARCHAR structured as JSON, utilizing PARSE\\_JSON().\nHowever, avoid filtering or joining on scalar attributes in complex\nVARIANT/VARCHAR values, as this can hinder pruning effectiveness.\n\nStrategic denormalization is another valid technique, especially for\nanalytical workloads. By pre-joining and storing relevant data together,\nyou reduce the number of join operations, benefiting from Snowflake's\ncolumnar storage and micro-partitioning. This co-locates frequently\naccessed attributes, improving data locality and enhancing\nmicro-partition pruning for faster query execution and optimized\nresource consumption.\n\n#### Carefully choose data types\n\nOptimizing application query performance in Snowflake relies on data\ntype selection during schema design. Specifically, the data types of\ncolumns used in filter predicates, join keys, and aggregation keys\nsignificantly influence the query optimizer's efficiency and can\nintroduce computational overhead.\n\nFor optimal performance, use numeric data types for join keys. Temporal\ndata types like DATE and TIMESTAMP\\_NTZ also generally outperform others\nin filtering and join key scenarios due to their numerical nature.\n\nCoordinating data types for common join keys across tables within the\nsame schema is crucial. Mismatched data types require explicit or\nimplicit type casting, which consumes CPU cycles and can reduce the\neffectiveness of dynamic (execution time) pruning, leading to measurable\nperformance degradation, especially on the probe side of a join.\n\nFinally, while collation is sometimes necessary, it introduces\nsignificant performance overhead and should be used sparingly, ideally\nonly for presentation values and never for join keys, as it can\ninterfere with crucial performance features like pruning.\n\n#### Refine data access\n\nOptimizing data access in Snowflake is crucial for achieving peak query\nperformance. Several techniques are recommended, starting with **Pruning** , which minimizes scanned micro-partitions by leveraging\nmetadata. Both static pruning (compile-time, based on WHERE clauses) and\ndynamic pruning (runtime, based on join filters) are utilized. Static\npruning is generally preferred due to its predictability.\n\n[Clustering](https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions) significantly enhances pruning by physically co-locating similar data\nwithin micro-partitions. This reduces the data range for filtered\ncolumns, leading to more efficient micro-partition elimination.\nEffective clustering also benefits join operations, aggregations, and\nDML (UPDATE, DELETE, MERGE) by reducing I/O requirements.\n\nThe [Search Optimization Service](https://docs.snowflake.com/en/user-guide/search-optimization-service) dramatically accelerates various query types, including selective point\nlookups, searches on character data (using SEARCH, LIKE, RLIKE), and\nqueries on semi-structured or geospatial data.\n\n[Materialized Views](https://docs.snowflake.com/en/user-guide/views-materialized) improve performance by pre-computing and storing expensive query\nresults, offering faster access for frequently executed or complex\noperations. They are particularly useful for aggregations and queries on\nexternal tables, with Snowflake handling automatic updates.\n\n[Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-about) also pre-compute query results, but offer continuous refreshment, ideal\nfor BI dashboards needing low-latency data. They leverage Snowflake's\nquery optimizations and can be further enhanced with clustering and\nsearch optimization.\n\nFinally, **Join Elimination** optimizes queries by removing unnecessary\njoins when they don't alter the result set, typically in primary/foreign\nkey relationships where joined columns aren't projected, significantly\nreducing data processing overhead.\n\n* Step-by-step Instruction\n  \n+ A Data-Driven Methodology for Choosing a Snowflake Clustering Key\n* Blog Articles\n  \n    + [Turbo-charge your Data Model with Snowflake’s Join Elimination](https://medium.com/snowflake/turbo-charge-your-data-model-with-snowflakes-join-elimination-4fedc8a47d26)\n    + [Automatic Clustering at Snowflake](https://medium.com/snowflake/automatic-clustering-at-snowflake-317e0bb45541)\n    + [Snowfake Clustering Demystified](https://medium.com/snowflake/snowflake-clustering-demystified-8042fa81289e)\n    + [Snowflake Dynamic Table Complete Guide](https://medium.com/snowflake/snowflake-dynamic-table-complete-guide-6-final-c363aa7e273a)\n\n### Data quality for better performance\n\nMaintaining high data quality is crucial for optimal query performance\nwithin your application workflow. Snowflake offers built-in features for\nautomatic data quality enforcement and declarative capabilities for\nexternal implementations.\n\nOperating on well-structured, high-quality data provides benefits across\nthe data lifecycle, from ingestion to advanced analytics and reporting.\nThese advantages lead to greater efficiency, reduced costs, and more\nreliable business insights.\n\n* **Cleaner SQL code and reduced development effort:** Clean data\n  minimizes defensive coding, simplifying SQL for transparency and\n  maintainability. This allows focus on business logic over data\n  cleansing, accelerating development, reducing support, and freeing\n  engineering resources. Predictable query behavior and fewer errors\n  result.\n* **Faster join performance:** Efficient join operations require\n  equality predicates, clean data, and matching data types. Inconsistent\n  data types or dirty data impede the optimizer, creating performance\n  bottlenecks. Data cleanliness and type consistency enable efficient\n  join algorithms, speeding query execution for complex analytical\n  workloads.\n\n#### Using data types for data quality enforcement\n\nProper datatype selection is a fundamental aspect of robust data quality\npractices. While often overlooked, correct datatype assignment provides\ninherent constraints and optimizations, significantly contributing to\ndata accuracy, consistency, and integrity. For instance, storing\nnumerical values as numbers, rather than strings, prevents invalid\nentries like \"abc\" and enables accurate mathematical operations.\nSimilarly, date/time datatypes ensure chronological order and allow for\nprecise time-based filtering and analysis.\n\nStricter data types offer significant advantages for database\nperformance, primarily by establishing a clear domain of valid values,\nwhich benefits the query optimizer, reduces storage utilization, and\nimproves join performance.\n\n**Optimizing query performance through stricter data types**\n\n* **Improved query optimizer efficiency:** Variability in data values\n  can lead to complex filters and join predicates, hindering query\n  performance. Stricter data types limit this variability, reducing\n  cardinality and enhancing micro-partition statistics.\n* \\*\\*Accurate cardinality estimation:\\*\\*Data statistics are gathered at\n  the micro-partition level. Varying values with the same meaning\n  distort the query optimizer's cardinality estimations. Stricter data\n  types, such as BOOLEAN or DATE, provide precise information, leading\n  to accurate cardinality estimates.\n* **Enhanced performance with numerical data types:** Stricter data\n  types, often numerical, optimize performance. This facilitates binary\n  predicates in filters and joins, resulting in faster database\n  application performance.\n\n**Reduced storage utilization**\n\n* **Efficient data storage:** Numerical data types generally consume\n  less storage due to their inherent efficiency in how computer systems\n  represent and process numbers. Integers and floating-point numbers\n  store in fixed-size memory blocks, with size determining range and\n  precision. This contrasts with variable-length types like strings or\n  complex objects.\n* **Improved data compression** occurs with strictly defined data types,\n  reducing cardinality within micro-partitions. Tighter constraints\n  increase value unification, enabling more efficient compression\n  algorithms. This minimizes storage footprint and accelerates data\n  retrieval and processing due to reduced data volume, optimizing\n  storage and performance in modern data warehousing and database\n  systems.\n* **Physical I/O reduction:** For large tables, storage optimization\n  significantly reduces the physical storage footprint, directly\n  decreasing the number of physical I/O operations for data access and\n  processing. This leads to substantial overall performance improvement.\n  Minimizing disk read/write needs allows systems to complete queries\n  and operations more quickly, enhancing responsiveness and efficiency\n  for users and applications, and optimizing resources for a more\n  efficient data processing pipeline.\n\n**Optimized join performance**\n\n* **Consistent data types for join keys:** For optimal query\n  performance, ensure columns used in equality join conditions have\n  identical data types. Mismatched data types lead to implicit\n  conversions, degrading performance. Meticulous data type assignment\n  during design, matching frequently joined columns across tables, is\n  crucial for a performant database application.\n* **Superior performance of numerical types in joins:** Numerical data\n  types generally offer better performance in equality join conditions\n  compared to VARCHAR.\n* **Temporal data type efficiency:** For optimal performance in join\n  keys and filters, consider using DATE and TIMESTAMP\\_NTZ. These data\n  types do not store timezone information, eliminating runtime\n  adjustments and allowing the optimizer to utilize Min/Max statistics\n  effectively.\n\n#### NOT NULL constraints for data quality enforcement\n\nSnowflake's NOT NULL constraints ensure data quality by requiring\nspecified columns to always contain a value, preventing incomplete or\nmissing data. This establishes a baseline for critical data points,\nreducing the need for later cleansing.\n\nBeyond data quality, NOT NULL constraints enhance query performance. The\noptimizer can make more efficient query plan decisions when a column is\nknown to contain no NULL values, especially in equality join conditions.\nThis allows for more streamlined equality predicates.\n\n**Performance benefits of NOT NULL constraints:**\n\n* **Simplified query logic:** Eliminates the need for separate IS\n  [NOT] NULL predicates, streamlining query construction. You can\n  write cleaner, more concise SQL, reducing complexity and improving\n  readability, maintainability, and debugging. This approach minimizes\n  potential errors, leading to more robust and efficient database\n  operations.\n* **Reduced query overhead:** Snowflake's query optimizer automatically\n  introduces a nullability filter when a nullable column is used in an\n  equality inner join. This additional filter consumes CPU time,\n  resulting in performance overhead that you can avoid by using NOT NULL\n  constraints.\n\n#### The role of primary key and unique constraints\n\nSnowflake supports PRIMARY KEY and UNIQUE constraints for SQL\ncompatibility, but these are not enforced. Maintaining data integrity is\nyour application's responsibility.\n\nExternal enforcement of these constraints is vital for data integrity in\nanalytical workloads, ensuring reliable reporting and correct\naggregations. Validation checks, deduplication, or upsert mechanisms in\nyour data pipelines ensure only valid, unique records are loaded.\n\nBeyond integrity, external enforcement benefits query performance. When\ndata is guaranteed unique externally, Snowflake's query optimizer\ngenerates more efficient execution plans by leveraging metadata and\nstatistics. This avoids unexpected data duplication in joins and\npromotes cleaner query code, reducing complexity.\n\n**Optimizing query performance with primary key and unique constraints**\n\n* **Enhanced query optimizer selectivity:** Enforcing Primary Key and\n  Unique constraints ensures cleaner data, which benefits\n  micro-partition statistics. Accurate record counts and distinct values\n  are crucial for the query optimizer to estimate selectivity precisely,\n  leading to more effective query plans.\n* **Enabling join elimination:** The RELY property, when set for a\n  Primary Key or Unique constraint, is a prerequisite for Snowflake's\n  Join Elimination. This dynamically removes unnecessary joins,\n  improving query performance, and relies on externally enforced\n  uniqueness.\n* **Reduced defensive coding:** External uniqueness enforcement\n  minimizes defensive SQL coding in downstream applications,\n  streamlining development and enhancing application robustness.\n\n#### Foreign key constraints\n\nSnowflake supports foreign key constraints for SQL compatibility but\ndoes not enforce them. Maintaining referential integrity is your\nresponsibility. Enforcing these constraints ensures data consistency and\naccuracy, improving downstream query performance by assuming valid table\nrelationships. This simplifies query logic and streamlines join\noperations, leading to faster, more predictable query execution.\n\n**Performance benefits of foreign key constraints**\n\n* **Snowflake Join Elimination:** Setting the RELY property on a foreign\n  key constraint enables Snowflake's Join Elimination, improving query\n  efficiency by removing unnecessary joins.\n* **INNER JOIN optimization:** When a foreign key is externally\n  enforced, INNER JOINs offer optimizations not available with OUTER\n  JOIN. Use OUTER JOIN only for inconsistent data where foreign key\n  constraints are not strictly enforced.\n\n### Data modeling for performance\n\nOptimizing data models can significantly enhance Snowflake's already\nimpressive performance. Most industry-standard data modeling techniques\nare applicable when designing a schema in Snowflake. The following\nsections explore common and model-specific considerations.\n\n#### Star Schema\n\nThe Star Schema model works well with Snowflake, favoring right-deep\nquery execution plans. This positions the largest fact table on the far\nprobe side of joins. The model also benefits from Join Elimination,\nwhich excludes optional dimensions (joined with OUTER JOIN) if they're\nnot part of the final projections.\n\n**Considerations:**\n\n* **Fact table clustering:** You should consider clustering large fact\n  tables. Clustering key candidates should include frequently used\n  filters, join keys, and a temporal attribute reflecting the natural\n  data ingestion order.\n* **Numerical data types for keys:** Use of numerical data types for key\n  columns involved in equality joins is strongly recommended for optimal\n  performance.\n\n#### Data Vault 2.0\n\n**Considerations:**\n\n* **Numerical keys:** For optimal performance, use numerical keys\n  instead of hash keys (e.g., MD5). Hash keys, being alphanumeric\n  VARCHAR, are less effective for clustering because Snowflake only\n  considers the first five bytes for clustering, ignoring the rest.\n  Their random nature also prevents natural record co-location, leading\n  to higher reclustering costs and suboptimal states.\n* **Strategic table clustering:** Data Vault schemas, with their higher\n  normalization, often involve queries across many large tables. To\n  improve performance, strategically cluster these tables using a shared\n  attribute that is also part of join and filter conditions. This\n  enhances pruning and improves query performance.\n* **Local materializations:** For performance issues when joining\n  multiple large tables, consider limited materializations using Dynamic\n  Tables to boost query execution speed.\n\n#### Very wide tables\n\nQuery compilation time in Snowflake can be affected by very wide tables\n(hundreds of columns). The optimizer processes more metadata at\ncompilation, extending the time from submission to execution. This is\nmore noticeable in complex queries referencing many columns or involving\nintricate logic, especially with deep dynamic views or [CTE hierarchies](https://docs.snowflake.com/en/user-guide/queries-cte) .\nWhile often minor for long-running analytical queries, it can impact\nshort interactive queries.\n\nA balanced schema design is key. Consider using a VARIANT data type for\nnumerous individual scalar data elements, or a large VARCHAR structured\nas JSON. The PARSE\\_JSON() function allows for easy data access.\nSnowflake automatically subcolumnarizes up to a set number of unique\nelements within a VARIANT, though this is not guaranteed and has\nrestrictions.\n\nAvoid combining scalar attributes frequently used for filtering and\njoins into a single VARIANT or VARCHAR, as this can reduce pruning\neffectiveness due to a lack of collected statistics for these complex\nvalues.\n\n#### Denormalization\n\nDenormalization can optimize query performance in Snowflake,\nparticularly for analytical tasks. While a normalized schema ensures\ndata integrity, it often necessitates multiple joins, adding\ncomputational overhead. Strategic denormalization pre-joins relevant\ndata, reducing the need for joins at query time.\n\nThis approach leverages Snowflake's columnar storage and\nmicro-partitioning. By co-locating frequently accessed attributes within\nthe same micro-partition, data locality improves for common queries,\nminimizing I/O operations. Micro-partition pruning is also enhanced,\nallowing the warehouse to efficiently skip irrelevant data during scans.\n\nStrategic denormalization significantly improves query performance and\noptimizes resource usage. By reducing complex joins and leveraging\nmicro-partition pruning, you can design schemas for highly performant\nanalytical workloads. This provides faster access to insights and a more\nresponsive user experience, balancing performance with data integrity.\n\n#### Finding the balance\n\nAchieving the right balance among these concepts requires careful\ntesting. There's no universal guideline or ideal degree of\ndenormalization; each data model and dataset is unique. Therefore,\nextensive testing is crucial to determine the most effective approach\nfor your specific business needs.\n\n### Data type for performance\n\nOptimizing query performance depends on the data type choices made\nduring schema design. The physical data type of a column can\nsignificantly impact the query optimizer's choices and introduce\ncomputational overhead. This is especially true when the column is used\nin filter predicates, join key predicates, or as an aggregation key.\n\n#### Data type recommendations\n\nConsider the following data type recommendations when designing a\nschema:\n\n* **Join key data types:** Use numerical data types for join keys for\n  superior performance. Snowflake often pushes join filters to the probe\n  side, and numerical values create a more robust filter.\n* **Temporal data types:** Understand the performance implications of\n  temporal data types. DATE and TIMESTAMP\\_NTZ generally perform better\n  in filtering and join key scenarios.\n* **Collation considerations:** Use collation only when essential due to\n  its performance impact. Avoid using collation for join keys.\n\n#### Data types and join performance\n\nFor optimal performance, use identical data types for common join keys\nacross tables in the same schema. Type casting of join keys, whether\nexplicit or implicit, introduces performance overhead due to CPU cycles\nand can hinder dynamic pruning.\n\nSnowflake automatically casts columns with lower precision to match the\ndata type of the opposite join key, but this still incurs the same\nperformance disadvantages as explicit casting. When type casting is\napplied to a key on the probe side of a join, it typically leads to\nmeasurable performance degradation. You can observe type casting in a\njoin on the query profile for each join operation.\n\nThe Snowflake query optimizer pushes join filters derived from equality\njoin conditions in inner joins to the probe side. Filters based on\nnumerical values offer superior initial filtering and enhanced\nperformance. DATE and TIMESTAMP\\_NTZ data types are numerical and share\nthese performance characteristics, meaning the impact of non-coordinated\ndata types also applies to them.\n\nSee ​​ [Data Type Considerations for Join Keys in Snowflake](https://medium.com/snowflake/data-type-considerations-for-join-keys-in-snowflake-304d515d2b91) for further exploration of this topic.\n\n#### Temporal data type choices and performance\n\nSnowflake offers various temporal data types, all stored numerically.\nFor optimal **filtering** performance, DATE and TIMESTAMP\\_NTZ are\nsuperior.\n\nThese types are offsets from EPOCH. DATE uses smaller numerical values,\nleading to better performance for equality join keys due to its smaller\nmemory and storage footprint. TIMESTAMP\\_NTZ offers greater precision\nwith only slightly lower performance.\n\nTIMESTAMP\\_TZ and TIMESTAMP\\_LTZ, which include a timezone component, are\nstored using UTC and a dynamic timezone offset. This dynamic calculation\nconsumes CPU cycles and interferes with pruning. While suitable for\npresentation, if precise filtering or joining on timestamp values with\ntimezones is needed, use two columns: one without a timezone (DATE or\nTIMESTAMP\\_NTZ) for pruning, and another with a timezone for accurate\nrepresentation.\n\n#### Collation\n\nCollation introduces measurable performance overhead, especially for\njoin keys. Numerical join keys are faster, regardless of collation. You\nshould restrict collation use to presentation values only for optimal\nperformance.\n\nEven with a collation definition at the DATABASE, SCHEMA, or TABLE\nlevel, data is physically stored without collation. The transformation\nis dynamic upon data access. While column statistics at the\nmicro-partition level reflect the default (no collation) state for\nmin/max values, filters and join keys must adhere to collation rules.\nThis misalignment hinders important performance features like pruning.\n\nSee [Performance implications of using collation](https://docs.snowflake.com/en/sql-reference/collation) for further exploration of this topic.\n\n### Summary\n\nTaking the time to thoughtfully choose data types can make a significant\ndifference in performance, particularly at scale. Data types matter in\nSnowflake, but not always in expected ways. Pay special attention to the\ndata types for join keys, temporal data types, and avoid the use of\ncollations where possible.\n\n## Data access\n\n### Pruning\n\nPruning eliminates unnecessary Snowflake scanning. This efficiency is\nmeasured by the ratio of unscanned to total micro-partitions during\ntable access. For instance, scanning only one out of 100\nmicro-partitions yields 99% pruning efficiency. Pruning decisions are\nbased on the minimum and maximum values stored as metadata for columns\ninvolved in filters or join keys.\n\n#### Static vs dynamic pruning\n\nSnowflake utilizes both static and dynamic pruning. Static pruning,\nwhich occurs during query compilation in the Cloud Services Layer, uses\nWHERE clause filters to optimize queries. Its impact on the execution\nplan is analyzed and factored into cost calculations.\n\nDynamic pruning, performed at runtime by the Virtual Warehouse, employs\njoin filters to prune unnecessary scans. Its effectiveness is not known\nduring compilation, thus it doesn't affect execution plan cost or table\njoin order.\n\nStatic pruning is generally preferred. Regardless, a query profile will\nalways show actual scanning statistics and the number of\nmicro-partitions scanned.\n\n### Clustering\n\nSnowflake performance is significantly enhanced by clustering, which\nminimizes micro-partition scans by co-locating frequently queried\nrecords. The primary goal is to reduce the range of values in\nmicro-partition statistics for columns used in query filter predicates,\nimproving pruning.\n\nTo select an effective clustering key, identify queries needing\noptimization and choose potential candidates based on filters and join\npredicates. Evaluate the cardinality of these candidates to determine an\noptimal combined clustering key cardinality relative to the total number\nof full-size micro-partitions. Finally, test the chosen key on a subset\nof actual data.\n\nClustering also benefits DML operations (UPDATE, DELETE, MERGE). By\nusing a specific clustering key combination, you can facilitate the\nco-location of records frequently modified by the same DML query. This\nreduces the number of micro-partitions affected by DML logic, leading to\nlower physical write I/O and improved query execution performance.\n\n### Search Optimization Service\n\nThe Snowflake [Search Optimization Service](https://docs.snowflake.com/en/user-guide/search-optimization-service) significantly improves query performance for faster response times and a\nbetter user experience. This service helps data consumers, from business\nanalysts to data scientists, gain insights with unmatched speed.\n\nA key benefit is the acceleration of selective point lookup queries,\nwhich return a small number of distinct rows. These queries are vital\nfor applications requiring immediate data retrieval, such as critical\ndashboards needing real-time data for decision-making.\n\nSearch Optimization Service also improves the speed of searches\ninvolving character data and IPv4 addresses using the [SEARCH](https://docs.snowflake.com/en/sql-reference/functions/search) and [SEARCH\\_IP](https://docs.snowflake.com/en/sql-reference/functions/search_ip) functions. This is particularly beneficial for applications relying on\ntext-based queries, such as log analysis or security monitoring, where\nquick identification of specific text patterns or IP addresses is\ncritical.\n\nThe service extends performance enhancements to substring and regular\nexpression searches, supporting functions like [LIKE](https://docs.snowflake.com/en/sql-reference/functions/like) , [ILIKE](https://docs.snowflake.com/en/sql-reference/functions/ilike) ,\nand [RLIKE](https://docs.snowflake.com/en/sql-reference/functions/rlike) .\nThis capability is vital for scenarios requiring fuzzy matching or\ncomplex pattern recognition, allowing for quicker and more comprehensive\ndata exploration without typical performance bottlenecks.\n\nFinally, search optimization delivers substantial performance\nimprovements for queries on semi-structured data (VARIANT, OBJECT, and\nARRAY columns) and geospatial data. For semi-structured data, it\noptimizes equality, [IN](https://docs.snowflake.com/en/sql-reference/functions/in) , [ARRAY\\_CONTAINS](https://docs.snowflake.com/en/sql-reference/functions/array_contains) , [ARRAYS\\_OVERLAP](https://docs.snowflake.com/en/sql-reference/functions/arrays_overlap) ,\nfull-text search, substring, regular expression, and NULL value\npredicates. For geospatial data, it speeds up queries using selected [GEOGRAPHY](https://docs.snowflake.com/en/sql-reference/functions-geospatial) functions. These optimizations are crucial for efficiently handling\ndiverse and complex data structures, ensuring modern applications can\nrapidly query and analyze all data types.\n\n### Materialized views\n\n[Materialized views](https://docs.snowflake.com/en/user-guide/views-materialized) improve query performance by pre-computing and storing data sets, making\nqueries inherently faster than repeatedly executing complex queries\nagainst base tables. This is especially beneficial for frequently\nexecuted or computationally complex queries, accelerating data retrieval\nand analysis.\n\nThese views speed up expensive operations like aggregation, projection,\nand selection. This includes scenarios where query results represent a\nsmall subset of the base table's rows and columns, or when queries\ndemand significant processing power, such as analyzing semi-structured\ndata or calculating time-intensive aggregates.\n\nMaterialized views also offer an advantage when querying external\ntables, which can sometimes exhibit slower performance. By materializing\nviews on these external sources, you can mitigate performance\nbottlenecks, ensuring quicker data access and more efficient analytical\nworkflows.\n\nSnowflake's implementation ensures data currency and transparent\nmaintenance. A background service automatically updates the materialized\nview as base table changes occur, eliminating manual upkeep and reducing\nerrors. This guarantees that data accessed through materialized views is\nalways current, providing consistent and reliable performance.\n\n### Leverage Dynamic Tables for query performance\n\nDynamic Tables boost query performance by materializing data. Unlike\nstandard views that re-execute queries, Dynamic Tables pre-compute and\nstore results, creating continuously refreshed data. This means complex\nqueries are computed once per refresh, not every time a user queries.\n\nThis benefits applications like BI dashboards and embedded analytics,\nwhere low latency is crucial. Directing these applications to a Dynamic\nTable makes end-user queries simple SELECT statements on pre-computed\nresults. This significantly improves performance by bypassing complex\nlogic, leading to quicker execution and better response for many\nconcurrent users.\n\nAs periodically refreshed tables, Dynamic Tables share performance\nadvantages with standard tables. Snowflake automatically collects\nstatistical metadata and applies advanced query optimizations. You can\nfurther optimize Dynamic Tables with features like automatic clustering,\nSearch Optimization Service for improved partition pruning, and\nadditional serverless features such as Materialized Views and the Query\nAcceleration Service.\n\n### Join elimination\n\n[Join elimination](https://docs.snowflake.com/en/user-guide/join-elimination) ,\na powerful Snowflake query optimization, significantly enhances query\nperformance by removing unnecessary joins. This occurs when the\noptimizer determines a join won't change the query result, typically\ninvolving primary/foreign key relationships declared with the RELY\nkeyword. Columns from the \"joined\" table must not be required in the\nfinal projection (e.g., not selected or used in WHERE, GROUP BY, or\nORDER BY clauses that would alter the outcome).\n\nThe primary benefit of join elimination is a substantial reduction in\nprocessed and transferred data, leading to faster query execution and\nlower compute costs. By eliminating a join, Snowflake avoids reading\nunnecessary data and performing the join operation, which is\nparticularly beneficial in complex queries. This intelligent\nsimplification allows Snowflake to focus computational resources on\nessential query components, delivering results more efficiently.\n\n### Data Lifecycle Management\n\nData Lifecycle Management (DLM) is crucial for optimizing Snowflake\nperformance. By setting clear policies for data retention, archiving,\nand deletion, organizations ensure that only actively used data resides\nin frequently accessed objects. This proactive approach minimizes data\nprocessed for queries, leading to faster execution and reduced compute\ncosts. Efficiently managed data also improves micro-partition pruning,\nmaking your dataset more concise.\n\nAs data ages and access declines, keeping seldom-accessed historical\ndata in active tables with many micro-partitions can create performance\noverhead during query compilation. Isolating historical data into\nseparate tables maintains peak query performance for frequently used\ndata, while ensuring full access for analytical purposes. This also\nallows for alternative clustering strategies that benefit analytical\nquery performance. Since data is often transferred in large batches,\nperiodic reordering may be unnecessary. You can choose manual data\nclustering by sorting records in each batch to reduce ongoing automatic\nclustering costs.\n\n## Architect for scalability and workload partitioning\n\n#### Overview\n\nTo achieve highly performant, scalable solutions on Snowflake, fully\nleverage its multi-cluster shared data architecture. This architecture\nseparates compute from storage, allowing independent scaling. Assigning\ndifferent workloads to dedicated virtual warehouses lets you match\ncompute resources to query complexity. This ensures one workload (e.g.,\ndata engineering) doesn't negatively impact another (e.g., a critical BI\ndashboard). Workload isolation typically improves cache hit ratios and\ncompute resource utilization, leading to faster performance and better\nprice-performance.\n\n#### Desired outcomes\n\nAdhering to this principle improves business agility and\ndecision-making. Quickly available data insights help you respond to\nmarket changes and adapt strategic plans. This principle also\ncontributes to greater data operation stability and reliability.\nWorkload isolation prevents poorly performing queries from crippling\nanalytical operations, creating a more robust data platform, increasing\nuser trust, and reducing troubleshooting. Finally, it leads to reduced\noperational costs and simplified performance administration.\nRight-sizing virtual warehouses avoids over-provisioning and\nunder-provisioning. Simplifying administration frees technical resources\nto focus on business advancement.\n\n### Recommendations\n\nHere are some specific and actionable recommendations to architect\nhigh-performance, scalable solutions on Snowflake:\n\n#### Optimize virtual warehouses for cost and performance\n\nOptimizing for cost and performance is a key best practice on Snowflake,\ninvolving the strategic tuning of warehouse-level parameters to match\nworkload needs. Leveraging parameters like AUTO\\_RESUME and AUTO\\_SUSPEND\nis a great way to ensure you're only paying for compute resources when\nqueries are actively running. You often right-size the warehouse's\nt-shirt size to match query complexity, while using multi-cluster\nsettings like MIN\\_CLUSTER\\_COUNT and MAX\\_CLUSTER\\_COUNT allows for\nautomatic scaling to handle any potential ebbs and flows of concurrent\nactivity. Finer control over how clusters scale and handle queries can\nbe achieved with the scaling policy and MAX\\_CONCURRENCY\\_LEVEL parameter,\nwhich helps teams achieve the best price-performance for their specific\nworkload.\n\n#### Implement strategies for handling concurrent queries\n\nEffectively handling concurrent queries is a key architectural\nconsideration on Snowflake, and the preferred approach is using\nmulti-cluster virtual warehouses which automatically scale compute\nresources in response to query load. You can fine-tune this behavior\nwith parameters like MIN\\_CLUSTER\\_COUNT and MAX\\_CLUSTER\\_COUNT to define\nthe scaling boundaries, and MAX\\_CONCURRENCY\\_LEVEL to control the number\nof queries per cluster. For predictable batch workloads, a great\nstrategy is to stagger scheduled jobs to reduce concurrency spikes and\nlessen demand on warehouses. Additionally, isolating large,\nrarely-executed scheduled jobs into a dedicated warehouse is a best\npractice, as it prevents resource contention and allows for programmatic\nresume and suspend to eliminate idle time and save on cost.\n\n#### Utilize Snowflake's serverless features\n\nSnowflake's serverless features abstract away the manual configuration\nof virtual warehouses by leveraging shared compute that is typically\nmetered by the second. In contrast, virtual warehouses are dedicated to\na customer and have a one-minute minimum for billing. This allows for\nbetter utilization of shared compute resources via an economy of scale,\nwhich in turn enables Snowflake to provide excellent price-performance.\nBy leveraging these services, teams can achieve significant compute\nefficiency gains for a variety of specific workloads.\n\nThe Search Optimization Service automatically builds a data structure\nthat drastically reduces the amount of data scanned for highly selective\nqueries. The Query Acceleration Service offloads parts of resource-heavy\nqueries to shared compute pools, which prevents long-running \"outlier\"\nqueries from monopolizing a warehouse. For repeatable, complex\naggregations, the Materialized View Service automatically maintains\npre-computed results, allowing subsequent queries to bypass\nrecomputation entirely. Finally, Serverless Tasks automatically manage\nand right-size the compute for scheduled jobs, eliminating the need for\nmanual warehouse configuration and ensuring efficient credit\nconsumption.\n\n#### Leverage Dynamic Tables for data engineering pipelines\n\nDynamic Tables are a powerful new feature that dramatically simplifies\nthe creation and management of data pipelines on Snowflake. By using a\ndeclarative SQL syntax, they automate the complex incremental DML\noperations required to keep a table up-to-date, eliminating the need for\nmanual orchestration, which can be tedious, suboptimal, and prone to\nerrors. Similar to materialized views, they pre-compute and store query\nresults, which significantly improves the performance of downstream\nqueries. This declarative approach simplifies pipeline development and\nmonitoring, leading to enhanced data engineering productivity and a more\nstreamlined architecture.\n\n## Optimize virtual warehouses for cost and performance\n\n#### Isolate workloads\n\nImplementing workload isolation with multiple virtual warehouses is\ncrucial for optimizing Snowflake performance. This strategy prevents\nresource contention by dedicating separate compute resources to distinct\ntasks, such as isolating long-running ETL from time-sensitive BI\nqueries. It also provides a robust mechanism for cost management and\naccountability, especially for organizations with a single Snowflake\naccount and many business units, by simplifying internal chargeback and\nencouraging teams to optimize their compute usage.\n\n**Examples include:**\n\n* A smaller, multi-cluster warehouse (e.g., X-Small or Small with an\n  upper bound of 5-10 clusters) is ideal for ad-hoc queries from\n  business users and analysts. This setup dynamically scales to meet\n  variable, bursty query loads, ensuring performance objectives are met\n  without over-provisioning and keeping costs in check.\n* A Medium or Large single-cluster warehouse suits data ingestion and\n  data engineering jobs. These jobs often process larger data sets and\n  require more compute, especially with complex transformations.\n* A very large dedicated warehouse (e.g., 2X-Large or higher) is best\n  for complex, high-volume monthly batch jobs. Isolating such jobs\n  prevents negative impact on others and allows for precise\n  right-sizing. You can programmatically resume the warehouse before the\n  job and suspend it afterward, avoiding idle time costs.\n\n#### Use meaningful names for warehouses\n\nUse descriptive names (e.g., BI\\_REPORTING\\_WH, ETL\\_LOADER\\_WH) to make it\neasy for users and administrators to understand the purpose of each\nwarehouse and prevent mixing workloads. This will also make it easier to\nunderstand dashboards and reports that provide insights into performance\nand cost metrics by warehouse name.\n\n#### Leverage auto-resume and auto-suspend\n\nWarehouses typically benefit from auto-resume (default). This allows a\nwarehouse to spin up automatically from a suspended state when a query\nis issued. Disabling this requires manual resumption via an ALTER\nWAREHOUSE RESUME command, leading to:\n\n* a query failing because it's issued against a suspended warehouse.\n* a manually resumed warehouse sitting idle, accruing credits without\n  servicing queries.\n\nConfigure virtual warehouses to automatically suspend after inactivity\n(e.g., 60-300 seconds). This saves credit when not in use, benefiting\nintermittent workloads. However, suspension flushes the data cache,\nwhich avoids expensive remote reads. For a BI dashboard, a longer\nsuspension (e.g., ten minutes) might be better to keep the cache warm.\nFor data engineering, where caching often yields less benefit, a shorter\nauto-suspend interval is often optimal.\n\n#### Right-size your warehouses\n\nStart with a smaller warehouse size and scale up if queries spill or\ntake too long. For slow queries, if stepping up the warehouse size\n(doubling CPU, memory, and SSD) does not roughly halve the query time,\nconsider a smaller size for better resource utilization. If necessary,\nperform performance profiling to identify why it isn't scaling linearly,\noften due to uneven processing.\n\nWarehouse size should primarily be driven by workload complexity, not\naverage or peak concurrency, which is often better handled by\nmulti-cluster warehouses. Increasing warehouse size for concurrency,\nespecially peak, typically results in over-provisioning and increased\ncredit consumption without a corresponding price-performance ratio.\n\nSpilling occurs when a fixed-size resource (memory or local SSD) is\nfully utilized, requiring additional resources. Local spilling moves\nmemory contents to SSD; remote spilling moves SSD contents to remote\nstorage. Excessive spilling, particularly remote, often signals an\nundersized warehouse for its assigned workload. The QUERY\\_HISTORY view\nin ACCOUNT\\_USAGE provides insights into local and remote spilling via\nbytes\\_spilled\\_to\\_local\\_storage and bytes\\_spilled\\_to\\_remote\\_storage\nattributes. These metrics can identify warehouses for upsizing due to\nexcessive spilling.\n\n#### Enable multi-cluster warehouses (MCWs)\n\nFor high-concurrency BI reporting, enable [multi-cluster warehouses](https://docs.snowflake.com/en/user-guide/warehouses-multicluster) (Enterprise Edition and above). This allows Snowflake to automatically\nscale out clusters when queries queue and scale in when load decreases,\nensuring consistent performance during peak times without\nover-provisioning.\n\nThe SCALING\\_POLICY parameter can be configured to influence scale-out\nbehavior with STANDARD (default) and ECONOMY values. While STANDARD\nsuits most workloads, ECONOMY can conserve credits by establishing a\nhigher bar for spinning up new clusters, leading to increased queuing.\nThis tradeoff may be worthwhile for cost-optimized workloads.\n\nMulti-cluster warehouses often provide better price-performance for\n\"bursty\" workloads by elastically adjusting clusters. However, if\nadditional compute is required for complex individual queries,\nincreasing cluster size is more appropriate, as a single query executes\nagainst a single cluster.\n\n#### Drive accountability via warehouse-level chargeback\n\nIn larger organizations with a single Snowflake account, workload isolation promotes accountability and effective administration. Virtual warehouses are often the dominant cost driver, so dedicating specific warehouses to business units provides a clear mechanism for internal chargeback. This drives cost control and empowers each team to manage their own compute usage. This simplifies governance, as each business unit can manage its dedicated warehouse with local control, minimizing the risk of one team's actions affecting another's workload. Always secure access to warehouses via an effective RBAC (Role-based access control) strategy to ensure only authorized users/roles/teams have access.\n\n#### Query Acceleration Service (and Scale Factor)\n\nEnable [Query Acceleration](https://docs.snowflake.com/en/user-guide/query-acceleration-service) on your warehouse to parallelize parts of qualifying queries, reducing\nthe need to over-provision for \"outlier\" queries with heavy table scans.\nThe **QUERY\\_ACCELERATION\\_MAX\\_SCALE\\_FACTOR** parameter defines the\nsupplemental compute available. While the default is eight, begin with a\nlower value (even one) to validate its benefits before increasing it to\noptimize price-performance.\n\n## Implement strategies for handling concurrent queries\n\n#### Tune max concurrency level\n\nFor multi-cluster warehouses, fine-tune concurrency with the\nMAX\\_CONCURRENCY\\_LEVEL parameter, which sets the maximum queries per\ncluster. Reducing this value can benefit resource-intensive queries by\nproviding more compute power, potentially improving throughput by\nminimizing concurrency overhead and optimizing resource utilization.\n\nThis parameter's \"unit of measurement\" is not strictly a query count,\nbut rather \"full, cluster-wide queries.\" While no single query counts\nfor more than one unit, some, like stored procedure CALL statements,\ncount for less. A CALL statement, being a single control thread, uses\nonly a fraction of a cluster's \"width,\" representing a fraction of a\nunit. Thus, multiple CALL statements might aggregate into one unit,\nmeaning a MAX\\_CONCURRENCY\\_LEVEL of eight could support more than eight\nconcurrent CALL statements. Snowflake automatically manages these\ncalculations for optimized resource utilization.\n\nWhile query concurrency can exceed MAX\\_CONCURRENCY\\_LEVEL due to lower\ndegrees of parallelism, fewer queries might be assigned to a cluster\nbefore queueing due to memory budgeting. Each query has a memory metric\nthat determines if a cluster can accept it without exceeding its budget.\nIf exceeded, the query is not assigned. If no clusters are available,\nthe query queues, awaiting capacity. Larger or more complex queries\nincrease the memory metric, reducing net concurrency for warehouses\nprocessing heavyweight queries.\n\nThis parameter also applies to single-cluster warehouses. Reducing its\nvalue will cause queueing. Without a multi-cluster warehouse (MCW),\nqueries will wait for capacity to free up when others complete, rather\nthan a new cluster spinning up. However, for some scenarios, tuning this\nvalue for a single-cluster warehouse may be appropriate.\n\n#### Stagger scheduled jobs\n\nFor predictable, high-concurrency workloads like scheduled batch jobs,\nstaggering their start times effectively avoids concurrency spikes,\nallowing more efficient use of warehouse resources. This prevents jobs\nfrom running simultaneously and competing for resources, mitigating the\nnegative impacts of concurrency spikes.\n\nSince most jobs begin with scan-based activity (I/O), even slight\nstaggering of heavyweight queries can prevent \"stampedes\" that arise\nfrom simultaneous dispatch. While Snowflake and cloud storage are highly\nscalable, making staggering not a strict requirement, it is a best\npractice for optimal resource utilization.\n\nThis principle also applies to concurrent queries dispatched from an\napplication. Introducing a slight delay, sometimes via a small random\noffset for each query, provides similar benefits to staggering scheduled\njobs.\n\n#### Monitor for queuing\n\nWhen queries queue on a warehouse, it's a signal that the warehouse\nmight be under-provisioned for the workload, and either a larger\nwarehouse size or a multi-cluster warehouse (with an increased maximum\ncluster count) would be beneficial. You can use Snowflake's [QUERY\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) view in the ACCOUNT\\_USAGE share to monitor query queuing and identify\nconcurrency issues. There are three queueing-related attributes in the\nview:\n\n* queued\\_provisioning\\_time\n  \n+ Warehouse start up (resume from suspended; less relevant here)\n* queued\\_repair\\_time\n  \n+ Warehouse repair (less common; not relevant here)\n* queued\\_overload\\_time\n\nWhen coupled with the following attributes that are also included in the\nview:\n\n* warehouse\\_id / warehouse\\_name\n* warehouse\\_size\n* start\\_time\n* end\\_time\n\nIt is fairly straightforward to identify warehouses that require\nadditional compute resources, and also whether this is periodic or\nsustained.\n\n#### Special considerations for concurrent stored procedures\n\nSnowflake stored procedures, invoked via a CALL statement, coordinate\nchild SQL statements. The CALL statement itself uses minimal warehouse\nresources, but child statements, executing as separate queries, can\nfully utilize a warehouse cluster. By default, child statements run on\nthe same warehouse as the parent.\n\nIn environments with extensive stored procedure use and high CALL\nstatement concurrency, parent and child statements can intertwine on a\nsingle warehouse. While lightweight parent statements are easily\nassigned, high concurrency can lead to child statements queuing. A\nparent statement cannot complete until all its children do. This can\ncause a subtle deadlock: many parent statements wait for children, but\nsome children are blocked due to insufficient warehouse capacity.\n\nTo prevent this, isolate parent and child statements on different\nwarehouses. This is achieved by having the parent issue a USE WAREHOUSE\ncommand before launching child statements. This simple strategy\neffectively avoids deadlocks in high stored procedure concurrency.\nAdditionally, using two warehouses allows each to be optimally\nconfigured for its specific purpose.\n\n## Utilize Snowflake's serverless features\n\n### Automatic Clustering Service\n\n[Automatic clustering](https://docs.snowflake.com/en/user-guide/tables-auto-reclustering) in Snowflake is a background service that continuously manages data\nreclustering for tables with a defined and enabled cluster key.\nClustering table data effectively, based on columns or expressions, is a\nhighly effective performance optimization. It directly supports pruning,\nwhere specific micro-partitions are eliminated from a query's table\nscan. This micro-partition elimination provides direct performance\nbenefits, as I/O operations, especially across a network to remote\nstorage, can be expensive. By clustering data to align with common\naccess paths, MIN-MAX ranges for columns become narrower, reducing the\nnumber of micro-partitions scanned for a query.\n\n[Defining the correct cluster key](https://docs.snowflake.com/en/user-guide/tables-clustering-keys) is crucial for achieving excellent query performance and optimizing\ncredit consumption. While the Automatic Clustering Service incurs credit\ncharges to maintain a table's clustering state, when implemented\ncorrectly, overall credit consumption should be lower, often by an order\nor two of magnitude, compared to not managing clustering. Best practices\nfor cluster key selection include:\n\n* The order of expressions in a multi-part cluster key is crucial. The\n  first expression, or \"leading edge,\" has the greatest impact on\n  clustering.\n* Currently, Snowflake ignores any expressions beyond the fourth in a\n  cluster key. Do not define a cluster key with more than four\n  expressions.\n* Even with four or fewer expressions, not all may affect clustering.\n  Clustering co-locates rows into micro-partitions to achieve narrow\n  MIN-MAX ranges. If the composite cluster key's cardinality exceeds the\n  number of micro-partitions, further co-location is not possible.\n* For activity-based tables that continually grow (e.g., sales, web\n  clicks), a date/time-based leading edge is often effective. Unless\n  older data is restated, micro-partitions stabilize quickly, reducing\n  churn. This benefits Search Optimization, Materialized Views, Dynamic\n  Tables, and Replication through reduced churn.\n* If a column has high cardinality but is valuable as an access path,\n  consider reducing its cardinality with a system function. Ensure the\n  function maintains the original column's partial ordering, segmenting\n  values into \"buckets\" that preserve relative order. To reduce the\n  cardinality of:\n  \n    + A string (VARCHAR), use LEFT().\n    + A timestamp, use TO\\_DATE to truncate to a DATE, or DATE\\_TRUNC for\n        other units (HOUR, WEEK, MONTH, etc.).\n    + An integer, use TRUNC(<COL>, <BUCKETSIZE>).\n* For string expressions in a cluster key, Snowflake rarely considers\n  more than five characters (never more than six). For multi-byte\n  non-Latin characters, this can be as low as one. Therefore, COL\\_X as a\n  cluster key expression is functionally equivalent to LEFT(COL\\_X, 5)\n  for Latin characters. To consider more characters, stitch them\n  together as multiple expressions, e.g., CLUSTER BY (LEFT(COL\\_X, 5),\n  SUBSTR(COL\\_X, 6, 5)) for the first ten characters.\n* Do not use functions like MOD, as they do not result in narrow MIN-MAX\n  ranges. For example, MOD(COL, 10) treats 1, 11, and 100000001 as\n  equivalent for clustering, but results in a very wide MIN-MAX range,\n  significantly diminishing pruning effectiveness.\n\n### Search Optimization (SO) Service\n\nFor highly selective queries on large tables, enable the [Search Optimization Service](https://docs.snowflake.com/en/user-guide/search-optimization-service) .\nThis serverless feature creates a persistent data structure (index) for\nefficient micro-partition pruning, significantly reducing scanned data\nand improving query performance. It is effective for point lookups and\nsubstring searches, even on semi-structured data.\n\nConsider Search Optimization indexes when clustering alone doesn't meet\nperformance objectives. Search Optimization enhances pruning without\nrequiring narrow MIN-MAX ranges, using Bloom filters to identify\nmicro-partitions that do not contain matching rows.\n\nClustering often affects Search Optimization's effectiveness.\nEstablishing a cluster key before assessing Search Optimization's\npruning is often advantageous, as altering clustering can change pruning\neffectiveness.\n\nSearch Optimization builds incrementally on registered micro-partitions.\nAn index may cover 0% to 100% of active micro-partitions. If a query is\nissued with partial coverage, the index is leveraged for covered\nmicro-partitions, providing an additional pruning option beyond MIN-MAX\npruning. Uncovered micro-partitions miss this secondary pruning. Heavy\ntable churn can diminish Search Optimization's effectiveness, so\nminimize unnecessary churn when the service is configured.\n\n### Materialized View (MV) Service\n\nFor frequently-run queries with repeated subquery results, such as\ncomplex aggregations, use a materialized view. The [Materialized View Service](https://docs.snowflake.com/en/user-guide/views-materialized) automatically refreshes the view when the underlying base table changes,\nallowing subsequent queries to use pre-computed results for faster\nperformance.\n\nSimilar to Search Optimization, Materialized View maintenance is\nincremental on active micro-partitions. If a table experiences extensive\nmicro-partition churn, performance benefits diminish. Queries\nencountering non-covered micro-partitions revert to the base table,\nimpacting performance.\n\nFor MVs that aggregate data (via DISTINCT or GROUP BY), results are\nstored per source micro-partition, requiring a query-time final\naggregation. This final aggregation is usually negligible, but if\nsignificant, consider alternative data materialization via manual ETL or\nDynamic Tables, balancing data access and latency.\n\n### Query Acceleration Service (QAS)\n\nTo speed up portions of a query workload on an existing warehouse,\nenable the Query Acceleration Service. This service automatically\noffloads parts of the query processing work, particularly large scans\nwith selective filters, to shared compute resources. It's ideal for ad\nhoc analytics or workloads with unpredictable data volumes, as it\nreduces the impact of resource-hungry \"outlier queries\" without needing\nto manually scale up the entire warehouse.\n\nIt is important to understand that QAS is limited to a subset of\noperation types within the processing of a query. Specifically, there\nare two primary scenarios where QAS can be employed within a query\ntoday:\n\n* For a COPY INTO, it can participate in the entire operation for a\n  subset of the source files that are being ingested. It can scan,\n  transform, and write out rows.\n* For SELECT and INSERT (including CTAS), QAS can only participate in\n  the following operation types:\n  \n    + Table Scan\n\nIf any other operation type is encountered within a query execution\nplan, QAS will share its partial result with the warehouse and will not\nparticipate in that query any further. This includes (but is not limited\nto):\n\n* Join\n* Sort\n* Insert (write data to new micro-partitions)\n\n### Serverless tasks\n\nFor scheduled, single-statement SQL or stored procedure-based workloads,\nconsider using serverless tasks instead of user-managed warehouses. With\nserverless tasks, Snowflake automatically manages the compute resources,\ndynamically allocating the optimal warehouse size for each run based on\npast performance. This eliminates the need for manual warehouse sizing,\nautomates cost management, and ensures that your data pipelines run\nefficiently without consuming credits when idle.\n\n## Leverage Dynamic Tables for data engineering pipelines\n\n### Improve downstream query performance\n\nDynamic Tables enhance query performance for downstream consumers by\nmaterializing data. Unlike standard views, which re-execute their\nunderlying query each time they are called, Dynamic Tables pre-compute\nand store query results, providing a periodically refreshed version of\nthe data. This means complex, performance-intensive queries, such as\nthose with multiple joins and aggregations, are computed once during the\nrefresh cycle, rather than every time the result is queried.\n\nThis significantly benefits applications like BI dashboards and embedded\nanalytics, where low latency is crucial. By directing these applications\nto a fast-responding Dynamic Table instead of a complex view, end-user\nqueries become simple SELECT statements on pre-computed results. This\ncan substantially boost performance, as the query engine bypasses\ntransformation logic, leading to quicker query execution and a more\nresponsive analytical experience for many concurrent users.\n\nAs materialized tables, Dynamic Tables offer powerful performance\nadvantages similar to standard tables. They collect the same statistical\nmetadata automatically, and Snowflake's advanced query optimizations\napply when Dynamic Tables are the query source. Effectively, from a\nquery perspective, Dynamic Tables function as materialized standard\ntables.\n\nAdditionally, Dynamic Tables are compatible with serverless features for\nfurther performance enhancements. You can apply Automatic Clustering for\nbetter partition pruning during data scanning. A Search Optimization\nService index can be built to accelerate scans for highly selective\nfilter criteria. Other serverless features, like Materialized Views and\nthe Query Acceleration Service, can also be layered on top of a Dynamic\nTable for even greater optimization.\n\n### Enhance organizational productivity\n\nDynamic Tables simplify data engineering by allowing you to define a\ntable's desired final state with a CREATE DYNAMIC TABLE AS SELECT...\nstatement, rather than complex MERGE or INSERT, UPDATE, DELETE commands.\nThis declarative approach removes the burden of managing incremental\nlogic, which is often error-prone.\n\nBeyond initial pipeline creation, Dynamic Tables automate data\norchestration by intelligently performing full or incremental refreshes,\napplying only necessary changes. This eliminates the need for manual\npipeline construction using STREAMS and TASKS, and facilitates chaining\nDynamic Tables for complex dependencies without external tools.\n\nAdministration and monitoring are also streamlined. Snowflake's\ndedicated Snowsight graphical interface provides a visual representation\nof your pipeline's status, refresh history, and dependencies,\nsimplifying troubleshooting and governance. This \"single pane of glass\"\nidentifies bottlenecks or failures quickly, replacing manual log\ninspection or metadata queries.\n\nUltimately, Dynamic Tables' declarative syntax, automated refresh logic,\nand integrated monitoring transform data engineering. You can configure\nthem to run on a set schedule or on-demand. By automating low-level\ntasks, Dynamic Tables enable data teams to focus on building new data\nproducts and driving business value, resulting in a more efficient and\nagile data platform.\n\n### [Best practices for Dynamic Tables](https://docs.snowflake.com/en/user-guide/dynamic-table-performance-guide)\n\n**Simplify the core definition:** Avoid overly complex single Dynamic\nTable definitions. For clarity and performance, it's a best practice to\nkeep the number of joined tables in a single definition to a minimum,\nideally no more than five. For more complex transformations, chain\nmultiple Dynamic Tables together, with each one building on the\nprevious.\n\n**Leverage dependency chaining:** Use the DOWNSTREAM keyword to specify\ndependencies between Dynamic Tables, which ensures that a dependent\ntable is only refreshed after its upstream source has been updated. This\nalso allows Snowflake to optimize processing by permitting refreshes on\na Dynamic Table to be deferred until it is required by a downstream\nDynamic Table.\n\n**Set the TARGET\\_LAG parameter appropriately:** The [TARGET\\_LAG parameter](https://docs.snowflake.com/en/user-guide/dynamic-tables-target-lag) controls the maximum delay of the data in the Dynamic Table relative to\nits source. It's crucial to set this value to the highest number that\nstill meets your business requirements. Setting a TARGET\\_LAG that is\nlower than necessary can cause more frequent, less efficient refreshes,\nwhich increases credit consumption and resource usage without providing\nany tangible business benefit.\n\n**Avoid high DML churn on source tables**\n\nExcessive DML operations like UPDATE or DELETE on a source table can\nsignificantly impact the performance of its dependent Dynamic Table.\nThis is because the underlying change data capture (CDC) mechanism has\nto process a higher volume of changes, which requires more compute and\ncan lead to slower refresh times. Designing data pipelines to be\nappend-only when possible is a key best practice for maximizing\nefficiency.\n\n**Utilize the IMMUTABLE WHERE clause**\n\nUse the **IMMUTABLE WHERE** clause in your Dynamic Table definition to\nspecify a predicate for immutable source data. This allows the refresh\nservice to avoid re-scanning historical data that is guaranteed not to\nchange, which can significantly improve the efficiency and performance\nof incremental refreshes.\n\n**Manage backfills with BACKFILL FROM clause**\n\nTo load historical data into a new Dynamic Table, or one that is\nundergoing a change to its schema definition, use the BACKFILL FROM\nparameter in the CREATE DYNAMIC TABLE syntax. This allows you to specify\na timestamp from which the initial refresh should begin, providing a\nsimple, declarative way to backfill the table with historical records.\nIt eliminates the need for manual, complex backfilling scripts.\n\n**Right-size the refresh warehouse:** Ensure the warehouse specified for\nthe Dynamic Table refresh is appropriately sized for the workload. A\nlarger warehouse can process large refreshes more quickly, while a\nsmaller one may be more cost-effective for frequent, smaller incremental\nupdates.\n\n**Monitor refresh history:** Regularly monitor the refresh history of\nyour Dynamic Tables using the DYNAMIC\\_TABLE\\_REFRESH\\_HISTORY view. This\nprovides insights into the refresh performance, latency, and costs,\nallowing you to fine-tune your definitions and warehouse sizes for\ncontinuous optimization.\n\n## Implement continuous performance monitoring and optimization\n\n### Overview\n\nEstablish comprehensive monitoring and logging to identify performance\nbottlenecks. Proactively optimize the system by analyzing queries and\nworkloads to adapt to evolving requirements.\n\n#### Desired outcome\n\nEffective performance monitoring and optimization yield a system with\npredictable performance and stable costs, even as data and applications\nevolve. This foundation supports future growth in data volume and query\nactivity, enabling better root cause analysis and cost management.\n\n### Recommendations\n\nPerformance optimization often follows the 80/20 rule, where a minimal\ninvestment can yield significant improvements. While definitive rules\nare elusive due to diverse workloads, these recommendations establish a\nfoundation for a performant, cost-stable workload on the Snowflake\nplatform.\n\n#### Understand how Snowflake works\n\nA technical understanding of the Snowflake architecture is crucial for\ndiagnosing performance issues and identifying optimization\nopportunities. While most workloads perform well without deep expertise,\nperformance tuning requires a greater investment in understanding the\nplatform's fundamentals.\n\n#### Measure performance\n\nObjective measurement is a prerequisite for meaningful optimization.\nWithout a clear and objective baseline, success cannot be defined,\nleaving initiatives without direction.\n\n#### Identify bottlenecks\n\nFocus optimization efforts on significant bottlenecks at the macro\n(application), workload, or micro (query) level. Addressing\ninconsequential components yields minimal overall improvement.\n\n#### Proactively optimize\n\nAddress performance proactively, not just reactively. Regular analysis\nof performance patterns and slow queries can prevent emergencies.\nEstablishing a performance baseline is key to tracking trends and\ndetermining if performance is improving or degrading over time.\n\n#### Thoroughly test performance optimizations\n\nPredicting query performance is difficult; therefore, testing is\nessential. Validate that changes improve the target workload without\nnegatively impacting others. Testing proves whether a proposed solution\nhas the desired effect.\n\n#### Meticulously validate AI suggestions\n\nAI can be a useful tool, but suggestions require critical validation.\nTest AI-driven recommendations thoroughly against platform knowledge and\nreliable sources before implementation.\n\n### Trade-offs & considerations\n\n#### Cost versus performance\n\nThe relationship between cost and performance is not linear. While some\noptimizations increase cost, others can reduce it. For example, a query\nthat takes 10 minutes on an XS warehouse and spills to remote storage\nmight complete in one minute on a Small warehouse without spilling. In\nthis case, selecting a larger, more expensive warehouse results in a\nfaster, cheaper query execution.\n\n#### Monitoring versus analysis\n\nSnowflake's rich performance data favors targeted analysis over constant\nmonitoring. A proactive approach to reviewing and comparing performance\ndata is required. The optimal balance depends on workload criticality;\nsensitive operations may benefit from continuous analysis, while stable\nprocesses can rely on retrospective analysis.\n\n#### Statistical rigor versus agility\n\nAccurate performance insights require statistically sound metrics.\nHowever, this rigor can slow down the optimization process. Balance the\nneed for statistically valid data with the need for agile\ndecision-making. Less rigorous measurements may suffice for initial\ntroubleshooting, with comprehensive testing reserved for validating\nmajor changes.\n\n#### Factors impacting performance\n\nNumerous factors can impact query and workload performance. This list is\na starting point for consideration.\n\n* **Data volume** : Significant changes in data volume can impact\n  performance, especially if a threshold is crossed that causes\n  operations like remote spilling.\n* **Data distribution** : Data value distribution affects micro-partition\n  scanning and overall performance. Changes can alter query plans and\n  the amount of data processed.\n* **Data clustering** : Clustering, the physical grouping of data in\n  micro-partitions, is critical. Poor clustering increases\n  micro-partition scanning, degrading performance. Maintaining good\n  clustering improves partition pruning.\n* **Data model** : The logical data organization profoundly impacts\n  performance. Poorly designed models can lead to inefficient query\n  plans, while overly wide tables can also present challenges.\n* **Query syntax** : The structure of a SQL query significantly affects\n  its execution plan. Inefficient syntax, like poor JOIN conditions, can\n  prevent the optimizer from choosing an efficient plan.\n* **View definitions** : A view's underlying definition impacts the\n  performance of queries that use it. A complex view can hide\n  performance problems and introduce computational overhead with every\n  execution.\n* **Data share changes** : Changes to shared data, such as new columns or\n  modified clustering keys, can impact consumer query performance.\n* **Search Optimization changes** : The Search Optimization Service can\n  accelerate point lookups. Adding or removing it can have a massive\n  impact on the performance of applicable queries.\n* **Use of Hybrid Tables** : Hybrid Tables blend OLTP and OLAP\n  capabilities. Their performance characteristics for analytical queries\n  may differ from standard tables, requiring specific workload\n  considerations.\n* **Warehouse characteristics** : Virtual warehouse configuration has a\n  direct impact on performance. This includes:\n  \n    + Virtual warehouse size\n    + Multi-Cluster Warehouse (MCW) settings\n    + MAX\\_CONCURRENCY\\_LEVEL\n\n    + Snowpark-optimized warehouse settings\n    + Warehouse generation (Gen1 vs. Gen2)\n* **Caching** : Snowflake uses metadata, result set, and warehouse data\n  caching. A valid cache hit accelerates query execution, while a cache\n  miss can be slower.\n* **Concurrency** : Multiple queries competing for resources can slow\n  individual execution times. Proper warehouse sizing and MCW\n  configuration can prevent concurrency bottlenecks.\n* **Queuing due to load** : When a warehouse is at capacity, new queries\n  are queued, increasing total execution time. This indicates a need for\n  a larger warehouse or more clusters.\n* **Queuing due to locks** : Locks ensure data integrity during DML\n  operations. While minimal, lock contention can occur and must be\n  resolved to maintain a responsive system.\n* **Snowflake updates** : Regular platform updates, which often include\n  performance enhancements, can occasionally alter query optimizer\n  behavior and execution plans, potentially impacting specific\n  workloads.\n\nThis list is not exhaustive, and some of these things are much more\nlikely than others.\n\n### Measuring performance\n\n#### Overview\n\nRigorous measurement is essential for achieving performance targets,\nconfirming improvements, and preventing performance issues. Snowflake\nprovides a wealth of performance data, which reduces the need for\nconstant monitoring. This data only gains meaning through consistent\nreview and comparative analysis.\n\n**Desired outcome**\n\nUnderstanding current performance allows identification of anomalies and\ntheir origins. It can also help understand how time spent within\nSnowflake relates to other parts of an application or pipeline.\n\n### Recommendations\n\nA robust performance measurement strategy is built on a clear purpose, a\ndefined scope, and a consistent process for evaluating critical\nworkloads. These recommendations are essential to properly measuring\nperformance:\n\n#### Clarify reasons for measuring performance\n\nIdentifying the reason for measuring performance guides the effort and\ntime invested. The common reasons for measuring the performance of a\nquery or workload include establishing a baseline, comparing to a\nbaseline, troubleshooting, controlling costs, and understanding\ncontributions to Service Level Objectives (SLOs) or performance targets.\n\n#### Define measurement scope\n\nDefine the dimensions of the analysis. Pinpoint the object of\nmeasurement, since performance in Snowflake can be evaluated at\ndifferent levels of granularity.\n\nUnderstanding the granularity of performance measurement guides the\ntechniques and tools employed.\n\n#### Identify priority workloads and queries\n\nThe choice of what to measure depends on the reason for performance\nanalysis and the desired granularity. There are different considerations\nfor measuring single queries, workloads , and workloads with\nconcurrency. The overall goal is to focus effort where it will have the\nmost impact.\n\n#### Select metrics\n\nCarefully choose and measure performance metrics in Snowflake to ensure\neffective analysis.There are many valid metrics, including overall query\nexecution time, query cost, and metrics like files scanned. Statistical\nsignificance is important in measuring performance.\n\n#### Define your measurement cadence\n\nEstablish a clear schedule and triggers for measuring performance.\nUnderstanding when to measure performance allows for focused effort and\nhelps avoid over-reacting to perceived performance problems. The goal is\nto provide a valid comparison point and understand the impact of various\nfactors on performance.\n\n### Clarify reasons for measuring performance\n\n#### Overview\n\nEstablishing a clear reason for measuring performance from the outset is\nparamount for understanding what data to examine and which strategic\ndirections to pursue.\n\n**Desired outcome**\n\nDefining the \"why\" behind a performance initiative guides the level of\neffort and ensures resources are directed to the most impactful areas.\n\n#### Recommendations\n\nCommon reasons for measuring the performance of a query or workload\ninclude:\n\n* **Establishing a baseline:** A baseline provides a reference point for\n  comparing future changes or problems. This comparison helps determine\n  the value of potential modifications and the importance of addressing\n  an issue. It is crucial to gather metrics in a statistically sound\n  manner to avoid skewed results from transitory differences.\n  \n    + **Query-Level baseline:** A single query is the simplest unit for a\n        baseline. Measure a query along several dimensions and record the\n        values for later comparison. Establishing a query-level baseline\n        often involves accessing information already stored by Snowflake.\n        Recording a timestamp and query ID for lookup in QUERY\\_HISTORY is\n        often sufficient, though longer retention may require storing this\n        data in a separate table. A baseline can be as simple as execution\n        time and micro-partitions scanned, or more detailed using\n        GET\\_QUERY\\_OPERATOR\\_STATS().\n    + **Workload-level baseline:** A workload is a logical grouping of\n        queries, often identified by query tags, user, or warehouse. Since\n        the number of queries in a workload can change, a simple\n        query-by-query analysis is often insufficient.\n* **Comparing to a baseline:** Once established, a baseline is used to\n  evaluate the impact of optimizations. When making changes, measure\n  performance against the baseline to quantify the improvement. This can\n  be applied to both individual queries and entire workloads.\n* **Troubleshooting:** A primary reason to collect performance data is\n  to make a query or workload faster. To quantify an improvement, one\n  must first measure the initial performance. Understanding the scale of\n  a performance problem helps determine the appropriate level of urgency\n  and resource allocation. Without a baseline, it is impossible to\n  determine if performance has degraded, or by how much.\n* **Understanding contributions to SLOs:** The performance of a query or\n  workload is often critical to a larger application or pipeline with a\n  Service Level Objective (SLO). Understanding the contribution of each\n  component is essential for overall performance improvement and for\n  assessing the severity of any performance issues.\n\n### Define measurement scope\n\n#### Overview\n\nA successful performance measurement strategy begins with a clear\ndefinition of the scope of the analysis. It is important to identify the\nspecific object of measurement, as performance in Snowflake can be\nevaluated at several different levels of granularity.\n\n**Desired outcome**\n\nThe level of granularity chosen for performance measurement will guide\nsubsequent decisions, such as the appropriate measurement techniques.\nThis clarity is essential for directing performance tuning efforts in\nthe most effective way.\n\n#### Recommendations\n\nPerformance can be measured at various levels of granularity. The most\ncommon granularities for performance measurement are:\n\n* **Single query:** The performance of a single query is often a\n  critical factor. A single query can be the most resource-intensive\n  component of a pipeline or application, or it may be executed with\n  such high frequency that its individual performance has a significant\n  cumulative impact. Analyzing individual queries can also reveal\n  patterns in performance or data modeling that can be applied to\n  improve a broader set of queries. A focus on individual query\n  performance is generally a necessary part of any effort to improve\n  overall performance or reduce costs.\n* **Workload:** In many cases, optimizing the performance of a single\n  query is not enough. An improvement to one query can sometimes lead to\n  a degradation in the performance of another. To achieve significant\n  performance gains, it may be necessary to analyze the performance of\n  all queries within a pipeline or application. A comprehensive approach\n  to workload analysis, including the establishment of a baseline and a\n  well-defined testing methodology, is essential for effective\n  regression testing and overall performance improvement.\n* **Concurrency:** Concurrency testing is a specialized area of testing\n  that is most likely to be something that is needed for high\n  concurrency and low latency applications. It is rare to engage in\n  concurrency testing to simply reduce costs or in the pursuit of better\n  metrics on paper. Concurrency testing generally involves using a\n  third-party app, such as JMeter, to run queries in specific patterns.\n\n#### Concurrency testing anti-patterns\n\nAs mentioned above, there are many potential anti-patterns for\nconcurrency testing. Here are several to avoid:\n\n* **Failing to simulate a realistic ramp-up:** Starting a concurrency\n  test with a sudden, massive spike of concurrent queries rather than a\n  gradual ramp-up. A sudden burst does not allow time for proper scaling\n  at the virtual warehouse and the cloud services layer, unless those\n  layers are specifically configured to handle bursting workloads. Such\n  a pattern can trigger queuing and auto-scaling. This pattern may not\n  be representative of how users naturally log on and begin their work\n  throughout the day, and may have negative performance patterns that\n  are not representative of a real workload.\n* **Not validating the test environment:** Failing to ensure the testing\n  environment (including data volume, data distribution, and data\n  clustering) is a true representation of the production environment. A\n  test run on a small, non-representative dataset may yield results that\n  are not applicable to the real-world workload.\n* **Failing to use separate warehouses for distinct workloads:** Lumping\n  together disparate workloads—such as ETL and BI dashboards—on the same\n  warehouse. This is an anti-pattern for performance in general, but in\n  concurrency testing, it can skew results and hide the true bottlenecks\n  for each workload type.\n* **Using a single, large warehouse for all workloads:** Testing a mix\n  of small, fast-running queries and large, resource-intensive queries\n  on a single, oversized warehouse. This can lead to resource contention\n  and queuing for the smaller queries, which could be handled more\n  efficiently on a smaller, separate warehouse. It also wastes credits\n  by over-provisioning for the smaller workloads.\n* **Ignoring the multi-cluster warehouse's scaling policies:** Not\n  understanding the difference between the \"Standard\" and \"Economy\"\n  scaling policies. Testing with \"Economy\" can lead to high queuing as\n  Snowflake attempts to fully utilize existing clusters before spinning\n  up new ones, while a workload requiring low latency might need the\n  \"Standard\" policy's more rapid scaling.\n* **Using an un-representative test workload:** Simulating concurrency\n  with a small, static set of queries that do not accurately reflect\n  real-world user behavior. A good concurrency test needs a mix of query\n  types, complexities, and data access patterns that mirror the\n  production workload.\n* **Overlooking the impact of caching:** Not accounting for the various\n  cache layers in Snowflake (result, warehouse, and metadata cache). A\n  poorly designed test may repeatedly hit the same cached result, giving\n  a false impression of performance under load. A true concurrency test\n  must be designed to invalidate the cache and simulate a realistic mix\n  of cold and warm queries. Disabling the result set cache is a bad idea\n  for concurrency testing - if the test is properly designed, the result\n  set cache will be used appropriately.\n* **Not establishing a baseline without concurrency:** Running a\n  concurrency test without first measuring the performance of individual\n  queries in isolation. Without this baseline, it is impossible to\n  attribute performance degradation to concurrency versus other factors,\n  like query complexity or data changes.\n* **Treating Snowflake like an OLTP database:** For optimal performance\n  in Snowflake, some level of denormalization is often advantageous.\n  Grouping DML operations logically, rather than executing a large\n  number of individual INSERT, UPDATE, DELETE, or MERGE operations that\n  affect only a few rows, tends to be more cost-efficient and perform\n  better.\n\n### Identify priority workloads and queries\n\n#### Overview\n\nAfter establishing the reason for performance analysis and the required\ngranularity [of what you’re measuring](https://docs.google.com/document/d/1LDeFasziRlYL1Z5t9BqJ_MwhECtJHbRDKG5BNWUAuwU/edit?tab=t.l1pikt37o4sn) ,\nthe next step is to identify the specific workloads or queries that will\nbe the focus of the investigation.\n\n**Desired outcome**\n\nTargeting performance measurements allows for the efficient allocation\nof time and resources to the areas where they will have the most\nsignificant impact.\n\n#### Recommendations\n\nThe selection of a query or workload for performance measurement may be\napparent based on the reason for measuring performance. However, if a\nspecific target has not yet been identified, the following\nrecommendations can help guide the selection process based on the chosen\nlevel of granularity.\n\n#### Single query\n\nIdentifying a single query for performance analysis from a large volume\nof queries requires a systematic approach. Queries can be prioritized\nbased on various dimensions, including:\n\n* **Cost:** The most expensive queries in terms of resource consumption.\n* **Expectations:** Queries that deviate from expected performance\n  norms.\n* **Criticality:** Queries that are essential to the functionality of an\n  application, dashboard, or pipeline.\n* **User impact:** Queries that directly affect the end-user experience.\n* **Historical performance:** Queries that have been identified as\n  problematic in the past.\n* **Complexity:** Queries with a high degree of complexity.\n* **Data access:** Queries that scan a large number of micro-partitions.\n* **Performance variance:** Queries that exhibit a wide range of\n  execution times.\n\nIt is important to focus on queries that are meaningful and relevant to\nthe overall performance goals. Analyzing random queries is unlikely to\nyield significant improvements. The insights gained from optimizing a\nsingle query can often be applied to other queries with similar\npatterns.\n\nThe QUERY\\_HISTORY view in ACCOUNT\\_USAGE is a valuable resource for\nidentifying queries that meet specific criteria. Aggregating data on the\nQUERY\\_PARAMETERIZED\\_HASH can help identify patterns across multiple\nexecutions of a query with different parameters, which is a common\ncharacteristic of workloads that support BI dashboards.\n\n#### Workload\n\nThe process of selecting a workload for performance analysis is similar\nto that of selecting a single query. However, the decision is often\ninfluenced by business-related factors, such as the impact of the\nworkload on a critical metric. A workload is typically chosen for\nanalysis because it is a source of user frustration or because it is\nassociated with high costs, rather than solely because of its technical\ncomplexity.\n\nA workload can be defined as a group of queries, and the criteria for\ngrouping can vary. Queries can be grouped by user, virtual warehouse, or\napplication. The use of query tags can be a helpful mechanism for\nidentifying and tracking the queries that constitute a specific workload\nin QUERY\\_HISTORY and other monitoring tools.\n\n#### Concurrency\n\nConcurrency testing is a specialized form of performance analysis that\nis typically reserved for applications with high-concurrency and\nlow-latency requirements. It is not generally employed for the sole\npurpose of cost reduction or for improving metrics that are not directly\nrelated to concurrency. Concurrency testing usually involves the use of\nthird-party tools, such as JMeter, to simulate specific query patterns.\n\n### Select metrics\n\n#### Overview\n\nAfter deciding to measure performance, the next step is to determine\nwhich metrics to use and how to apply them. Using inappropriate metrics\nor applying them incorrectly can be counterproductive. A thoughtful and\ndeliberate approach to choosing and measuring metrics ensures a better\nreturn on the time and effort invested in performance analysis.\n\n#### Recommendations\n\nThere are two main aspects to consider when selecting metrics for\nperformance measurement: what to measure and how to measure it. The\nchoices made will depend on the overall goals of the performance\nanalysis and the specific context of the measurement. It is crucial to\nensure that the measurement methodology is valid.\n\n#### What to measure\n\n* **Query execution time:** The most common metric for analyzing the\n  performance of a single query or a workload is its execution time. In\n  Snowflake, it is generally not beneficial to focus on individual\n  components of the execution time, such as compilation time, as a\n  longer compilation time can sometimes result in a shorter overall\n  execution time.\n* **Query cost:** There is a nuanced relationship between performance\n  and cost in Snowflake. A query that runs for 2 minutes on an X-Small\n  warehouse may cost the same as the same query running for 1 minute on\n  a Small warehouse. It is even possible for a query to be both faster\n  and cheaper on a larger warehouse. Rigorous testing is essential to\n  determine if changes that might seem to increase costs, such as using\n  a larger warehouse or clustering a table, actually lead to a reduction\n  in overall cost.\n* **Files scanned and other metrics:** Other metrics should also be\n  considered. For example, the number of files scanned can be a key\n  performance indicator, especially when working with large datasets.\n  Minimizing the amount of data that needs to be processed is often a\n  crucial step in improving performance.\n\n#### How to measure\n\nOnce a metric has been selected, it is important to decide how to\nmeasure it. A single execution of a query or workload is not a\nsufficient basis for a reliable measurement due to the many variables\nthat can affect performance, such as warehouse startup times and cache\npopulation. For a representative sample, it is recommended to run each\nquery or workload at least 10 times, preferably at different times of\nthe day and on different days. The results can then be aggregated to\nprovide a single, comparable number. Here are a few examples of common\naggregations:\n\n* **Mean/average:** The average of a metric across multiple executions\n  provides a good understanding of what to expect. While the average\n  will not be the exact value for every execution, it is a simple and\n  widely understood calculation that can be used to put performance in\n  the context of a larger pipeline or application.\n* **P90 or higher:** When meeting a Service Level Objective (SLO) is the\n  primary goal, edge cases become more important. The 90th percentile\n  (P90) can provide insight into what to expect when conditions are not\n  ideal. This is particularly useful in concurrency testing but can also\n  be applied to other types of performance analysis.\n* **Sum across multiple executions:** The sum of a metric across\n  multiple executions can be a useful way to incorporate both the\n  expected performance and the variance into a single metric, although\n  it does not provide an easy way to understand the performance in\n  context.\n\n#### Performance metric anti-patterns\n\nWhen measuring performance, it is important to avoid common pitfalls\nthat can lead to inaccurate conclusions.\n\n* **Basing analysis on a single query execution** \\- Basing performance\n  conclusions on a single execution of a query should be avoided, as\n  this approach lacks statistical significance. Numerous transient\n  factors can cause a single execution to run unusually fast or slow,\n  leading to flawed assumptions based on a best-case or worst-case\n  scenario rather than on a representative performance sample.\n* **Focusing on isolated execution phases** \\- Avoid focusing on isolated\n  components of query execution, such as compilation time. A longer\n  compilation phase can sometimes result in a more optimized execution\n  plan and a shorter total execution time. The primary metric for\n  analysis should be the total elapsed time for the query, not the\n  duration of its individual phases.\n\n### Define your measurement cadence\n\n#### Overview\n\nWhile there are often specific motivations for measuring performance,\nsuch as investigating a reported issue, establishing a regular cadence\nfor performance measurement is a critical practice. Even basic\nmeasurements, like recording a timestamp and query ID, become more\nvaluable when collected as part of a structured approach.\n\n**Desired Outcome**\n\nA well-defined cadence for performance measurement allows for the\nfocused allocation of time and effort, ensuring that performance\nanalysis is conducted when it is most impactful.\n\n#### Recommendations\n\nPerformance should be measured at several key moments to ensure a\ncomprehensive understanding of the system's behavior. The following\ncatalysts and cadences are recommended:\n\n* **Establish a baseline:** A performance baseline provides a reference\n  point for all future comparisons. Without a baseline, it is difficult\n  to objectively determine if performance has degraded, or to quantify\n  the severity of a performance issue. A defined measurement\n  methodology, combined with a baseline, allows for a straightforward\n  comparison to identify and assess performance problems.\n* **Before and after significant changes** : It is important to validate\n  or re-establish a performance baseline before any significant change,\n  such as an application upgrade, a data model alteration, or a\n  substantial data change. This ensures a recent and valid point of\n  comparison. Following the change, performance should be measured again\n  to assess the impact and to establish a new baseline for future\n  analysis.\n* **Testing potential changes:** Snowflake's table cloning capabilities\n  provide an efficient and cost-effective way to test the impact of\n  potential changes. This is particularly useful for regression testing,\n  where the goal is to determine if a change will negatively affect\n  performance before it is implemented in production.\n* **On a periodic interval:** Regular performance checks are valuable\n  even in the absence of known issues. Periodic measurement can help\n  identify gradual performance degradation caused by changes in data or\n  other subtle factors within an application or pipeline. For workloads\n  with annual peak periods, it may be beneficial to retain performance\n  data, such as that found in QUERY\\_HISTORY, for longer than the\n  standard retention period to allow for year-over-year comparisons.\n\n### Identifying bottlenecks\n\n#### Overview\n\nIdentifying performance bottlenecks is a critical step in any\noptimization process. To ensure that time and resources are used\neffectively, it is essential to focus on the parts of a query or\nworkload where improvements will have the most significant impact on\noverall performance metrics.\n\n#### Recommendations\n\nA multi-faceted approach is recommended for identifying performance\nbottlenecks. It's best to begin with a holistic understanding of the\napplication's context to focus optimization efforts where they will have\nthe most impact. Analysis can then be performed at two levels using\nSnowflake's native tools.\n\n#### Tools for query-level analysis\n\nFor granular analysis of individual queries, several tools provide\ndetailed insights into the execution plan and performance\ncharacteristics.\n\n* **Query profile:** The query profile offers\n  a detailed visual representation of a query's execution. It is the\n  primary tool for identifying common issues such as poor\n  micro-partition pruning, join explosions, remote spilling, and the\n  specific operators consuming the most resources. The profile displays\n  actual execution statistics after a query has completed (or a partial\n  view while it is running) but is not accessible programmatically. Data\n  is available for 14 days.\n* **GET\\_QUERY\\_OPERATOR\\_STATS():** For programmatic access to the data\n  found in the query profile, use the [GET\\_QUERY\\_OPERATOR\\_STATS()](https://docs.snowflake.com/en/sql-reference/functions/get_query_operator_stats) function. This table function returns detailed, operator-level\n  statistics in a structured format, making it ideal for automated\n  analysis or for capturing performance data for long-term storage and\n  comparison.\n* **EXPLAIN:** The [EXPLAIN](https://docs.snowflake.com/en/sql-reference/sql/explain) command provides a look at a query's logical execution plan _before_ it runs. It is useful for understanding the optimizer's choices but\n  should be treated as an estimate, as it does not contain the actual\n  run-time statistics available in the query profile.\n\n#### Tools for workload-level analysis\n\nAnalyzing performance across an entire workload requires a broader\nperspective. The following ACCOUNT\\_USAGE views are invaluable for\nidentifying trends and high-impact queries.\n\n* **QUERY\\_HISTORY:** The [QUERY\\_HISTORY view](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) is the foundational tool for workload analysis. It contains a\n  comprehensive record of executed queries, allowing for the\n  identification of performance patterns. It is particularly useful for:\n  \n    + Finding queries with specific characteristics, such as long queue\n        times (queued\\_overload\\_time).\n    + Comparing workload performance against a baseline, especially when\n        using query tags for easy filtering.\n    + Aggregating metrics for queries that have the same structure but\n        different literal values, using the query\\_parameterized\\_hash.\n* **Specialized views:**\n  \n    + [AGGERGATE\\_QUERY\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/aggregate_query_history) :\n        This view is comparable to QUERY\\_HISTORY when using hybrid tables.\n    + [QUERY\\_ATTRIBUTION\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_attribution_history) **:** This view is designed to provide a more accurate picture of the\n        compute cost of a query, taking concurrency into account. It is\n        excellent when the primary concern is attributing costs to specific\n        queries or workloads.\n    + [QUERY\\_INSIGHTS](https://docs.snowflake.com/en/sql-reference/account-usage/query_insights) **:** This view proactively identifies queries that match known\n        performance anti-patterns. It can help pinpoint queries to\n        investigate, but it is important to note that not every insight\n        requires immediate action. See [the documentation for details on the types of insights currently available and what they mean](https://docs.snowflake.com/en/user-guide/query-insights) .\n    + [TABLE\\_QUERY\\_PRUNING\\_HISTORY](https://docs.snowflake.com/sql-reference/account-usage/table_query_pruning_history) and [COLUMN\\_QUERY\\_PRUNING\\_HISTORY](https://docs.snowflake.com/sql-reference/account-usage/table_query_pruning_history) **:** These views provide targeted data on micro-partition pruning\n        effectiveness. Analyzing this data can help identify tables that may\n        benefit from a clustering key. See [Tim Sander’s article on using the pruning history views](https://medium.com/snowflake/supercharging-snowflake-pruning-using-new-account-usage-views-52530b24bf2e) for more information.\n\n### Meticulously validating AI suggestions\n\n#### Overview\n\nAI can be a source of suggestions for performance improvement. However,\nall suggestions require critical evaluation and comprehensive testing\nbefore implementation. An unverified recommendation can waste time and\ncredits or even degrade performance.\n\n#### Recommendations\n\n* **Apply foundational knowledge** Evaluate AI suggestions against a\n  foundational understanding of the Snowflake platform. For complex or\n  costly recommendations, consult the Snowflake documentation, conduct\n  further research, or engage with Snowflake staff like Sales Engineers,\n  Solutions Architects, and if appropriate, Technical Support.\n* **Consider data timeliness** AI training data can be months or years\n  old, failing to reflect Snowflake's rapid evolution. A suggestion\n  might be correct based on old information but suboptimal given new\n  features.\n* **Test thoroughly** Performance improvements involve time and cost.\n  Thoroughly test all suggestions in a controlled environment to verify\n  they meet business requirements. Proper validation prevents unexpected\n  issues, saving resources and improving outcomes.\n\n## Conclusion\n\nA strong understanding of the Snowflake platform provides the necessary\ncontext to assess whether an AI suggestion warrants further\ninvestigation and testing.\n\nFor additional information, see [Optimizing performance in Snowflake](https://docs.snowflake.com/en/guides-overview-performance)\n\nUpdated Nov 21, 2025\n\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\n\\*\n\n\\*\n\n\\* Country United States Canada United Kingdom Germany France Australia Japan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo The Democratic Republic of The Cook Islands Costa Rica Cote D'Ivoire (Ivory Coast) Croatia (Hrvatska) Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Heard and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jordan Kazakhstan Kenya Kiribati Korea Republic of (South) Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia Federated States of Moldova Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Occupied Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Reunion Romania Russian Federation Saint Helena Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and The South Sandwich Island Spain Sri Lanka Suriname Svalbard and Jan Mayen Islands Swaziland Sweden Switzerland Taiwan Tajikistan Tanzania United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Venezuela Viet Nam Virgin Islands (British) Virgin Islands (U.S.) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe\n\n\\*\n\nAdd me to the list to receive dedicated product updates and general availability emails.\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\n\nSubscribe Now\n\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n* [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n* [Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n* [Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n* [Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n* [Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n* [Technology](https://www.snowflake.com/en/solutions/industries/technology/)\n\nLearn * [Resource Library](https://snowflake.com/en/resources/)\n* Live Demos\n* [Fundamentals](https://www.snowflake.com/en/fundamentals/)\n* [Training](https://www.snowflake.com/en/resources/learn/training/)\n* [Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n* [Snowflake University](https://learn.snowflake.com/en/)\n* [Developer Guides](https://www.snowflake.com/en/developers/guides)\n* [Documentation](https://docs.snowflake.com/)\n\n* [Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n* [Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\n* Cookie Settings\n* [Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Legal](https://www.snowflake.com/en/legal/)\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"],"full_content":null},{"url":"https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/","title":"FinOps on Snowflake: Built-In Cost and Performance Control","publish_date":null,"excerpts":["Data for Breakfast Around the World\n\nDrive impact across your organization with data and agentic intelligence.\n\nregister now\n\npricing options\n\noverview\n\ncost & performance optimization\n\npricing calculator\n\n###### Resources\n\nPricing calculator overview Pricing calculator FAQs Snowflake Performance Index\n\n[_Image Source_](https://squadrondata.com/org-impact-comparison-spark-based-saas-vs-snowflake/)\n\n###### COST MANAGEMENT AND PERFORMANCE OPTIMIZATION\n\n# FinOps on Snowflake\n\nTime is money – save both with Snowflake.\n\nexplore the economic impact of snowflake\n\n## Save time on platform management. Spend it on what matters more.\n\n## Go from painstaking configurations to a proven, fully-managed service\n\nSince its founding in 2012, Snowflake has provided automated cluster management, maintenance and upgrades — all without downtime — so you can spend time on valuable data projects\n\nGet **out-of-the-box governance and security through Snowflake Horizon Catalog** without extra configurations or protocols\n\n## Go from piecemeal dashboards to built-in cost & performance management\n\n* Get granular visibility, control and optimization of Snowflake spend through a unified Cost Management Interface .\n* Check query performance easily to proactively save on costs.\n* Automatically benefit from regular rollouts of performance improvements across all workloads.\n\n## Maximize your Snowflake spend\n\n* Add flexibility in how you use funds committed in your Snowflake Capacity contract.\n* Deploy partner solutions faster by simplifying finance and procurement processes.\n* Bundle your spend to increase your buying power with Snowflake and partners.\n\nlearn more\n\n###### OUR CUSTOMERS\n\n## Saving time on platform admin. Getting to market faster.\n\nTravelpass CTC Natwest\n\nTravel and Hospitality “Now, we aren’t so focused on how to build things. We are focused more on what to build.” Dan Shah  \nManager of Data Science Read the story * **1 week** for 130 Dynamic Tables to be in production after migration\n* **65%** cost savings switching from Databricks to Snowflake\n\nRead the case study Financial Services “Now with fewer ephemeral failures and higher visibility in Snowflake, we have a platform that’s much easier and cost-effective to operate than managed Spark.” David Trumbell  \nHead of Data Engineering, CTC Read the story * **1st** data availability deadline was hit everyday for the 1st time\n* **54%** cost savings switching from managed Spark to Snowflake\n\nRead the case study Financial Services “The speed at which we’ve delivered wouldn’t have been possible with other providers.” Kaushik Ghosh Dastidar  \nHead of ESG Cloud Solutions, NatWest Read the story * **6x** reduction in onboarding time from 3 months to 2 weeks\n* **$750K** saved in salaries & staff training costs\n\nRead the case study\n\n[Resource #### Snowflake Joins the FinOps Foundation Snowflake joins The FinOps Foundation as a Premier Enterprise Member to provide thought leadership and set industry financial best practices. Read more](https://www.finops.org/members/snowflake/)\n\nResource #### Snowflake Pricing Calculator Curious about Snowflake pricing? Our Snowflake pricing calculator shows credit usage, warehouse costs, and total expenses. Access calculator\n\nGuide #### Definitive Guide to Managing Spend in Snowflake Learn about considerations for consumption models such as Snowflake's, frameworks for better managing spend, and more. Get the guide\n\n## Even More To Explore\n\n#### Snowflake Documentation\n\nAccess documentation on Managing Costs and Optimizing Performance in Snowflake.\n\nRead about Managing Costs\n\nRead about Optimizing Performance\n\n#### On-Demand Cost Governance Training\n\nLearn how to successfully examine, control, and optimize Snowflake costs.\n\nRegister Now\n\n#### Professional Services\n\nEngage Snowflake’s Professional Services for expert advice on optimizing your use of Snowflake.\n\nDiscover Professional Services\n\n#### Priority Support\n\nLearn more about how our Priority Support team can help you reduce consumption spend through performance monitoring, observability, and management.\n\nLearn about Priority Support\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\nstart for free\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\n\\*\n\n\\*\n\n\\* Country United States Canada United Kingdom Germany France Australia Japan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo The Democratic Republic of The Cook Islands Costa Rica Cote D'Ivoire (Ivory Coast) Croatia (Hrvatska) Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Heard and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jordan Kazakhstan Kenya Kiribati Korea Republic of (South) Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia Federated States of Moldova Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Occupied Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Reunion Romania Russian Federation Saint Helena Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and The South Sandwich Island Spain Sri Lanka Suriname Svalbard and Jan Mayen Islands Swaziland Sweden Switzerland Taiwan Tajikistan Tanzania United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Venezuela Viet Nam Virgin Islands (British) Virgin Islands (U.S.) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe\n\n\\*\n\nAdd me to the list to receive dedicated product updates and general availability emails.\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\n\nSubscribe Now\n\nIndustries * Advertising, Media & Entertainment\n* Financial Services\n* Healthcare & Life Sciences\n* Manufacturing\n* Public Sector\n* Retail & Consumer Goods\n* Technology\n\nLearn * Resource Library\n* Live Demos\n* Fundamentals\n* Training\n* Certifications\n* Snowflake University\n* Developer Guides\n* Documentation\n\n* Privacy Policy\n* Site Terms\n* Communication Preferences\n* Cookie Settings\n* Do Not Share My Personal Information\n* Legal\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")\n\n\\* Private preview, <sup>†</sup> Public preview, <sup>‡</sup> Coming soon"],"full_content":null},{"url":"https://www.flexera.com/blog/finops/snowflake-native-apps/","title":"Snowflake Native Apps 101: Build and monetize data apps (2026)","publish_date":"2026-01-27","excerpts":["Flexera Open Primary Navigation\n\n* Solutions\n  \n  Business challenge\n  \n    + Software renewals and audits\n    + Software license management and optimization\n    + SaaS spend management\n    + Cloud cost management\n    + IT asset lifecycle management\n    + CMDB data quality\n    + Accurate IT inventory\n    + Security and regulatory risk management\n    + Sustainable IT\n    + AI-powered transformation\n    + Public sector\n  \n  Spend management by vendor\n  \n    + IBM\n    + Oracle\n    + Microsoft\n    + SAP\n    + VMWare\n    + ServiceNow\n    + AWS\n    + Salesforce\n    + BMC\n    + Adobe\n  \n  [Flexera is a Leader in 2025 cloud financial management tools](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\n  \n  Discover recognized CFM vendors to watch in the 2025 Gartner® Magic Quadrant™\n  \n  [View report](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\n* Products\n  \n  Flexera One\n  \n    + IT Visibility\n    + ITAM\n    + Snow Atlas\n    + Cloud License Management\n    + SaaS Management\n  \n    + FinOps\n    + Cloud Cost Optimization\n    + Cloud Commitment Management\n    + Container Optimization\n    + Virtual Machine Optimization\n    + Data Cloud Optimization\n  \n    + Application Readiness\n    + Security\n    + Integrations\n    + Technology Intelligence Platform\n    + All Products\n  \n  [Introducing Flexera One SaaS Management](https://www.flexera.com/products/flexera-one/saas-management)\n  \n  Discover comprehensive SaaS visibility for taming SaaS sprawl, wasted spend and compliance risks.\n  \n  [Book your personalized demo](https://www.flexera.com/products/flexera-one/saas-management)\n* Success\n  \n  Customer Success\n  \n  Services & Training\n  \n    + Services\n    + Training\n  \n  Support\n  \n    + [Flexera support portal](https://community.flexera.com/s/support-hub)\n    + [Flexera product documentation](https://docs.flexera.com)\n    + [Snow product documentation](https://docs.snowsoftware.io/)\n  \n    + Technology Intelligence Awards\n    + [Flexera community](https://community.flexera.com/s/)\n  \n  [2025 Technology Intelligence Awards](https://info.flexera.com/ITAM-REPORT-State-of-IT-Asset-Management)\n  \n  The results are in—see how our 2025 winners and honorable mentions are shaping the future of Technology Intelligence.\n  \n  [See the winners](https://www.flexera.com/customer-success/awards)\n* Resources\n  \n  Resources\n  \n    + Webinars\n    + Videos\n    + Datasheets\n    + Whitepapers & reports\n  \n    + Blog\n    + Case studies\n    + Events\n    + Analyst research\n    + Glossary\n    + Demos & trials\n    + Business value calculator\n  \n  [Flexera 2026 IT Priorities Report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)\n  \n  AI ROI, sustainability, cost and risk: Discover the latest IT trends shaping tomorrow’s IT landscape in Flexera’s 2026 IT Priorities Report.\n  \n  [View report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)\n* About\n  \n  Company\n  \n    + About\n    + Careers\n    + Contact us\n    + Leadership\n  \n  Partners\n  \n    + Partner program\n    + Partner locator\n  \n  Press center\n  \n    + Press releases\n    + Articles\n    + Awards\n  \n  Social responsibility\n  \n    + ESG\n    + Belonging and inclusion\n  \n  [The Flexera 2025 State of the Cloud Report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)\n  \n  How are GenAI rising cloud costs, security and sustainability shaping your cloud strategies in 2025?\n  \n  [View report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)\n\nCustomers Open External Links\n\n* [Community](https://community.flexera.com/)\n* [Product login](https://app.flexera.com/login)\n* [Spot login](https://console.spotinst.com/auth/signIn)\n* [Partner Portal](https://partnerhub.flexera.com/)\n\nSearch\n\nBook a demo\n\n1. Home\n2. Blog\n3. [FinOps](https://www.flexera.com/blog/finops/)\n4. Snowflake Native Apps 101: Build and monetize data apps (2026)\n\n### [FinOps](https://www.flexera.com/blog/finops/)\n\nSubscribe\n\nTopics\n\nSaaS Management FinOps IT Visibility IT Asset Management Product News Application Readiness Security Perspectives\n\n[FinOps](https://www.flexera.com/blog/finops/)\n\n# Snowflake Native Apps 101: Build and monetize data apps (2026)\n\n[](https://www.flexera.com/blog/author/pramit/ \"Pramit Marattha\")\n\n[Pramit Marattha](https://www.flexera.com/blog/author/pramit/ \"Pramit Marattha\")\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-native-apps%2F&title=Snowflake%20Native%20Apps%20101%3A%20Build%20and%20monetize%20data%20apps%20%282026%29&source=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-native-apps%2F) [](https://twitter.com/intent/tweet?source=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-native-apps%2F&text=Snowflake%20Native%20Apps%20101%3A%20Build%20and%20monetize%20data%20apps%20%282026%29%20https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-native-apps%2F) \n\nThis post originally appeared on the chaosgenius.io blog. Chaos Genius has been [acquired by Flexera](https://www.flexera.com/more/ProsperOps-Chaos-Genius) .\n\nBuilding and deploying [data applications](https://www.snowflake.com/guides/applications/) often comes with unnecessary complexity. Snowflake has significantly reduced this by introducing **Snowflake Native Apps** . Snowflake Native Apps allow you to build, run and monetize data apps directly within the Snowflake ecosystem. No external integrations. No infrastructure headaches. Snowflake previously supported [Connected Apps](https://www.snowflake.com/guides/connected-apps) , which operated outside Snowflake, connected via [APIs](https://docs.snowflake.com/en/api-reference) and required manual data movement. Native Apps, by contrast, run entirely within Snowflake, leveraging its secure and scalable infrastructure . They’re built using the _Snowflake Native App Framework_ , allowing you to develop secure, scalable and integrated Snowflake apps that leverage Snowflake’s environment without leaving the Snowflake platform.\n\nIn this article, we will discuss the features, benefits and inner workings of Snowflake Native Apps and walk you through the step-by-step process of creating a Snowflake Native App and publishing and monetizing your Native Apps through the Snowflake Marketplace.\n\n## What Are Snowflake Native Applications?\n\n**Snowflake Native Apps** are applications built using the Snowflake Native App Framework, which allows developers to create, test and deploy applications directly within Snowflake’s Data Cloud. These Native Apps leverage Snowflake’s core features like stored procedures , user-defined functions (Snowflake UDFs) and the [Snowpark API](https://docs.snowflake.com/en/developer-guide/snowpark/index) , all while keeping the data secure by running the code on your data stored in Snowflake. Native apps are available on the Snowflake Marketplace . You can discover and install them quickly, just like downloading an app on your smartphone.\n\n**TL;DR:  \n** **What They Are** — Snowflake Native Apps run entirely within Snowflake, eliminating the need to move data out for processing.  \n**How They Work** — Snowflake Native Apps operate in your Snowflake account, using your existing data without external connections. Developers create and distribute them via the Snowflake Marketplace.  \n**Why Use Them** :\n\n* **Fast** — Snowflake Native Apps process data directly in Snowflake, reducing latency.\n* **Secure** — Snowflake Native Apps don’t access or store data outside your account, relying on Snowflake’s encryption and access controls.\n* **Scalable** — Snowflake Native Apps can grow alongside your Snowflake resources, handling larger datasets or more users.\n\nSnowflake Native Apps (Source: [Snowflake](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/) )\n\n### Key Technical Characteristics of Snowflake Native Apps\n\n#### 1) **Native Integration with Snowflake Services**\n\nSnowflake Native Apps work directly with Snowflake’s core services. They use stored procedures , user-defined functions (Snowflake UDFs UDFs) and the Snowpark API, making them efficient and seamless.\n\n#### 2) **Simplified Development and Testing**\n\nYou can build Snowflake Native Applications using the Snowflake Native App Framework. Snowflake Native App Framework streamlines development and testing. You can create, test and deploy Snowflake apps within Snowflake, reducing development time.\n\n#### 3) **Monetization via Snowflake Marketplace**\n\nProviders can list and sell Snowflake apps in the Snowflake Marketplace . Consumers install these apps directly into their Snowflake accounts, simplifying deployment and making app monetization straightforward.\n\n#### 4) **Security and Governance**\n\nSnowflake Native Applications don’t transfer data outside the platform. Providers can package their logic securely, protecting intellectual property. Users maintain control of access permissions, with data security managed by Snowflake’s encryption and governance features .\n\n#### 5) **Data Sharing**\n\nSnowflake Native Apps leverage [Snowflake’s secure data sharing](https://docs.snowflake.com/en/user-guide/data-sharing-intro) . Apps can access shared datasets without creating duplicates or requiring data to leave the user’s Snowflake account.\n\n#### 6) **Advanced Feature Support**\n\nSnowflake Native Applications use Snowflake features like:\n\n* User-Defined Functions (Snowflake UDFs)\n* Stored Procedures\n* [Snowpark](https://docs.snowflake.com/en/developer-guide/snowpark/index)\n\n#### 7) **Versioning and Patching**\n\nSnowflake Native App Framework supports versioning and patching, allowing you to manage updates easily. This keeps your apps up to date.\n\n#### 8) **Streamlit Integration**\n\nYou can integrate your apps with Streamlit , which allows you to create interactive dashboards within Snowflake. While the integration is still evolving, it supports embedding visualizations in your apps for end-user analytics.\n\n#### 9) **Encapsulation in Application Packages**\n\nSnowflake Native Apps are bundled into packages containing all necessary components (e.g., data content, application logic, metadata and setup scripts). This makes deployment simple and reduces compatibility issues.\n\n#### 10) **Source Control and Tool Integration**\n\nYou can integrate with external tools like [IDEs](https://en.wikipedia.org/wiki/Integrated_development_environment) , [CI/CD pipelines](https://www.redhat.com/en/topics/devops/what-cicd-pipeline) and source control systems. This flexibility helps teams adopt DevOps practices when developing Snowflake Native Applications.\n\n#### 11) **AI and ML Workflows**\n\nSnowflake Native Applications support machine learning and AI tasks through Snowpark. You can integrate external ML libraries, process training data and deploy models directly within Snowflake.\n\n#### 12) **Cross-Cloud Deployment (AWS + Azure + with limitations on GCP)**\n\nSnowflake Native Applications are compatible with multiple clouds ( [AWS](https://www.snowflake.com/en/why-snowflake/partners/all-partners/aws/) , [Azure](https://www.snowflake.com/en/why-snowflake/partners/all-partners/microsoft/) and limit on [GCP](https://www.snowflake.com/technology-partners/google-cloud-platform/) ). This feature enables users to run apps across different infrastructures, though GCP support has limitations.\n\n## What Are the Benefits of Snowflake Native Apps — For Providers?\n\nSnowflake Native Apps offer significant advantages for providers looking to build, distribute and monetize their applications within the Snowflake ecosystem. Here are some of the benefits:\n\n### ➥ Simplified Development\n\nProviders can use Snowflake’s tools to build apps that leverage Snowflake’s high availability and scalability. This completely eliminates the need for separate infrastructure, reducing development overhead and speeding up time to market​.\n\nSnowflake Native Apps run within customers’ Snowflake accounts, meaning data stays within their environment, which minimizes the complexity of data integration and security management​.\n\n### ➥ **On-Platform Monetization Opportunities**\n\nProviders can easily sell and monetize their Snowflake apps directly within the Snowflake ecosystem via Snowflake Marketplace , bypassing the need for third-party systems.\n\n### ➥ **Easy Management**\n\nSince apps are hosted within Snowflake’s infrastructure, you don’t need to manage separate resources. Snowflake offers always-on availability, global reach, built-in governance and secure operations.\n\n### ➥ **Data Protection**\n\nSnowflake Native Apps interact directly with the customer’s Snowflake account. This approach keeps customer data within their environment, reducing security risks and compliance burdens. You avoid the complexity of managing sensitive data, as Snowflake’s internal security and governance capability can handle it for you.\n\n### ➥ **Zero Infrastructure Management and Lower Operational Costs**\n\nTraditional Snowflake apps often require providers to pay for their own compute and storage. With Snowflake Native Applications, you leverage the customer’s compute resources, reducing your operating costs and improving profit margins.\n\n### ➥ **Access to New Customers**\n\nSnowflake Marketplace gives you exposure to a global Snowflake customer base. Here, customers can find, test and purchase your apps. This built-in distribution network simplifies how you reach and onboard users while enabling seamless deployment directly into customer environments.\n\n## What Are the Benefits of Snowflake Native Apps — For Consumers?\n\nSnowflake Native Apps offer significant advantages for end-users, enhancing their ability to integrate, utilize and manage data applications seamlessly within the Snowflake ecosystem. Here are some of the key benefits of Native Snowflake apps for consumers:\n\n### ➥ **Quick and Easy Access**\n\nConsumers can easily search for Snowflake Native Applications on the Snowflake Marketplace or in private listings and directly install and use them with a single click.\n\n### ➥ **Secure Data Use**\n\nSince Snowflake Native Applications run directly in the consumer’s Snowflake account, data does not need to leave the platform. This eliminates the risks associated with data transfers and ensures compliance with Snowflake’s robust governance and security controls.\n\n### ➥ **Enhanced Performance**\n\nSnowflake Native apps utilize the consumer’s Snowflake compute resources, which ensures optimal performance tailored to the existing workload. Consumers benefit from Snowflake’s underlying scalability and processing power, resulting in faster query execution and application responsiveness​.\n\n### ➥ **Streamlined Trial and Deployment**\n\nConsumers can easily test applications via trial periods. Transitioning from trial to full versions is simple and data generated during the trial is retained for future use, making the adoption process seamless and risk-free.\n\n### ➥ **Simplified Application Management**\n\nConsumers can manage access privileges, event logging and app-related tasks directly within their Snowflake account.\n\n## How Do Snowflake Native Applications Work?\n\nSnowflake Native Applications leverage the _Snowflake Native App Framework_ to build and deploy data-driven applications directly within the Snowflake ecosystem. These Snowflake apps harness Snowflake’s core features—secure data sharing, analytics, compute and governance—enabling seamless integration and monetization, without requiring data to move outside the platform. The framework supports applications ranging from analytical tools to fully containerized services.\n\nSnowflake Native App Framework allows:\n\n* Providers to share data, business logic and application interfaces (e.g., Streamlit apps, stored procedures) using [Snowpark API](https://docs.snowflake.com/en/developer-guide/snowpark/index) , [Python](https://www.python.org/) , [SQL](https://www.w3schools.com/sql/) and [JavaScript](https://www.w3schools.com/js/) .\n* Applications to be listed as free or paid offerings on the Snowflake Marketplace or shared privately with select accounts.\n* Developers to benefit from streamlined testing environments, version control via external repositories and detailed logging for troubleshooting.\n* Built-in support for structured and unstructured event logging to streamline troubleshooting and performance tracking.\n* Integration with Streamlit to build interactive, user-friendly visual interfaces.\n\nOn top of that, the Snowflake Native Framework also provides an enhanced developer experience, including:\n\n* A unified testing environment for app lifecycle management.\n* Integration with version control systems for seamless code and resource management.\n* Incremental app updates with versioning and patch support.\n\nSnowflake Native Apps extend the functionality of features like Secure Data Sharing and Collaboration, ensuring providers can offer scalable, governed applications to other Snowflake users.\n\n### Architecture of the Snowflake Native App Framework\n\nThe architecture of the Snowflake Native App Framework operates on a provider-consumer model:\n\n* **Provider** — Creates and shares data and application logic using the framework.\n* **Consumer** — Installs and interacts with applications shared by providers.\n\nSnowflake Native Applications are packaged as **Application Packages** , which contains the necessary logic, metadata and configuration to deploy a Snowflake Native App. This includes:\n\n* **Manifest file** : Configuration details, including setup script locations and versioning.\n* **Setup script** : Contains SQL commands for installation and updates.\n\nThe provider publishes the Snowflake Native app via:\n\n* **Marketplace Listings** — Accessible to all Snowflake users for broad distribution.\n* **Private Listings** — Targeted sharing with specific accounts across regions.\n\nUpon installation, Snowflake creates a corresponding database object for the app, which runs the setup script to establish necessary resources in the consumer’s account. Additional configurations like logging or privilege grants can be applied post-installation.\n\nSnowflake Native App Architecture\n\n### How do Snowflake Native Applications work with Snowpark Container Services?\n\nFor advanced use cases, Snowflake Native Applications can utilize [**Snowpark Container Services**](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) , which enable Snowflake apps to manage containerized workloads within Snowflake. This approach supports high-performance applications, such as machine learning and AI-driven analytics, without externalizing data.\n\nComponents unique to containerized Snowflake apps:\n\n* **Services specification file** — Applications reference container images stored in the provider’s repository.\n* **Compute pool** — A collection of virtual machine nodes where containerized workloads execute.\n\nSnowflake Native App with Snowpark Container Architecture\n\nFeatures of Snowpark Container Services include:\n\n* Support for AI/ML workloads with low latency.\n* Persistent data and model storage close to compute resources for efficiency.\n* Seamless deployment across GPU/CPU configurations.\n\n## Step-by-Step Guide to Create a Snowflake Native App\n\nNow that we have covered the features, benefits and inner workings/architecture of Snowflake Native Applications, let’s dive into a step-by-step guide on how you can create a simple Snowflake Native App on Snowflake.\n\nBefore you start building a Snowflake Native App, make sure you meet these requirements:\n\n## Prerequisite:\n\n* **Snowflake CLI** : Install the Snowflake CLI on your machine.\n* [**Visual Studio Code**](https://code.visualstudio.com/) : If you haven’t already installed VSCode, download it from here.\n* **ACCOUNTADMIN Role** : Use the ACCOUNTADMIN role for all steps in this tutorial.\n* **Second Snowflake Account** : If installing your app from a private listing, you need access to a second Snowflake account.\n* **Set a Current Warehouse** : Use the USE WAREHOUSE command to specify the active warehouse.\n* **SQL Command Session** : Run all SQL commands within the same session. The session context must remain consistent as the steps build on one another.\n\nLet’s kick off by covering the main pieces you’ll be working with.\n\n**Application Package**\n\nThink of this as a container for your entire application. It holds:\n\n* Shared data\n* Application logic\n* Deployment artifacts\n\n**Setup Script**\n\nThe blueprint of your application’s initialization. This script:\n\n* Creates schemas\n* Sets up application roles\n* Defines initial objects and permissions\n\n**Manifest File**\n\nYour app’s configuration metadata. It tells Snowflake:\n\n* Which files to include\n* How to structure the deployment\n* Runtime behaviors\n\n**Project Definition File**\n\nDefines deployment specifics:\n\n* Application package name\n* Stages\n* Artifacts to include\n\n### **Step 1** —Configuring the Environment\n\n```\nCREATE DATABASE IF NOT EXISTS my_app_db;\nUSE DATABASE my_app_db;\nCREATE SCHEMA IF NOT EXISTS my_app_schema;\n```\n\nThis sets up a dedicated space for your app’s data and logic.\n\nNow make sure you have an active warehouse set for executing your SQL commands. You can create a new warehouse or use an existing one:\n\n```\nCREATE WAREHOUSE IF NOT EXISTS <warehouse> WITH WAREHOUSE_SIZE = 'SMALL';\nUSE WAREHOUSE <warehouse>;\n```\n\n### **Step 2** —Setting Up Snowflake CLI\n\nTo get started, you need to install and configure Snowflake CLI. It allows users to execute SQL queries and carry out a wide range of DDL and DML operations.\n\nTo download Snowflake CLI, first download from the [SnowSQL download page](https://www.snowflake.com/en/developers/downloads/snowsql/) and then open a new terminal window. Execute the following code to test your connection:\n\n```\nsnow connection add\n```\n\nOnce you have done that, enter all the credentials when prompted.\n\nAdding Snowflake CLI credentials\n\nThat’s it!\n\n> For detailed instructions on installing Snowflake CLI, [refer to the official documentation](https://docs.snowflake.com/developer-guide/snowflake-cli/installation/installation) .\n> \n> \n\n### **Step 3** —Initializing a New Project\n\nNow that your environment is set up, you will initialize a new project folder for your app using the Snowflake CLI. Open your terminal and execute:\n\n```\nsnow init --template app_basic snowflake_native_app_demo\n```\n\nInitializing Snowflake Native App project\n\nThis command creates a directory named **snowflake\\_native\\_app\\_demo** , which contains a basic structure for your Snowflake Native App project.\n\nFolder structure of Demo Snowflake Native App\n\nHere’s what is inside the app directory:\n\nFolder structure of Demo Snowflake Native App\n\n**OR**\n\nYou can clone the starter project which is provided by Snowflake by running the following command:\n\n```\ngit clone https://github.com/Snowflake-Labs/sfguide-getting-started-with-native-apps.git\n```\n\nCloning Starter Snowflake Native App project\n\n### **Step 4** —Creating Application Files\n\nNavigate to the **snowflake\\_native\\_app\\_demo** directory and create several essential files:\n\n**➥ Configuring Setup Script**\n\nThis SQL script runs automatically when a consumer installs your app. Modify **app/setup\\_script.sql** with the following content (add comment for now, we will update the script later on):\n\n```\n-- Snowflake Native App Setup script\n```\n\n**➥ Configuring Manifest File**\n\nThis YAML file contains configuration information about your app. Create/Update **app/manifest.yml** with this:\n\n**➥ Configuring Project Definition File**\n\nThis YAML file defines objects that can be deployed to Snowflake. Create/Update **snowflake.yml** in the root of your project with this:\n\n```\ndefinition_version: 2\nentities:\n\t\tsnowflake_native_app_demo_package:\n\t\t\ttype: application package\n\t\t\t stage: stage_content.hello_snowflake_stage\n\t\t\t manifest: app/manifest.yml\n\t\t\t identifier: snowflake_native_app_demo_package\n\t\t\tartifacts:\n\t\t\t\t - src: app/*\n\t\t\t\t\t\tdest: ./\n\t demo_snowflake_native_app:\n\t\t\t type: application\n\t\t\tfrom:\n\t\t\t\t\ttarget: snowflake_native_app_demo_package\n\t\t\t debug: false\n```\n\nThis file is central to setting up a Snowflake Native App. It specifies key details about the app’s configuration and how resources are managed within Snowflake.\n\nTo create an application package, you need the [CREATE APPLICATION PACKAGE](https://docs.snowflake.com/en/sql-reference/sql/create-application-package) privilege. If your role doesn’t have this, you can grant it with the Snowflake CLI:\n\n```\nsnow sql -q \"GRANT CREATE APPLICATION PACKAGE ON ACCOUNT TO ROLE accountadmin\" -c <connection_name>\n```\n\nGranting Create Application Package to account admin\n\n> Replace **connection\\_name** with the connection name specified in your **config.toml** file.\n> \n> \n\n**snowflake.yml** file defines objects and configuration details. Here’s what it includes:\n\n**a) Application Package**\n\nServes as the container for app-related objects. Example: snowflake\\_native\\_app\\_demo\\_package\n\n**b) Application Object**\n\nCreated from the application package. Example: **demo\\_snowflake\\_native\\_app**\n\n**c) Named Stage**\n\nHolds application files. Example: **stage\\_content.hello\\_snowflake\\_stage** . The stage name is schema-qualified and created inside the application package.\n\nIt stores files for setup scripts or runtime use.\n\n**Artifacts Section**\n\nSpecifies file rules for deployment. Example: Files in app/ are uploaded to the root of the stage.\n\nExample mappings:\n\n```\ntutorial/app/manifest.yml → @snowflake_native_app_demo_package.stage_content.hello_snowflake_stage\ntutorial/app/README.md → @snowflake_native_app_demo_package.stage_content.hello_snowflake_stage\ntutorial/app/setup_script.sql → @snowflake_native_app_demo_package.stage_content.hello_snowflake_stage\n```\n\n> Use <% … %> syntax for dynamic referencing. Example: <% ctx.entities.pkg.identifier %> accesses the package identifier.\n> \n> \n\n**Debug Mode**\n\nThe debug field in **snowflake.yml** is set to **false** for production. Debug mode is typically enabled by default during development.\n\n**➥ Configuring README File**\n\nThis file provides a description of your application. Create **app/README.md** with a brief description:\n\n```\nThis is a demo Snowflake Native App.\n```\n\n### **Step 5** —Writing the Snowflake Native Application Logic\n\nNow let’s get to the essence of Snowflake Native App development—adding application logic and installing your first app. This involves creating stored procedures, setting up application roles and configuring privileges for seamless execution. Here’s how you can achieve this:\n\n**➥ Add a Stored Procedure to the Setup Script**\n\nThe setup script is central to your app’s functionality. By extending it, you can define key components like roles and stored procedures.\n\nFirst let’s define an Application Role. To do so, add the following to your **setup\\_script.sql** file:\n\n```\nCREATE APPLICATION ROLE IF NOT EXISTS snowflake_native_app_public;\nCREATE SCHEMA IF NOT EXISTS native_app_core;\nGRANT USAGE ON SCHEMA native_app_core TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nAs you can see, this creates an application-specific role ( _snowflake\\_native\\_app\\_public_ ) and a schema (native\\_app\\_core) for the app. And the role is restricted to the app’s context and manages access to app-specific objects.\n\nNext, add a stored procedure that your app can call:\n\n```\nCREATE OR REPLACE PROCEDURE native_app_core.HELLO()\n\tRETURNS STRING\n\tLANGUAGE SQL\n\tEXECUTE AS OWNER\n\tAS\nBEGIN\n\t RETURN 'Demo Snowflake Native App!';\nEND;\n```\n\nThis stored procedure outputs a simple greeting—useful as a test or base for more complex logic.\n\nFinally, grant the application role permission to use the procedure:\n\n```\nGRANT USAGE ON PROCEDURE native_app_core.hello() TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nNow this role can access and execute the stored procedure.\n\nHere is how your **setup\\_script.sql** file should look like:\n\nWriting the Snowflake Native Application Logic\n\nOnce your setup script includes the necessary logic, install the app in stage dev mode using the Snowflake CLI. Dev mode lets you test app behavior before deploying it to production.\n\nTo test it, run the following Snowflake CLI command:\n\n```\nsnow app run -c connection_name\n```\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\nAs you can see, if the command runs successfully, it outputs a URL where you can see your app in Snowsight.\n\nChecking a Deployed Snowflake Native App in Snowsight\n\nTo run the stored procedure that you added to **setup\\_script.sql** in a previous section, run the following Snowflake CLI command:\n\n```\nsnow sql -q \"call demo_snowflake_native_app.native_app_core.hello()\" -c connection_name\n```\n\nYou should see the following result/output:\n\nRunning deployed Stored procedure via Snowflake CLI\n\n### **Step 6** —Adding Data Content to Your Snowflake Native App\n\nIn this step, you’ll enhance your Snowflake Native App by incorporating shared data content. This involves creating and sharing a table within your app package and granting access to app users. Additionally, you’ll create a view for secure data access by consumers.\n\nFirst, let’s create a table to share with the app. To do so, you can add a table to your app package by writing a SQL script and specifying its execution in the project definition file.\n\nSo let’s create and populate the table. To do that, you need to create a folder **scripts** and inside that folder, create a file called **shared\\_content.sql** . Then, add the following code:\n\n```\nUSE APPLICATION PACKAGE <% ctx.entities.snowflake_native_app_demo_package.identifier %>;\n\nCREATE SCHEMA IF NOT EXISTS shared_data;\nUSE SCHEMA shared_data;\nCREATE TABLE IF NOT EXISTS wealthy_individuals (\n\t\tid INT,\n\t\t name VARCHAR,\n\t\tstatus VARCHAR\n);\n\nTRUNCATE TABLE wealthy_individuals;\n\nINSERT INTO wealthy_individuals VALUES\n\t\t (1, 'Elon', 'Billionaire'),\n\t\t(2, 'Bernard', 'Billionaire'),\n\t\t(3, 'Jeff', 'Billionaire'),\n\t\t (4, 'Warren', 'Billionaire'),\n\t\t(5, 'Larry', 'Billionaire'),\n\t\t(6, 'Sergey', 'Billionaire'),\n\t\t (7, 'Gautam', 'Billionaire'),\n\t\t(8, 'Carlos', 'Billionaire'),\n\t\t (9, 'Mukesh', 'Billionaire'),\n\t\t(10, 'Bill', 'Millionaire'),\n\t\t(11, 'Mark', 'Millionaire'),\n\t\t(12, 'Larry', 'Millionaire'),\n\t\t (13, 'Michael', 'Millionaire'),\n\t\t(14, 'Aman', 'Millionaire'),\n\t\t(15, 'Warren', 'Billionaire');\n```\n\nNow, grant access to the table and schema using:\n\n```\nGRANT USAGE ON SCHEMA shared_data TO SHARE IN APPLICATION PACKAGE <% ctx.entities.snowflake_native_app_demo_package.identifier %>;\nGRANT SELECT ON TABLE wealthy_individuals TO SHARE IN APPLICATION PACKAGE <% ctx.entities.snowflake_native_app_demo_package.identifier %>;\n```\n\nThe placeholder `<% ctx.entities.snowflake_native_app_demo_package.identifier %>` dynamically resolves to your application package identifier during deployment.\n\n**Add the Script to the Project Definition File**\n\nUpdate the **snowflake.yml** file to include the new script in post-deployment hooks:\n\n```\ndefinition_version: 2\nentities:\n\t snowflake_native_app_demo_package:\n\t\t\t type: application package\n\t\t\tstage: stage_content.hello_snowflake_stage\n\t\t\tmanifest: app/manifest.yml\n\t\t\tidentifier: snowflake_native_app_demo_package\n\t\t\t artifacts:\n\t\t\t\t\t- src: app/*\n\t\t\t\t\t dest: ./\n\t\t\t meta:\n\t\t\t\t post_deploy:\n\t\t\t\t\t\t - sql_script: scripts/shared_content.sql\n\t\tdemo_snowflake_native_app:\n\t\t\ttype: application\n\t\t\t from:\n\t\t\t\t target: snowflake_native_app_demo_package\n\t\t\tdebug: false\n```\n\nNext, modify the **setup\\_script.sql** file to create a view that app consumers can use to query the data.\n\n**Create a Versioned Schema:**\n\n```\nCREATE OR ALTER VERSIONED SCHEMA code_schema;\nGRANT USAGE ON SCHEMA code_schema TO APPLICATION ROLE snowflake_native_app_public;\n```\n\n**Create the View:**\n\n```\nCREATE VIEW IF NOT EXISTS code_schema.accounts_view\n\t AS SELECT roll_number, NAME, VALUE\n\t FROM shared_data.wealthy_individuals;\nGRANT SELECT ON VIEW code_schema.accounts_view TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nAfter adding the table and view, test the updated app to make sure everything works as expected. Let’s deploy the updates:\n\n```\nsnow app run -c connection_name\n```\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\nThis uploads the edited files to the stage, runs **scripts/shared\\_content.sql** and deploys the app.\n\nNow, finally to verify the data access, fire the command below:\n\n```\nsnow sql -q \"SELECT * FROM demo_snowflake_native_app.code_schema.accounts_view\" -c connection_name\n```\n\nVerifying Snowflake data access\n\nThe output should list the sample data from the accounts table.\n\n### **Step 7** —Adding Python Code to Your Snowflake Native App\n\nLet’s enhance our Snowflake Native App by incorporating Python-based logic using User-Defined Functions (Snowflake UDFs). We’ll add both an inline Python UDF and one that references an external Python module.\n\nInline Python UDFs enable you to embed Python logic directly within your setup script. To do so, update your setup script to include the following code:\n\n```\nCREATE OR REPLACE FUNCTION code_schema.squareroot(i INT)\nRETURNS INT\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.11'\nHANDLER = 'squareroot_py'\nAS\n$$\ndef squareroot_py(i):\n\t return i * i\n$$;\n\nGRANT USAGE ON FUNCTION code_schema.squareroot(INT) TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nAs you can see, this:\n\n* Creates a Python UDF named squareroot in the code\\_schema schema.\n* Uses Python 3.11 runtime.\n* Grants the necessary usage privilege to the snowflake\\_native\\_app\\_public role.\n\nNow let’s reference a Python file for modular and reusable logic. First, let’s add this code to your setup script:\n\n```\nCREATE OR REPLACE FUNCTION code_schema.cuberoot(i INT)\nRETURNS FLOAT\nLANGUAGE PYTHON\nRUNTIME_VERSION = 3.11\nIMPORTS = ('/python/cube_python.py')\nHANDLER = 'cube_python.cuberoot';\n\nGRANT USAGE ON FUNCTION code_schema.cuberoot(INT) TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nNow let’s add the external Python file. To do this, in the project folder, create a subdirectory: **python/** . Inside this folder, create a file named **cube\\_python.py** with the following content:\n\n```\ndef cuberoot(i):\n\t return i * i * i\n```\n\nUpdate the project definition file to include the Python file:\n\n```\nartifacts:\n\t- src: python/cube_python.py\n```\n\nAfter adding the Python UDFs, deploy and validate their functionality.\n\n**Deploy the Updates:**\n\n```\nsnow app run -c connection_name\n```\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\nThis uploads the updated files to the stage and deploys the app.\n\n**Test the Inline Python UDF:**\n\n```\nsnow sql -q \"SELECT demo_snowflake_native_app.code_schema.squareroot(2)\" -c connection_name\n```\n\nOutput:\n\nTesting the Inline Python UDF – Snowflake Native App\n\n**Test the External Python UDF:**\n\n```\nsnow sql -q \"SELECT demo_snowflake_native_app.code_schema.cuberoot(2)\" -c connection_name\n```\n\nOutput:\n\nTesting the External Python UDF – Snowflake Native App\n\n### **Step 8** —Adding a Streamlit App to Your Snowflake Native App\n\nNow, let’s integrate a Streamlit-based user interface into your Snowflake Native App. Streamlit is an open-source framework designed for building interactive data applications, offering features for data visualization and user interaction.\n\nHead over to your project folder and create a subdirectory named **streamlit/** . Inside the streamlit folder, create a file named **streamlit\\_app.py** . Then, add the following Python code to **streamlit\\_app.py** :\n\n```\n# Import python packages\nimport streamlit as st\nfrom snowflake.snowpark import Session\n\nst.title(\"Demo Snowflake Native Application\")\nst.write(\n\t\t\"\"\"Demo of Snowflake Native Application\"\"\")\n\n# Get the current credentials\nsession = Session.builder.getOrCreate()\n\n# Create an example data frame\ndata_frame = session.sql(\"SELECT * FROM code_schema.accounts_view\")\n\n# Execute the query and convert it into a Pandas data frame\nqueried_data = data_frame.to_pandas()\n\n# Display the Pandas data frame as a Streamlit data frame.\nst.dataframe(queried_data, use_container_width=True)\n```\n\nNext, add the following to the artifacts section of the project definition file:\n\n```\nartifacts:\n\t - src: streamlit/streamlit_app.py\n```\n\nThis is how your **snowflake.yml** should look:\n\n```\ndefinition_version: 2\nentities:\n\t\tsnowflake_native_app_demo_package:\n\t\t\ttype: application package\n\t\t\t stage: stage_content.hello_snowflake_stage\n\t\t\t manifest: app/manifest.yml\n\t\t\t identifier: snowflake_native_app_demo_package\n\t\t\tartifacts:\n\t\t\t\t - src: app/*\n\t\t\t\t\t\tdest: ./\n\t\t\t\t - src: python/cube_python.py\n\t\t\t\t\t- src: streamlit/streamlit_app.py\t\t\t\t\n\t\t\tmeta:\n\t\t\t\t\tpost_deploy:\n\t\t\t\t\t\t- sql_script: scripts/shared_content.sql\n\t demo_snowflake_native_app:\n\t\t\t type: application\n\t\t\tfrom:\n\t\t\t\t\ttarget: snowflake_native_app_demo_package\n\t\t\t debug: false\n```\n\nThen, create the Streamlit object. To do that, add the following to the end of the **setup\\_script.sql** file:\n\n```\nCREATE STREAMLIT IF NOT EXISTS code_schema.snowflake_streamlit_native_app\n\tFROM '/streamlit'\n\t MAIN_FILE = '/streamlit_app.py'\n;\n```\n\nAdd the following statement to the same script to allow the application role to access the Streamlit object:\n\n```\nGRANT USAGE ON STREAMLIT code_schema.hello_snowflake_streamlit TO APPLICATION ROLE snowflake_native_app_public;\n```\n\nDeploy the updates by running the following command to update the app:\n\n```\nsnow app run -c connection_name\n```\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\nAfter deployment, a URL will be printed in the console. Navigate to this URL to interact with the app. Click the **snowflake\\_streamlit\\_native\\_app** tab to view the Streamlit interface.\n\nChecking a Deployed Streamlit App in Snowsight\n\n### **Step 9** —Versioning Your Snowflake Native App\n\nLets formalize your Snowflake Native App by creating a version. While previous steps utilized “stage development” mode for rapid iteration, adding a version is essential for listing the application package and sharing it with other Snowflake users.\n\nRun the following command to create version V1 for the **snowflake\\_native\\_app\\_demo\\_package** application package:\n\n```\nsnow app version create v1 -c connection_name\n```\n\nVersioning Snowflake Native App\n\nHere is how you would check the version of the app to see whether it was added successfully or not. To do so, run the following command:\n\n```\nsnow app version list -c connection_name\n```\n\nVersioning Snowflake Native App\n\n### **Step 10** —Installing and Testing the Versioned App\n\nTo install and test the versioned app, run the following command to install the app based on the created version:\n\nDeploying and Running Snowflake Native App via Snowflake CLI\n\n### **Step 11** —Viewing Snowflake Native App in Snowsight\n\nAfter finalizing your Snowflake Native App, you can use Snowsight, Snowflake’s web interface, to explore your app visually, instead of relying solely on SQL commands.\n\nTo start, sign in to Snowsight using your Snowflake credentials. Once logged in, switch to the **ACCOUNTADMIN** role to ensure you have the necessary permissions. You can do this by selecting your username from the navigation menu, opening the account menu and switching from the current active role (e.g., PUBLIC) to **ACCOUNTADMIN** . Next, navigate to the **Data Products** section and select **Apps** from the menu.\n\nNavigating to Data Products > Apps section\n\nLocate your application, **DEMO\\_SNOWFLAKE\\_NATIVE\\_APP** , in the list.\n\nChecking a Deployed Snowflake Native App in Snowsight\n\nClicking on it opens the **Read Me** tab, which displays the content of the **README.md** file you created earlier in this tutorial.\n\nTo view the Streamlit interface, find and select **SNOWFLAKE\\_STREAMLIT\\_NATIVE\\_APP** .\n\nChecking a Deployed Streamlit App in Snowsight\n\n## Monetization and Distribution of Snowflake Native App\n\nNow that we have covered the detailed steps of how you can create and build Snowflake Native Applications from scratch, let’s go through the process of how you can monetize your Snowflake Native Applications via Snowflake Marketplace. But before that, let’s actually understand what Snowflake Marketplace is.\n\nThe Snowflake Marketplace is a platform where users can discover, evaluate and purchase a variety of products, including third-party data, data services, Snowflake Native Apps and AI products. It serves as a public data exchange integrated within the Snowflake Data Cloud, facilitating seamless and secure transactions between data providers and consumers.\n\nThe Snowflake marketplace offers various types of data products, such as:\n\n* Raw datasets\n* Refined and enriched data\n* Historical datasets for forecasting and machine learning\n* Real-time data streams (like weather or traffic updates)\n* Specialized identity or audience data for analytics\n* Snowflake Native Applications\n* Pre-built data pipelines and transformations\n\nSnowflake Marketplace leverages Snowflake’s architecture to facilitate the secure sharing of data and applications. Transactions are managed natively, eliminating the need for third-party billing systems. Vendors can offer their products through various pricing models, such as pay-as-you-go, one-time payment, usage-based payment, or subscription-based plans, while benefiting from Snowflake’s built-in analytics to track customer engagement.\n\nLet’s jump right into the juicy part of the article: a step-by-step guide to monetizing Snowflake Native Applications via Snowflake Marketplace.\n\n### Step-By-Step Monetization Process of a Snowflake Native App via Snowflake Marketplace\n\n#### **Step 1** —Prepare Your Snowflake Native Application Package\n\nBefore monetization, make sure your Snowflake Native App meets Snowflake’s submission requirements:\n\n* The app must be fully functional upon installation, with all necessary resources and configurations included.\n* It must not depend on external systems for core functionality.\n* Should Leverage Snowflake-stored or shared data for operations.\n* Include the following files:\n  \n    + **manifest.yml** (outlines app permissions and dependencies).\n    + **readme.md** (describes app functionality, post-installation setup steps and sample usage SQL).\n    + Setup scripts and external components like Streamlit files or UDF code\n\n#### **Step 2** —Define the Default Release Directive\n\nNow, set the release directive to specify the app version and patch available for distribution:\n\nList available versions and patches:\n\n```\nsnow app version list -c connection_name\n```\n\nThen, set the default release directive:\n\n```\nsnow sql -q \"ALTER APPLICATION PACKAGE <your_app_package> SET DEFAULT RELEASE DIRECTIVE VERSION = v1 PATCH = 0\"\n```\n\nVersioning Snowflake Native App\n\nAs you can see, this command sets version **v1** and patch **0** as the default for your app, ensuring it is ready for deployment​.\n\n#### **Step 3** —Create and Configure a Private Listing on the Snowflake Marketplace\n\nTo share your app via the Snowflake Marketplace, start by signing in to Snowsight and navigating to **Data Products > Provider Studio** .\n\nNavigating to Provider Studio in Snowflake\n\nClick **\\+ Listing** to create a new listing and proceed with configuration. Enter a name for the listing and specify the discovery permissions, choosing whether the listing will be public or restricted to specific consumers (e.g., select “ **Only specified consumers** ” for private sharing and select “ **Anyone on the Marketplace** ” for public listing).\n\nCreating a private listing for only specified consumers – Snowflake Native App\n\nAttach the application package you prepared earlier as the core data content for the listing. Provide a detailed description outlining your app’s features and usage scenarios. If creating a private listing, add the account identifiers of intended consumers in the “ **Add** **Consumer accounts** ” section. Finally, publish your listing for approval.\n\nCreating a private listing for only specified consumers – Snowflake Native App\n\n#### **Step 4** —Create and Configure a Public Paid Listing on the Snowflake Marketplace\n\nNow, to create a public listing, you need to first contact your Snowflake business development partner to approve your paid listing. If you don’t have a business development partner, you’ll need to [submit a case with Marketplace Operations](https://snowflakecommunity.force.com/s/provider-onboarding-case) . Before proceeding, verify that your [role has the required privileges to create a listing](https://other-docs.snowflake.com/en/collaboration/provider-becoming) .\n\nOnce everything is in place, you need to log in to Snowsight and go to Data Products > Provider Studio from the menu. Select **\\+ Listing** to open the Create Listing window. Here, name your listing and set its visibility. To make the listing publicly discoverable, choose “ **Anyone on the Marketplace** ” under the discovery settings.\n\nCreating a paid private listing for only specified consumers – Snowflake Marketplace\n\nNext, you need to decide how consumers will access your data product: choose “ **Free** ” for no-cost access, “ **Personalized/Limited Trial** ” to offer a trial version with full access upon request, or “ **Paid** ” if you plan to charge consumers directly.\n\nAfter setting the access type, click **Next** to generate a draft listing. Lastly, refine and configure the draft by including all necessary details to ready it for publication on the Snowflake Marketplace.\n\n#### **Step 5** —Submit Listing for Approval\n\nAll listings on the Snowflake Marketplace must undergo a review and approval process before publication. If a listing is rejected, review the provided feedback, make the necessary updates and resubmit it for approval.\n\nBefore publishing make sure that your listing configuration is complete, you have the **ACCOUNTADMIN** role or **OWNERSHIP** privilege for the associated data product and all sample SQL queries in the listing are validated successfully. To submit your listing, sign in to Snowsight, navigate to **Data Products ➤ Provider Studio** , go to the Listings tab, select your draft listing and click **Submit for Approval** .\n\nSubmitting listing for final approval from Snowflake\n\n#### **Step 6** —Final Approval and Publishing\n\nOnce submitted, Snowflake will review your listing and provide an **Approved** or **Denied** status. If denied, review the feedback, make the necessary updates and resubmit the listing. After receiving approval, return to the **Listings** tab, select your approved listing and click **Publish** . Upon publication, the listing will be visible to consumers in all current and future Snowflake Marketplace regions. Regional availability can be managed through cross-cloud auto-fulfillment settings and you can create referral links for direct access to your listing.\n\n## Further Reading\n\n* [Snowflake Native Apps](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/)\n* [What Are Native Apps?](https://www.snowflake.com/guides/what-are-native-apps/)\n* [About the Snowflake Native App Framework](https://docs.snowflake.com/en/developer-guide/native-apps/native-apps-about)\n* [Snowflake Native App Framework on AWS and Azure](https://www.snowflake.com/en/blog/native-app-framework-available-aws-azure/)\n* [Introducing the Snowflake Native App Framework](https://www.snowflake.com/en/blog/introducing-snowflake-native-application-framework/)\n* [Getting Started with Snowflake Native Apps](https://quickstarts.snowflake.com/guide/getting_started_with_native_apps/)\n* [Snowflake Native Apps Example](https://github.com/snowflakedb/native-apps-examples)\n\n## Conclusion\n\nAnd that’s a wrap! Snowflake Native Apps are built using the Snowflake Native App Framework. This allows developers to create, test and launch apps right in Snowflake. The framework simplifies the process of building, launching and integrating advanced tools. It ensures security and governance by tapping into the Snowflake ecosystem. For providers, these apps provide an easy way to sell their solutions on the Snowflake Marketplace, reaching thousands of customers. Meanwhile, consumers get instant access to the apps without needing a complex setup.\n\nIn this article, we have covered:\n\n* What are Native Apps in Snowflake?\n* Key features and characteristics of Snowflake Native Apps\n* What are the benefits of Snowflake Native Apps for providers?\n* What are the benefits of Snowflake Native Apps for consumers?\n* How do Snowflake Native Apps work?\n* Step-by-step guide to create a Snowflake Native App\n* Monetization and distribution of Snowflake Native Apps\n* Step-by-step monetization process via Snowflake Marketplace\n\n… and so much more!\n\n## FAQs\n\n**What are Native Apps in Snowflake?**\n\nSnowflake Native Apps are designed specifically to operate within the Snowflake ecosystem without requiring external access or movement of sensitive data outside its environment.\n\n**How can I develop and test a Snowflake Native App locally?**\n\nDevelopers can set up their environments using tools like VSCode along with necessary extensions provided by Snowflakes such as CLI support.\n\n**Can I share my Snowflake Native App with other users?**\n\nYes! Once published on the marketplace after meeting compliance requirements.\n\n**Does the Snowflake Native App framework support logging and monitoring?**\n\nYes! Snowflake Native App framework includes telemetry tools that allow developers to monitor application performance post-deployment.\n\n**What is Streamlit’s role in Snowflake Native apps?**\n\nStreamlit allows developers to create interactive web interfaces that enhance user engagement directly within their Snowflake Native Applications.\n\n**Can I update my Snowflake Native App after deployment?**\n\nYes! Providers have control over release cycles allowing them to push updates seamlessly even after deployment.\n\nRelated posts:\n\n* [Empowering users with intuitive, actionable data: introducing Data Explorer](https://www.flexera.com/blog/it-visibility/empowering-users-with-intuitive-actionable-data-introducing-data-explorer/ \"Empowering users with intuitive, actionable data: introducing Data Explorer\")\n* [From Spot Eco to Flexera One Cloud Commitment Management: A new era of automated cloud cost optimization](https://www.flexera.com/blog/finops/from-spot-eco-to-flexera-one-cloud-commitment-management-a-new-era-of-automated-cloud-cost-optimization/ \"From Spot Eco to Flexera One Cloud Commitment Management: A new era of automated cloud cost optimization\")\n* [The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)](https://www.flexera.com/blog/finops/the-practical-finops-roadmap-series-what-to-do-before-you-start-practicing-finops-1-4/ \"The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)\")\n* [FinOps and ITAM: A unified approach to optimizing technology investments](https://www.flexera.com/blog/finops/finops-and-itam-a-unified-approach-to-optimizing-technology-investments/ \"FinOps and ITAM: A unified approach to optimizing technology investments\")\n* [Elevate your packaging efficiency: Introducing ‘My Requests’ in AdminStudio](https://www.flexera.com/blog/application-readiness/elevate-your-packaging-efficiency-introducing-my-requests-in-adminstudio/ \"Elevate your packaging efficiency: Introducing ‘My Requests’ in AdminStudio\")\n* [What’s new at Flexera: May 2025](https://www.flexera.com/blog/product/whats-new-at-flexera-may-2025/ \"What’s new at Flexera: May 2025\")\n\n### Want to know more?\n\nTechnology is evolving rapidly—and it's important to stay on top of the latest trends and critical insights. Check out the latest blogs related to FinOps below.\n\nFinOps\n\n## [2025 State of the Cloud](https://info.flexera.com/CM-REPORT-State-of-the-Cloud?lead_source=Website%20Visitor&id=Blog-Resources \"2025 State of the Cloud\")\n\nMarch 12, 2024\n\nFinOps\n\n## [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Request \"Cloud Cost Optimization demo\")\n\nFebruary 22, 2023\n\nFinOps\n\n## [Practical Guide for a Successful Cloud Journey](https://info.flexera.com/CM-GUIDE-Successful-Cloud-Journey?lead_source=Website%20Visitor&id=Blog-Resources \"Practical Guide for a Successful Cloud Journey\")\n\nFebruary 9, 2022\n\nFinOps\n\n## [Cloud Migration and Modernization Datasheet](https://www.flexera.com/sites/default/files/datasheet-foundation-cloudscape.pdf \"Cloud Migration and Modernization Datasheet\")\n\nFinOps\n\n## [Agentic FinOps for AI: autonomous optimization for Snowflake, Databricks and AI cloud costs](https://www.flexera.com/blog/finops/agentic-finops-for-ai-autonomous-optimization-for-snowflake-databricks-and-ai-cloud-costs/ \"Agentic FinOps for AI: autonomous optimization for Snowflake, Databricks and AI cloud costs\")\n\nFebruary 12, 2026\n\nFinOps\n\n## [Snowflake BUILD 2025: Quick recap of new features](https://www.flexera.com/blog/finops/snowflake-build-2025/ \"Snowflake BUILD 2025: Quick recap of new features\")\n\nJanuary 28, 2026\n\n×\n\nGet updates delivered to your inbox\n\nSubscribe\n\n## How can we help?\n\nSales Team\n\n[Community](https://community.flexera.com/s/)\n\nSubscribe\n\nFlexera\n\n* [](https://www.linkedin.com/company/flexera?elqTrackId=62e00a6465d449b0824c83c70706dff9&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"LinkedIn\")\n* [](https://twitter.com/flexera?elqTrackId=ab8f06bd7aea498e807592d19ac2ab00&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"Twitter\")\n* [](https://www.instagram.com/weareflexera?elqTrackId=fcfa0064605a42baaebfabe8fedd5c50&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"Instagram\")\n* [](https://www.youtube.com/user/FlexeraSoftware?elqTrackId=c6e9107020754655a13aca3ac7aa3cd4&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"YouTube\")\n\nBusiness challenge\n\n* Software renewals and audits\n* Software license management and optimization\n* SaaS spend management\n* Cloud cost management\n* IT asset lifecycle management\n* CMDB data quality\n* Accurate IT inventory\n* Security and regulatory risk management\n* Sustainable IT\n* AI-powered transformation\n* Public sector\n\nSpend management by vendor\n\n* IBM\n* Oracle\n* Microsoft\n* SAP\n* VMWare\n* ServiceNow\n* AWS\n* Salesforce\n* BMC\n* Adobe\n\nProducts\n\n* Flexera One\n* Snow Atlas\n* Application Readiness\n* Security\n* Integrations\n* All products\n\nCompany\n\n* About\n* Careers\n* Contact us\n* [Get support](https://community.flexera.com/s/support-hub)\n* Leadership\n* Media / press center\n* Partners\n* [Revenera.com](https://www.revenera.com)\n\n\\+1.800.374.4353\n\n* [English](https://www.flexera.com)\n* [Deutsch](https://www.flexera.de)\n\n* Privacy policy\n* Terms and conditions\n* Site map"],"full_content":null}],"errors":[],"warnings":[{"type":"warning","message":"Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.","detail":null}],"usage":[{"name":"sku_extract_excerpts","count":6}]}

{"search_id":"search_524af27749ce40228c61eb9324181811","results":[{"url":"https://docs.snowflake.com/en/user-guide/warehouses-multicluster","title":"Multi-cluster warehouses | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg™\nApache Iceberg™ Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Virtual warehouses Multi-cluster\n ... \nSection Title: Multi-cluster warehouses ¶ > What is a multi-cluster warehouse? ¶ > Maximized vs. auto-scale ¶\nContent:\nAs the number of concurrent user sessions and/or queries for the warehouse increases, and queries start to queue due to\ninsufficient resources, Snowflake automatically starts additional clusters, up to the maximum number defined for the warehouse.\nSimilarly, as the load on the warehouse decreases, Snowflake automatically shuts down clusters to reduce the number of\nrunning clusters and, correspondingly, the number of credits used by the warehouse.\nTo help control the usage of credits in Auto-scale mode, Snowflake provides a property, SCALING_POLICY, that determines the scaling policy\nto use when automatically starting or shutting down additional clusters. For more information, see Setting the scaling policy for a multi-cluster warehouse (in\nthis topic).\nTo create a multi-cluster warehouse, see Creating a multi-cluster warehouse (in this topic).\n ... \nSection Title: Multi-cluster warehouses ¶ > Creating a multi-cluster warehouse ¶\nContent:\nAll other tasks for multi-cluster warehouses (except for the remaining tasks described in this topic) are identical to single-cluster warehouse tasks .\nSection Title: Multi-cluster warehouses ¶ > Setting the scaling policy for a multi-cluster warehouse ¶\nContent:\nTo help control the credits consumed by a multi-cluster warehouse running in Auto-scale mode, Snowflake provides scaling policies.\nSnowflake uses the scaling policies to determine how to adjust the capacity of your multi-cluster warehouse\nby starting or shutting down individual clusters while the warehouse is running. You can specify a scaling policy\nto make Snowflake prioritize responsiveness and throughput for the queries in that warehouse, or to minimize costs\nfor that warehouse.\nThe scaling policy for a multi-cluster warehouse only applies if it is running in Auto-scale mode.\nIn Maximized mode, all clusters run concurrently, so there is no need to start or shut down individual clusters.\nSnowflake supports the following scaling policies:\nSection Title: Multi-cluster warehouses ¶ > Setting the scaling policy for a multi-cluster warehouse ¶\nContent:\n| Policy | Description | A new cluster starts… | An idle or lightly loaded cluster shuts down… |\n| Standard (default) | Prevents/minimizes queuing by favoring starting additional clusters over conserving credits. | When a query is queued, or if Snowflake estimates the currently running clusters don’t have |  |\n| enough resources to handle any additional queries, Snowflake increases the number of clusters |  |  |  |\n| in the warehouse. |  |  |  |\nSection Title: Multi-cluster warehouses ¶ > Setting the scaling policy for a multi-cluster warehouse ¶\nContent:\nFor warehouses with a MAX_CLUSTER_COUNT of 10 or less, Snowflake starts one additional cluster.\n ... \nSection Title: Multi-cluster warehouses ¶ > Setting the scaling policy for a multi-cluster warehouse ¶\nContent:\nNote\nA third scaling policy, Legacy, was formerly provided for backward compatibility. Legacy has been removed.\nAll warehouses that were using the Legacy policy now use the default Standard policy.\nYou can set the scaling policy for a multi-cluster warehouse when it is created or at any time afterwards,\neither in Snowsight or using SQL:\nSection Title: Multi-cluster warehouses ¶ > Setting the scaling policy for a multi-cluster warehouse ¶\nContent:\nSnowsight :\nWhen you select Multi-cluster Warehouse under Advanced Options in the New Warehouse dialog,\nyou can select the scaling policy from the Scaling Policy drop-down list.For an existing multi-cluster warehouse, in the navigation menu, select Compute » Warehouses . Then select Edit under the More menu ( … ).In the Scaling Policy field, select the desired value from the drop-down list.TipYou only see the Scaling Policy drop-down list when the warehouse you selected is a multi-cluster warehouse,\nand the maximum clusters value is higher than the minimum clusters value.\nSQL :\nExecute a CREATE WAREHOUSE or ALTER WAREHOUSE command with `SCALING_POLICY` set to the desired value.\nFor example, in SQL:\nCopy\n ... \nSection Title: Multi-cluster warehouses ¶ > Increasing or decreasing clusters for a multi-cluster warehouse ¶\nContent:\nThe effect of changing the maximum and minimum clusters for a running warehouse depends on whether it is running in\nMaximized or Auto-scale mode:\nMaximized:↑ max & min :\nSpecified number of clusters start immediately.\n↓ max & min :\nSpecified number of clusters shut down when they finish executing statements and the auto-suspend period elapses.\nAuto-scale:↑ max :\nIf `new_max_clusters > running_clusters` , no changes until additional clusters are needed.\n↓ max :\nIf `new_max_clusters < running_clusters` , excess clusters shut down when they finish executing statements and the scaling policy conditions are met.\n↑ min :\nIf `new_min_clusters > running_clusters` , additional clusters immediately started to meet the minimum.\n↓ min :\nIf `new_min_clusters < running_clusters` , excess clusters shut down when they finish executing statements and the scaling policy conditions are met."]},{"url":"https://www.flexera.com/blog/finops/snowflake-query-tuning-part1/","title":"Snowflake query optimization: A comprehensive guide 2026 (part 1)","publish_date":"2026-01-27","excerpts":["Section Title: Snowflake query optimization: A comprehensive guide 2026 (part 1)\nContent:\nBusinesses today have access to more data than ever before and with that abundance of data comes the need to analyze and utilize it. This is exactly where cloud-based data warehouses like Snowflake come in, which offer a powerful solution for analyzing, querying and monitoring data quickly and efficiently. However, with millions of queries running on these data warehouses, it’s essential to monitor query performance and optimize these workloads.\nIn this article, we’ll guide you through some best practices for optimizing Snowflake queries. This guide is divided into two-part series. In this part (part 1), we’ll discuss different Snowflake features that can enhance query performance. In the next part (part 2) , we’ll delve deeper into some common mistakes to avoid while writing Snowflake queries.\nSection Title: ... > 1) Choosing the Right Virtual Warehouse size\nContent:\nChoosing the “right-sized” Snowflake warehouse is critical for optimizing queries as it reduces Snowflake costs and maximizes Snowflake query performance.\nTo choose the ideal Snowflake warehouse size, it’s important to consider the specific needs/requirements of each individual query, like for complex queries that require extensive calculation, a larger warehouse is preferable; for simple queries, a smaller warehouse should suffice. Small warehouse sizes may also be sufficient for queries that don’t require much compute resources or for scenarios where cost optimization is a MAIN concern. Therefore, when scaling up or down the Snowflake warehouse size, it’s crucial to strike a perfect balance between the specific requirements of each query and select the appropriate or “right-sized” warehouse size to optimize query performance and reduce Snowflake costs.\nSection Title: ... > 1) Choosing the Right Virtual Warehouse size\nContent:\nIt is advised to start with the smallest warehouse size, X-SMALL and measure your workload’s performance. A good indicator of your warehouse being too small is the local and remote spillage shown by your queries. Read more about local and remote spillage and their possible solutions [here](https://community.snowflake.com/s/article/Performance-impact-from-local-and-remote-disk-spilling) . If Snowflake’s [Warehouse Load monitor](https://docs.snowflake.com/en/user-guide/warehouses-load-monitoring) shows a significant amount of queuing, you can make the Warehouse a multi-cluster warehouse with a maximum cluster size of 2 or 4.\n ... \nSection Title: ... > 4) Data clustering and micro-partitioning\nContent:\nIn Snowflake, all data is loaded by default into small chunks known as “micro-partitions”. Whenever the data is loaded into Snowflake tables, it automatically divides them into these micro-partitions, each containing between 50 MB to 500 MB of uncompressed data. As a result, each micro-partition corresponds to a group of rows and is arranged in a columnar fashion.\nLet’s suppose you have a huge table with a size of more than a terabyte or even a petabyte scale. It is crucial not only to store and manage such enormous data volumes but also to maximize snowflake query performance on such tables.\n ... \nSection Title: ... > 5) Using Snowflake Query Acceleration\nContent:\nLet’s say that when you run a query in Snowflake, it first checks to see if it has enough resources to run the query. If the cluster is already too busy, the query gets queued and put on hold, which can be a big problem when running massive workloads that include both short queries and long queries. Long-running queries also stop other queries from running. Most of the time, moving to a bigger warehouse is the only viable option. But if you run big queries on an X-LARGE cluster, execution times will be faster for sure. But as we’ve already talked about, this isn’t the best solution because short queries don’t use all of the resources, which can lead to huge overall query costs. Fortunately, the Query Acceleration Service (QAS) can act as a powerful additional cluster that’s temporarily available to deploy alongside your existing warehouses. When needed, it takes on some of the grunt work.\n ... \nSection Title: Snowflake query optimization: A comprehensive guide 2026 (part 1) > FAQs\nContent:\nSnowflake caching involves storing query results in caches at the Cloud Services and Data Warehouses layers. Caching improves query performance by reducing the need for remote storage access.\n**How does partition pruning work in Snowflake?**\nSnowflake uses partition pruning by utilizing metadata statistics to narrow down the search to specific micro-partitions, significantly improving query performance on large tables.\n**How can I identify and improve queries with poor performance in Snowflake?**\nMonitoring query history, examining statistics such as partitions scanned and rows fetched and identifying areas for improvement can help optimize queries in Snowflake.\n**What is the impact of warehouse size on query performance in Snowflake?**\nIncreasing warehouse size in Snowflake maximizes throughput by distributing the workload across more nodes, but it doesn’t directly increase query speed."]},{"url":"https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/","title":"FinOps on Snowflake: Built-In Cost and Performance Control","excerpts":["Section Title: FinOps on Snowflake\nContent:\nTime is money – save both with Snowflake.\n[explore the economic impact of snowflake](https://www.snowflake.com/resource/forrester-the-total-economic-impact-of-the-snowflake-ai-data-cloud/?utm_cta=website-cost-and-performance-forrester-tei)\nSection Title: FinOps on Snowflake > Go from painstaking configurations to a proven, fully-managed service\nContent:\nSince its founding in 2012, Snowflake has provided automated cluster management, maintenance and upgrades — all without downtime — so you can spend time on valuable data projects\nGet **out-of-the-box governance and security through Snowflake Horizon Catalog** without extra configurations or protocols\nSection Title: FinOps on Snowflake > Go from piecemeal dashboards to built-in cost & performance management\nContent:\nGet granular visibility, control and optimization of Snowflake spend through a unified Cost Management Interface .\nCheck query performance easily to proactively save on costs.\nAutomatically benefit from regular rollouts of performance improvements across all workloads.\nSection Title: FinOps on Snowflake > Maximize your Snowflake spend\nContent:\nAdd flexibility in how you use funds committed in your Snowflake Capacity contract.\nDeploy partner solutions faster by simplifying finance and procurement processes.\nBundle your spend to increase your buying power with Snowflake and partners.\nlearn more\nSection Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\nTravelpass CTC Natwest\nTravel and Hospitality “Now, we aren’t so focused on how to build things. We are focused more on what to build.” Dan Shah\nManager of Data Science Read the story * **1 week** for 130 Dynamic Tables to be in production after migration\n**65%** cost savings switching from Databricks to Snowflake\nRead the case study Financial Services “Now with fewer ephemeral failures and higher visibility in Snowflake, we have a platform that’s much easier and cost-effective to operate than managed Spark.” David Trumbell\nHead of Data Engineering, CTC Read the story * **1st** data availability deadline was hit everyday for the 1st time\n**54%** cost savings switching from managed Spark to Snowflake\nSection Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\nRead the case study Financial Services “The speed at which we’ve delivered wouldn’t have been possible with other providers.” Kaushik Ghosh Dastidar\nHead of ESG Cloud Solutions, NatWest Read the story * **6x** reduction in onboarding time from 3 months to 2 weeks\n**$750K** saved in salaries & staff training costs\nRead the case study\n[Resource #### Snowflake Joins the FinOps Foundation Snowflake joins The FinOps Foundation as a Premier Enterprise Member to provide thought leadership and set industry financial best practices. Read more](https://www.finops.org/members/snowflake/)\nResource #### Snowflake Pricing Calculator Curious about Snowflake pricing? Our Snowflake pricing calculator shows credit usage, warehouse costs, and total expenses. Access calculator\nSection Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\n[Guide #### Definitive Guide to Managing Spend in Snowflake Learn about considerations for consumption models such as Snowflake's, frameworks for better managing spend, and more. Get the guide](https://www.snowflake.com/en/resources/white-paper/definitive-guide-to-managing-spend-in-snowflake/)\nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation\nContent:\nAccess documentation on Managing Costs and Optimizing Performance in Snowflake.\n[Read about Managing Costs](https://docs.snowflake.com/en/user-guide/cost-management-overview)\n[Read about Optimizing Performance](https://docs.snowflake.com/en/guides-overview-performance)\nSection Title: FinOps on Snowflake > ... > Snowflake Documentation > On-Demand Cost Governance Training\nContent:\nLearn how to successfully examine, control, and optimize Snowflake costs.\n[Register Now](https://learn.snowflake.com/en/courses/OD-FINOPS/)\nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation > Professional Services\nContent:\nEngage Snowflake’s Professional Services for expert advice on optimizing your use of Snowflake.\n[Discover Professional Services](https://www.snowflake.com/snowflake-professional-services/)\nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation > Priority Support\nContent:\nLearn more about how our Priority Support team can help you reduce consumption spend through performance monitoring, observability, and management.\n[Learn about Priority Support](https://www.snowflake.com/support/priority-support/)\nSection Title: FinOps on Snowflake > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\n[start for free](https://signup.snowflake.com/)\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\nLearn * Resource Library\nLive Demos\nFundamentals\nTraining\nCertifications\nSnowflake University\nDeveloper Guides\nDocumentation\nPrivacy Policy\nSite Terms\nCommunication Preferences\nCookie Settings\nDo Not Share My Personal Information\nLegal\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")\n* Private preview, † Public preview, ‡ Coming soon"]},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization - Snowflake","excerpts":["Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\nThe best practice is to set the [STATEMENT_TIMEOUT_IN_SECONDS parameter](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) at different levels—for the account, warehouse, specific users, or\nindividual sessions—to tailor controls to different workload\npatterns, such as allowing longer timeouts for ETL warehouses\ncompared to BI warehouses. [Queued timeout policies](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) can also help remove queries that eclipse a reasonable time\nthreshold and could have been run elsewhere by users trying to\nreceive a response. **Policy-based automation** can also cancel queries pre-emptively.\nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\nAn example is using stored procedures that leverage the [SYSTEM$CANCEL_QUERY](https://docs.snowflake.com/en/sql-reference/functions/system_cancel_query) function to terminate statements that exceed predefined runtime\nthresholds or contain ill-advised logic, such as exploding joins. This approach allows you to more finely customize the types of\nqueries you want to cancel, as you have full control over defining\nthe stored procedure logic. **Auto-suspend policies** : Auto-suspend policies are a foundational\ncost control for virtual warehouses, automatically suspending a\nwarehouse after a defined period of inactivity.\n ... \nSection Title: Cost Optimization > Control > Overview > Govern resource creation and administration\nContent:\nTo prevent uncontrolled spend as organizations scale, it's essential to\nhave a clear management strategy for Snowflake resources, most notably,\nvirtual warehouses. This strategy should encompass a defined\nprovisioning process, ongoing object management, and automated platform\nenforcement to foster agility while maintaining financial discipline.\n**Centralized vs. decentralized management**\nOrganizations tend to adopt one of two primary approaches to managing\nSnowflake resources:\n ... \nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nSeparate warehouses by workload (e.g., ELT versus analytics versus\ndata science)\nWorkload size in bytes should match the t-shirt size of the warehouse\nin the majority of the workloads–larger warehouse size doesn’t always\nmean faster\nAlign warehouse size for optimal cost-performance settings\n[Utilize Multi-Cluster Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-considerations) configuration to solve for high concurrency\nUtilize [Query Acceleration Services](https://docs.snowflake.com/en/user-guide/query-acceleration-service) to help with infrequent, large data scans\nFor memory-intensive workloads, use a warehouse type of Snowpark\nOptimized or higher memory resource constraint configurations as\nappropriate\nSet appropriate auto-suspend settings - longer for high cache use,\nlower for no cache reuse\nSet appropriate warehouse query timeout settings for the workload and\nthe use cases it supports.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nAuto suspend\nMulti-cluster settings\nWarehouse resource constraints\nWarehouse type\nTo maintain an optimal balance between cost and performance, regularly\nmonitor your resource usage (e.g., weekly or monthly) and set up\nresource monitors to alert you to high credit consumption. When workload\ndemands change, adjust your settings as needed.\n**Warehouse consolidation**\nIf you find yourself with an excess of provisioned warehouses or a shift\nin workloads necessitating consolidation, apply the aforementioned\nprinciples. Begin with the least utilized warehouses and migrate their\nworkloads to an existing warehouse that handles similar tasks.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Leverage Managed Services\nContent:\n**[Query Acceleration Service](https://docs.snowflake.com/en/user-guide/query-acceleration-service) (QAS):** QAS is a serverless feature that provides a burst of additional\ncompute resources to accelerate specific parts of a query, rather than\nreplacing an appropriately sized warehouse. It's particularly beneficial\nfor large I/O operation queries, eliminating the need to manually scale\nwarehouses up or down. QAS also helps speed up query execution when\ntable clustering cannot be altered due to other workload dependencies. A\ncost-benefit analysis should always be performed to ensure that the\ncredit consumption from QAS is justified by the performance improvement\nand the avoided cost of a larger warehouse."]},{"url":"https://www.snowflake.com/en/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/","title":"Automatic Concurrency Scaling in Snowflake | Snowflake Auto Scaling","publish_date":"2024-08-05","excerpts":["Section Title: Category\nContent:\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\nApplications\nApr 28, 2016 | 5 min read\nSection Title: ... > The challenge with concurrency\nContent:\nToday we take a major step forward by extending our [elastic architecture](https://www.snowflake.com/cloud-data-platform/) to solve another major pain point in existing on-premises and cloud data warehousing solutions: how to run massively concurrent workloads at scale in a single system. Have you had the following experiences when building mission-critical applications that incorporate data analytics: * My application can only support a certain level of user concurrency due to the underlying data warehouse, which only allows 32-50 concurrent user queries.\nSection Title: ... > The challenge with concurrency\nContent:\nTo build my application, I need to acquire multiple data warehouse instances in order to isolate numerous workloads and users from each other. This adds to costs and complexity.\nWe have built our own scheduling policies around the data warehouse. We use query queues to control and prioritize incoming queries issued by our numerous users.\nDuring peak times, users are getting frustrated because their requests are getting queued or fail entirely.\nAt Snowflake, we separate compute from storage by introducing the unique concept of virtual data warehouses. That concept makes it possible to instantly resize virtual warehouses or pause them entirely. In addition, because of that concept Snowflake is the only cloud data warehousing solution that allows concurrent workloads to run without impacting each other.\n ... \nSection Title: ... > The challenge with concurrency\nContent:\nImagine you didn’t need users to adjust their workloads to accommodate data warehouse bottlenecks.\nImagine the data warehouse itself could detect increasing workloads and add additional compute resources as needed or shut-down/pause compute resources when workload activities subside again.\nImagine your application could scale out-of-the-box with one single (virtual) data warehouse without the need to provision additional data warehouses.\nImagine a world without any scheduling scripts and queued queries - a world in which you can leverage a smart data warehousing service that ensures all your users get their questions answered within the application’s SLA.\nWith Snowflake, we allow you to do that all of this for real, not just in your imagination, with our new multi-cluster data warehouse feature.\nSection Title: ... > Multi-cluster data warehouses\nContent:\nA virtual warehouse represents a number of physical nodes a user can provision to perform data warehousing tasks, e.g. running analytical queries. While a user can instantly resize a warehouse by choosing a different size (e.g. from small to 3X large), until now a virtual data warehouse in Snowflake always consisted of one physical cluster. With the recent introduction of multi-cluster warehouses, Snowflake supports allocating, either statically or dynamically, more resources for a warehouse by specifying additional clusters for the warehouse.\nThe figure above shows a multi-cluster DW that consists of three compute clusters. All compute clusters in the warehouse are of the same size. The user can choose from two different modes for the warehouse: * **Maximized:** When the warehouse is started, Snowflake always starts all the clusters to ensure maximum resources are available while the warehouse is running.\nSection Title: ... > Multi-cluster data warehouses\nContent:\n**Auto Scaling:** Snowflake starts and stops clusters as needed to dynamically manage the workload on the warehouse.\nAs always, in Snowflake a user can either leverage the user interface or use SQL to specify the minimum/maximum number of clusters per multi-cluster DW:\nSection Title: ... > Create Warehouse UI wizard\nContent:\nCreate Warehouse SQL script\nSection Title: ... > Create Warehouse UI wizard\nContent:\nSimilar to regular virtual warehouses, a user can resize all additional clusters of a multi-cluster warehouse instantly by choosing a different size (e.g. XS, S, M, L, …) either through the UI or programmatically via corresponding SQL DDL statements. In auto-scale mode, Snowflake automatically adds or resumes additional clusters (up to the maximum number defined by user) as soon as the workload increases. If the load subsides again, Snowflake shuts down or pauses the additional clusters. No user interaction is required - this all takes place transparently to the end user. For these decisions, internally, the query scheduler takes into account multiple factors. There are two main factors considered in this context: 1. The memory capacity of the cluster, i.e. whether clusters have reached their maximum memory capacity\n2. The degree of concurrency in a particular cluster, i.e.\nSection Title: ... > Create Warehouse UI wizard\nContent:\nwhether there are many queries executing concurrently on the cluster\nAs we learn more from our customers’ use cases, we will extend this feature further and share interesting use cases where multi-cluster data warehouses make a difference. Please stay tuned as we continue reinventing modern data warehousing and analytics by leveraging the core principles of cloud computing. *We would like to thank our co-authors, Florian Funke and Benoit Dageville, for their significant contributions as main engineer and architect, respectively, to making multi-cluster warehouses a reality. As always, keep an eye on the [blog](https://www.snowflake.com/blog) and our Snowflake Twitter feed [(@SnowflakeDB](https://twitter.com/SnowflakeDB) ) f or updates on Snowflake Computing. *"]},{"url":"https://www.unraveldata.com/resources/snowflake-multi-cluster-warehouse-done-right/","title":"Snowflake Multi Cluster Warehouses Done Right | Unravel Data","publish_date":"2025-12-01","excerpts":["Portal](https://customers.unraveldata.com/login?ec=302&startURL=%2Fs%2F)\n[Documentation](https://docs.unraveldata.com/?lang=en)\nContact Us\nSELF-GUIDED TOUR\nFREE HEALTH CHECK\nSELF-GUIDED TOUR\nFREE HEALTH CHECK\nSnowflake\nSection Title: Snowflake Multi Cluster Warehouses Done Right\nContent:\nOrganizations configure Snowflake multi cluster warehouse deployments with maximum cluster settings that rarely align with actual concurrency patterns. A warehouse set to scale up to 10 clusters might consistently run on 2-3 clusters, creating unnecessary capacity […]\n13 min read\nOrganizations configure Snowflake multi cluster warehouse deployments with maximum cluster settings that rarely align with actual concurrency patterns. A warehouse set to scale up to 10 clusters might consistently run on 2-3 clusters, creating unnecessary capacity overhead. Another warehouse capped at 3 clusters experiences sustained query queuing during peak periods, degrading performance for business-critical workloads.\nRight-sizing maximum cluster limits requires understanding actual concurrency patterns rather than theoretical capacity needs.\n ... \nSection Title: Snowflake Multi Cluster Warehouses Done Right > **Configuring Minimum Clusters and Scaling Policy**\nContent:\nOrganizations can combine minimum cluster, maximum cluster, and scaling policy settings to match specific workload patterns:\n**Always-on high concurrency:** MIN=3, MAX=6, Standard policy (maintains baseline capacity with aggressive scaling for peaks)\n**Cost-optimized variable load:** MIN=1, MAX=4, Economy policy (minimal baseline with conservative scaling)\n**Performance-critical with headroom:** MIN=2, MAX=8, Standard policy (strong baseline with substantial scale-up capacity)\n**Batch processing:** MIN=1, MAX=3, Economy policy (low baseline, limited scaling for scheduled workloads)\nConfiguration adjustments can occur at any time, even while the warehouse runs and processes queries.\n ... \nSection Title: ... > **Enterprise Challenges in Multi-Cluster Optimization**\nContent:\nA warehouse showing peak concurrency of 50 queries once during a quarterly business review doesn’t necessarily need maximum clusters sized for that single event. Distinguishing between recurring patterns worth configuring for and outlier events worth handling through temporary manual scaling requires sophisticated analysis. Scaling policy effectiveness varies by workload but isn’t immediately obvious from metrics.\nA warehouse running Economy policy might show acceptable performance for most queries while occasionally queuing time-sensitive operations that impact business productivity.\n ... \nSection Title: Snowflake Multi Cluster Warehouses Done Right > **Automated Multi-Cluster Optimization**\nContent:\nUnravel’s FinOps Agent moves from insight to action for Snowflake multi cluster warehouse optimization. Rather than just identifying configuration opportunities based on usage patterns, it automatically implements optimal cluster settings based on actual concurrency demands and configurable governance policies – all built natively on Snowflake system tables.\nThe FinOps Agent continuously analyzes warehouse load history, query patterns, and cluster utilization across your entire Snowflake environment.\n ... \nSection Title: Snowflake Multi Cluster Warehouses Done Right > **Automated Multi-Cluster Optimization**\nContent:\nThe agent’s concurrency analysis automatically categorizes workload patterns into stable high-concurrency, variable moderate-concurrency, batch processing, and development workloads based on historical query execution, timing patterns, and resource consumption characteristics.\nThis enables automatic identification of optimal configuration without manual workload classification.\nThe system determines appropriate cluster limits and scaling policies for each workload category. For maximum cluster optimization, the FinOps Agent analyzes peak concurrency patterns across different time windows – hourly peaks for daily patterns, daily peaks for weekly patterns, weekly peaks for monthly patterns. It identifies recurring peak levels that justify higher maximum clusters versus outlier events that don’t warrant permanent capacity increases.\nSection Title: Snowflake Multi Cluster Warehouses Done Right > **Automated Multi-Cluster Optimization**\nContent:\nConfiguration recommendations account for both typical and peak requirements with appropriate headroom.\nMinimum cluster and scaling policy optimization happens automatically based on workload performance sensitivity and cost efficiency goals. The agent detects user-facing workloads requiring rapid response times and recommends Standard scaling policy with higher minimum clusters. Batch processing workloads receive Economy policy recommendations with minimal baseline clusters. Dynamic optimization adapts recommendations as workload patterns evolve.\nSeasonal and event-driven adjustments occur automatically through calendar-based configuration changes.\n ... \nSection Title: Snowflake Multi Cluster Warehouses Done Right > **Automated Multi-Cluster Optimization**\nContent:\nIf maximum cluster reductions create sustained queuing beyond acceptable thresholds, the system automatically increases limits. If minimum cluster increases don’t eliminate observed queuing, scaling policy adjustments or further capacity additions occur automatically. Closed-loop optimization ensures configuration changes deliver intended outcomes. Organizations using Unravel’s automated multi-cluster optimization typically achieve 25-35 percent sustained cost reduction while maintaining or improving query performance.\nThe optimization happens continuously – as workloads evolve, user counts change, or business patterns shift, the FinOps Agent adapts Snowflake multi cluster warehouse configuration to maintain optimal efficiency."]},{"url":"https://yukidata.com/snowflake-finops-guide/","title":"Snowflake FinOps: Complete Guide to Automated Cost Optimization | Yuki","publish_date":"2025-09-19","excerpts":["Section Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > What is FinOps for Snowflake?\nContent:\n**Credit-based consumption:** At $2-4 per credit (depending on your region and edition) with cost scaling linearly from X-Small (1 credit/hour) to 6X-Large (512 credits/hour).\n**Query-level optimization:** Using QUERY_HISTORY and WAREHOUSE_METERING_HISTORY views to identify expensive queries before they ruin your budget.\n**Dynamic warehouse scaling:** Beyond basic auto-suspend, implementing intelligent scaling based on queue depth and query complexity\n**Storage and** [**data transfer**](https://yukidata.com/blog/snowflake-data-transfer-costs-complete-guide/) **:** Long-term storage and cross-regional replication can add significant costs if not managed with lifestyle policies and governance.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > What is FinOps for Snowflake?\nContent:\nThe big challenge here? Manual management. That same hands-on approach that worked so well for traditional infrastructure of the past breaks down when you apply the same method to millions of queries and dozens of ever-changing warehouses.\n*With Yuki, warehouse optimization is automated with a single toggle – no more manual resizing or monitoring.*\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > The Compliance Complexity\nContent:\nEnterprise FinOps needs control, not just cost reduction. You’ll find manual approaches falling short of this:\n**Granular spend attribution** across cost centers and teams\n**Real-time budget enforcement** to prevent runaway costs\n**Audit trails** for compliance and chargeback scenarios\n**Role-based access** maintaining security while enabling autonomy\nThese challenges only continue to compound as you scale, making manual management not just expensive, but impossible to maintain.\nSection Title: ... > 6 Ways to Improve Efficiency for Your Snowflake FinOps Setup\nContent:\nThe key to an efficient (and not super expensive) Snowflake FinOps setup is to be systematic with your approach. Address immediate optimization opportunities, then move on to more long-term scalability.\nSection Title: ... > : Implement Granular Cost Attribution\nContent:\n**The problem:** Snowflake’s native cost reporting only shows warehouse-level spending. You need query- and team-level attribution for effective chargeback.\n**The solution:** Build automated tagging and attribution using Snowflake’s metadata like this:\n```\n-- Set up cost attribution table\nCREATE TABLE cost_attribution AS\nSELECT\n  qh.query_id,\n  qh.user_name,\n  qh.warehouse_name,\n  qh.database_name,\n  qh.schema_name,\n  qh.credits_used_cloud_services,\n  -- Extract team from username pattern or use session context\n  CASE\n    WHEN qh.user_name ILIKE '%analytics%' THEN 'Analytics Team'\n    WHEN qh.user_name ILIKE '%eng%' THEN 'Engineering Team'\n    ELSE 'General'\n  END as team_attribution,\n  qh.start_time\nFROM snowflake.account_usage.query_history qh\nWHERE qh.start_time >= dateadd(day, -30, current_timestamp());\n```\nYou can use this to pull key metrics like:\n ... \nSection Title: ... > : Automated Warehouse Scaling\nContent:\n**The problem:** Snowflake’s auto-suspend helps with idle time, but it doesn’t actually optimize warehouse size when it comes to workload complexity.\n**The solution:** Implement workload-aware scaling. For example, use Snowflake’s WAREHOUSE_LOAD_HISTORY to track query depth and concurrency, then programmatically adjust sizes via Snowflake Python Connector or Snowpark API:\n```\nimport snowflake.connector\n\nconn = snowflake.connector.connect(\n    user='USER',\n    password='PASSWORD',\n    account='ACCOUNT'\n)\n\ncur = conn.cursor()\ncur.execute(\"\"\"\nSELECT AVG(avg_queued_load)\nFROM snowflake.account_usage.warehouse_load_history\nWHERE warehouse_name='COMPUTE_WH'\nAND start_time >= dateadd(minute, -5, current_timestamp());\n\"\"\")\n\nqueue_depth = cur.fetchone()[0]\n\nif queue_depth > 50:\n    cur.execute(\"ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = 'LARGE'\")\nelif queue_depth < 10:\n    cur.execute(\"ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = 'SMALL'\n```\nSection Title: ... > : Automated Warehouse Scaling\nContent:\nThis approach can help you [reduce warehouse costs](https://yukidata.com/blog/snowflake-warehouse-optimization-guide/) by 20-40% because it lets you match your compute size to actual workload requirements.\n ... \nSection Title: ... > : Automated Governance and Compliance\nContent:\n**The problem:** Manual governance doesn’t scale. Maintaining control without slowing growth becomes more and more impossible as teams grow.\n**The solution:** Intelligent guardrails that maintain control *and* enable autonomy:\n**Role-based resource limits** that prevent unauthorized warehouse creation or sizing\n**Automated compliance monitoring** to keep data governance policies in place\n**Policy-driven scaling** that allows you to apply optimization rules based on workload classification\n**Audit-ready reporting** so you can get complete visibility for compliance and chargeback\nThese six approaches build out a foundation for your FinOps platform to operate automatically, letting your teams focus on innovation while maintaining growth.\n*Yuki lets you organize Snowflake costs into business domains, apply budgets, and monitor daily usage trends – making FinOps governance actionable.*\n ... \nSection Title: ... > Fix #3: Optimizing Warehouse Sizing\nContent:\nThis lets you align your warehouse size with actual usage so you can avoid the trap of running a Large warehouse for a workload that only needs a Small.\n```\nSELECT\n  warehouse_name,\n  avg(avg_running) as avg_concurrent_queries,\n  avg(avg_queued_load) as avg_queue_depth\nFROM snowflake.account_usage.warehouse_load_history\nWHERE start_time >= dateadd(day, -7, current_timestamp())\nGROUP BY warehouse_name;\n```\nIf your avg_queue_depth > 100: Scale up your warehouse\nIf avg_concurrent_queries < 1: Consider smaller warehouses or using a longer auto-suspend"]},{"url":"https://docs.snowflake.com/en/user-guide/warehouses-considerations","title":"Warehouse considerations | Snowflake Documentation","excerpts":["Section Title: Warehouse considerations ¶ > Creating a warehouse ¶\nContent:\nWhen creating a warehouse, the two most critical factors to consider, from a cost and performance perspective, are:\nWarehouse size (that is, available compute resources)\nManual vs automated management (for starting/resuming and suspending warehouses).\nThe number of clusters in a warehouse is also important if you are using Snowflake Enterprise Edition (or higher) and multi-cluster warehouses . For more details, see Scaling Up vs Scaling Out (in this topic).\n ... \nSection Title: Warehouse considerations ¶ > ... > Using the default warehouse for Notebook apps ¶\nContent:\nPreview Feature — Open\nAvailable to all accounts.\nEach account is provisioned with the SYSTEM$STREAMLIT_NOTEBOOK_WH warehouse that is specifically designed to run Notebook Python code. This multi-cluster X-Small warehouse helps reduce cluster fragmentation, optimize costs, and improve bin-packing efficiency. For more\ndetails, see Default warehouse for notebooks .\n ... \nSection Title: Warehouse considerations ¶ > Scaling up vs scaling out ¶\nContent:\nSnowflake supports two ways to scale warehouses:\nScale up by resizing a warehouse.\nScale out by adding clusters to a multi-cluster warehouse (requires Snowflake Enterprise Edition or\nhigher).\nSection Title: Warehouse considerations ¶ > Scaling up vs scaling out ¶ > Warehouse resizing improves performance ¶\nContent:\nResizing a warehouse generally improves query performance, particularly for larger, more complex queries. It can also help reduce the\nqueuing that occurs if a warehouse does not have enough compute resources to process all the queries that are submitted concurrently. Note\nthat warehouse resizing is not intended for handling concurrency issues; instead, use additional warehouses to handle the workload or use a\nmulti-cluster warehouse (if this feature is available for your account).\nSnowflake supports resizing a warehouse at any time, even while running. If a query is running slowly and you have additional queries of\nsimilar size and complexity that you want to run on the same warehouse, you might choose to resize the warehouse while it is running; however,\nnote the following:\nSection Title: Warehouse considerations ¶ > Scaling up vs scaling out ¶ > Warehouse resizing improves performance ¶\nContent:\nAs stated earlier about warehouse size, larger is not necessarily faster; for smaller, basic queries that are already executing quickly,\nyou may not see any significant improvement after resizing.\nResizing a running warehouse does not impact queries that are already being processed by the warehouse; the additional compute resources,\nonce fully provisioned, are only used for queued and new queries.\nResizing between a 5XL or 6XL warehouse to a 4XL or smaller warehouse results in a brief period during which the customer is charged\nfor both the new warehouse and the old warehouse while the old warehouse is quiesced.\nTip\n ... \nSection Title: Warehouse considerations ¶ > ... > Multi-cluster warehouses improve concurrency ¶\nContent:\nEnterprise Edition Feature\nTo inquire about upgrading to Enterprise Edition, please contact [Snowflake Support](https://docs.snowflake.com/user-guide/contacting-support) .\nMulti-cluster warehouses are designed specifically for handling queuing and performance issues\nrelated to large numbers of concurrent users and/or\nqueries. In addition, multi-cluster warehouses can help automate this process if your number of users/queries tend to fluctuate.\nWhen deciding whether to use multi-cluster warehouses and the number of clusters to use per multi-cluster warehouse, consider the\nfollowing:\nSection Title: Warehouse considerations ¶ > ... > Multi-cluster warehouses improve concurrency ¶\nContent:\nIf you are using Snowflake Enterprise Edition (or a higher edition), all your warehouses should be configured as multi-cluster\nwarehouses. Unless you have a specific requirement for running in Maximized mode, multi-cluster warehouses should be configured to run in Auto-scale\nmode, which enables Snowflake to automatically start and stop clusters as needed. When choosing the minimum and maximum number of clusters for a multi-cluster warehouse:Minimum :\nKeep the default value of `1` ; this ensures that additional clusters are only started as needed. However, if\nhigh-availability of the warehouse is a concern, set the value higher than `1` . This helps ensure multi-cluster warehouse availability\nand continuity in the unlikely event that a cluster fails. Maximum :\nSet this value as large as possible, while being mindful of the warehouse size and corresponding credit costs.\nSection Title: Warehouse considerations ¶ > ... > Multi-cluster warehouses improve concurrency ¶\nContent:\nFor example, an\nX-Large multi-cluster warehouse with maximum clusters = `10` will consume 160 credits in an hour if all 10 clusters run\ncontinuously for the hour.\nSection Title: Warehouse considerations ¶ > ... > Multi-cluster warehouses improve concurrency ¶\nContent:\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nHow are credits charged for warehouses?\nHow does query composition impact warehouse processing?\nHow does warehouse caching impact queries?\nCreating a warehouse\nScaling up vs scaling out\nRelated content\nWorking with resource monitors\nData loading considerations\nLanguage: **English**\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês"]},{"url":"https://docs.snowflake.com/en/user-guide/performance-query-warehouse","title":"Optimizing warehouses for performance | Snowflake Documentation","excerpts":["Guides Performance optimization Optimizing warehouses for performance\nSection Title: Optimizing warehouses for performance ¶\nContent:\nIn the Snowflake architecture, virtual warehouses provide the computing power that is required to execute queries. Fine-tuning the compute\nresources provided by a warehouse can improve the performance of a query or set of queries.\nA warehouse owner or administrator can try the following warehouse-related strategies as they attempt to improve the performance of one or\nmore queries. As they adjust a warehouse based on one of these strategies, they can test the change by re-running the query and checking its execution time .\nWarehouse-related strategies are just one way to boost the performance of queries. For performance strategies involving how data\nis stored, refer to Optimizing storage for performance .\nSection Title: Optimizing warehouses for performance ¶\nContent:\n| Strategy | Description |\n| Reduce queues | Minimizing queuing can improve performance because the time between submitting a query and getting its results is longer when the |\n| query must wait in a queue before starting. |  |\n| Resolve memory spillage | Adjusting the available memory of a warehouse can improve performance because a query runs substantially slower when a warehouse runs |\n| out of memory, which results in bytes “spilling” onto storage. |  |\n| Increase warehouse size | The larger a warehouse, the more compute resources are available to execute a query or set of queries. |\n| Try query acceleration | The query acceleration service offloads portions of query processing to serverless compute resources, which speeds up the processing |\n| of a query while reducing its demand on the warehouse’s compute resources. |  |\nSection Title: Optimizing warehouses for performance ¶\nContent:\n| Strategy | Description |\n| Reduce queues | Minimizing queuing can improve performance because the time between submitting a query and getting its results is longer when the |\n| query must wait in a queue before starting. |  |\n| Resolve memory spillage | Adjusting the available memory of a warehouse can improve performance because a query runs substantially slower when a warehouse runs |\n| out of memory, which results in bytes “spilling” onto storage. |  |\n| Optimize the warehouse cache | Query performance improves if a query can read from the warehouse’s cache instead of from tables. |\n| Limit concurrently running queries | Limiting the number of queries that are running concurrently in a warehouse can improve performance because there are fewer queries |\n| putting demands on the warehouse’s resources. |  |\n ... \nSection Title: Optimizing warehouses for performance ¶\nContent:\nOverview of warehouses\nExploring execution times\nOptimizing storage for performance"]},{"url":"https://billigence.com/snowflake-autoscaling-and-concurrency/","title":"Snowflake Warehouses | Autoscaling & Concurrency - Billigence","publish_date":"2024-11-04","excerpts":["Section Title: Snowflake Warehouses | Autoscaling & Concurrency\nContent:\n[Snowflake](https://www.snowflake.com/en/) is a cloud-based data platform that enables organisations to store, process and analyse their data using a scalable and flexible architecture. Its architecture consists of three layers: storage, compute and services. The storage layer is responsible for storing data, the compute layer processes queries and analyses, and the services layer handles tasks such as query optimisation, security and metadata management.  This blog will explore an important feature, [Snowflake](https://billigence.com/snowflake) warehouses , which refers to the compute layer of the architecture.\nSnowflake’s warehouses are virtual compute clusters that process queries and perform analytical tasks on data stored in the platform’s storage layer. The warehouses can be scaled up or down in size and number of clusters to match the workload demands and the data processing requirements.\nSection Title: Snowflake Warehouses | Autoscaling & Concurrency > **Snowflake Warehouses Explained**\nContent:\n[Warehouses](https://billigence.com/blog/big-data-data-warehouses/) are a crucial part of the Snowflake data platform because they allow organisations to achieve high-performance data processing and analytics. Snowflake offers various warehouse sizes to suit different business needs, ranging from X-Small to 4X-Large and beyond. Organisations can choose the appropriate warehouse size based on their data processing requirements and budget.\nMoreover, warehouses enable Snowflake to offer unique features like autoscaling and concurrency. Autoscaling automatically adjusts the number of clusters in a warehouse to match the workload demand and optimise resource utilisation. Concurrency, on the other hand, refers to the ability of Snowflake warehouses to process multiple queries simultaneously, without sacrificing performance or accuracy. We’ll discuss autoscaling and concurrency in more detail later.\nSection Title: Snowflake Warehouses | Autoscaling & Concurrency > **Snowflake Warehouses Explained**\nContent:\nIn traditional data warehouses, multiple users querying the same data set can lead to contention for resources and slow down query performance. Snowflake’s architecture eliminates this problem by distributing the workload across multiple virtual warehouses, allowing each user to access and analyse the data they need without being affected by the queries of others.\nSection Title: Snowflake Warehouses | Autoscaling & Concurrency > **Warehouse Sizes**\nContent:\nWarehouses can be configured to be of standard size or multi-cluster. For the standard warehouse, you can choose from eight sizes. Snowflake makes the sizing convenient by classifying warehouse sizes into t-shirt sizes: X-Small, Small, Medium, Large, X-Large, 2X-Large, 3X-Large, 4X-Large.\nChoose which warehouse sizes fits the needs of the use case. For the multi-cluster warehouse, compute resources are configured to scale as query needs change. There are two modes in multi-cluster: auto-scale and maximised.\nMaximised: When the warehouse is started, Snowflake always starts all the clusters to ensure maximum resources are available while the warehouse is running.\nAuto Scaling: Snowflake starts and stops clusters as needed to dynamically manage the workload on the warehouse.\nSection Title: Snowflake Warehouses | Autoscaling & Concurrency > **Warehouse Sizes**\nContent:\nWarehouses are a key component of the Snowflake data platform, allowing users to scale their compute resources on demand to meet changing query workloads. The flexibility and elasticity of virtual warehouses make them particularly well-suited for organisations that need to support dynamic workloads with varying query requirements.\nChoosing the right size warehouse is important for optimising performance and cost. If a warehouse is too small, queries may run slower than expected, and if it’s too large, it may result in unnecessary costs. To choose the right size warehouse, customers should consider the size of their dataset, the complexity of their queries, and the number of concurrent users accessing the data.\nSection Title: Snowflake Warehouses | Autoscaling & Concurrency > **Warehouse Autoscaling**\nContent:\nOne of the benefits of Snowflake is the ability to scale warehouses up or down as needed, without having to move data or change application code. This allows customers to adjust their compute resources based on changing workloads or business needs, resulting in cost savings and improved performance.\nAutoscaling in Snowflake refers to the automatic adjustment of warehouse size based on demand. When a customer runs queries on Snowflake, the system monitors the workload and automatically adjusts the size of the warehouse to meet the demand. This means that when there is high demand, Snowflake will automatically scale up the warehouse size to provide more computing power. Conversely, when demand decreases, Snowflake will automatically scale down the warehouse size to save costs.\n ... \nSection Title: ... > [Report Whistleblowing](https://www.app.nntb.cz/en-us/page/UGFnZTozNDI5Nw==)\nContent:\nMenu Close\n[Privacy Policy](https://billigence.com/privacy-policy/)\n[Privacy Policy](https://billigence.com/privacy-policy/)\nMenu Close\n[Report Whistleblowing](https://www.nntb.cz/c/billigencewhistleblowing)\n[Report Whistleblowing](https://www.nntb.cz/c/billigencewhistleblowing)\n[](https://billigence.com/billigence-iso27001-information-management-security-systems-certified/)\nIllustrations by [Storyset](https://storyset.com/) | Icons by [Flaticon](https://www.flaticon.com/) | Animations by [LottieFiles](https://lottiefiles.com/)\n[Careers](https://billigence.com/careers/)\n[Privacy Policy](https://billigence.com/privacy-policy/)\n[Contact Us](https://billigence.com/contact-us/)\n[Careers](https://billigence.com/careers/)\n[Privacy Policy](https://billigence.com/privacy-policy/)\n[Contact Us](https://billigence.com/contact-us/)"]},{"url":"https://www.snowflake.com/en/blog/adaptive-compute-smarter-warehouses/","title":"Introducing Even Easier-to-Use Snowflake Adaptive Compute with Better Price/Performance ","publish_date":"2025-06-03","excerpts":["Section Title: Introducing Even Easier-to-Use Snowflake Adaptive Compute with Better Price/Performance\nContent:\nIn today’s AI-powered world, organizations must continuously find new ways to accelerate time to value from all their data to stay ahead of the competition. For decades, data and AI platforms have required organizations to spend valuable time and resources on cluster spin-up and spin-down, disruptive upgrades, manual performance tuning and laborious optimizations. In 2012, Snowflake revolutionized the data world with our virtual warehouses that had automated cluster management, maintenance, upgrades and regular performance improvements — all without any downtime — for a truly easy-to-use service. Continuously improving our compute to make warehouses even faster and even easier to use, with better price/performance has been a key priority.\nSection Title: Introducing Even Easier-to-Use Snowflake Adaptive Compute with Better Price/Performance\nContent:\nEvery day, we strive to be the market leader in fast performance for all workloads and continue to iterate rapidly to automatically improve performance by doing the heavy lifting on behalf of our customers. In fact, Snowflake is excited to announce that over the past 12 months ending May 2, 2025, Snowflake has delivered **2.1x faster performance for core analytics workloads** through **Standard Warehouse – Generation 2 (Gen2)** (generally available). Gen2 is an updated version of Snowflake’s current virtual Standard Warehouse that has upgraded hardware and additional performance enhancements. Snowflake is also excited to announce a new service called **Snowflake Adaptive Compute** (in private preview) that takes away the undifferentiated heavy lifting of making infrastructure choices to save time and operational overhead. Adaptive Compute enables enhanced ease of use with better price/performance.\nSection Title: Introducing Even Easier-to-Use Snowflake Adaptive Compute with Better Price/Performance\nContent:\nSnowflake will support warehouses created using the Adaptive Compute service that are called **Adaptive Warehouses** (in private preview). ## Even easier to use With Adaptive Compute, minimal configuration is required as Snowflake will automatically select the appropriate cluster size(s), the number of clusters, and auto-suspend/resume duration for jobs on your behalf. Users no longer need to think about the warehouse size, concurrency settings, multi-cluster warehouse settings, or auto-suspend/resume semantics. Even query routing is done intelligently to the right-sized clusters without any user action.\nSection Title: Introducing Even Easier-to-Use Snowflake Adaptive Compute with Better Price/Performance\nContent:\nWith Adaptive Warehouses created using this new compute service, users still interact with many familiar aspects of current virtual warehouses including having a similar billing model, getting granular information from ACCOUNT_USAGE views and the ability to use FinOps tools such as budgets and resource monitors . However, all jobs running on all the Adaptive Warehouses in an account go to a shared pool of resources within that account.\nSection Title: Introducing Even Easier-to-Use Snowflake Adaptive Compute with Better Price/Performance\nContent:\nConverting from a standard virtual warehouse to an Adaptive Warehouse is a simple alter command that’s done without any downtime. Customers have told us it takes a non-trivial amount of time and effort to consolidate workloads because warehouse names often are hard coded in pipelines and scripts, which could lead to disruption. With Adaptive Warehouses, users can simply convert their production workloads in batches, while still maintaining existing warehouse names, policies, permissions and showback/chargeback reporting structure. ## Even better price/performance At Snowflake, our product philosophy is focused on continuously and automatically enhancing performance for customers by regularly refining our core engine and doing the heavy lifting on behalf of our customers.\nSection Title: Introducing Even Easier-to-Use Snowflake Adaptive Compute with Better Price/Performance\nContent:\nIn addition to being effortless to use, Adaptive Compute will leverage the latest and greatest hardware and performance enhancements available from Snowflake. Adaptive Compute also has optimized resource sharing as queries leverage a shared pool of clusters within your account to maximize efficiency. ## What customers are saying “We are excited to be part of the private preview of Adaptive Warehouses, which will allow us to consolidate many warehouses across different workloads while still being able to effectively use built-in FinOps capabilities like Budgets. Adaptive Warehouses will make it effortless for Pfizer to manage and optimize our warehouses while delivering outsized performance improvements,” said Steve Ring, Director of Enterprise Database Solutions at Pfizer. ## Learn more * Get started with Standard Warehouse – Generation2\n ... \nSection Title: ... > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\n[start for free](https://signup.snowflake.com/)\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\nLearn * Resource Library\nLive Demos\nFundamentals\nTraining\nCertifications\nSnowflake University\nDeveloper Guides\nDocumentation\nPrivacy Policy\nSite Terms\nCommunication Preferences\nCookie Settings\nDo Not Share My Personal Information\nLegal\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"]},{"url":"https://yukidata.com/snowflake-concurrency-optimization/","title":"How to Handle High Concurrency and Big Spikes in Snowflake | Yuki","publish_date":"2025-05-21","excerpts":["Section Title: How to Handle High Concurrency and Big Spikes in Snowflake\nContent:\nBy Amir Peres\nApril 23, 2025 | 5 min read\nA nightmare Snowflake scenario that we’re all too familiar with? Dealing with large bursts of queries, without paying for oversized warehouses you hardly use.\nIf you’ve ever watched your queries stack up in a queue because you hit the warehouse concurrency limit, you know it’s more than a minor inconvenience.\nInstead of simply making our warehouse bigger or relying on multi-cluster scaling alone, Ideveloped a more effective approach that’s helped clients save **up to 30% on monthly Snowflake bills.**\nIn this article, I’ll explain:\nWhat concurrency is\nWhy bigger warehouses aren’t always better\nWhat the real problem is here (spoiler: it’s concurrency limits)\nHow to conquer your unpredictable queries (with real world examples!)\nHow you can plan your Snowflake setup to handle spikes in query traffic\nTips for tuning Snowflake concurrency\n ... \nSection Title: ... > Why Bigger Warehouses Alone Aren’t Enough\nContent:\nSnowflake sets a maximum number of queries that can run simultaneously on any single warehouse. This means that once you hit that limit, new queries go into a queue and wait until one of the active queries finishes.\nIf you want to avoid long wait times, you might think, “Let’s just switch to a larger warehouse or enable multi-cluster mode.”\nThat can help for a while, but it may also raise your bill if you’re only using that extra power during short, intense bursts.\nHere’s exactly why bigger warehouses can mean bigger bills in the long run:\n**Short Spikes Can Be Costly**\nThis is what I was talking about above. If your traffic is unpredictable, you get stuck withspinning up large warehouses or multiple clusters only to use them for a rare 20 minute period. Snowflake’s auto-suspend feature does help, but that time spent until shut down bill waits for no one.\n**Multi-Cluster Scaling Takes Time to Kick In**\nSection Title: ... > Why Bigger Warehouses Alone Aren’t Enough\nContent:\nEven though Snowflake automatically adds clusters when concurrency spikes, it can take around 20 seconds for a new cluster to be fully ready to handle queries. Those 20 seconds matter if you have urgent dashboards or real-time analytics that need immediate responses.\n**One Size Doesn’t Fit All**\nSome queries are quick (like small selects), others are huge (like complex aggregative queries), and they shouldn’t always share the same resources in a one-size-fits-all approach.\nI’ve seen a lot of Snowflake setups struggle with unpredictability. That’s why at [Yuki](https://yukidata.com/) , we set out to create a more balanced method that can handle up and down workloads while keeping costs in check.\nSection Title: How to Handle High Concurrency and Big Spikes in Snowflake > The Core Issue: Concurrency Limits\nContent:\nSnowflake’s concurrency model is simple on paper: each warehouse has a specific number of “slots” for running queries. When all slots are taken, new queries have to wait. That wait time is what frustrates end users – especially if you’re dealing with business dashboards or applications that need fast results.\nTo see how your queries are performing under concurrency, you can look at Snowflake’s built-in “Account Usage” views. One helpful table is SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY, which logs information about each query.\nFor example, you can run:\nSection Title: How to Handle High Concurrency and Big Spikes in Snowflake > The Core Issue: Concurrency Limits\nContent:\n```\n-- This query checks how many queries are running in parallel per second,\n\n-- so you can see when concurrency might be peaking.\n ... \nSection Title: How to Handle High Concurrency and Big Spikes in Snowflake > ... > Complete Real-Time Awareness\nContent:\nYuki continuously checks each warehouse’s concurrency level, queue length, and recent query performance. This helps you:\n**Proactively route new queries** to avoid building large queues.\n**Monitor spin-up times** of new warehouses.\nBecause Yuki manages the traffic, we’re able to monitor warehouse statistics **without** directly querying the Snowflake warehouses, such as:\n```\nSHOW WAREHOUSES;\n```\nSection Title: ... > Managing Snowflake Query Unpredictability: A Real-World Example\nContent:\nOne of our clients offers a reporting tool for their customers, and they experienced a big spike in queries at any hour during the day.\n**Before Yuki:** They thought about simply doubling the size of their Snowflake warehouse or relying heavily on multi-cluster scaling. Both approaches are valid, but they were worried about the potential cost and large queues that will affect the SLA.\n**After Yuki:** The moment concurrency started rising, we allowed immediate queries to jump to less-busy warehouses with minimum queue times. As a result, queue times dropped from over a minute to under 5 seconds, and monthly Snowflake bills went down roughly 30%.\n ... \nSection Title: ... > Final Tips for Tuning Concurrency in Snowflake\nContent:\nIf your concurrency jumps at certain times every day (like end-of-day reporting), schedule a slight ramp-up for that period. Don’t rely only on multi-cluster scaling to magically save you. Proactive planning can prevent many issues.\n**Revisit Strategy Often**\nYour concurrency patterns might shift from quarter to quarter as your user base or data volumes change. Check your concurrency data and warehouse usage at least monthly.\n**Invest in a Third-Party Tool**\nLook into getting a third-party tool like Yuki. Constantly monitoring and optimizing Snowflake performance is a whole job in and of itself, but it doesn’t have to be, especially if you look into investing in a plug-and-play opportunity like Yuki."]}],"usage":[{"name":"sku_search","count":1},{"name":"sku_extract_excerpts","count":2}]}

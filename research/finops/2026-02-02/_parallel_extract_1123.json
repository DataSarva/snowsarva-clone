{"extract_id":"extract_15dd1b79b41c471a9a1f845c2d15bca1","results":[{"url":"https://docs.snowflake.com/en/sql-reference/account-usage","title":"Account Usage | Snowflake Documentation","publish_date":"2021-11-01","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\n[Get started](/en/user-guide-getting-started)\n[Guides](/en/guides)\n[Developer](/en/developer)\n[Reference](/en/reference)\n[Release notes](/en/release-notes/overview)\n[Tutorials](/en/tutorials)\n[Status](https://status.snowflake.com)\n[Reference](/en/reference) [General reference](/en/sql-reference) [SNOWFLAKE database](/en/sql-reference/snowflake-db) Account Usage\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > Overview of Account Usage schemas [¶]( \"Link to this heading\")\nContent:\nACCOUNT_USAGE :\nViews that display object metadata and usage metrics for your account.\nIn general, these views mirror the corresponding views and table functions in the Snowflake [Snowflake Information Schema](info-schema) , but\nwith the following differences:\nRecords for dropped objects included in each view.\nLonger retention time for historical usage data.\nData latency.\nFor more details, see [Differences Between Account Usage and Information Schema]() (in this topic). For more details about each\nview, see [ACCOUNT_USAGE Views]() (in this topic).\nREADER_ACCOUNT_USAGE :\nViews that display object metadata and usage metrics for all the reader accounts that have been created for\nyour account (as a [Secure Data Sharing](../guides-overview-sharing) provider).\nThese views are a small subset of the ACCOUNT_USAGE views that apply to reader accounts. Also, each view in this schema contains an\nadditional `READER_ACCOUNT_NAME` column for filtering results by reader account.\nFor more details about each view, see [READER_ACCOUNT_USAGE Views]() (in this topic).\nNote that these views are empty if no reader accounts have been created for your account.\nSection Title: Account Usage [¶]( \"Link to this heading\") > Differences between Account Usage and Information Schema [¶]( \"Link to this heading\")\nContent:\nThe Account Usage views and the corresponding views (or table functions) in the [Snowflake Information Schema](info-schema) utilize identical\nstructures and naming conventions, but with some key differences, as described in this section:\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > Differences between Account Usage and Information Schema [¶]( \"Link to this heading\")\nContent:\nFor more details, see the following sections.\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > ... > Data latency [¶]( \"Link to this heading\")\nContent:\nDue to the process of extracting the data from Snowflake’s internal metadata store, the account usage views have some natural latency:\nFor most of the views, the latency is 2 hours (120 minutes).\nFor the remaining views, the latency varies between 45 minutes and 3 hours.\nFor details, see the list of views for each schema (in this topic). Also, note that these are all maximum time lengths; the actual\nlatency for a given view when the view is queried may be less.\nIn contrast, views/table functions in the [Snowflake Information Schema](info-schema) do not have any latency.\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > ACCOUNT_USAGE views [¶]( \"Link to this heading\")\nContent:\nThe ACCOUNT_USAGE schema contains the following views:\nSection Title: Account Usage [¶]( \"Link to this heading\") > ACCOUNT_USAGE views [¶]( \"Link to this heading\")\nContent:\n| View | Type | Latency [1] | Edition [3] | Notes |\n| [ACCESS_HISTORY](account-usage/access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_ACCESS_HISTORY](account-usage/aggregate_access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_QUERY_HISTORY](account-usage/aggregate_query_history) | Historical | 3 hours |  |  |\n| [AGGREGATION_POLICIES](account-usage/aggregation_policies) | Object | 2 hours |  |  |\n| [ALERT_HISTORY](account-usage/alert_history) | Historical | 3 hours |  | Data retained for 1 year. |\n| [ANOMALIES_DAILY](account-usage/anomalies_daily) | Historical | 3 hours |  | Data retained for 1 year. |\n| [APPLICATION_DAILY_USAGE_HISTORY](account-usage/application_daily_usage_history) | Historical | 24 hours |  | Data retained for 1 year. |\n| [APPLICATION_SPECIFICATION_STATUS_HISTORY](account-usage/application_specification_status_history) | Historical | 1 hour |  | Data retained for 1 year. |\n| [APPLICATION_SPECIFICATIONS](account-usage/application_specifications) | Historical | 1 hour |  | Data for deleted app specifications is retained for 1 year. |\n| [ARCHIVE_STORAGE_DATA_RETRIEVAL_USAGE_HISTORY](account-usage/archive_storage_data_retrieval_usage_history) | Historical | 1 hour |  | Data retained for 1 year. |\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > ACCOUNT_USAGE views [¶]( \"Link to this heading\")\nContent:\n| View | Type | Latency [1] | Edition [3] | Notes |\n| [ACCESS_HISTORY](account-usage/access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_ACCESS_HISTORY](account-usage/aggregate_access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_QUERY_HISTORY](account-usage/aggregate_query_history) | Historical | 3 hours |  |  |\n| [AGGREGATION_POLICIES](account-usage/aggregation_policies) | Object | 2 hours |  |  |\n| [CONTACTS](account-usage/contacts) | Object | 3 hours |  |  |\n| [COPY_FILES_HISTORY](account-usage/copy_files_history) | Historical |  |  | Data retained for 1 year. |\n| [COPY_HISTORY](account-usage/copy_history) | Historical | 2 hours [2] |  | Data retained for 1 year. |\n| [CORTEX_AISQL_USAGE_HISTORY](account-usage/cortex_aisql_usage_history) | Historical |  |  | Data retained for 1 year. |\n| [CORTEX_ANALYST_USAGE_HISTORY](account-usage/cortex_analyst_usage_history) | Historical | One hour |  | Data retained for 1 year. |\n| [CORTEX_DOCUMENT_PROCESSING_USAGE_HISTORY](account-usage/cortex_document_processing_usage_history) | Historical | 1 hour |  | Data retained for 1 year. |\n| [CORTEX_FINE_TUNING_USAGE_HISTORY](account-usage/cortex_fine_tuning_usage_history) | Historical | 1 hour |  | Data retained for 1 year. |\nSection Title: Account Usage [¶]( \"Link to this heading\") > ACCOUNT_USAGE views [¶]( \"Link to this heading\")\nContent:\n| View | Type | Latency [1] | Edition [3] | Notes |\n| [ACCESS_HISTORY](account-usage/access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_ACCESS_HISTORY](account-usage/aggregate_access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_QUERY_HISTORY](account-usage/aggregate_query_history) | Historical | 3 hours |  |  |\n| [AGGREGATION_POLICIES](account-usage/aggregation_policies) | Object | 2 hours |  |  |\n| [CORTEX_FUNCTIONS_QUERY_USAGE_HISTORY](account-usage/cortex_functions_query_usage_history) | Historical |  |  | Data retained for 1 year. |\n| [CORTEX_FUNCTIONS_USAGE_HISTORY](account-usage/cortex_functions_usage_history) | Historical |  |  | Data retained for 1 year. |\n| [CORTEX_PROVISIONED_THROUGHPUT_USAGE_HISTORY](account-usage/cortex_provisioned_throughput_usage_history) | Historical |  |  | Data retained for 1 year. |\n| [CORTEX_REST_API_USAGE_HISTORY](account-usage/cortex_rest_api_usage_history) | Historical |  |  | Data retained for 1 year. |\n| [CORTEX_SEARCH_DAILY_USAGE_HISTORY](account-usage/cortex_search_daily_usage_history) | Historical | 3 hours |  | Data retained for 1 year. |\n| [CORTEX_SEARCH_SERVING_USAGE_HISTORY](account-usage/cortex_search_serving_usage_history) | Historical | 1 hour |  | Data retained for 1 year. |\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > ACCOUNT_USAGE views [¶]( \"Link to this heading\")\nContent:\n| View | Type | Latency [1] | Edition [3] | Notes |\n| [ACCESS_HISTORY](account-usage/access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_ACCESS_HISTORY](account-usage/aggregate_access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_QUERY_HISTORY](account-usage/aggregate_query_history) | Historical | 3 hours |  |  |\n| [AGGREGATION_POLICIES](account-usage/aggregation_policies) | Object | 2 hours |  |  |\n| [NOTEBOOKS_CONTAINER_RUNTIME_HISTORY](account-usage/notebooks_container_runtime_history) | Historical | 3 hours |  |  |\n| [OBJECT_ACCESS_REQUEST_HISTORY](account-usage/object_access_request_history) | Historical | 3 hours |  |  |\n| [OBJECT_DEPENDENCIES](account-usage/object_dependencies) | Historical | 3 hours |  |  |\n| [ONLINE_FEATURE_TABLE_REFRESH_HISTORY](account-usage/online_feature_table_refresh_history) | Historical | 3 hours |  |  |\n| [OPENFLOW_USAGE_HISTORY](account-usage/openflow_usage_history) | Historical | 3 hours |  |  |\n| [OUTBOUND_PRIVATELINK_ENDPOINTS](account-usage/outbound_privatelink_endpoints) | Object | 2 hours | Business Critical (or higher) | Data for deleted endpoints is retained for 1 year. |\n| [PASSWORD_POLICIES](account-usage/password_policies) | Object | 2 hours |  |  |\n| [PIPES](account-usage/pipes) | Object | 2 hours |  |  |\nSection Title: Account Usage [¶]( \"Link to this heading\") > ACCOUNT_USAGE views [¶]( \"Link to this heading\")\nContent:\n| View | Type | Latency [1] | Edition [3] | Notes |\n| [ACCESS_HISTORY](account-usage/access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_ACCESS_HISTORY](account-usage/aggregate_access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_QUERY_HISTORY](account-usage/aggregate_query_history) | Historical | 3 hours |  |  |\n| [AGGREGATION_POLICIES](account-usage/aggregation_policies) | Object | 2 hours |  |  |\n| [PIPE_USAGE_HISTORY](account-usage/pipe_usage_history) | Historical | 3 hours |  | Data retained for 1 year. |\n| [POLICY_REFERENCES](account-usage/policy_references) | Object | 2 hours |  |  |\n| [POSTGRES_STORAGE_USAGE_HISTORY](account-usage/postgres_storage_usage_history) | Historical | 3 hours |  | Data retained for 1 year. |\n| [PRIVACY_BUDGETS](account-usage/privacy_budgets) | Object | 24 hours | Enterprise Edition (or higher) |  |\n| [PRIVACY_POLICIES](account-usage/privacy_policies) | Object | 2 hours | Enterprise Edition (or higher) |  |\n| [PROCEDURES](account-usage/procedures) | Object | 2 hours |  |  |\n| [PROJECTION_POLICIES](account-usage/projection_policies) | Object | 2 hours |  |  |\n| [QUERY_ACCELERATION_ELIGIBLE](account-usage/query_acceleration_eligible) | Historical | 3 hours |  | Data retained for 1 year. |\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > ACCOUNT_USAGE views [¶]( \"Link to this heading\")\nContent:\n| View | Type | Latency [1] | Edition [3] | Notes |\n| [ACCESS_HISTORY](account-usage/access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_ACCESS_HISTORY](account-usage/aggregate_access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_QUERY_HISTORY](account-usage/aggregate_query_history) | Historical | 3 hours |  |  |\n| [AGGREGATION_POLICIES](account-usage/aggregation_policies) | Object | 2 hours |  |  |\n| [REPLICATION_GROUPS](account-usage/replication_groups) | Object | 2 hours |  |  |\n| [REPLICATION_USAGE_HISTORY](account-usage/replication_usage_history) | Historical | 3 hours |  | Data retained for 1 year. |\n| [RESOURCE_MONITORS](account-usage/resource_monitors) | Object | 2 hours |  |  |\n| [ROLES](account-usage/roles) | Object | 2 hours |  |  |\n| [ROW_ACCESS_POLICIES](account-usage/row_access_policies) | Object | 2 hours |  |  |\n| [SCHEMATA](account-usage/schemata) | Object | 2 hours |  |  |\n| [SEARCH_OPTIMIZATION_BENEFITS](account-usage/search_optimization_benefits) | Historical | 6 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [SEARCH_OPTIMIZATION_HISTORY](account-usage/search_optimization_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > ... > ACCOUNT_ USAGE schema SNOWFLAKE database roles [¶]( \"Link to this heading\")\nContent:\nIn addition, you can grant finer control to accounts using SNOWFLAKE Database roles.\nFor more information on database roles, see [database roles](../user-guide/security-access-control-considerations.html) .\n[ACCOUNT_USAGE](#) schemas have four defined SNOWFLAKE database roles, each granted the SELECT privilege on specific views.\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > Querying the Account Usage views [¶]( \"Link to this heading\")\nContent:\nThis section includes considerations when querying the Account Usage views along with query examples.\n ... \nSection Title: Account Usage [¶]( \"Link to this heading\") > ... > Reconciling cost views [¶]( \"Link to this heading\")\nContent:\nThere are several Account Usage views that contain data related to the cost of compute resources, storage, and data transfers. If you are trying to reconcile these views against a corresponding view in the [ORGANIZATION_USAGE schema](organization-usage) , you must first set the timezone of the session to UTC.\nFor example, if you are trying to reconcile ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY to the account’s data in ORGANIZATION_USAGE.WAREHOUSE_METERING_HISTORY, you must run the following command before querying the Account Usage view:\nCopy\nSection Title: Account Usage [¶]( \"Link to this heading\") > Querying the Account Usage views [¶]( \"Link to this heading\") > Examples [¶]( \"Link to this heading\")\nContent:\nThe following examples show some typical/useful queries using the views in the [ACCOUNT_USAGE]() schema.\nNote\nThese examples assume the SNOWFLAKE database and the ACCOUNT_USAGE schema are in use for the current session. The examples also\nassume the ACCOUNTADMIN role (or a role granted IMPORTED PRIVILEGES on the database) is in use. If they are not in use, execute\nthe following commands before running the queries in the examples:Copy"],"full_content":null},{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/adding-custom-event-billing","title":"Add billable events to an application package | Snowflake Documentation","publish_date":null,"excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\n[Get started](/en/user-guide-getting-started)\n[Guides](/en/guides)\n[Developer](/en/developer)\n[Reference](/en/reference)\n[Release notes](/en/release-notes/overview)\n[Tutorials](/en/tutorials)\n[Status](https://status.snowflake.com)\n[Developer](/en/developer) [Snowflake Native App Framework](/en/developer-guide/native-apps/native-apps-about) Add billable events to an app\nSection Title: Add billable events to an application package [¶]( \"Link to this heading\")\nContent:\n[](../../_images/logo-snowflake-black.png) Feature — Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see [Support for private connectivity, VPS, and government regions](limitations.html) .\nWhen you use Custom Event Billing for a Snowflake Native App, you can charge for specific types of application usage in addition to the existing\nusage-based pricing plans. To set it up, you must perform two high-level steps:\nSet up your application package to emit billable events by following the steps in this topic.\n[Select a usage-based pricing plan with billable events](../../collaboration/provider-listings-pricing-model) for the listing you use to publish your Snowflake Native App to consumers.\nThis topic describes how to set up your application package to emit billable events using the [SYSTEM$CREATE_BILLING_EVENT](../../sql-reference/functions/system_create_billing_event) and [SYSTEM$CREATE_BILLING_EVENTS](../../sql-reference/functions/system_create_billing_events) system functions.\nSection Title: ... > Overview of billable events in an application package [¶]( \"Link to this heading\")\nContent:\nYou can set up your application package to emit billable events in response to specific usage events so that you can charge consumers based on\nhow much they use your Snowflake Native App.\nFor example, you can add a billable event to charge a consumer a specific amount for each call to a stored procedure in your Snowflake Native App.\nTo add billable events to an application package, do the following:\nCreate stored procedures to define which usage events trigger calls to the [SYSTEM$CREATE_BILLING_EVENT](../../sql-reference/functions/system_create_billing_event) and [SYSTEM$CREATE_BILLING_EVENTS](../../sql-reference/functions/system_create_billing_events) system functions.NoteYou cannot test the output of the system function at this stage. This system function can only be called from a Snowflake Native App\ninstalled in a consumer account.\nAdd those stored procedures to the setup script of the application package.\nImportant\nSnowflake supports billable events that are emitted by calling the system function within a stored procedure in the application,\nas outlined by the examples in this topic.\nSnowflake does not support other methods of calculating the base charge for billable events, such as methods that use the output of a\ntable or user-defined function that outputs consumer activity or methods that use telemetry logged in an event table.\nSection Title: ... > Overview of billable events in an application package [¶]( \"Link to this heading\")\nContent:\nIf you’re uncertain whether a proposed implementation will be supported, contact your Snowflake account representative.\nSection Title: Add billable events to an application package [¶]( \"Link to this heading\") > Billable event examples [¶]( \"Link to this heading\")\nContent:\nThe examples in this section show how to create stored procedures to emit billable events for common billing\nscenarios. Each of these examples calls the `createBillingEvent` function.\nSection Title: ... > Call the SYSTEM$CREATE_ BILLING_ EVENT system function [¶]( \"Link to this heading\")\nContent:\nThe following example shows how to create a wrapper function in a stored procedure to call the [SYSTEM$CREATE_BILLING_EVENT](../../sql-reference/functions/system_create_billing_event) system function.\nNote\nYou can call this system function in a stored procedure written in JavaScript, Python, or Java.\nThis example creates a JavaScript stored procedure named `custom_event_billing` in the schema version that is accessible to the procedures that emit billing. The stored procedure creates a helper function called `createBillingEvent` which takes arguments that correspond to the typed parameters expected by the SYSTEM$CREATE_BILLING_EVENT system function.\nFor more details about the parameters and the required types, see [SYSTEM$CREATE_BILLING_EVENT](../../sql-reference/functions/system_create_billing_event) .\nCopy\nThe examples in this topic call this helper function.\nSection Title: ... > Batch multiple billing events with the SYSTEM$CREATE_ BILLING_ EVENTS system function [¶]( \"Link to this heading\")\nContent:\nThe following example stored procedure shows how to batch multiple Snowflake Native App billing events with the SYSTEM$CREATE_BILLING_EVENTS system function. By using batches, you save time, reduce the likelihood of exceeding call limits, and ensure your billing events are set up correctly.\nFor more details about the parameters and the required types, see [SYSTEM$CREATE_BILLING_EVENTS](../../sql-reference/functions/system_create_billing_events) .\nCopy\nSection Title: ... > Example: Billing based on calls to a stored procedure [¶]( \"Link to this heading\")\nContent:\nThe following example shows how to create a stored procedure to emit a billable event when a consumer calls\nthat stored procedure in a Snowflake Native App.\nAdd this example code to your setup script in the same stored procedure that defines the helper function:\nCopy\nThis example code creates a stored procedure that calls the `createBillingEvent` function to emit a billable event\nwith the class name `PROCEDURE_CALL` and a base charge of `1.0` .\nNote\nThe types of the arguments passed to the `createBillingEvent` function must correspond to the typed parameters\nexpected by the [SYSTEM$CREATE_BILLING_EVENT](../../sql-reference/functions/system_create_billing_event) system function.\nSection Title: ... > Example: Billing based on rows consumed by a Snowflake Native App [¶]( \"Link to this heading\")\nContent:\nThe following example shows how to create a stored procedure to emit a billable event based on the number of\nrows consumed within a table in the consumer account.\nAdd this example code to your setup script in the same stored procedure that defines the helper function:\nCopy\nThis example code creates a stored procedure that calls the `createBillingEvent` function to emit a billable event\nwith the class name `ROWS_CONSUMED` and a calculated base charge of `2.5` multiplied by the number of rows in the `db_1.public.t1` table in the consumer account.\nNote\nThe types of the arguments passed to the `createBillingEvent` function must correspond to the typed parameters\nexpected by the [SYSTEM$CREATE_BILLING_EVENT](../../sql-reference/functions/system_create_billing_event) system function.\nSection Title: ... > Example: Billing based on the number of rows ingested [¶]( \"Link to this heading\")\nContent:\nThe following example shows how to create a stored procedure to emit a billable event based on the number of rows\ningested into a table.\nAdd this example code to your setup script in the same stored procedure that defines the helper function:\nCopy\nThis example code creates a stored procedure that calls the `createBillingEvent` function to emit a billable event\nwith the class name `ROWS_CHANGED` and a calculated base charge of `2.5` multiplied by the number of rows\ningested in the `db_1.target_table` table.\nNote\nThe types of the arguments passed to the `createBillingEvent` function must correspond to the typed parameters\nexpected by the [SYSTEM$CREATE_BILLING_EVENT](../../sql-reference/functions/system_create_billing_event) system function.\nSection Title: ... > Example: Billing based on monthly active rows [¶]( \"Link to this heading\")\nContent:\nMonthly active rows are the number of rows inserted or updated for the first time within a calendar month. Some\nproviders use this metric to only charge consumers for unique rows updated in a month. You can modify this example to instead\ncount unique users, or identify a unique data load location to determine a base charge.\nThe following example shows how to create a stored procedure to emit a billable event based on the number of\nmonthly active rows. Add this example code to your setup script in the same stored procedure that defines the helper function:\nCopy\nThis example code creates a stored procedure that determines the number of monthly active rows using a merge query to identify unique\nrows. The example then calculates the base charge using the value of the `monthlyActiveRows` variable and the `billing_quantity` .\nThe base charge is then passed to the `createBillingEvent` function.\nNote\nThe types of the arguments passed to the `createBillingEvent` function must correspond to the typed parameters\nexpected by the [SYSTEM$CREATE_BILLING_EVENT](../../sql-reference/functions/system_create_billing_event) system function.\nIn your setup script, add this stored procedure after the [stored procedure that calls the SYSTEM$CREATE_BILLING_EVENT system function]() .\nSection Title: ... > Snowpark Python example: Billing based on rows consumed [¶]( \"Link to this heading\")\nContent:\nTo write your stored procedure in Snowpark Python to bill based on rows consumed by your Snowflake Native App, use the following example:\nCopy\nThis example code creates a stored procedure that defines a helper method that calls the SYSTEM$CREATE_BILLING_EVENT system function,\nas well as a method that calls that helper method, `createBillingEvent` , to emit a billable event\nwith the class name `ROWS_CONSUMED` and a base charge calculated by multiplying a price of `2.5` US dollars by the number of rows in\nthe `db_1.public.t1` table in the consumer account.\nNote\nThe types of the arguments passed to the `createBillingEvent` function must correspond to the typed parameters\nexpected by the [SYSTEM$CREATE_BILLING_EVENT](../../sql-reference/functions/system_create_billing_event) system function.\nSection Title: Add billable events to an application package [¶]( \"Link to this heading\") > Test custom event billing [¶]( \"Link to this heading\")\nContent:\nTo make sure that you set up Custom Event Billing properly and that billable events are emitted for usage events as you expect,\ndo the following:\nUpdate your application package:\nUpdate your setup script to include the stored procedures that emit billable events.\nUpdate your application package with the new setup script.\nUpdate the version and release directive for your application package.\nShare the application package with a consumer account in your organization that you have access to:\n[Create a private listing](../../collaboration/provider-listings-creating-publishing.html) .\nAdd [Custom Event Billing as the pricing plan](../../collaboration/provider-listings-pricing-model.html) for the listing.\nShare it with the consumer account.\nSign in to the consumer account using Snowsight.\nInstall the Snowflake Native App.\nConfirm that the [stored procedures successfully emit billable events]() .\nConfirm that the [listing is set up properly]() .\nNote\nWhen you test Custom Event Billing, you must [set up a payment method](../../collaboration/consumer-listings-paying) but you will not be charged for usage within your organization.\nSection Title: ... > Validate whether the stored procedures emit billable events [¶]( \"Link to this heading\")\nContent:\nWhile signed in to the consumer account with which you shared your listing, call the stored procedures that you added to your Snowflake Native App.\nFor example, to test the stored procedure created for [billing based on monthly active rows]() , do the following:\nSign in to the consumer account in Snowsight.\nOpen a worksheet and set the context to `db_1.public` .\nRun the following SQL statement:CopyIf the stored procedure returns `Success` , your code is working.\nNote\nIf you run these SQL commands in the provider account that you used to create the application package, you see an error.\nSection Title: ... > Validate the custom event billing pricing plan [¶]( \"Link to this heading\")\nContent:\nTo validate the consumer experience of a Snowflake Native App and confirm that the listing and application package are set up properly, you can query\nthe [MARKETPLACE_PAID_USAGE_DAILY View](../../collaboration/views/marketplace-paid-usage-daily-ds) in the DATA_SHARING_USAGE schema of the shared SNOWFLAKE database.\nNote\nDue to latency in the view, run these queries at least two days after first using the Snowflake Native App.\nTo confirm that billable events are successfully generated by a Snowflake Native App and listing,\nrun the following SQL statement in the consumer account that you shared the listing with:\nNote\nReplace the PROVIDER_ACCOUNT_NAME and PROVIDER_ORGANIZATION_NAME values with those of the provider account.\nCopy\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\n[Share your feedback](/feedback)\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\n[Privacy Notice](https://www.snowflake.com/privacy-policy/) [Site Terms](https://www.snowflake.com/legal/snowflake-site-terms/) Cookies Settings © 2026 Snowflake, Inc. All Rights Reserved.\nOn this page\n[Overview of billable events in an application package]()\n[Billable event examples]()\n[Test custom event billing]()\nRelated content\nSection Title: ... > Validate the custom event billing pricing plan [¶]( \"Link to this heading\")\nContent:\n[About the Snowflake Native App Framework](/developer-guide/native-apps/native-apps-about)\n[SYSTEM$CREATE_BILLING_EVENT](/developer-guide/native-apps/../../sql-reference/functions/system_create_billing_event)\n[Paid listings pricing models](/developer-guide/native-apps/../../collaboration/provider-listings-pricing-model)\nLanguage: **English**\n[English](/en/developer-guide/native-apps/adding-custom-event-billing)\n[Français](/fr/developer-guide/native-apps/adding-custom-event-billing)\n[Deutsch](/de/developer-guide/native-apps/adding-custom-event-billing)\n[日本語](/ja/developer-guide/native-apps/adding-custom-event-billing)\n[한국어](/ko/developer-guide/native-apps/adding-custom-event-billing)\n[Português](/pt/developer-guide/native-apps/adding-custom-event-billing)"],"full_content":null},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization","publish_date":null,"excerpts":["Section Title: Cost Optimization > Recommendations\nContent:\n**Business Impact**\n**Consider cost as a design constraint:** Integrate cost\nconsiderations into the architecture and design process from the\nvery beginning, making it a key non-functional requirement alongside\nperformance, security, and reliability. **Quantify value:** Develop metrics to quantify the business value\ndelivered by cloud resources (e.g., revenue per Snowflake Credit,\ncost per customer, efficiency gains). **Trade-off analysis:** Understand the inherent trade-offs between\ncost, performance, reliability, and security, and make informed\ndecisions that align with business priorities. **Measure business value KPIs baseline:** Once metrics to quantify\nbusiness value are identified and trade-offs between cost,\nperformance, and reliability are established, you need to document a\n“baseline measurement” in order to track progress again. Furthermore, you should establish a regular cadence for refreshing\nthis measurement to ensure value realization is in line with\nexpectations and business goals. **Visibility**\n**Understand Snowflake’s resource billing models:** Review\nSnowflake’s billing models to align technical and non-technical\nresources on financial drivers and consumption terminology.\n ... \nSection Title: Cost Optimization > Business Impact > Overview > Consider cost as a design constraint\nContent:\nAt the ingestion layer, best practices include balancing latency versus\ncost by selecting appropriate services (e.g., Snowpipe, Snowpipe\nStreaming, or third-party tools) and choosing the right storage format\n(e.g., native tables, Iceberg). For transformations, design with\nfrequency versus SLA in mind to ensure data freshness matches the\nbusiness need. For analytics, apply schema design best practices such as\nthoughtful clustering key choices and pruning strategies to reduce\nconsumed credits. In distribution, optimize data transfer by monitoring\negress patterns and applying cost-saving practices like the [Snowflake Data Transfer Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) .\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\nIt is essential to review Snowflake's billing models to align technical\nand non-technical resources on financial drivers and consumption\nterminology. Snowflake's elastic, credit-based consumption model charges\nseparately for compute (Virtual Warehouses, Compute Pools, etc),\nstorage, data transfer, and various serverless features (e.g., Snowpipe,\nAutomatic Clustering, Search Optimization, Replication/Failover, AI\nServices). Understanding the interplay of these billing types ensures\nyou can attribute costs associated with each category’s unique usage\nparameters. High-level categories are below.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\nThe cost varies by feature and is outlined\nin [Snowflake’s Credit Consumption Document](/legal-files/CreditConsumptionTable.pdf) . **Cloud services layer:** This encompasses essential background\nservices, including query compilation, metadata management,\ninformation schema access, access controls, and authentication. Usage\nfor cloud services is only charged if the daily consumption of cloud\nservices exceeds 10% of the daily usage of virtual warehouses. **AI features:** Snowflake additionally offers artificial intelligence\nfeatures that run on Snowflake-managed compute resources, including\nCortex AISQL functions (e.g. COMPLETE, CLASSIFY, etc. ), Cortex\nAnalyst, Cortex Search, Fine Tuning, and Document AI. The usage of\nthese features, often with tokens, are converted to credits to unify\nwith the rest of Snowflake’s billing model. Details are listed in the\nCredit Consumption Document. **Data transfer:** Data transfer is the process of moving data into\n(ingress) and out of (egress) Snowflake. This generally happens via\negress on cross-region [data replication](https://docs.snowflake.com/en/user-guide/account-replication-cost) , [copying into/out of stage, function calls](https://docs.snowflake.com/user-guide/cost-understanding-data-transfer) ,\nand cross/same region [SPCS data transfer](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) .\nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\nDepending on the cloud provider and the region used during data\ntransfer, charges vary. **Data sharing & rebates:** Snowflake offers an opt-out Data\nCollaboration rebate program that allows customers to offset credits\nby data consumed with shared outside organizations. This rebate is\nproportional to the consumption of your shared data by consumer\nSnowflake accounts. See the latest terms and more details [here](/en/legal/) .\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\nTo deliver clear and actionable consumption insights, it is essential to\nleverage the rich usage data that Snowflake natively provides. The\nfoundation for all cost visibility is the **SNOWFLAKE** database, which\ncontains two key schemas for this purpose: [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) (for granular, account-level data) and [ORGANIZATION_USAGE](https://docs.snowflake.com/en/sql-reference/organization-usage) (for a consolidated view across all accounts).\nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n| Metric Category | Description | Key Metrics | Primary Data Sources |\n| Compute & query metrics | Understand the cost of query execution, warehouse consumption, and overall compute health. These are often the most dynamic and largest portion of your spend. | - Credits used: total credits by warehouse |  |\n| - Query performance: execution time, bytes scanned, compilation time, parameterized query hash |  |  |  |\n| - Warehouse health: % idle time, queueing, spilling, concurrency | - `ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY` (hourly warehouse credit usage) |  |  |\n| - `ACCOUNT_USAGE.QUERY_HISTORY` (detailed query metrics and associated warehouses) |  |  |  |\n| Storage metrics | Costs for compressed data, including active data, Time Travel, and Fail‑safe. | - Storage volume (avg monthly compressed GB/TB) |  |\n| - Inactive storage (Time Travel, Fail‑safe) |  |  |  |\n| - Storage growth rates |  |  |  |\n| - Table access (stale/unused) | - `ACCOUNT_USAGE.TABLE_STORAGE_METRICS` |  |  |\n| - `ACCOUNT_USAGE.DATABASE_STORAGE_USAGE_HISTORY` |  |  |  |\n| - `ACCOUNT_USAGE.ACCESS_HISTORY` |  |  |  |\n| Serverless & AI metrics | Track credit consumption by Snowflake‑managed services and AI features. | - Credits used by service |  |\n| - Cost per credit‑consuming events | - `ACCOUNT_USAGE.<Serverless Feature>_HISTORY` |  |  |\n| - `ORGANIZATION_USAGE.METERING_DAILY_HISTORY` |  |  |  |\nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n| Metric Category | Description | Key Metrics | Primary Data Sources |\n| Compute & query metrics | Understand the cost of query execution, warehouse consumption, and overall compute health. These are often the most dynamic and largest portion of your spend. | - Credits used: total credits by warehouse |  |\n| - Query performance: execution time, bytes scanned, compilation time, parameterized query hash |  |  |  |\n| - Warehouse health: % idle time, queueing, spilling, concurrency | - `ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY` (hourly warehouse credit usage) |  |  |\n| - `ACCOUNT_USAGE.QUERY_HISTORY` (detailed query metrics and associated warehouses) |  |  |  |\n| - AI views such as `CORTEX_FUNCTIONS_USAGE_HISTORY` , `CORTEX_ANALYST_USAGE_HISTORY` , `DOCUMENT_AI_USAGE_HISTORY` |  |  |  |\n| Data transfer | Cost of moving data into (ingress) and out of (egress) Snowflake, especially cross‑region/cloud. | - Bytes transferred |  |\n| - Transfer cost by destination |  |  |  |\n| - Replication vs. egress | - `ACCOUNT_USAGE.DATA_TRANSFER_HISTORY` |  |  |\n| - `ORGANIZATION_USAGE.DATA_TRANSFER_DAILY_HISTORY` |  |  |  |\n| Financial metrics | Translate credits to currency and provide org‑wide spend view. | - Overall dollar spend (daily) |  |\n| - Spend by service type | - `ORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY` |  |  |\n| - `ORGANIZATION_USAGE.RATE_SHEET_DAILY` |  |  |  |\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Snowsight's built-in cost management capabilities:** Snowsight\nprovides pre-built visuals for usage and credit monitoring directly\nwithin the [Snowflake Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) . It allows filtering by tags (e.g., view cost by department tag),\ncredit consumption by object types, and cost insights to optimize the\nplatform. **Creating custom dashboards or Streamlit apps for different stakeholder groups:** Snowsight facilitates the creation of custom\ndashboards using ACCOUNT_USAGE and ORGANIZATION_USAGE views. Custom\ncharts in the Dashboards feature and Streamlit apps can both be easily\nshared. Combined with cost allocation and tagging, this allows for\ntailored views for finance managers (aggregated spend), engineering\nmanagers (warehouse utilization), or data analysts (query\nperformance). **Integrating with third-party BI tools for advanced analytics:** Connecting to Snowflake from tools like Tableau, Power BI, Looker, or\ncustom applications offers highly customizable and extensive control\nover cost data visualization. Cloud-specific third-party data programs\n(FinOps platforms) offer easier setup and more out-of-the-box\nSnowflake cost optimization insights.\nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Leverage Cortex Code (In Preview):** This AI Assistant capability\nallows users to query cost and usage data in ACCOUNT_USAGE views using\nnatural language natively in the Snowsight UI.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nFor deeper integration and automation, organizations can review [anomalies programmatically](https://docs.snowflake.com/en/user-guide/cost-anomalies-class) using the SQL functions and views available within the SNOWFLAKE.LOCAL\nschema. This approach is important for enabling automation and\nscalability, allowing cost governance to be embedded directly into\noperational workflows, such as feeding anomaly data into third-party\nobservability tools or triggering automated incident response playbooks.\nA key best practice is to utilize this programmatic access to build\ncustom reports and dashboards that align with specific financial\nreporting needs and to create advanced, automated alerting mechanisms\nthat pipe anomaly data into established operational channels, such as\nSlack or PagerDuty.\n**Custom Anomaly Detection & Notification**\n ... \nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\nForecasting Snowflake consumption should be a strategic business\nfunction, not a mere technical prediction. The goal is to establish a\ntransparent basis for budgeting and optimizing ROI by linking\nconsumption directly to measurable business outcomes. In a dynamic,\nusage-based environment where compute costs are the most volatile\nelement of the bill, a robust framework must integrate quantitative\nanalysis of historical usage with qualitative insights into future\nbusiness drivers. The following framework outlines how to build and\nmaintain a comprehensive consumption forecast.\n**Establish the Baseline**\nThis phase focuses on understanding the source of spend and establishing\ngranular cost accountability.\n**Identify demand drivers and unit economics:** To understand what\ndrives Snowflake spend, correlate historical credit, storage, and data\ntransfer usage with key business metrics like cost per customer or per\ntransaction. Use Snowflake's [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) schema, including the WAREHOUSE_METERING_HISTORY and QUERY_HISTORY\nviews, as the primary data sources for this analysis.\n**Granular cost attribution:** Accurately tie costs back to business\nteams or workloads by implementing a mandatory tagging strategy for\nall warehouses and queries. Align these tags with your organization's\nfinancial structure to provide clear cost segmentation.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Limit data transfer\nContent:\nData egress, the transfer of data from one cloud provider or region to\nanother, can incur substantial costs, particularly when handling large\ndata volumes. Implementing appropriate tools and best practices is\nessential to minimize these data transfer expenses and maximize business\nvalue when data egress is necessary.\n**Tooling: Enable proactive cost management**\nLeverage Snowflake's native features to gain visibility and control over\ndata transfer costs before they become a significant expense.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nWorkload optimization focuses on identifying the efficiency of your data\nprocessing activities within Snowflake. This involves a holistic\napproach encompassing the review of query syntax, data pipelines, table\nstructures, and warehouse configurations to minimize resource\nconsumption and improve performance. By addressing inefficiencies across\nthese areas, organizations can significantly reduce costs and accelerate\ndata delivery.\n**Query syntax optimization**\nInefficient queries often lead to excessive and hidden credit\nconsumption. Organizations can identify performance bottlenecks and\nunderstand the cost impact of specific SQL patterns by using Snowflake\nfeatures and adhering to SQL code best practices. This enables\ndevelopment teams to create more efficient and cost-effective code by\nhighlighting poor performing queries. Refer to the Performance\nOptimization Pillar of the Snowflake Well-Architected Framework for\ndetails on how to do this.\n**Utilize query history & insights for highlevel monitoring**\nFor broader visibility across all workloads, the Snowsight UI and the\nACCOUNT_USAGE schema are indispensable."],"full_content":null}],"errors":[],"warnings":null,"usage":[{"name":"sku_extract_excerpts","count":3}]}

{"extract_id":"extract_a4d12470c9a64f1f94e00aa6d21b0f17","results":[{"url":"https://blog.altimate.ai/does-size-matter-what-else-is-important-in-choosing-a-snowflake-warehouse","title":"Choosing the Right Snowflake Warehouse: It's not Rocket Science","publish_date":"2024-11-02","excerpts":["[Hashnode](https://hashnode.com/?utm_source=https%3A%2F%2Fblog.altimate.ai&utm_medium=referral&utm_campaign=blog_header_logo&utm_content=logo) Altimate.ai | Data Engineering Blog\n\nOpen search (press Control or Command and K) Toggle theme Open menu\n\n[Hashnode](https://hashnode.com/?utm_source=https%3A%2F%2Fblog.altimate.ai&utm_medium=referral&utm_campaign=blog_header_logo&utm_content=logo) Altimate.ai | Data Engineering Blog\n\nOpen search (press Control or Command and K)\n\nToggle theme Subscribe [Write](https://hn.new)\n\n## Command Palette\n\nSearch for a command to run...\n\n# Does Size Matter? What else is important in choosing a Snowflake Warehouse?\n\nChoosing the Best Warehouse Size in Snowflake\n\nUpdated November 2, 2024\n\n•\n\n12 min read\n\n[J](https://hashnode.com/@John-ryan-uk)\n\n[John Ryan](https://hashnode.com/@John-ryan-uk)\n\nAfter 30 years of experience building multi-terabyte data warehouse systems, I spent five incredible years at Snowflake as a Senior Solution Architect, helping customers across Europe and the Middle East deliver lightning-fast insights from their data.\nIn 2023, I joined Altimate.AI, which uses generative artificial intelligence to provide Snowflake performance and cost optimization insights and maximize customer return on investment.\n\nHaving worked with over 50 Snowflake customers globally, I'm frequently asked the question:\n\n_\"What warehouse size do I need?\"_\n\nThe answer (familiar to any experienced IT specialist), is...\"It depends\".\n\nIn this article, I'll discuss warehouse size and why although important, it's often a distraction from the real challenge - delivering excellent throughput, performance and value for money.\n\n## TL;DR Take-Aways\n\n1. **Size Isn't everything:** Warehouse Size is not that important. Get over it.\n2. **Don't Scale Up:** Although remarkably easy, it's a great way to burn through cash (and eventually lose your job).\n3. **Warehouse Count Matters:** If you have more than 20 warehouses - you're probably wasting hundreds of thousands of dollars.\n4. **Never give a user what they want:** You need to find out what they need!\n\n## What Problem(s) are we trying to Solve?\n\nWhile working for a London based investment bank, I spent a year migrating an Oracle 11g data warehouse to a bigger machine having already spent a eighteen months tuning the existing system to improve query performance.\n\nWe'd cut hours off the batch ELT execution time, but we still struggled with end-user query performance. The diagram below illustrates the challenge we faced.\n\nOur challenge was we had a hugely over-complex ELT process loading and transforming millions of trades every three hours, but during busy periods it took four hours to complete - effectively running continuously.\n\nOn the other hand, we had hundreds of frustrated traders and data analysts needing sub-second query response times. The diagram below illustrates how this played out, with end-users (in blue) fighting for machine resources with the batch ETL system (in red).\n\n> \"Inside Every Big Problem, is Hundreds of Little Problems Dying to get Out\".\n> \n> \n\nIf indeed we had Snowflake, we could have solved the problem by executing each workload on a separate virtual warehouse. They could even have a different warehouse size as illustrated in the diagram below.\n\nHowever, if we focus too much upon warehouse size, we're missing the point.\n\nThe real challenge is contention for resources between batch processing and end-user queries, and the fact that batch transformations prioritize throughput, whereas end-users need fast query performance. This leads to the first best practice: Separate your workloads.\n\n## Separate Workloads to Avoid Contention\n\nWhile the size of the warehouse is important, it's even more important to separate workloads by type. Broadly speaking, workload types include:\n\n* **Data Loading:** Using COPY commands to load data into Snowflake. [Data Loading](https://articles.analytics.today/best-practices-for-using-bulk-copy-to-load-data-into-snowflake) tends to favor an `XSMALL` warehouse as each file loads on a single CPU and in most cases cannot benefit from scaling up the warehouse. Equally, a second dedicated warehouse for [large data loads](https://articles.analytics.today/best-practices-for-using-bulk-copy-to-load-data-into-snowflake) is also recommended where sensible.\n* **Huge Batch Transformations:** Which tend to process large data volumes and require more complex SQL operations including sort operations. These prioritize throughput over performance of individual queries, and tend to be submitted on a regular schedule. Finally, unlike end-user queries, individual query response time is not as critical as getting the overall job done as quickly as possible.\n* **Real-Time Processing:** Which tend to process smaller, incremental data sets but need very fast performance.\n* **Data Analysis or Data Science:** These workloads tend to scan massive data volumes (which need a bigger warehouse), while also prioritizing query performance (as queries are submitted by users).\n* **End User Dashboards:** These workloads tend to scan smaller data volumes where individual query performance is critical even as the number of concurrent users increase.\n\nWhile the recommendations above may appear common sense, I seldom find them used. In fact, the most common deployment method I've seen is separation of workloads by team.\n\n## Avoid Workload Separation by Team\n\nThe diagram below illustrates a situation I've seen in almost every large Snowflake deployment I've worked on.\n\nThe above diagram shows how each team is given a set of warehouses, one for Sales, another for Marketing and yet another for Finance workloads. This separation is often driven by the need to chargeback warehouse costs to different teams, but it's both unnecessary and hugely inefficient.\n\nThe challenges with this deployment include:\n\n* **Poor Infrastructure Management:** As each team manages it's own warehouses, the size, configuration and infrastructure management is distributed across the entire business. As it's not feasible for each team to have a Snowflake expert, it often leads to wasteful deployment and poor throughput and query performance.\n* **Workload Contention:** Data Engineering teams focus on code and delivery schedules and often lack the skills and experience to manage conflicting workloads. This leads to the same machine being used for mixed purposes including end-user queries and batch processing on the same warehouse.\n* **Significant Waste:** It's not unusual to find over a hundred virtual warehouses with multiple same-size machines executing similar workloads. This leads to significant waste as there's often many warehouses running at low utilization with similar workloads executed by different teams.\n\n## Focus on the Workload not Warehouse Size\n\nThe most common question I'm asked by Data Engineers is:\n\n_\"What warehouse size should I run this query on\"_ .\n\nI would describe this sort of thinking as: _\"Looking down the wrong end of a telescope\"_ . There are several critical mistakes implied by the above question:\n\n* **Focusing on the Query:** Most transformation pipelines involve a sequence of operations, (or a Job), not just a single query. Focussing on an individual query is missing the bigger picture. It's not feasible to execute every query on the optimum warehouse. We need to focus on the JOB.\n* **Focusing on the Warehouse Size:** While a [bigger warehouse](https://articles.analytics.today/snowflake-virtual-warehouses-what-you-need-to-know) often leads to faster results it's not always the case. You need to focus on the workload size - the time taken to process the data and deliver results.\n* **Ignoring the Priority:** One of the most important requirement of any task is _\"How quickly do you need the results?\"_ . This indicates the priority, whether the workloads needs faster performance, high throughput or neither. If a batch process taking 30 minutes is acceptable, why bother running it on a larger warehouse with a potentially higher cost?\n* **Ignoring Configuration Options:** [Virtual Warehouses](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse) can be configured with an `AUTO_SUSPEND` time, `WAREHOUSE TYPE` (memory size), `CLUSTER_COUNT` and `SCALING POLICY` in addition to warehouse size. You should be aware of these options when deciding [which warehouse to use](https://articles.analytics.today/snowflake-virtual-warehouses-what-you-need-to-know) .\n* **Ignoring Performance Options:** Snowflake provides several performance tuning options including, [Query Acceleration Service](https://articles.analytics.today/how-snowflake-query-acceleration-service-boosts-performance) , [Cluster Keys](https://articles.analytics.today/snowflake-cluster-keys-and-micro-partition-elimination-best-practices) and [Search Optimization Service](https://articles.analytics.today/best-practices-snowflake-search-optimisation-services) to help maximize query performance. A focus on warehouse size ignores these options.\n* **Ignoring the Cost:** The question about warehouse size implies the need to maximize query performance, but this hyper-focus on performance leads to ignoring query cost. The diagram below illustrates the real balance every Data Engineer needs to be aware of, balancing the conflicting requirements of Throughput, Performance and Cost.\n\nThe biggest single difference between an on-premises databases and Snowflake is that on-premises systems have a fixed up-front cost and limited resources, whereas on Snowflake the cost depends upon usage and machine resources are unlimited.\n\nWe therefore need to be aware of all three potential priorities, and (for example, with over-night batch processing), if neither throughput nor performance is critical, we must focus on cost.\n\n## Workload Frequency and Warehouse Size\n\nAnother common warehouse deployment mistake I see is high frequency tasks on an inappropriate (too large) warehouse. This is often driven by the need to quickly deliver near real-time incremental results to end-users. It typically involves a short, repeatedly executed scheduled job which is run every few minutes, running 24x7 on a MEDIUM or LARGE warehouse.\n\nTo understand why this is a bad mistake, consider how Snowflake bills for warehouse time.\n\nWhen a warehouse is resumed, there is a minimum 60 second charge, and a minimum `AUTO_SUSPEND` of 60 seconds (although the default time is 10 minutes).\n\nEven assuming the minimum 60 second auto-suspend time, a job which runs every five minutes and executes for just two minutes has an actual charge of three minutes as a result of the `AUTO_SUSPEND` time.\n\nAny job executed every five minutes executes 105,120 times per year, and assuming $3.00 per credit costs $15,768. If however, the job were executed on a `MEDIUM` size warehouse, the costs increases to over $63,000.\n\nWorst still, since each query in the job is short and processes relatively small data volumes, there's little benefit in increasing warehouse size as the queries won't run much faster, but the cost of the `AUTO_SUSPEND` time has increased to over $21,000 - more than the original cost on an `XSMALL` warehouse.\n\n## Avoid Resizing the Warehouse\n\nSnowflake recommends experimenting with different warehouse sizes and because it's easy to resize, most customers starting with a SMALL virtual warehouse and then increase the size as performance demands.\n\nThe diagram above illustrates a common scenario whereby users start with a `SMALL` warehouse, then increase size to a `MEDIUM` and then `LARGE` . As additional workloads are added, they find query performance suffers, but they also discover that increasing warehouse size again to an `XLARGE` leads to a significant increase in cost, so the warehouse is resized back down again.\n\nAs a Snowflake Solution Architect and Instructor to hundreds of Snowflake Data Engineers, and I've consistently advised:\n\n> \"Don't increase the Virtual Warehouse Size\"\n> \n> \n\nTo understand why, consider the \"Workload Frequency\" section above. It's likely that any virtual warehouse will include a combination of workloads, from huge batch jobs run once per day, to short, fast jobs run every few minutes.\n\nAssuming a virtual warehouse suspends after 60 seconds, a job which runs for an hour spends just 1.6% of the time waiting to suspend, whereas a job which runs for two minutes spends an additional 50% of the time waiting to suspend.\n\nWhen you increase warehouse size, you double the cost per hour, but you're also doubling the cost of the idle time.\n\n## Mixed Workload: A Case Study\n\nI recently worked with a Snowflake customer to help reduce costs when they were spending millions of dollars per year. Their most expensive warehouse wasn't as you might expect, an `X6LARGE` size, but a `MEDIUM` size warehouse that accounted for around $200,000 per year.\n\nThe graph below illustrates the results of my analysis where I found 70% of queries completed in under five seconds while just 1% took over an hour. Clearly we had identified a warehouse with a highly mixed workload.\n\nThere were two important insights from the above:\n\n1. With 70% of queries completing in under five seconds on a warehouse with a small number of queries (just 1%), taking up to an hour to complete, this is clearly a warehouse with a mixed workload.\n2. Queries taking under 5 seconds are considerably more cost effective on a smaller warehouse. My personal benchmark tests demonstrate that queries taking around 5 seconds run around 30% faster when executed on a `MEDIUM` rather than `XSMALL` warehouse, but the charge rate is 400% higher.\n\nFurther investigation revealed that the warehouse had been increased to a `LARGE` size, but was quickly reversed as the cost increased significantly and the customer simply accepted the poor performance.\n\nIn conclusion:\n\n* 70% of the queries complete in under 5 seconds are running these on a MEDIUM size warehouse is hugely wasteful.\n* It's likely the warehouse size was increased to `MEDIUM` size to ensure the longer running queries (the 1% of queries taking 10-60 minutes) finished quickly.\n* It would be sensible to identify the longer running queries and move them to a `MEDIUM` size warehouse, and then resize the warehouse to a `SMALL` or `XSMALL` to save money.\n\n## Snowflake Virtual Warehouse Sizing: What are we doing wrong?\n\nLet's quickly summarize the mistakes being made:\n\n1. **Deployment by Team:** Allocation of warehouse is delegated to individual teams without the expertise or experience to manage these remarkably expensive resources.\n2. **Mixed Workloads:** As there's little understanding of workload allocation, warehouses tend to have a mixed workload with short frequently executed jobs on the same warehouse as long running queries. This makes it increasingly expensive to scale up for performance.\n3. **Focusing on Warehouse Size:** As it's easy to resize a warehouse and it often results in a 50% performance improvement, this becomes a the simple fix for performance issues.\n4. **Reliance on Scaling up:** It's easy to scale up a warehouse size. However, because warehouses run a mixed workload, it becomes increasingly expensive to scale up, and instead we rely upon scale out which simply multiplies out the extent of inefficient workload deployment.\n5. **Focus on Performance:** Everyone would prefer their results delivered more frequently and faster which leads to a focus on [query performance tuning](https://articles.analytics.today/boost-your-snowflake-query-performance-with-these-10-tips) , ignoring the need to control costs or maximize throughput.\n6. **Ignorance of Configuration or Performance Tools:** As warehouse size is a simple concept, we ignore the more sophisticated options including the `SCALING POLICY` or advanced features like [Cluster Keys](https://blog.altimate.ai/snowflake-data-clustering-expert-advice-to-tune-query-performance) .\n\n## Virtual Warehouse Sizing: Best Practices\n\n> \"Never give a customer what they asks for! Give them what they NEED! And remember, it's your job to understand what they need\" - Dr David. Pearson\n> \n> \n\n1. **Understand System Requirements:** Be aware of the trade-off of Cost, Performance and Throughput. If you're aware of the the conflicting requirements you might deliver a solution that gives the best value to the business - and keep yourself in a job.\n2. **Understand the Business Requirements:** Keeping in mind the overall requirements, agree the need for both data refresh frequency and query performance. Make business users aware of the real cost of their request. For example a job executing every five minutes on an `XSMALL` warehouse costs over $15,000 per year whereas the same job executed once per hour costs just $1,300.\n3. **Deploy by Workload not Team:** The diagram below illustrates an optimum warehouse deployment whereby workloads deployed to reduce contention between workloads rather than by different teams.\n\nThe above diagram illustrates how a single `XSMALL` warehouse is used for data loading, while there's a range of different size transformation warehouses for ELT processing. Finally, users with similar workload sizes share warehouses while Data Scientists use a much larger dedicated warehouse based upon workload size and performance needs.\n\n1. **Focus on the JOB not the Query:** Around 80% of Snowflake compute costs are the result of scheduled jobs, and it's easy to identify the steps in a job using the [QUERY\\_TAG](https://docs.snowflake.com/en/sql-reference/sql/alter-session) . Using this technique, we can quickly summarize costs and execution time by JOB, and ensure each job is assigned to the correct warehouse size. It can also be used to quickly identify frequently executed short jobs on large warehouses. Moving these from a `MEDIUM` to an `XSMALL` will reduce costs by 75%.\n2. **Move the JOB instead of resizing the warehouse:** To avoid (or resolve) a mixed workload, having identified the poorly deployed jobs, move them to an appropriate warehouse size. This means moving jobs that [spill to storage](https://articles.analytics.today/improve-snowflake-query-speed-by-preventing-spilling-to-storage) to a larger warehouse, while short, frequently executed jobs are moved to a smaller (less expensive) warehouse.\n3. **Understand Snowflake Performance Features:** In addition to a range of features to [tune query performance](https://articles.analytics.today/boost-your-snowflake-query-performance-with-these-10-tips) , Snowflake supports a number of [warehouse parameters](https://articles.analytics.today/snowflake-virtual-warehouses-what-you-need-to-know) , along with advanced features including [Cluster Keys](https://articles.analytics.today/snowflake-cluster-keys-and-micro-partition-elimination-best-practices) , [Query Acceleration Service](https://articles.analytics.today/how-snowflake-query-acceleration-service-boosts-performance) and [Search Optimization Service](https://articles.analytics.today/best-practices-snowflake-search-optimisation-services) to help improve query performance. While increasing warehouse size might improve performance, in many cases it won't help, in which case alternative techniques need to be considered.\n4. **Control Warehouse Deployment:** Avoid a sprawl of hundreds of virtual warehouses with inappropriate (and undocumented) configuration and mixed workloads. Provide a centrally coordinated centre of excellence to oversee warehouse deployment and [monitor key warehouse metrics](https://articles.analytics.today/monitor-snowflake-usage-cost) to maximize business value.\n\n* * *\n\nDeliver more performance and cost savings with Altimate AI's cutting-edge AI teammates. These intelligent teammates come pre-loaded with the insights discussed in this article, enabling you to implement our recommendations across millions of queries and tables effortlessly. Ready to see it in action? Request a recorded demo by just sending a chat message ( [here](https://app.myaltimate.com/contactus) ) and discover how AI teammates can transform your data team\n\n[](https://app.myaltimate.com/contactus)\n\n\\ \\\n\n## More from this blog\n\n### Teaching Claude Code the Art of Data Engineering: Introducing Altimate Skills How we achieved 19% improvement on real-world data tasks — and what it taught us about AI-assisted data development. Jan 22, 2026 · 12 min read\n\nSubscribe to the newsletter\n\nGet new posts delivered to your inbox.\n\nSubscribe\n\n### Adaptive Compute vs. Auto Tune: A Practical Guide to Optimizing Snowflake Warehouses Managing Snowflake warehouses efficiently is a constant balancing act. Set them too large, and you're burning money on idle compute. Too small, and your queries slow to a crawl, frustrating users and missing SLAs. For many organizations, this manual ... Jul 30, 2025 · 12 min read\n\n### Supercharging Cursor IDE: How the dbt Power User Extension’s Embedded MCP Server Unlocks AI-Driven dbt Development Introduction We’re excited to announce a major new capability in the dbt Power User VSCode extension: an embedded Model Context Protocol (MCP) server. This MCP server is now built directly into the extension, acting as a bridge between AI-powered dev... Mar 21, 2025 · 18 min read\n\n### Could AI Teammates Be the Secret to Efficient Snowflake Cost Management? In the past five years, Snowflake has disrupted the database, data storage, and analytics industry by massively simplifying the entire data and analytics landscape. However, a new wave of innovation is horizon - Artificial Intelligence and AI Teammat... Nov 13, 2024 · 9 min read\n\n### Simple Question: Is Snowflake Expensive? The answer is - It Depends! Around five years ago, I joined Snowflake in the UK as a Senior Solution Architect, helping customers across Europe and the Middle East get up to speed with this new technology from an unknown startup - called Snowflake. Back then, Snowflake was the ... Sep 25, 2024 · 8 min read\n\nA\n\nAltimate.ai | Data Engineering Blog\n\n15 posts published\n\nWe explore the application of AI in Data Engineering to accelerate development, eliminate the drudgery of maintaining pipelines, optimize data platforms, and deliver better data products.\n\n[](https://x.com/AltimateInc) [](https://github.com/AltimateAI) [](https://www.linkedin.com/company/altimate-ai/) [](https://www.youtube.com/@AltimateAI) [](https://www.altimate.ai/)\n\n* Archive\n* [Privacy](https://hashnode.com/privacy)\n* [Terms](https://hashnode.com/terms)\n\nSitemap RSS\n\n[](https://hashnode.com/?utm_source=https%3A%2F%2Fblog.altimate.ai&utm_medium=referral&utm_campaign=blog_footer_logo&utm_content=logo)"],"full_content":null},{"url":"https://www.flexera.com/blog/finops/snowflake-warehouse-sizes/","title":"8 best practices for choosing right Snowflake warehouse sizes (2026)","publish_date":"2026-01-27","excerpts":["Flexera Open Primary Navigation\n\n* Solutions\n  \n  Spend management by vendor\n  \n  [Flexera is a Leader in 2025 cloud financial management tools](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\n  \n  Discover recognized CFM vendors to watch in the 2025 Gartner® Magic Quadrant™\n  \n  [View report](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\n* Products\n  \n  Flexera One\n  \n    + IT Visibility\n    + ITAM\n    + Snow Atlas\n    + Cloud License Management\n    + SaaS Management\n  \n    + FinOps\n    + Cloud Cost Optimization\n    + Cloud Commitment Management\n    + Container Optimization\n    + Virtual Machine Optimization\n    + Data Cloud Optimization\n  \n    + Application Readiness\n    + Security\n    + Integrations\n    + Technology Intelligence Platform\n    + All Products\n  \n  [Introducing Flexera One SaaS Management](https://www.flexera.com/products/flexera-one/saas-management)\n  \n  Discover comprehensive SaaS visibility for taming SaaS sprawl, wasted spend and compliance risks.\n  \n  [Book your personalized demo](https://www.flexera.com/products/flexera-one/saas-management)\n* Success\n  \n  Customer Success\n  \n  Services & Training\n  \n    + Services\n    + Training\n  \n  Support\n  \n    + [Support portal](https://community.flexera.com/s/support-hub)\n    + [Product documentation](https://docs.flexera.com)\n  \n    + Technology Intelligence Awards\n    + [Flexera community](https://community.flexera.com/s/)\n  \n  [2025 Technology Intelligence Awards](https://info.flexera.com/ITAM-REPORT-State-of-IT-Asset-Management)\n  \n  The results are in—see how our 2025 winners and honorable mentions are shaping the future of Technology Intelligence.\n  \n  [See the winners](https://www.flexera.com/customer-success/awards)\n* Resources\n  \n  Resources\n  \n    + Webinars\n    + Videos\n    + Datasheets\n    + Whitepapers & reports\n  \n    + Blog\n    + Case studies\n    + Events\n    + Analyst research\n    + Glossary\n    + Demos & trials\n    + Business value calculator\n  \n  [Flexera 2026 IT Priorities Report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)\n  \n  AI ROI, sustainability, cost and risk: Discover the latest IT trends shaping tomorrow’s IT landscape in Flexera’s 2026 IT Priorities Report.\n  \n  [View report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)\n* About\n  \n  Company\n  \n    + About\n    + Careers\n    + Contact us\n    + Leadership\n  \n  Partners\n  \n    + Partner program\n    + Partner locator\n  \n  Press center\n  \n    + Press releases\n    + Articles\n    + Awards\n  \n  Social responsibility\n  \n    + ESG\n    + Belonging and inclusion\n  \n  [The Flexera 2025 State of the Cloud Report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)\n  \n  How are GenAI rising cloud costs, security and sustainability shaping your cloud strategies in 2025?\n  \n  [View report](https://info.flexera.com/CM-REPORT-State-of-the-Cloud)\n\nCustomers Open External Links\n\n* [Community](https://community.flexera.com/)\n* [Product login](https://app.flexera.com/login)\n* [Spot login](https://console.spotinst.com/auth/signIn)\n* [Partner Portal](https://partnerhub.flexera.com/)\n\nSearch\n\nBook a demo\n\n1. Home\n2. Blog\n3. [FinOps](https://www.flexera.com/blog/finops/)\n4. 8 best practices for choosing right Snowflake warehouse sizes (2026)\n\n### [FinOps](https://www.flexera.com/blog/finops/)\n\nSubscribe\n\nTopics\n\nSaaS Management FinOps IT Visibility IT Asset Management Product News Application Readiness Security Perspectives\n\n[FinOps](https://www.flexera.com/blog/finops/)\n\n# 8 best practices for choosing right Snowflake warehouse sizes (2026)\n\n[](https://www.flexera.com/blog/author/pramit/ \"Pramit Marattha\")\n\n[Pramit Marattha](https://www.flexera.com/blog/author/pramit/ \"Pramit Marattha\")\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-warehouse-sizes%2F&title=8%20best%20practices%20for%20choosing%20right%20Snowflake%20warehouse%20sizes%20%282026%29&source=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-warehouse-sizes%2F) [](https://twitter.com/intent/tweet?source=https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-warehouse-sizes%2F&text=8%20best%20practices%20for%20choosing%20right%20Snowflake%20warehouse%20sizes%20%282026%29%20https%3A%2F%2Fwww.flexera.com%2Fblog%2Ffinops%2Fsnowflake-warehouse-sizes%2F) \n\nThis post originally appeared on the chaosgenius.io blog. Chaos Genius has been [acquired by Flexera](https://www.flexera.com/more/ProsperOps-Chaos-Genius) .\n\nSnowflake provides a scalable, secure and cost-effective solution for storing and analyzing large volumes of data in real-time. However, choosing the appropriate Snowflake warehouse sizes can be a daunting task as it significantly impacts both Snowflake costs and query performance .\n\nIn this article, we’ll cover the detailed steps you need to take to ensure your Snowflake virtual warehouse is the “ **right size** ”, striking the perfect balance between price and performance.\n\n## What is a Snowflake virtual warehouse?\n\nSnowflake Virtual Warehouse or simply referred to as “ _warehouse_ ”, is a cluster of computational resources used for running queries and tasks within the Snowflake platform. It functions as an on-demand resource, separate from any data storage system and is similar to a virtual machine (VM).\n\nOne of the key features of Snowflake’s Virtual Warehouse is the separation of storage and compute infrastructure, meaning that the compute resources utilized for analytics query processing are not tied to the data loading characteristics or overall storage volume of a given instance.\n\nThe Snowflake Virtual Warehouse comprises compute nodes that work together to provide increased computational power. The number of nodes available for a query doubles with each increase in warehouse size, starting with only one node for an X-Small warehouse and scaling up to 512 nodes for a 6X-Large warehouse. This doubling of processing power with each unit increase in size halves the elapsed query processing time every time the warehouse size is scaled up.\n\nFor instance, a **Medium** warehouse has **4 nodes** in Snowflake, while a **Large** warehouse has **8 nodes** . When a query is executed on a warehouse, Snowflake may utilize as many nodes as are available in parallel to execute the query. This means that a Large warehouse (8 nodes) will complete the same task twice as fast as a Medium warehouse (4 nodes).\n\nSnowflake offers two types of virtual warehouses: the flexible **SnowStandard** and the **Snowpark-optimized** Warehouses.\n\nThe Standard virtual warehouses come in sizes ranging from X-SMALL to 6X-Large. On the other hand, the SnowPark-optimized virtual warehouse also comes in sizes ranging from X-SMALL to 6X-Large.\n\n> **Note** : Snowpark-optimized warehouses are not supported on **X-Small** or **SMALL** warehouse sizes.\n> \n> \n\nSnowPark-optimized virtual warehouse is designed for high memory requirements, such as ML workloads, ML training datasets and other memory-intensive tasks. It provides 16 times more memory per node than a standard virtual warehouse, making it an excellent choice for handling such workloads on a single virtual warehouse.\n\n## How do cost & performance change as Snowflake warehouse sizes increases?\n\nThe size of a Snowflake virtual warehouse affects both cost and performance. All warehouses, regardless of size, are charged based on the amount of time they are running, whether actively processing queries or waiting for one to be issued. The hourly costs—measured in Snowflake credits—are also doubled with every increase in warehouse size.\n\nSnowflake uses t-shirt sizing names for their warehouses. The available sizes range from **X-SMALL** to **6X-Large** and for most Snowflake users, the **X-SMALL** warehouse is sufficient, providing ample power to effectively handle massive datasets, depending on the complexity of the workload.\n\nAs previously stated, there are two types of virtual warehouses: **SnowStandard** and **Snowpark-optimized** and each type charges credits differently.\n\n> Knowing how Snowflake’s resources are billed is crucial to understanding how to use the platform. While we’ll touch on some of the essential aspects briefly, for a thorough explanation of Snowflake credit costs and ways to decrease the Snowflake costs, we recommend reading our two articles: 8 Ways to Decrease Snowflake Costs & 4 Best Snowflake Cost Estimation Tools\n> \n> \n\nFor **standard virtual warehouses** , the credit/hour table looks like this:\n\n|Warehouse Size |Credits per Hour |\n| --- | --- |\n|X-Small |1 |\n|Small |2 |\n|Medium |4 |\n|Large |8 |\n|X-Large |16 |\n|2X-Large |32 |\n|3X-Large |64 |\n|4X-Large |128 |\n|5X-Large |256 |\n|6X-Large |512 |\n\nFor Snowpark-optimized virtual warehouses, the credit/hour table looks like this:\n\n|Warehouse Size |Credits per Hour |\n| --- | --- |\n|X-Small |Not available |\n|Small |Not available |\n|Medium |6 |\n|Large |12 |\n|X-Large |24 |\n|2X-Large |48 |\n|3X-Large |96 |\n|4X-Large |192 |\n|5X-Large |384 |\n|6X-Large |768 |\n\n> **Note** : Snowflake charges a **minimum of 60 seconds** per query on a running or resized warehouse. Even if a query runs for only a few seconds, the user will be charged for a full minute of usage. Also, Snowflake offers **serverless compute** and **cloud service compute** , which have different credit structures. Check this our article to learn more about it.\n> \n> \n\nAs for performance, increasing the warehouse size can significantly reduce processing time for complex queries using large tables and/or multiple joins. This is because the most computationally heavy operations— [TableScans, ExternalScans and Joins](https://docs.snowflake.com/en/user-guide/ui-query-profile) —benefit from the additional working memory that comes with larger warehouse sizes. However, if a query’s results have a lot of uneven/skewed values, increasing the warehouse size might not improve performance as much as expected.\n\nFinally, now that we know what a Snowflake virtual warehouse is and how much it costs, it’s time to get into the article’s core. Let’s review how to “ **right-size** ” your virtual warehouse for optimal Snowflake performance.\n\n## Steps to Effectively Right-Size Your Snowflake Virtual Warehouse\n\nSelecting the right warehouse size in Snowflake can significantly impact performance, potentially saving hours of waiting for failed queries and quickly delivering results for complex queries. To find the best warehouse size, it’s crucial to regularly perform tests that help identify the right size. Whether you’re setting up a new warehouse or adjusting the size of an existing one, following these steps can help you find the optimal size for your needs.\n\n### 1) Start Small and Scale Up as Needed\n\nTo effectively Right-Size Your Snowflake Virtual Warehouse, it’s essential to start small and gradually increase the size until you find the sweet spot of maximum performance at the lowest cost.\n\nStart with the X-SMALL warehouse and run workloads. If queries are slow, increase the size incrementally until performance is satisfactory. Remember that each size increase doubles the cost, so ensure queries are at least 2x faster. X-SMALL or SMALL warehouses may suffice for small-scale operations, while larger sizes (X-LARGE to 6X-LARGE) are suitable for bulk loading and heavy calculations in large-scale environments.\n\nHere are a few essential points to consider in order to create the optimal balance between cost and performance:\n\n* Start with an X-SMALL warehouse and gradually increase the size until the query duration stops halving.\n* Choose a Snowflake warehouse sizes that offers the best cost-to-performance ratio, usually one smaller than the largest warehouse that fully utilizes the query.\n* If increasing the Snowflake warehouse sizes only results in a small decrease in query time, sticking with the smaller warehouse may be more cost-effective. But, if faster performance is necessary, a larger Snowflake warehouse sizes can be selected, but returns may start diminishing.\n\n### 2) Automate Warehouse Suspension + Resumption\n\nSelecting the initial Snowflake warehouse sizes is important, but Snowflake offers a solution to automate warehouse suspension and resumption, which provides more flexibility to start with a larger size and adjust the size based on workloads.\n\nBy setting up automatic warehouse suspension, warehouses can be configured to auto-suspend after no activity (default is 10 minutes). To strike a balance between performance and cost, it is suggested to use a 60-second auto-suspend instead of the default 600 seconds.\n\nThis approach is useful as Snowflake charges credits based on when warehouses run, not when processing requests. Hence, automatic warehouse suspension prevents warehouses from consuming credits when they are not being used. Note that when a warehouse suspends, its local cache gets cleared. Therefore, if there are repeating queries that scan the same tables, setting the warehouse auto-suspend too small will lead to a drop in performance.\n\nOn the other hand, auto-resume gives you more control over costs and access to warehouses. If you prefer manual control over when your warehouse resumes processing, you can disable auto-resume. Otherwise, the warehouse will spin back up whenever new queries are submitted.\n\nAutomating warehouse suspension and resumption can effectively manage costs and ensure that resources are only consumed when necessary. This approach provides more flexibility to businesses/users that have unpredictable workloads or need to adjust Snowflake warehouse sizes frequently.\n\n### 3) Check if Your Snowflake Warehouse is Under or Over Provisioned\n\nChoosing the appropriate Snowflake warehouse sizes in Snowflake is essential for determining whether the warehouse is under or over-provisioned. This evaluation assists in optimizing resource allocation, controlling costs and ensuring efficient query performance.\n\n#### Is it under-provisioned?\n\nAn **under-provisioned** Snowflake warehouse may not have sufficient resources to handle the workload, leading to sluggish query performance and potential bottlenecks. This can negatively impact the user experience and hinder the ability to process large volumes of data in a timely manner. To identify under-provisioning, monitor performance indicators such as query execution time, queue time and the number of queued queries. If these metrics consistently show poor performance, increasing the warehouse size to allocate more resources may be necessary.\n\n#### Is it over-provisioned?\n\nAn **over-provisioned** Snowflake warehouse may have more resources than required, resulting in unnecessary costs without providing any significant performance improvements. To identify over-provisioning, analyze the warehouse’s resource utilization, such as CPU and memory usage. If these metrics consistently show low utilization, it may be more cost-effective to reduce the warehouse size.\n\n### 4) Monitor Disk spillage\n\nIt’s crucial to monitor both local and remote disk spillage. In Snowflake, when a warehouse cannot fit an operation in memory, it starts spilling data first to the local disk of a warehouse node and then to remote storage. This process, called disk spilling, leads to decreased performance and can be seen in the query profile as “ **Bytes spilled to local/remote storage** .” When the amount of spilled data is significant, it can cause noticeable degradation in warehouse performance.\n\nTo decrease the impact of spilling, the following steps can be taken:\n\n* Increase the size of the warehouse, which provides more memory and local disk space.\n* Review the query for optimization, especially if it’s new query.\n* Reduce the amount of data processed, such as improving partition pruning or projecting only the needed columns.\n* Decrease the number of parallel queries running in the warehouse.\n\n### 5) Determine Optimal Costs and Performance\n\nOptimizing the cost and performance of Snowflake virtual warehouses is critical. The warehouse’s size plays a significant role in the speed of CPU-bound queries. Increasing the size of the Snowflake warehouse will boost query speed until the resources are fully utilized. Beyond this point, larger Snowflake warehouse sizes will not enhance performance and costs can rise without any improvement in query speed/performance.\n\nTo achieve the optimal balance between performance and cost, start with an X-SMALL warehouse and gradually scale it up until the query duration stops halving. This indicates that the warehouse resources are fully utilized and helps you identify the sweet spot of maximum performance at the lowest cost. Also, analyze the historical usage patterns of your Snowflake warehouse to identify any usage patterns or spikes in resource utilization that may indicate an incorrectly sized warehouse. By doing so, you can adjust the Snowflake warehouse size to align with your workload demands and ensure optimal performance.\n\n### 6) Slow or Frequently Timing-Out Queries\n\nIf you notice that your queries are consistently slow or frequently timing out, it could indicate that the Snowflake warehouse size is too small for the workload. Sluggish query performance can indicate that the warehouse lacks sufficient resources to handle the workload efficiently. Consider monitoring the query durations and examining whether increasing the warehouse size improves query speed.\n\n### 7) Reviewing Snowflake Query History for Errors\n\nExamining the Snowflake Query History can provide valuable insights into any errors related to Snowflake warehouse sizes. Look out for error messages such as “ **Warehouse full** ” or “ **Insufficient credit** “, which can indicate that the warehouse is unable to accommodate the query workload. Identifying and addressing these errors ensures that the Snowflake warehouse size aligns with your workload demands.\n\n### 8) Using Snowflake Observability Tools (Chaos Genius)\n\nSnowflake’s Resource monitoring is an important step in reducing Snowflake costs, but it may not provide the level of detail needed to make an informed decision. That’s exactly where Snowflake observability tools like [Chaos Genius](https://www.chaosgenius.io/) come into play!!\n\nChaos Genius uses advanced analytics and machine learning to monitor Snowflake virtual warehouse in real-time, providing automated recommendations to improve warehouse utilization and recommendations on how to right-size them. The tool also offers a dashboard with detailed metrics and insights to quickly identify any potential issues, enabling users to optimize the Snowflake usage, reduce Snowflake costs and improve the overall Snowflake performance.\n\n[Chaos Genius](https://www.chaosgenius.io/) dashboard\n\nDon’t miss the opportunity to reduce Snowflake costs and transform your business. Schedule a demo with us right now!\n\n## Conclusion\n\nSnowflake virtual warehouse is a game-changer for businesses seeking to store, process and analyze large amounts of data in real-time. Its scalability and affordability make it a popular choice for organizations striving to stay ahead of the data game. Yet, deciding on the right size and structure for a Snowflake Virtual Warehouse can be a daunting task. In this article, we’ve delved into the nitty-gritty details of Snowflake’s virtual warehouse, including how its credits are charged.\n\nHere is a summary of what we covered in the steps needed to right-size the Snowflake warehouse:\n\n* Start with a small Snowflake warehouse size and gradually scale up to find the sweet spot of maximum performance at the lowest cost.\n* Automate warehouse suspension and resumption to manage costs and optimize performance.\n* Check if Your Snowflake Warehouse is under or over Provisioned\n* Monitor Disk spillage\n* Determine the optimal costs and performance based on workload and usage patterns.\n* Review Snowflake Query History for errors related to warehouse size.\n* Use Snowflake observability tools like Chaos Genius.\n\nSo, by following the steps outlined in this article and carefully considering Snowflake warehouse sizes, you can optimize your Snowflake virtual warehouse for maximum cost-effectiveness and Optimal query performance.\n\nUltimately, it’s all about finding the Goldilocks solution—not too big, not too small, but just right. So, take the time to get to know your data and query usage patterns and don’t be afraid to experiment a little.\n\n## FAQs\n\n**How many compute nodes are there in each Snowflake warehouse size?**\n\nSnowflake warehouses are composed of compute nodes. The X-Small warehouse has 1 node, the Small warehouse has 2 nodes, the Medium warehouse has 4 nodes and so on. Each node has 8 cores/threads, irrespective of the cloud provider.\n\n**Can a Snowflake warehouse run multiple queries simultaneously?**\n\nYes, a Snowflake warehouse can handle multiple queries concurrently.\n\n**How can I determine the best Snowflake warehouse size for my query?**\n\nStart with the X-Small warehouse and increase the size until the query duration no longer halves, indicating that the warehouse is not fully utilized. To find the best cost-to-performance ratio, choose a warehouse size one step smaller than the maximum.\n\n**What virtual warehouse sizes are available in Snowflake?**\n\nSnowflake offers virtual warehouses in various sizes, ranging from X-Small to 6X-Large. Each size represents a doubling of resources and credit consumption compared to the previous size.\n\n**How should I approach finding the right Snowflake warehouse size?**\n\nExperiment and iterate by starting small and gradually scaling up. Regularly test and monitor performance indicators to identify the warehouse size that delivers the best balance between cost and performance.\n\nRelated posts:\n\n* [Navigating the SaaS security maze: Tips to protect your business](https://www.flexera.com/blog/saas-management/navigating-the-saas-security-maze-tips-to-protect-your-business/ \"Navigating the SaaS security maze: Tips to protect your business\")\n* [Governing the SaaS explosion: How to establish control and minimize risk](https://www.flexera.com/blog/saas-management/governing-the-saas-explosion-how-to-establish-control-and-minimize-risk/ \"Governing the SaaS explosion: How to establish control and minimize risk\")\n* [Now available: GraphQL Query Generator (Preview Release)](https://www.flexera.com/blog/it-visibility/now-available-graphql-query-generator-preview-release/ \"Now available: GraphQL Query Generator (Preview Release)\")\n* [The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)](https://www.flexera.com/blog/finops/the-practical-finops-roadmap-series-what-to-do-before-you-start-practicing-finops-1-4/ \"The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)\")\n* [What’s new at Flexera: August 2025](https://www.flexera.com/blog/product/whats-new-at-flexera-august-2025/ \"What’s new at Flexera: August 2025\")\n* [Kubernetes pods vs containers: 4 key differences and how they work together](https://www.flexera.com/blog/finops/kubernetes-architecture-kubernetes-pods-vs-containers-4-key-differences-and-how-they-work-together/ \"Kubernetes pods vs containers: 4 key differences and how they work together\")\n\n### Want to know more?\n\nTechnology is evolving rapidly—and it's important to stay on top of the latest trends and critical insights. Check out the latest blogs related to FinOps below.\n\nFinOps\n\n## [2025 State of the Cloud](https://info.flexera.com/CM-REPORT-State-of-the-Cloud?lead_source=Website%20Visitor&id=Blog-Resources \"2025 State of the Cloud\")\n\nMarch 12, 2024\n\nFinOps\n\n## [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Request \"Cloud Cost Optimization demo\")\n\nFebruary 22, 2023\n\nFinOps\n\n## [Practical Guide for a Successful Cloud Journey](https://info.flexera.com/CM-GUIDE-Successful-Cloud-Journey?lead_source=Website%20Visitor&id=Blog-Resources \"Practical Guide for a Successful Cloud Journey\")\n\nFebruary 9, 2022\n\nFinOps\n\n## [Cloud Migration and Modernization Datasheet](https://www.flexera.com/sites/default/files/datasheet-foundation-cloudscape.pdf \"Cloud Migration and Modernization Datasheet\")\n\nFinOps\n\n## [New Flexera One Cloud Cost Optimization capabilities: Built for partners, designed for growth](https://www.flexera.com/blog/finops/new-flexera-one-cloud-cost-optimization-capabilities-built-for-partners-designed-for-growth/ \"New Flexera One Cloud Cost Optimization capabilities: Built for partners, designed for growth\")\n\nFebruary 24, 2026\n\nFinOps\n\n## [FinOps enters its technology value era: Insights from the State of FinOps 2026](https://www.flexera.com/blog/finops/finops-enters-its-technology-value-era-insights-from-the-state-of-finops-2026/ \"FinOps enters its technology value era: Insights from the State of FinOps 2026\")\n\nFebruary 20, 2026\n\n×\n\nGet updates delivered to your inbox\n\nSubscribe\n\n## How can we help?\n\nSales Team\n\n[Community](https://community.flexera.com/s/)\n\nSubscribe\n\nFlexera\n\n* [](https://www.linkedin.com/company/flexera?elqTrackId=62e00a6465d449b0824c83c70706dff9&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"LinkedIn\")\n* [](https://twitter.com/flexera?elqTrackId=ab8f06bd7aea498e807592d19ac2ab00&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"Twitter\")\n* [](https://www.instagram.com/weareflexera?elqTrackId=fcfa0064605a42baaebfabe8fedd5c50&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"Instagram\")\n* [](https://www.youtube.com/user/FlexeraSoftware?elqTrackId=c6e9107020754655a13aca3ac7aa3cd4&elq=00000000000000000000000000000000&elqaid=6833&elqat=2&elqCampaignId=&elqcst=272&elqcsid=143 \"YouTube\")\n\n* Privacy policy\n* Terms and conditions\n* Site map"],"full_content":null},{"url":"https://ternary.app/blog/snowflake-cost-optimization/","title":"Top 8 Snowflake Cost Optimization Strategies to Reduce Cost","publish_date":"2025-09-22","excerpts":["Ternary named a Leader in the 2025 ISG Provider Lens® for FinOps Platforms . [Download the report](https://ternary.app/isg-names-ternary-a-leader/) .\n\n✕\n\nBlog\n\n# Snowflake cost optimization: 8 proven strategies for reducing costs\n\n**Last updated:** September 22, 2025\n\nTernary Team\n\nSnowflake has quickly become a favorite as a data warehousing platform, and for good reasons. This makes many teams jump in, only to realize later that their [Snowflake spend](https://ternary.app/blog/manage-your-snowflake-costs/) is climbing faster than expected.\n\nThis is because managing Snowflake costs often becomes a sticking point despite the platform being impressive from a tech POV. That’s where Snowflake cost optimization comes in.\n\nIn this guide, we’ll explore how Snowflake pricing works and how you can stay in control without giving up performance or flexibility.\n\n## What is Snowflake cost optimization?\n\n### Snowflake cost components\n\nSnowflake’s architecture has 3 layers:\n\n1. Storage\n2. Compute (which Snowflake calls virtual warehouses)\n3. Cloud services\n\nThese layers are billed separately, and Snowflake pricing is usage-based, meaning you get charged for what you actually use.\n\n#### Storage\n\nEvery file, every table, every backup, all adds up in Snowflake. Snowflake charges a monthly fee based on the average amount of storage used over the month. The data is stored in compressed format. Depending on what kind of data you’re working with, like if you’re pulling in a bunch of raw CSVs versus more compact file types, the compression can significantly lower Snowflake storage costs.\n\n#### Compute layer\n\nVirtual warehouses are basically compute clusters that run your queries and handle your data loads. They scale independently, and you can have more than one running at a time. The key thing to know here: compute is paid using Snowflake credits. You spin up a warehouse, and you start spending credits. You pause it, it stops costing you. Sounds fair, but it also means you’ve got to stay on top of usage.\n\n#### Cloud services layer\n\nThis layer handles all the coordination across the platform, such as authentication, metadata management, and query optimization. It also runs on Snowflake credits, but the cost here is usually a smaller percentage compared to compute. Still, it adds up if you’ve got a lot going on, especially with serverless features in play.\n\nSpeaking of credits, let’s clarify this: a Snowflake credit is a unit that measures usage.\n\nOne credit = one unit of usage. Simple. You’re charged credits whenever you’re running a virtual warehouse, leveraging cloud services, or tapping into Snowflake’s serverless features.\n\nOne more component that Snowflake charges is data transfer between cloud regions or providers. This applies if you’re using features like external tables or exporting data from Snowflake to a data lake.\n\nThese Snowflake cost components vary depending on whether you’re on Amazon Web Services (AWS), Microsoft Azure, or Google Cloud (GCP), and the pricing structure for that is a bit more granular.\n\nLook at the tables below for Snowflake data transfer charges for AWS, Azure, and GCP:\n\n[AWS pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ] [Azure pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ] [GCP pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ]\n\n### An example of how Snowflake calculates cost\n\n**Note:** This example is courtesy of Snowflake.\n\nSuppose we have a customer using Snowflake Capacity Standard Service with Premier Support in the U.S.\n\nThey do 3 main things:\n\n1. Load data nightly using a small virtual warehouse.\n2. Support 8 users working 10 hours a day, 5 days a week, using a medium virtual warehouse.\n3. Store 4 TB of compressed data on Snowflake.\n\n#### Data loading costs\n\n|**Warehouse used** |Small Standard Virtual Warehouse |\n| --- | --- |\n|**Rate** |2 credits per hour |\n|**Usage** |2\\.5 hours daily for 31 days/month |\n|**Monthly Credits** |2 credits/hour × 2.5 hours/day × 31 days = 155 credits/month |\n\n#### User activity costs\n\n|**Users** |8 users |\n| --- | --- |\n|**Warehouse used** |Medium Standard Virtual Warehouse |\n|**Rate** |4 credits per hour |\n|**Usage** |10 hours/day, 20 workdays/month |\n|**Monthly credits for users** |4 credits/hour × 10 hours/day × 20 days = 800 credits/month |\n|**Total monthly credits (users + loading)** |800 + 155 = 955 credits/month |\n\n#### Storage costs\n\n|**Data stored** |4TB compressed |\n| --- | --- |\n|**Rate** |$23 per TB/month |\n|**Annual storage cost** |4 TB × $23 × 12 months = $1,104/year |\n\n#### Virtual warehouse cost\n\n|**Credits used per year** |955 credits/month × 12 = 11,460 credits/year |\n| --- | --- |\n|**Rate per credit** |$2 (with 5% discount: × 0.95) |\n|**Annual compute cost** |11,460 × $2 × 0.95 = $21,774/year |\n\n#### Total annual cost\n\n|**Storage** |$1,104 |\n| --- | --- |\n|**Virtual warehouse** |$21,774 |\n|**Grand Total** |$22,878 per year |\n\n## 8 best practices and techniques for optimizing your Snowflake spend and reducing costs\n\n### 1\\. Disable automatic clustering on tables that are barely touched\n\nWhile automatic clustering can improve query performance, it runs on serverless compute. This means it racks up Snowflake credits whether anyone’s actually using the table or not.\n\nIf the table is only getting hit a few times a week, that background compute activity is just silently chipping away at your budget. This is where smart Snowflake cost optimization begins.\n\nLook for tables with automatic clustering enabled that barely get queried, say, fewer than 100 times per week. Ask yourself if these tables are part of a disaster recovery setup or being shared with another account. If not, it’s probably safe to hit pause.\n\nTo suspend automatic clustering, run:\n\n|**ALTER** **TABLE** your\\_table\\_name **SUSPEND** RECLUSTER; |\n| --- |\n\nThis one step alone can help reduce Snowflake costs tied to unnecessary background compute.\n\n### 2\\. Drop materialized views that don’t pull their weight\n\nMaterialized views store precomputed results so queries can run faster.\n\nBut at the same time, they come with both storage and serverless compute costs to keep everything up to date. So if a materialized view is only being queried, say, ten times a week? You’re paying for upkeep that’s barely getting used.\n\nIt’s a solid move to suspend or remove any materialized views that aren’t actively helping performance. This falls right into the category of low-effort, high-impact snowflake cost optimization. But again, double-check if the view exists for data sharing or backup purposes before you go on a deletion spree.\n\nTo drop a materialized view, run:\n\n|**DROP** **MATERIALIZED** **VIEW** your\\_view\\_name; |\n| --- |\n\n### 3\\. Remove unused search optimization paths\n\nSearch optimization can speed up point lookups and analytical queries, but just like everything else in Snowflake, that speed boost doesn’t come free.\n\nThese access paths require extra storage and compute resources to stay in sync with your data.\n\nIf Snowflake tells you that a particular search optimization path is being used fewer than ten times a week, it might be time to rethink things. Especially if you’re trying to reduce Snowflake costs without compromising your actual workloads.\n\nYou can remove search optimization with a simple command:\n\n|**ALTER** **TABLE** your\\_table\\_name **DROP** **SEARCH** OPTIMIZATION; |\n| --- |\n\n### 4\\. Clean out large tables that haven’t been touched in a week\n\nThe massive tables that sit there eating up storage and haven’t been queried at all in the past week not only inflate your Snowflake storage costs, but they also clutter up your environment and slow down everything from data discovery to data warehouse optimization and management.\n\nIf a table isn’t serving any purpose (besides reminding you of a project from six months ago), drop it. But again, always check if it’s being used for recovery or data sharing before swinging the axe.\n\nTo delete a table, run:\n\n|**DROP** **TABLE** your\\_table\\_name; |\n| --- |\n\n### 5\\. Use transient or temporary tables for short-lived data\n\nHave you ever created a permanent table just to delete it 12 hours later?\n\nWhen you’re dealing with short-lived data, using a permanent table doesn’t make sense.\n\nSnowflake charges extra for things like Time Travel and Fail-safe on permanent tables even if they don’t stick around long enough to need it.\n\nInstead, go with a transient or temporary table.\n\nThese are lighter-weight options that skip the fancy durability features and save you money in the process.\n\nIt’s one of the simplest cost optimization techniques to implement, and it can have a real impact, especially in workflows where data turnover is high.\n\nTo create a transient table, use:\n\n|**CREATE** TRANSIENT **TABLE** your\\_table\\_name (…); |\n| --- |\n\n### 6\\. Allow multi-cluster warehouses to scale down\n\nMulti-cluster warehouses can be incredibly powerful, especially when you’ve got a high volume of concurrent queries.\n\nBut if you’ve locked the cluster count at a fixed number, say, 3 minimum and 3 maximum, you’re forcing Snowflake to keep all clusters running at all times, even when demand doesn’t justify it.\n\nThat’s wasted compute and wasted credits. Snowflake is built to scale, so let it.\n\nLower the minimum cluster count so the warehouse can scale down during slower periods. It won’t affect performance during peak hours, but it’ll quietly reduce credit consumption when traffic drops.\n\nTo adjust the scaling behavior, run:\n\n|ALTER WAREHOUSE your\\_warehouse\\_name **SET** MIN\\_CLUSTER\\_COUNT = 1; |\n| --- |\n\n### 7\\. Reduce transaction lock wait times with batch updates\n\nA sneaky Snowflake cost drain is when queries get blocked by transaction locks.\n\nThis happens when multiple users run updates or merges on the same table at the same time. Each command locks the table, and while other queries are waiting, they’re still racking up cloud services credits. So even though nothing’s happening, you’re paying for the wait.\n\nTo avoid this, change how your updates work. Use batch inserts into temporary tables instead of single-row updates. Then run periodic merges from the temp table to the main one. This cuts down on locks and lets Snowflake handle things more efficiently.\n\nFor workflows that receive a steady stream of new data, consider using a scheduled task to handle updates at intervals, say, every 15 minutes, instead of processing every change as it comes in.\n\nIt’s a small shift, but it adds up fast. And it’s one of those Snowflake optimization techniques that improves both performance and billing.\n\n### 8\\. Reduce the frequency and scope of cloning operations\n\nCloning in Snowflake [saves a ton of resources](https://ternary.app/blog/cloud-cost-savings/) compared to full copies.\n\nBut if you’re cloning entire databases or schemas over and over again, that metadata usage starts to pile up.\n\nAnd since cloning relies on cloud services, doing it frequently means your costs quietly creep up.\n\nSo instead of cloning full environments, clone only what you actually need, maybe just a single table instead of an entire schema.\n\nAlso, take a hard look at how often your teams are running these clones. If it’s part of an automated process, make sure it’s not firing more often than it needs to.\n\n## What is a KPI in Snowflake?\n\nFor Snowflake cost optimization, KPIs are your best friend.\n\nSnowflake offers a bunch of performance metrics that, when tracked together, paint a full picture. These include basically anything that has a noticeable impact on credit usage, query speed, or system efficiency.\n\n### Snowflake performance index (SPI)\n\n[Snowflake Performance Index (SPI)](https://www.snowflake.com/en/pricing-options/performance-index/) is a macro-level view of how much performance has improved over time across typical customer workloads.\n\nIt tracks millions of jobs every month to give a reliable baseline for measuring how well Snowflake is optimizing things under the hood.\n\nThis tracking gradually surfaces improvements like query improvements, data ingestion speed, replication efficiency, and more.\n\nThe best part is that many of these performance gains happen automatically, so you benefit without needing to change your code or reconfigure anything.\n\n## How to pick a Snowflake FinOps and cost optimization tool\n\nThere are a lot of moving parts in Snowflake, and the right tool should help you control the chaos, not add to it.\n\nHere’s how to make the right call:\n\n### Define your objectives\n\nBefore you even look at tool comparisons, figure out what you’re trying to solve.\n\nDo you need detailed cost visibility, like knowing exactly who or what is burning through credits, or are you looking for smart, automated recommendations that can flag optimization opportunities without you digging through logs for hours?\n\nClarifying your goals will keep you from chasing features you don’t actually need.\n\n### Decide how much automation you want\n\nSome tools give you suggestions. Others take action like auto-suspending idle warehouses, resizing compute, or flagging inefficient queries in real time.\n\nAsk yourself: are you comfortable letting the tool make changes, or do you prefer having the final say? The answer will help you zero in on a solution that fits your workflow.\n\n### Make sure the tool supports granular cost allocation\n\nYou want to be able to [break down costs](https://ternary.app/blog/cloud-cost-analysis/) by user, warehouse, role, or even by specific workloads.\n\nThe more detail you have, the easier it is to hold teams accountable and find areas where spend can be trimmed.\n\n### Check for query performance tuning support\n\nLook for tools that not only track slow or costly queries but also help you understand why they’re inefficient, whether it’s due to joins, filters, or warehouse sizing.\n\n### Prioritize customizable dashboards and reporting\n\nEvery stakeholder needs different data. Finance might want a monthly credit burn summary, while engineering needs real-time warehouse spikes.\n\nThe tool should let you build dashboards and reports that speak to your team’s needs without having to export everything to spreadsheets every week.\n\n### Evaluate ease of integration and scalability\n\nLast but not least, think about how well the tool fits into your current stack.\n\nDoes it integrate smoothly with your Snowflake environment? Can it handle your current workload and scale as you grow?\n\nSome tools might look great for small setups but fall apart once things get complex.\n\n## Final thoughts\n\nAt the end of the day, Snowflake cost optimization comes down to how much visibility you have.\n\nThe more visibility you have into your Snowflake usage, the better decisions you can make to keep costs in check.\n\nThat’s exactly where [Ternary](https://ternary.app/blog/manage-your-snowflake-costs/) can help.\n\nTernary gives your team the insights they need to manage Snowflake spend with confidence.\n\n**Get a clearer view of your Snowflake costs.**\n\n[Request a demo](https://ternary.app/demo/)\n\n## FAQ\n\n### How much do Snowflake credits cost?\n\nThe cost of Snowflake credits depends on your chosen cloud provider, region, and pricing tier. Credits are consumed when using compute, cloud services, or serverless features.\n\n### What is the biggest contributor to Snowflake costs?\n\nCompute is usually the biggest cost driver in Snowflake. Virtual warehouses charge based on per-second usage, and costs vary with warehouse size and workload.\n\n### Does Snowflake charge for storing old data?\n\nYes, Snowflake charges for storage based on the average compressed data stored per month. Keeping outdated or unused data increases your storage costs.\n\n## Related articles\n\n[](https://ternary.app/blog/manage-your-snowflake-costs/) Blog [Harness Snowflake’s power—without the pain](https://ternary.app/blog/manage-your-snowflake-costs/)\n\n[](https://ternary.app/blog/hidden-costs/) Blog [Hidden costs that can supersize your cloud bill—and how to manage them](https://ternary.app/blog/hidden-costs/)\n\n[](https://ternary.app/)\n\nAvailable as a SaaS platform and a self-hosted solution, Ternary manages more than $7.5B in multi-cloud spend across leading enterprises and managed service providers.\n\n[](https://www.linkedin.com/company/ternaryinc/) [](https://twitter.com/ternaryinc) [](https://www.youtube.com/@ternaryinc)\n\n* [Privacy policy](https://ternary.app/privacy-policy/)\n* [LLM info](https://ternary.app/llm-info/)"],"full_content":null},{"url":"https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb/","title":"Why Your Snowflake Bill is High and How to Fix It with a Hybrid Approach","publish_date":"2026-02-23","excerpts":["BACK TO LEARN\n\n# Why Your Snowflake Bill is High and How to Fix It with a Hybrid Approach\n\n17 min read BY Manveer Chawla\n\nYour Snowflake bill is high primarily because of its compute billing model, which enforces a [60-second minimum charge](https://docs.snowflake.com/en/user-guide/cost-understanding-compute) each time a warehouse resumes. This creates a significant \"idle tax\" on the frequent, short-running queries common in BI dashboards and ad-hoc analysis. You're often paying for compute you don't actually use.\n\nA surprisingly high bill for a modest amount of data is frustrating. We see it all the time. The immediate question is, \"Why is my bill so high when my data isn't that big?\" The cost isn't driven by data at rest, it's driven by data in motion, specifically by compute patterns. For many modern analytical workflows, the bill inflates from thousands of frequent queries accumulating disproportionately high compute charges.\n\nIf you don't address this, you'll face budget overruns, throttled innovation, or pressure to undertake a costly and risky platform migration. The solution isn't always abandoning a powerful platform like Snowflake. You can augment it intelligently instead.\n\nThis guide provides a practical playbook for understanding the root causes of high Snowflake costs and a strategy for reducing them using internal optimizations and a [modern hybrid architecture](https://motherduck.com/docs/concepts/architecture-and-capabilities/) .\n\n## The Real Reason Your Snowflake Bill is So High\n\nTo control costs effectively, you need to diagnose the problem first. The primary driver of inflated Snowflake bills for bursty, interactive workloads is the platform's billing model for compute. It creates a significant [\"Big Data Tax\"](https://motherduck.com/videos/the-death-of-big-data-and-why-its-time-to-think-small-jordan-tigani-ceo-motherduck/) .\n\nSnowflake bills for compute per-second, but only after a 60-second minimum is met each time a virtual warehouse resumes from a suspended state. A query that takes only five seconds to execute gets billed for a full minute of compute time. In this common scenario, you're paying for 55 seconds (over 91%) of compute resources that sit idle.\n\nHere's what this looks like on a timeline. For a 5-second query, the billed duration on Snowflake versus a usage-based platform like MotherDuck is stark.\n\n**Snowflake (X-Small Warehouse):**\n\n**MotherDuck (Pulse Compute):**\n\nWhen a BI dashboard executes 20 quick queries upon loading, each taking three seconds, this single page view could trigger 1,200 seconds (20 minutes) of billed compute time. The actual work took only one minute.\n\nThis problem gets worse with warehouse sizing. Each incremental size increase in a Snowflake warehouse [doubles its credit consumption rate](https://docs.snowflake.com/en/user-guide/cost-understanding-compute) . We often see teams defaulting to 'Medium' or 'Large' warehouses for all tasks. That creates a 4x to 8x cost premium for workloads that could easily run on an 'X-Small' warehouse.\n\nThis combination of minimum billing increments and oversized compute creates exponential cost leak. Serverless features like [Automatic Clustering](https://docs.snowflake.com/en/user-guide/cost-understanding-overall) and [Materialized Views](https://docs.snowflake.com/en/user-guide/cost-understanding-overall) consume credits in the background too, contributing to credit creep that's difficult to trace without diligent monitoring.\n\n|Warehouse Size |Credits per Hour |Relative Cost |\n| --- | --- | --- |\n|X-Small |1 |1x |\n|Small |2 |2x |\n|Medium |4 |4x |\n|Large |8 |8x |\n|X-Large |16 |16x |\n\n## First Aid: A Playbook to Immediately Optimize Snowflake\n\nBefore considering architectural changes, you can achieve significant savings by optimizing your existing Snowflake environment. These internal fixes are your first line of defense against cost overruns. They can often reduce spend by 20-40%.\n\n### 1\\. Master Warehouse Management (Set AUTO\\_SUSPEND to 60s)\n\nSet aggressive yet intelligent warehouse timeouts. For most workloads, set the [`AUTO_SUSPEND` parameter](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) to exactly 60 seconds. This ensures the warehouse suspends after one minute of inactivity, stopping credit consumption. Setting it lower than 60 seconds is counterproductive. A new query arriving within that first minute could trigger a second 60-second minimum charge.\n\nRight-size warehouses by defaulting to smaller configurations. Use 'X-Small' warehouses by default and only scale up when a specific workload fails to meet its performance SLA. Consolidate workloads onto fewer, appropriately sized warehouses to prevent warehouse sprawl. Multiple underutilized compute clusters add up on your bill.\n\nWe helped one analytics team save approximately $38,000 annually by moving its BI queries from a Medium to a Small warehouse. They accepted a marginal 4-second increase in query time.\n\n### 2\\. Leverage Snowflake's Caching Layers (Result & Warehouse)\n\nSnowflake's multi-layered cache is one of its most powerful cost-saving features. Not using it leaves money on the table.\n\n**Result Cache:** If you run the exact same query as one run previously (by anyone in the account) and the underlying data hasn't changed, Snowflake returns the results instantly from a global result cache. No warehouse starts. That's free compute. It's especially effective for BI dashboards where multiple users view the same default state.\n\n**Warehouse Cache (Local Disk Cache):** When a query runs, the required data from storage gets cached on the SSDs of the active virtual warehouse. Subsequent queries that need the same data read it from this much faster local cache instead of remote storage. This dramatically speeds up queries and reduces I/O. Keeping a warehouse warm for related analytical queries can be beneficial.\n\nDesign workloads to maximize cache hits through consistent query patterns.\n\n### 3\\. Optimize Inefficient Queries (Prune Partitions & Avoid SELECT \\*)\n\nPoorly written queries burn credits unnecessarily. While comprehensive query tuning is a deep topic, these practices provide immediate savings:\n\n**Avoid `SELECT *` :** Select only the columns you need. This reduces the amount of data processed and moved, improving caching and query performance.\n\n**Filter Early and Prune Partitions:** Apply `WHERE` clauses that filter on a table's clustering key as early as possible. This lets Snowflake prune massive amounts of data from being scanned. It's the single most effective way to speed up queries on large tables.\n\n**Use `QUALIFY` for Complex Window Function Filtering:** Instead of using a subquery or CTE to filter window function results, use the [`QUALIFY` clause](https://docs.snowflake.com/en/sql-reference/constructs/qualify) . It's more readable and often more performant.\n\n### 4\\. Implement Cost Guardrails with Resource Monitors\n\nImplement [resource monitors](https://docs.snowflake.com/en/user-guide/resource-monitors) as a critical safety net. Resource monitors track credit consumption and trigger actions like sending notifications or automatically suspending compute when usage hits predefined thresholds. They're the most effective tool for preventing budget overruns from runaway queries or misconfigured pipelines.\n\n```\n-- Create a monitor that notifies at 75% and suspends at 100% CREATE OR  REPLACE RESOURCE MONITOR monthly_etl_monitor\n WITH  CREDIT_QUOTA  = 5000 \nTRIGGERS  ON 75 PERCENT  DO NOTIFY\n         ON 100 PERCENT  DO SUSPEND;\n -- Assign the monitor to a warehouse ALTER  WAREHOUSE etl_heavy_wh  SET  RESOURCE_MONITOR  =  monthly_etl_monitor;\n```\n\nActively monitor serverless feature costs too. Query the [`serverless_task_history`](https://docs.snowflake.com/en/sql-reference/functions/serverless_task_history) view to track credits consumed by Automatic Clustering, Search Optimization, and other background tasks. This helps you understand your hidden costs and tune these features appropriately.\n\nThese internal fixes can significantly lower your Snowflake bill. To eliminate entire categories of spend, particularly from non-production workloads, you need a different approach to compute location.\n\n## Go Local: Slashing Dev & Test Costs with DuckDB\n\nA substantial portion of cloud data warehouse spend gets consumed by non-production workloads. Every [`dbt run`](https://github.com/duckdb/dbt-duckdb) , data validation script, and ad-hoc analysis performed by engineers during development consumes expensive cloud compute credits. By adopting a local-first development workflow, you can shift this entire category of work off the cloud and reduce these costs to zero.\n\nDuckDB makes this shift possible. It's a fast, in-process analytical database designed to run complex SQL queries directly on your laptop or within a CI/CD runner. DuckDB queries data files like Parquet and [CSV](https://duckdb.org/docs/data/csv/) directly. You don't need to load data into a separate database for local development. Engineers can build, test, and iterate on data models and pipelines locally before incurring any cloud costs.\n\nThis workflow saves money and dramatically improves developer velocity. You shorten the feedback loop from minutes (waiting for a cloud warehouse to provision and run) to seconds.\n\nA typical local development pattern in Python is straightforward. You can prototype rapidly without any cloud interaction.\n\n```\nimport  duckdb\n import  pandas  as  pd\n\n # Analyze a local Parquet file instantly # No cloud warehouse, no compute credits consumed \ndf = duckdb.sql( \"\"\"\n    SELECT\n        product_category,\n        COUNT(DISTINCT order_id) as total_orders,\n        AVG(order_value) as average_value\n    FROM 'local_ecommerce_data.parquet'\n    WHERE order_date >= '2024-01-01'\n    GROUP BY ALL\n    ORDER BY total_orders DESC;\n\"\"\" ).df()\n print (df)\n```\n\nRunning analytics locally is powerful for development. For sharing insights and powering production dashboards, this local-first approach extends into a hybrid architecture.\n\n## The Hybrid Solution: MotherDuck for Cost-Effective Interactive Analytics\n\nMotherDuck is a serverless data warehouse as a service built on DuckDB. It provides a simpler, [Scale-up solution](https://motherduck.com/learn-more/select-olap-solution-postgres) for workloads that are inefficient on traditional distributed data warehouses. It directly solves the idle tax problem by replacing the provisioned warehouse model with per-query, usage-based compute that bills in [one-second increments](https://motherduck.com/product/pricing/) .\n\nThis billing model profoundly impacts the cost of interactive analytics. Let's quantify the savings with a realistic scenario.\n\n### Breaking Down the Costs: A Tale of Two Queries\n\nConsider a common BI dashboard used by an operations team. It refreshes every 5 minutes during an 8-hour workday to provide timely updates. Each refresh executes 10 small queries to populate various charts. Each query takes 4 seconds to run.\n\n**Workload Parameters:**\n\n* **Queries per refresh:** 10\n* **Execution time per query:** 4 seconds\n* **Refresh frequency:** Every 5 minutes (12 refreshes per hour)\n* **Operational hours:** 8 hours/day, 22 days/month\n\n**Snowflake Cost Calculation (X-Small Warehouse):**\n\nBecause of the high frequency, the team can't let the warehouse suspend between refreshes without incurring repeated 60-second minimums. Their most cost-effective option is running an X-Small warehouse continuously during the workday.\n\n* **Total active hours per month:** 8 hours/day \\* 22 days/month = 176 hours\n* **Credits consumed per hour (X-Small):** 1\n* **Total credits per month:** 176 hours \\* 1 credit/hour = 176 credits\n* **Estimated Monthly Cost (@ $3.00/credit):** 176 credits \\* $3.00/credit = \\*\\*$528\\*\\*\n\nThis assumes perfect management. A more common scenario where the warehouse runs 24/7 would cost **$2,160** (720 hours \\* 1 credit/hr \\* $3.00/credit).\n\n**MotherDuck Cost Calculation ( Pulse Compute ):**\n\nMotherDuck bills only for the actual compute time used by queries.\n\n* **Total queries per month:** 10 queries/refresh \\* 12 refreshes/hr \\* 8 hrs/day \\* 22 days/month = 21,120 queries\n* **Total execution time per month:** 21,120 queries \\* 4 seconds/query = 84,480 seconds\n* **Total execution hours:** 84,480 seconds / 3,600 s/hr = 23.47 hours\n* **Estimated Monthly Cost (@ $0.25/CU-hour, assuming 1 CU):** 23\\.47 CU-hours \\* $0.25/CU-hour = \\*\\*$5.87\\*\\*\n\nEven assuming a more complex query consuming 4 Compute Units, the cost would only be **$23.48** . This example shows how a usage-based model eliminates waste for bursty workloads, reducing costs by over 95% in this scenario.\n\nThis calculation focuses on compute cost, the primary driver. While negligible for this interactive pattern, a full TCO analysis would include data storage and egress, where MotherDuck also offers competitive pricing.\n\nMotherDuck's architecture introduces [**\"dual execution.\"**](https://motherduck.com/docs/concepts/architecture-and-capabilities/) Its query planner intelligently splits work between the local DuckDB client and the MotherDuck cloud service. This minimizes data transfer and latency by performing filters and aggregations locally before sending smaller, pre-processed datasets to the cloud. This hybrid model works ideal for interactive analytics, BI dashboards, and ad-hoc queries on sub-terabyte hot data.\n\n```\n-- Connect to MotherDuck from any DuckDB-compatible client \nATTACH  'md:' ;\n\n -- This query joins a large cloud table with a small local file. -- The filter on the local file is pushed down, so only matching -- user_ids are ever requested from the cloud, minimizing data transfer. SELECT \n  cloud_events.event_name,\n  cloud_events.event_timestamp,\n  local_users.user_department\n FROM  my_db.main.cloud_events\n JOIN  read_csv_auto( 'local_user_enrichment.csv' )  AS  local_users\n   ON  cloud_events.user_id  =  local_users.user_id\n WHERE  local_users.is_priority_user  = TRUE ;\n```\n\n### Proven in Production: Real-World Case Studies of Significant Cost Savings\n\nThe savings from this new architecture aren't just theoretical. Companies are already using this model to achieve significant results.\n\n> **Case Study: [Definite Slashes Costs by 70%](https://motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck/)** The SaaS company Definite migrated its entire data warehouse from Snowflake to a DuckDB-based solution. The results were quick and significant, achieving an **over 70% reduction** in their data warehousing expenses. In their detailed write-up, the engineering team noted that even after accounting for the migration effort, the savings freed up a significant portion of their budget for core product development.\n> \n> \n\n> **Case Study: [Okta Eliminates a $60,000 Monthly Snowflake Bill](https://motherduck.com/learn-more/reduce-cloud-data-warehouse-costs-duckdb-motherduck/)** Okta's security engineering team needed to process trillions of log records for threat detection, with data volumes spiking daily. Their Snowflake solution was costing approximately **$2,000 per day ($60,000 monthly)** . By building a clever system that used thousands of small DuckDB instances running in parallel on serverless functions, they significantly reduced their processing costs. This case shows that even at a large scale, the DuckDB ecosystem can be much cheaper than traditional cloud warehouses.\n> \n> \n\n> **Case Study: [A 79% BI Spend Reduction with a Simple Caching Layer](https://www.reddit.com/r/dataengineering/comments/1mk85dn/how_we_used_duckdb_to_save_79_on_snowflake_bi/)** A data engineering team shared their story of implementing a smart caching layer for their BI tool. Instead of having every dashboard query hit Snowflake directly, they routed smaller, frequent queries to a DuckDB instance that served cached results. Large, complex queries were still sent to Snowflake. The impact was a **79% immediate reduction** in their Snowflake BI spend, and average query times sped up by 7x. This highlights the power of a hybrid \"best tool for the job\" approach.\n> \n>\n\n## A Framework for Workload Triage\n\nUnderstanding the tool landscape is one thing. Systematically deciding which of your workloads belong where requires a data-driven approach. Rather than relying on [abstract performance benchmarks](https://motherduck.com/videos/lies-damn-lies-and-benchmarks/) , you should analyze query history to classify every workload and route it to the most efficient engine.\n\nThe two most important axes for classification are **Execution Time** and **Query Frequency** . Consider a third axis too: **data freshness requirements** . A dashboard needing near real-time data has different constraints than one running on a nightly batch refresh.\n\nA simple 2x2 matrix provides a clear framework for triage:\n\n* **Low Execution Time, High Frequency:** Short, bursty queries that run often.\n* **Low Execution Time, Low Frequency:** Quick, sporadic, ad-hoc queries.\n* **High Execution Time, Low Frequency:** Long-running, scheduled batch jobs.\n* **High Execution Time, High Frequency:** Often an anti-pattern indicating a need for data modeling or architectural redesign. It can occur in complex, near-real-time operational analytics.\n\nYou can analyze Snowflake's [`query_history`](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) using SQL to categorize your workloads. This query provides a starting point. We use `MEDIAN` instead of `AVG` for execution time because it's more robust to outliers and gives a better sense of typical query duration.\n\n```\n-- Analyze query patterns over the last 30 days WITH  query_stats  AS  (\n     SELECT \n        warehouse_name,\n        user_name,\n        query_id,\n        execution_time  / 1000 AS  execution_seconds\n     FROM \n        snowflake.account_usage.query_history\n     WHERE \n        start_time  >=  DATEADD( 'day' ,  -30 ,  CURRENT_TIMESTAMP ())\n         AND  warehouse_name  IS NOT NULL AND  execution_status  = 'SUCCESS' \n)\n SELECT \n    warehouse_name,\n    user_name,\n     COUNT (query_id)  AS  query_count,\n    MEDIAN(execution_seconds)  AS  median_execution_seconds,  -- More robust than AVG CASE WHEN  query_count  > 1000 AND  median_execution_seconds  < 30 THEN 'Interactive BI / High Frequency' WHEN  query_count  <= 1000 AND  median_execution_seconds  < 60 THEN 'Ad-Hoc Exploration' WHEN  median_execution_seconds  >= 300 THEN 'Batch ETL / Heavy Analytics' ELSE 'General Purpose' END AS  workload_category\n FROM \n    query_stats\n GROUP BY \n    warehouse_name, user_name\n ORDER BY \n    query_count  DESC ;\n```\n\nOnce categorized, map these workloads to the optimal tool:\n\n1. **Interactive BI / High Frequency (Short & Bursty):** Prime candidates for migration to **MotherDuck** . The per-second, usage-based billing model eliminates the idle tax, offering dramatic cost savings for dashboards and [embedded analytics](https://motherduck.com/learn-more/customer-facing-analytics-database/) .\n2. **Ad-Hoc Exploration (Short & Sporadic):** This category fits well with **MotherDuck** or local **DuckDB** . For queries on smaller datasets or local files, DuckDB provides instant, free execution. For shared datasets, MotherDuck offers a cost-effective cloud backend.\n3. **Batch ETL / Heavy Analytics (Long & Scheduled):** These large, resource-intensive jobs often work best on **Snowflake** . Its provisioned warehouses provide predictable performance for multi-terabyte transformations. Its mature ecosystem simplifies complex data pipelines.\n4. **Development & CI/CD:** Move all non-production workloads to local **DuckDB** , regardless of their characteristics. This completely eliminates cloud compute costs during development and testing.\n\n## When the Hybrid Approach Isn't the Right Fit: Sticking with Snowflake\n\nTo build an effective architecture, you need to know a tool's limitations. The hybrid approach isn't a universal solution. Certain workloads are best suited for a mature, large-scale data warehouse like Snowflake. Acknowledging this builds trust and leads to better technical decisions.\n\n**Massive Batch ETL/ELT:** For scheduled jobs processing many terabytes of data, Snowflake's provisioned compute model provides predictable power and performance. The 60-second minimum doesn't matter for jobs that run for hours.\n\n**[Enterprise-Grade Governance and Security](https://trust.snowflake.com/) :** Organizations with complex data masking requirements, deep Active Directory integrations, or strict regional data residency rules often rely on Snowflake's mature and comprehensive features.\n\n**Highly Optimized, Long-Running Workloads:** If you have a workload that already runs consistently on a warehouse and maximizes its uptime (like a data science cluster running for 8 hours straight), the idle tax isn't a problem. There's little cost benefit to moving it.\n\nThe goal of a hybrid architecture is using the right tool for the right job, not replacing a tool that's already performing efficiently.\n\n## The Modern Alternatives Landscape: Where Does MotherDuck Fit?\n\nWhile the Snowflake-and-MotherDuck hybrid model effectively addresses many common workloads, the broader data platform market offers other specialized solutions. Understanding where they fit provides a complete picture for architectural decisions.\n\nData lake query engines like [Starburst](https://www.starburst.io/) and [Dremio](https://www.dremio.com/) are powerful for organizations wanting to query data directly in object storage like S3. They offer flexibility but often come with significant operational overhead.\n\nFor use cases demanding sub-second latency at very high concurrency (like real-time observability), specialized engines like [ClickHouse](https://clickhouse.com) often provide superior price-performance.\n\nWithin classic cloud data warehouses, [Google BigQuery](https://cloud.google.com/bigquery/) presents a different pricing model. Its on-demand, per-terabyte-scanned pricing can be cost-effective for sporadic forensic queries. But it carries the risk of a runaway query where a single mistake leads to a massive bill.\n\nMotherDuck carves a unique niche. It combines the serverless simplicity of BigQuery with the efficiency of a local-first workflow powered by DuckDB. This makes it highly cost-effective and productive for teams focused on speed, iteration, and interactive analytics. You don't get the cost penalty of a traditional warehouse or the operational complexity of a data lake.\n\n|Workload Type |Recommended Primary Tool |Rationale |\n| --- | --- | --- |\n|**Local Dev/Testing** |DuckDB |Eliminates cloud compute cost for non-production work. |\n|**Interactive Dashboards (<5TB)** |MotherDuck |Per-second billing avoids idle tax on bursty query patterns. |\n|**Large Batch ETL (>10TB)** |Snowflake |Predictable performance and mature features for heavy jobs. |\n|**Real-Time Observability (High QPS)** |ClickHouse |Optimized architecture for sub-second latency at high concurrency. |\n|**Sporadic Forensic Queries** |BigQuery (On-Demand) / MotherDuck |Pay-per-use model is efficient for unpredictable, infrequent queries. |\n\n## Conclusion and Path Forward\n\nThe path to a more efficient and cost-effective analytics stack doesn't require abandoning existing investments. You augment them intelligently. By adopting a three-tiered strategy, organizations gain control over their cloud data warehouse spending while empowering teams with better tools.\n\nThe strategy is simple:\n\n1. **Tune:** Implement Snowflake-native optimizations like 60-second auto-suspend timers, right-sized warehouses, and resource monitors to immediately reduce waste.\n2. **Go Local:** Shift all development and testing workloads to a local-first workflow with DuckDB. This eliminates an entire category of cloud compute spend.\n3. **Go Hybrid:** Use the workload triage framework to identify bursty, interactive workloads. Offload them to MotherDuck, replacing the idle tax with fair, usage-based billing.\n\nThis hybrid architecture uses each platform's strengths. Snowflake handles massive, scheduled batch processing and enterprise governance. The DuckDB/MotherDuck ecosystem handles cost-effective development, ad-hoc exploration, and interactive analytics.\n\nStart with your own data. Analyze your Snowflake `query_history` using the provided script. If you see a high volume of queries with median execution times under 30 seconds, that workload is a prime candidate for migration.\n\nFrom there:\n\n1. **Audit:** Use the provided SQL scripts to identify your most expensive and inefficient warehouses.\n2. **Experiment:** [Download DuckDB](https://duckdb.org/docs/installation/) and run your next data model test locally.\n3. **Prototype:** [Sign up for MotherDuck's free tier](https://app.motherduck.com/signup) , upload a dataset, and connect a BI tool to experience the performance and simplicity firsthand.\n\nBy taking these steps, teams transform their analytics budget from a source of stress into a driver of innovation.\n\nStart using MotherDuck now!\n\n[Try 21 Days Free](https://app.motherduck.com/?auth_flow=signup)\n\n## FAQS\n\n### Why is my Snowflake bill so high when my data isn’t that big?\n\nYour bill is likely high due to compute costs, which often account for over 80% of the total. Snowflake's pricing includes a 60-second minimum charge every time a warehouse activates, creating an \"idle-compute tax\" on the short, frequent queries common in development and BI.\n\n### What are cost-effective alternatives to Snowflake for data warehousing?\n\nFor many modern analytical workloads, a hybrid architecture using DuckDB for local processing and MotherDuck for a serverless cloud backend is a highly cost-effective alternative. This model is designed to eliminate idle compute costs and can reduce data warehousing bills by 70-90%.\n\n### Can any tools reduce my Snowflake spend by handling queries locally?\n\nYes. The open-source DuckDB Snowflake Extension allows you to query data from your Snowflake warehouse directly within a local DuckDB instance. This lets you handle development, testing, and iterative analysis on your laptop for free, significantly reducing Snowflake credit consumption.\n\n### How does a hybrid local-cloud analytics model optimize costs?\n\nIt shifts the bulk of analytics work—especially development and ad-hoc queries—from an expensive, minute-metered cloud warehouse to your local machine, where it's free. You only use the serverless cloud backend (like MotherDuck) for collaboration or larger queries, paying only for the actual seconds of compute used.\n\n### How can we optimize Snowflake resource allocation without hurting performance?\n\nStart by right-sizing warehouses (default to XS/S), setting aggressive auto-suspend policies (30-120 seconds), and consolidating workloads. For a bigger impact, offload development and BI workloads to a hybrid DuckDB/MotherDuck architecture to isolate and reduce the most inefficient costs.\n\n### Is MotherDuck a full replacement for Snowflake?\n\nFor many startups and teams with data under a few terabytes, it can be a full replacement. For enterprises with petabyte-scale batch processing needs, it serves as a powerful complement to offload expensive interactive and development workloads that Snowflake handles inefficiently.\n\n### Can MotherDuck connect to my existing BI tools like Tableau or Power BI?\n\nYes. MotherDuck and DuckDB support standard connection protocols like JDBC and ODBC, allowing them to integrate with most major BI and data visualization tools. You can power dashboards from either a local DuckDB instance or the MotherDuck serverless backend.\n\n## Additional Resources\n\n[Blog The Data Warehouse TCO: A Guide to the True Costs of Snowflake, BigQuery, and Redshift](https://motherduck.com/learn-more/data-warehouse-tco/) [Blog MotherDuck: A Faster, Cost-Effective BigQuery Alternative](https://motherduck.com/learn-more/bigquery-alternative-motherduck/)"],"full_content":null}],"errors":[],"warnings":[{"type":"warning","message":"Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.","detail":null}],"usage":[{"name":"sku_extract_excerpts","count":4}]}
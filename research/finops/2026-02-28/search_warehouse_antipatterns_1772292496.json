{"search_id":"search_61c75b0c84764734a5f6ccdef1d372e6","results":[{"url":"https://community.snowflake.com/s/article/Misconception-of-larger-warehouse-sizes-costing-more","title":"Misconception of larger warehouse sizes costing more","excerpts":["Loading\n× Sorry to interrupt\nCSS Error\nRefresh\nCREATE ACCOUNT SIGN IN\nKNOWLEDGE BASE ARTICLES\nCan't find what you're looking for? **Ask The Community**\n**Sign Up for snowflake** **communications**\n[Privacy Notice](https://www.snowflake.com/privacy-policy/) | [Site Terms](https://www.snowflake.com/legal/snowflake-community-terms-of-service/) | Cookie Settings | [Do not Share My personal Information](https://www.snowflake.com/privacy-policy/)\n[unsubscribe here](https://info.snowflake.com/2024-Preference-center.html) or customize your communication preferences\n[](https://twitter.com/SnowflakeDB \"Snowflake Twitter\")\n[](https://www.linkedin.com/uas/login?session_redirect=%2Fcompany%2F3653845 \"Snowflake LinkedIn\")\n[](https://www.youtube.com/user/snowflakecomputing \"Snowflake YouTube\")\n[](https://www.facebook.com/snowflakedb/ \"Snowflake Facebook\")\nLoading\nMisconception of larger warehouse sizes costing more"]},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization - Snowflake","excerpts":["Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Business Impact > Overview > Consider cost as a design constraint\nContent:\nAt the ingestion layer, best practices include balancing latency versus\ncost by selecting appropriate services (e.g., Snowpipe, Snowpipe\nStreaming, or third-party tools) and choosing the right storage format\n(e.g., native tables, Iceberg). For transformations, design with\nfrequency versus SLA in mind to ensure data freshness matches the\nbusiness need. For analytics, apply schema design best practices such as\nthoughtful clustering key choices and pruning strategies to reduce\nconsumed credits. In distribution, optimize data transfer by monitoring\negress patterns and applying cost-saving practices like the [Snowflake Data Transfer Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) .\nSection Title: Cost Optimization > Business Impact > Overview > Quantify value\nContent:\nTo maximize organizational outcomes, Snowflake consumption must be\nexplicitly tied to measurable business value and clearly communicated in\nterms that resonate with stakeholders. Establishing baselines using\nSnowflake’s [Account Usage views](https://docs.snowflake.com/en/sql-reference/account-usage) creates a reference point, while tracking the current state highlights\ntrends in performance and consumption. Defining explicit goal\nstates—such as reduced cost per decision, improved time-to-market, or\nbroader data access—ties workloads directly to outcomes that matter to\nstakeholders. Outliers that diverge from these goals should be flagged\nfor review and optimization to prevent wasted resources. Best practices\ninclude applying unit economic measures related to your field (e.g.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\n**Compute (Virtual Warehouses, Snowpark Container Services, Openflow):** This is often the most dynamic and largest portion of\nSnowflake spend. Virtual Warehouses are billed per-second after an\ninitial 60-second minimum when active, with credit consumption\ndirectly proportional to warehouse size (e.g., an “X-Small” Gen1\nwarehouse consumes one credit per hour, a 'Small' consumes two credits\nper hour, doubling with each size increase) while SPCS (via [compute pools](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) )\nare billed for all uptime with a minimum of five minutes. [Openflow](https://docs.snowflake.com/en/user-guide/data-integration/openflow/cost) is billed per second of runtime with a 60-second minimum.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nCost Anomaly Detection is a critical component of visibility that\nleverages machine learning to continuously monitor credit consumption\nagainst historical spending patterns, automatically flagging significant\ndeviations from the established baseline. This proactive monitoring is\nessential for preventing budget overruns and identifying inefficiencies,\nshifting the organization from a reactive to a proactive cost management\nposture to mitigate financial risk. As a best practice, you should\ninitially review anomaly detection on the entire account to gain a broad\nview, then dive deeper into a more granular review for individual\nhigh-spend warehouses. [This approach](https://docs.snowflake.com/en/user-guide/cost-anomalies) allows for more targeted analysis and assigns clear ownership for\ninvestigating any flagged anomalies. There are several methods for\nanomaly detection supported by Snowflake:\n ... \nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\n**Implement resource monitors** : [Resource monitors](https://docs.snowflake.com/en/user-guide/resource-monitors) are a powerful feature for tracking and controlling credit consumption\nacross virtual warehouses or for the entire account. Their primary\nimportance lies in their ability to enforce strict budget limits,\npreventing cost overruns by automatically [triggering actions](https://docs.snowflake.com/en/user-guide/resource-monitors) ,\nsuch as sending notifications and/or suspending warehouses when credit\nusage reaches a defined quota. For effective governance, it is a best\npractice to create multiple resource monitors at different\ngranularities (e.g., per-department, per-project) with escalating\nactions, such as notifying administrators at 80% usage and suspending\nall assigned warehouses at 100% usage, to cap spending.\n ... \nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\nThe best practice is to set the [STATEMENT_TIMEOUT_IN_SECONDS parameter](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) at different levels—for the account, warehouse, specific users, or\nindividual sessions—to tailor controls to different workload\npatterns, such as allowing longer timeouts for ETL warehouses\ncompared to BI warehouses. [Queued timeout policies](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) can also help remove queries that eclipse a reasonable time\nthreshold and could have been run elsewhere by users trying to\nreceive a response. **Policy-based automation** can also cancel queries pre-emptively.\n ... \nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\nBy default, all\nwarehouses have auto-suspend enabled, however this feature can be\ndisabled, so it’s important to [monitor warehouse auto-suspend configuration](https://docs.snowflake.com/en/user-guide/warehouses-considerations) and ensure proper access controls are set to restrict users from disabling the auto-suspend setting. The best practice for balancing cost versus performance is to reduce the auto-suspend policy to the minimum possible (generally above 60 seconds) without affecting query caching and performance (SLA) expectations.\n ... \nSection Title: Cost Optimization > Control > Overview > Govern resource creation and administration\nContent:\n**Limit resource creation:** Restrict the ability to create or modify\nvirtual warehouses and other high-cost resources to a small number of\ntrusted roles to prevent uncontrolled growth. **Establish a transparent workflow:** Create a clear, simple workflow\nfor provisioning resources, especially for larger warehouses. For\nexample, any request for a warehouse of medium size or larger should\nrequire a business justification and an assigned cost center. **Near real-time visibility:** Triggered visibility is non-negotiable\nfor monitoring resource creation and resizing. Configure alerts that\nnotify FinOps or CoE teams whenever a new warehouse is created or an\nexisting one is modified outside of a provisioning workflow. This\nallows for immediate review and prevents overprovisioning. **Enforce tagging:** Make a mandatory tagging strategy a prerequisite\nfor all resource creation.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nSeparate warehouses by workload (e.g., ELT versus analytics versus\ndata science)\nWorkload size in bytes should match the t-shirt size of the warehouse\nin the majority of the workloads–larger warehouse size doesn’t always\nmean faster\nAlign warehouse size for optimal cost-performance settings\n[Utilize Multi-Cluster Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-considerations) configuration to solve for high concurrency\nUtilize [Query Acceleration Services](https://docs.snowflake.com/en/user-guide/query-acceleration-service) to help with infrequent, large data scans\nFor memory-intensive workloads, use a warehouse type of Snowpark\nOptimized or higher memory resource constraint configurations as\nappropriate\nSet appropriate auto-suspend settings - longer for high cache use,\nlower for no cache reuse\nSet appropriate warehouse query timeout settings for the workload and\nthe use cases it supports.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nOptimize dashboards and reports by [reusing the warehouse SSD cache](https://docs.snowflake.com/en/user-guide/warehouses-considerations) for repeated select queries. This can be achieved by configuring a\nlonger warehouse autosuspend setting.\nLoading large volumes of data using [optimal file sizes](https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare) and utilizing all available threads in a virtual warehouse\nProcessing large data sets for AI workloads using memory-optimized\nwarehouses\n**Warehouse sizing**\nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nMapping the workload to the [right warehouse size](https://docs.snowflake.com/en/user-guide/performance-query-warehouse-size) and configuration is an important consideration of warehouse design.\nThis should consider several factors like query completion time,\ncomplexity, data size, query volume, SLAs, queuing, and balancing\noverall cost objectives. Warehouse sizing involves a cost-benefit\nanalysis that balances performance, cost, and human expectations. Humans\noften have expectations that their queries will not be queued or take a\nlong time to complete, so it is recommended to have dedicated warehouses\nfor teams.\nRecommendations for choosing the right-sized warehouse include:\nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nFollow the principles outlined above and understand that this is a\ncontinuous improvement process.\nChoose a size based on the estimated or actual workload size and\nmonitor.\nUtilize Snowflake's extensive telemetry data, such as [QUERY_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) and [WAREHOUSE_METERING_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history) ,\nto validate that the warehouse size is impacting the metrics you care\nabout in the direction you intend.\n**Optimal warehouse settings**\nWhile Snowflake strives for minimal knobs and self-managed tuning, there\nare situations where selecting the right settings for warehouses can\nhelp with optimal cost and/or performance. Some of the key [warehouse settings](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse) include\n ... \nSection Title: Cost Optimization > Optimize > Overview > Leverage Managed Services\nContent:\n**[Query Acceleration Service](https://docs.snowflake.com/en/user-guide/query-acceleration-service) (QAS):** QAS is a serverless feature that provides a burst of additional\ncompute resources to accelerate specific parts of a query, rather than\nreplacing an appropriately sized warehouse. It's particularly beneficial\nfor large I/O operation queries, eliminating the need to manually scale\nwarehouses up or down. QAS also helps speed up query execution when\ntable clustering cannot be altered due to other workload dependencies. A\ncost-benefit analysis should always be performed to ensure that the\ncredit consumption from QAS is justified by the performance improvement\nand the avoided cost of a larger warehouse.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\n**Leverage the query profile for deep-dive analysis**\nAfter identifying problematic queries, the [Query Profile](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) is an essential tool for understanding the execution plan of a query. It\nprovides a detailed, step-by-step breakdown of every operator involved,\nfrom data scanning to final result delivery. To gain visibility into\ninefficiencies, analysts and developers should regularly use the Query\nProfile to identify common anti-patterns like:\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\n**File and batch sizing:** Optimal performance is achieved with files\nsized 100–250 MB compressed. Too few large files or too many very\nsmall files reduce load parallelism and reduce efficiency. **Parallelism:** Size and configure your warehouse cluster to match\nthe number and typical size of files to be loaded. (e.g., an XS\nwarehouse has eight threads; to utilize it fully, you need at least\neight files). **File organization:** Partition files in external stages by logical\npaths (date, region, etc.) to allow selective/cost-effective loads and\nenable easy partition-level reloading. **Pattern filtering:** Use COPY's pattern and files parameter to\nprecisely select the right files for each load, particularly to avoid\nscanning entire stages. **Resource management:** Use resource monitors and low auto-suspend\nsettings on load warehouses to minimize idle compute costs.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nTo determine tables that can most benefit from re-ordering how data is\nstored, you can review Snowflake’s [best practice](https://medium.com/snowflake/supercharging-snowflake-pruning-using-new-account-usage-views-52530b24bf2e) on how to analyze the TABLE_QUERY_PRUNING_HISTORY and\nCOLUMN_QUERY_PRUNING_HISTORY account usage views. Fundamentally,\nreducing the percentage of partitions in each table pruned to the\npercentage of rows returned in a query will lead to the most optimized\ncost and performance for any given workload.\nA table’s Ideal pruning state is scanning the same % of rows matched as\npartitions read, minimizing unused read rows.\n**Warehouse optimization**\nWarehouse concurrency, type, and sizing can impact the execution\nperformance and cost of queries within Snowflake. Review the compute\noptimization section for more information into the tuning of the\nwarehouse and its effect on cost and performance."]},{"url":"https://docs.snowflake.com/en/user-guide/cost-optimize","title":"Optimizing cost - Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nEN\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg™\nApache Iceberg™ Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic tables\nStreams and tasks\nRow timestamps\ndbt Projects on Snowflake\nData Unloading\nStorage lifecycle policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Optimization\nSection Title: Optimizing cost ¶\nContent:\nThis topic summarizes the features and strategies you can use to optimize Snowflake to reduce costs and maximize your spend.\nUsing cost insights to save\nLearn how to use cost insights to optimize Snowflake for cost within a particular account.\nOptimizing cloud services for cost\nLearn how to adjust your cloud services usage to reduce costs.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nRelated content\nManaging cost in Snowflake\nUnderstanding overall cost"]},{"url":"https://www.reddit.com/r/snowflake/comments/10dt8bc/how_do_you_right_size_a_snowflake_warehouse/","title":"how do you right size a snowflake warehouse? - Reddit","publish_date":"2023-01-16","excerpts":["Skip to main content Open menu Open navigation  Go to Reddit Home\nr/snowflake A chip A close button\nExpand user menu Open settings menu\nGo to snowflake r/snowflake\nr/snowflake\nUnofficial subreddit for discussion relating to the Snowflake Data Cloud\nMembers •\nZealousideal_Zebra_9\nSection Title: how do you right size a snowflake warehouse?\nContent:\nIs there a guide or anything that anyone uses? I need to figure out if my warehouses are too large or too small Share\nSection Title: Related Answers Section\nContent:\nRelated Answers\nGuide for sizing Snowflake warehouses\nUnderstanding Snowflake warehouse clusters\nTop features of Snowflake for data analysts\nHow to integrate Snowflake with Python\nCommon pitfalls when using Snowflake\nPublic\nAnyone can view, post, and comment to this community\n0 0\nSection Title: Related Answers Section > Top Posts\nContent:\n[Reddit reReddit: Top posts of January 16, 2023 * * *](https://www.reddit.com/posts/2023/january-16-1/global/)\n[Reddit reReddit: Top posts of January 2023 * * *](https://www.reddit.com/posts/2023/january/global/)\n[Reddit reReddit: Top posts of 2023 * * *](https://www.reddit.com/posts/2023/global/)\nExpand Navigation Collapse Navigation"]},{"url":"https://www.reddit.com/r/snowflake/comments/1pypvbh/inherited_snowflake_mess_large_warehouses_running/","title":"Inherited snowflake mess... large warehouses running small workloads - Reddit","excerpts":["Ir al contenido principal Abrir menú Abrir navegación  Ir al inicio de Reddit\nr/snowflake\nDescargar la app Descargar la app de Reddit [Iniciar sesión](https://www.reddit.com/login/) Inicia sesión en Reddit\nExpandir menú de usuario Abrir menú Ajustes\nIr a snowflake r/snowflake\nr/snowflake\nUnofficial subreddit for discussion relating to the Snowflake Data Cloud\nVisitantes semanales Contribuciones semanales •\nHenryWolf22\nSection Title: Inherited snowflake mess... large warehouses running small workloads\nContent:\nTook over FinOps for our Snowflake deployment last quarter, and holy shit, the waste is staggering. Previous team sized everything as large warehouses back in 2023 because god know why.\nRan the numbers and found ~80% of our ETL jobs could run perfectly fine on Small warehouses. We're talking batch processes that finish in 15 minutes regardless of warehouse size because they're I/O bound, not compute bound. 25% of our Snowflake spend is pure waste from oversized warehouses.\nAnyone got advice on convincing the team to downsize without pushback, or other quick wins to cut this crap? Compartir\nSection Title: Sección de respuestas relacionadas\nContent:\nRespuestas relacionadas\n[Top features of Snowflake for data analysts](https://www.reddit.com/answers/f4d7f117-b6ae-4cfb-81ce-4b996a65d39a/?q=Top+features+of+Snowflake+for+data+analysts&source=PDP)\n[Managing costs in Snowflake effectively](https://www.reddit.com/answers/a1a5d834-70cd-4840-b69f-f991586cb379/?q=Managing+costs+in+Snowflake+effectively&source=PDP)\n[How to secure data in Snowflake environments](https://www.reddit.com/answers/de4cb7a7-b958-40d4-a0e2-36b023daa39a/?q=How+to+secure+data+in+Snowflake+environments&source=PDP)\n[Best resources for learning Snowflake quickly](https://www.reddit.com/answers/de498612-3da4-442e-8a26-2e5aea2d74ef/?q=Best+resources+for+learning+Snowflake+quickly&source=PDP)\n[Real-world success stories using Snowflake](https://www.reddit.com/answers/a4271acd-0472-469e-9eee-7a2468a5cbb9/?q=Real-world+success+stories+using+Snowflake&source=PDP)\n¿Primera vez en Reddit?\nCrea una cuenta y conéctate con un mundo de comunidades.\nSection Title: Sección de respuestas relacionadas\nContent:\nContinuar con correo electrónico\nContinuar con número de teléfono\nAl continuar, aceptas nuestros [Acuerdo de usuario](https://www.redditinc.com/policies/user-agreement) y reconoces que entiendes la [Política de privacidad](https://www.redditinc.com/policies/privacy-policy) .\nPública\nCualquiera puede ver, publicar y comentar en esta comunidad\n0 0\nExpandir la navegación Esconder navegación"]},{"url":"https://docs.snowflake.com/en/user-guide/performance-query-warehouse","title":"Optimizing warehouses for performance | Snowflake Documentation","excerpts":["Guides Performance optimization Optimizing warehouses for performance\nSection Title: Optimizing warehouses for performance ¶\nContent:\nIn the Snowflake architecture, virtual warehouses provide the computing power that is required to execute queries. Fine-tuning the compute\nresources provided by a warehouse can improve the performance of a query or set of queries.\nA warehouse owner or administrator can try the following warehouse-related strategies as they attempt to improve the performance of one or\nmore queries. As they adjust a warehouse based on one of these strategies, they can test the change by re-running the query and checking its execution time .\nWarehouse-related strategies are just one way to boost the performance of queries. For performance strategies involving how data\nis stored, refer to Optimizing storage for performance .\nSection Title: Optimizing warehouses for performance ¶\nContent:\n| Strategy | Description |\n| Reduce queues | Minimizing queuing can improve performance because the time between submitting a query and getting its results is longer when the |\n| query must wait in a queue before starting. |  |\n| Resolve memory spillage | Adjusting the available memory of a warehouse can improve performance because a query runs substantially slower when a warehouse runs |\n| out of memory, which results in bytes “spilling” onto storage. |  |\n| Increase warehouse size | The larger a warehouse, the more compute resources are available to execute a query or set of queries. |\n| Try query acceleration | The query acceleration service offloads portions of query processing to serverless compute resources, which speeds up the processing |\n| of a query while reducing its demand on the warehouse’s compute resources. |  |\nSection Title: Optimizing warehouses for performance ¶\nContent:\n| Strategy | Description |\n| Reduce queues | Minimizing queuing can improve performance because the time between submitting a query and getting its results is longer when the |\n| query must wait in a queue before starting. |  |\n| Resolve memory spillage | Adjusting the available memory of a warehouse can improve performance because a query runs substantially slower when a warehouse runs |\n| out of memory, which results in bytes “spilling” onto storage. |  |\n| Optimize the warehouse cache | Query performance improves if a query can read from the warehouse’s cache instead of from tables. |\n| Limit concurrently running queries | Limiting the number of queries that are running concurrently in a warehouse can improve performance because there are fewer queries |\n| putting demands on the warehouse’s resources. |  |\nSection Title: Optimizing warehouses for performance ¶\nContent:\nTip\nOptimizing a warehouse for query performance is more straightforward when the warehouse runs similar workloads. For example, if a\nwarehouse runs significantly different queries, the cost of a performance enhancement might be wasted on a query that does not benefit\nfrom the optimization.\nFor general guidelines about distributing workloads to your organization’s warehouses, see the Analyzing Your Workloads section of\nthe [Managing Snowflake’s Compute Resources](https://www.snowflake.com/blog/managing-snowflakes-compute-resources/) (Snowflake blog).\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nRelated content\nSection Title: Optimizing warehouses for performance ¶\nContent:\nOverview of warehouses\nExploring execution times\nOptimizing storage for performance"]},{"url":"https://www.flexera.com/blog/finops/snowflake-warehouse-sizes/","title":"8 best practices for choosing right Snowflake warehouse sizes (2026)","publish_date":"2026-01-27","excerpts":["SolutionsSpend management by vendor[Flexera is a Leader in 2025 cloud financial management tools](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)Discover recognized CFM vendors to watch in the 2025 Gartner® Magic Quadrant™[View report](https://info.flexera.com/CM-REPORT-Gartner-Magic-Quadrant-Cloud-Financial-Management-Tools)\nProductsFlexera One[Introducing Flexera One SaaS Management](https://www.flexera.com/products/flexera-one/saas-management)Discover comprehensive SaaS visibility for taming SaaS sprawl, wasted spend and compliance risks.\n[Book your personalized demo](https://www.flexera.com/products/flexera-one/saas-management)\nIT Visibility\nITAM\nSnow Atlas\nCloud License Management\nSaaS Management\nFinOps\nCloud Cost Optimization\nCloud Commitment Management\nContainer Optimization\nVirtual Machine Optimization\nData Cloud Optimization\nApplication Readiness\nSecurity\nIntegrations\nTechnology Intelligence Platform\nAll Products\nSuccessCustomer SuccessServices & TrainingSupport[2025 Technology Intelligence Awards](https://info.flexera.com/ITAM-REPORT-State-of-IT-Asset-Management)The results are in—see how our 2025 winners and honorable mentions are shaping the future of Technology Intelligence.\n ... \nCustomers Open External Links\n[Community](https://community.flexera.com/)\n[Product login](https://app.flexera.com/login)\n[Spot login](https://console.spotinst.com/auth/signIn)\n[Partner Portal](https://partnerhub.flexera.com/)\nSearch\nBook a demo\nHome\nBlog\n[FinOps](https://www.flexera.com/blog/finops/)\n8 best practices for choosing right Snowflake warehouse sizes (2026)\n ... \nSection Title: 8 best practices for choosing right Snowflake warehouse sizes (2026)\nContent:\nSnowflake provides a scalable, secure and cost-effective solution for storing and analyzing large volumes of data in real-time. However, choosing the appropriate Snowflake warehouse sizes can be a daunting task as it significantly impacts both Snowflake costs and query performance .\nIn this article, we’ll cover the detailed steps you need to take to ensure your Snowflake virtual warehouse is the “ **right size** ”, striking the perfect balance between price and performance.\n ... \nSection Title: ... > How do cost & performance change as Snowflake warehouse sizes increases?\nContent:\nThe size of a Snowflake virtual warehouse affects both cost and performance. All warehouses, regardless of size, are charged based on the amount of time they are running, whether actively processing queries or waiting for one to be issued. The hourly costs—measured in Snowflake credits—are also doubled with every increase in warehouse size.\nSnowflake uses t-shirt sizing names for their warehouses. The available sizes range from **X-SMALL** to **6X-Large** and for most Snowflake users, the **X-SMALL** warehouse is sufficient, providing ample power to effectively handle massive datasets, depending on the complexity of the workload.\nAs previously stated, there are two types of virtual warehouses: **SnowStandard** and **Snowpark-optimized** and each type charges credits differently.\nSection Title: ... > How do cost & performance change as Snowflake warehouse sizes increases?\nContent:\nKnowing how Snowflake’s resources are billed is crucial to understanding how to use the platform. While we’ll touch on some of the essential aspects briefly, for a thorough explanation of Snowflake credit costs and ways to decrease the Snowflake costs, we recommend reading our two articles: 8 Ways to Decrease Snowflake Costs & 4 Best Snowflake Cost Estimation Tools\nFor **standard virtual warehouses** , the credit/hour table looks like this:\n ... \nSection Title: ... > How do cost & performance change as Snowflake warehouse sizes increases?\nContent:\nFor Snowpark-optimized virtual warehouses, the credit/hour table looks like this:\n ... \nSection Title: ... > How do cost & performance change as Snowflake warehouse sizes increases?\nContent:\nFinally, now that we know what a Snowflake virtual warehouse is and how much it costs, it’s time to get into the article’s core. Let’s review how to “ **right-size** ” your virtual warehouse for optimal Snowflake performance.\nSection Title: ... > Steps to Effectively Right-Size Your Snowflake Virtual Warehouse\nContent:\nSelecting the right warehouse size in Snowflake can significantly impact performance, potentially saving hours of waiting for failed queries and quickly delivering results for complex queries. To find the best warehouse size, it’s crucial to regularly perform tests that help identify the right size. Whether you’re setting up a new warehouse or adjusting the size of an existing one, following these steps can help you find the optimal size for your needs.\nSection Title: ... > 1) Start Small and Scale Up as Needed\nContent:\nTo effectively Right-Size Your Snowflake Virtual Warehouse, it’s essential to start small and gradually increase the size until you find the sweet spot of maximum performance at the lowest cost.\nStart with the X-SMALL warehouse and run workloads. If queries are slow, increase the size incrementally until performance is satisfactory. Remember that each size increase doubles the cost, so ensure queries are at least 2x faster. X-SMALL or SMALL warehouses may suffice for small-scale operations, while larger sizes (X-LARGE to 6X-LARGE) are suitable for bulk loading and heavy calculations in large-scale environments.\nHere are a few essential points to consider in order to create the optimal balance between cost and performance:\nSection Title: ... > 1) Start Small and Scale Up as Needed\nContent:\nStart with an X-SMALL warehouse and gradually increase the size until the query duration stops halving.\nChoose a Snowflake warehouse sizes that offers the best cost-to-performance ratio, usually one smaller than the largest warehouse that fully utilizes the query.\nIf increasing the Snowflake warehouse sizes only results in a small decrease in query time, sticking with the smaller warehouse may be more cost-effective. But, if faster performance is necessary, a larger Snowflake warehouse sizes can be selected, but returns may start diminishing.\n ... \nSection Title: ... > 3) Check if Your Snowflake Warehouse is Under or Over Provisioned\nContent:\nChoosing the appropriate Snowflake warehouse sizes in Snowflake is essential for determining whether the warehouse is under or over-provisioned. This evaluation assists in optimizing resource allocation, controlling costs and ensuring efficient query performance.\n ... \nSection Title: 8 best practices for choosing right Snowflake warehouse sizes (2026) > ... > Is it over-provisioned?\nContent:\nAn **over-provisioned** Snowflake warehouse may have more resources than required, resulting in unnecessary costs without providing any significant performance improvements. To identify over-provisioning, analyze the warehouse’s resource utilization, such as CPU and memory usage. If these metrics consistently show low utilization, it may be more cost-effective to reduce the warehouse size.\n ... \nSection Title: ... > 5) Determine Optimal Costs and Performance\nContent:\nOptimizing the cost and performance of Snowflake virtual warehouses is critical. The warehouse’s size plays a significant role in the speed of CPU-bound queries. Increasing the size of the Snowflake warehouse will boost query speed until the resources are fully utilized. Beyond this point, larger Snowflake warehouse sizes will not enhance performance and costs can rise without any improvement in query speed/performance.\nSection Title: ... > 5) Determine Optimal Costs and Performance\nContent:\nTo achieve the optimal balance between performance and cost, start with an X-SMALL warehouse and gradually scale it up until the query duration stops halving. This indicates that the warehouse resources are fully utilized and helps you identify the sweet spot of maximum performance at the lowest cost. Also, analyze the historical usage patterns of your Snowflake warehouse to identify any usage patterns or spikes in resource utilization that may indicate an incorrectly sized warehouse. By doing so, you can adjust the Snowflake warehouse size to align with your workload demands and ensure optimal performance.\n ... \nSection Title: ... > 8) Using Snowflake Observability Tools (Chaos Genius)\nContent:\nSnowflake’s Resource monitoring is an important step in reducing Snowflake costs, but it may not provide the level of detail needed to make an informed decision. That’s exactly where Snowflake observability tools like [Chaos Genius](https://www.chaosgenius.io/) come into play!!\nChaos Genius uses advanced analytics and machine learning to monitor Snowflake virtual warehouse in real-time, providing automated recommendations to improve warehouse utilization and recommendations on how to right-size them. The tool also offers a dashboard with detailed metrics and insights to quickly identify any potential issues, enabling users to optimize the Snowflake usage, reduce Snowflake costs and improve the overall Snowflake performance.\n[Chaos Genius](https://www.chaosgenius.io/) dashboard\nDon’t miss the opportunity to reduce Snowflake costs and transform your business. Schedule a demo with us right now!\n ... \nSection Title: 8 best practices for choosing right Snowflake warehouse sizes (2026) > Conclusion\nContent:\nStart with a small Snowflake warehouse size and gradually scale up to find the sweet spot of maximum performance at the lowest cost.\nAutomate warehouse suspension and resumption to manage costs and optimize performance.\nCheck if Your Snowflake Warehouse is under or over Provisioned\nMonitor Disk spillage\nDetermine the optimal costs and performance based on workload and usage patterns.\nReview Snowflake Query History for errors related to warehouse size.\nUse Snowflake observability tools like Chaos Genius.\nSo, by following the steps outlined in this article and carefully considering Snowflake warehouse sizes, you can optimize your Snowflake virtual warehouse for maximum cost-effectiveness and Optimal query performance.\nSection Title: 8 best practices for choosing right Snowflake warehouse sizes (2026) > Conclusion\nContent:\nUltimately, it’s all about finding the Goldilocks solution—not too big, not too small, but just right. So, take the time to get to know your data and query usage patterns and don’t be afraid to experiment a little.\nSection Title: 8 best practices for choosing right Snowflake warehouse sizes (2026) > FAQs\nContent:\n**How many compute nodes are there in each Snowflake warehouse size?**\nSnowflake warehouses are composed of compute nodes. The X-Small warehouse has 1 node, the Small warehouse has 2 nodes, the Medium warehouse has 4 nodes and so on. Each node has 8 cores/threads, irrespective of the cloud provider.\n**Can a Snowflake warehouse run multiple queries simultaneously?**\nYes, a Snowflake warehouse can handle multiple queries concurrently.\n**How can I determine the best Snowflake warehouse size for my query?**\nStart with the X-Small warehouse and increase the size until the query duration no longer halves, indicating that the warehouse is not fully utilized. To find the best cost-to-performance ratio, choose a warehouse size one step smaller than the maximum.\n**What virtual warehouse sizes are available in Snowflake?**\nSection Title: 8 best practices for choosing right Snowflake warehouse sizes (2026) > FAQs\nContent:\nSnowflake offers virtual warehouses in various sizes, ranging from X-Small to 6X-Large. Each size represents a doubling of resources and credit consumption compared to the previous size.\n**How should I approach finding the right Snowflake warehouse size?**\nExperiment and iterate by starting small and gradually scaling up. Regularly test and monitor performance indicators to identify the warehouse size that delivers the best balance between cost and performance.\nRelated posts:\nSection Title: 8 best practices for choosing right Snowflake warehouse sizes (2026) > FAQs\nContent:\n[Navigating the SaaS security maze: Tips to protect your business](https://www.flexera.com/blog/saas-management/navigating-the-saas-security-maze-tips-to-protect-your-business/ \"Navigating the SaaS security maze: Tips to protect your business\")\n[Governing the SaaS explosion: How to establish control and minimize risk](https://www.flexera.com/blog/saas-management/governing-the-saas-explosion-how-to-establish-control-and-minimize-risk/ \"Governing the SaaS explosion: How to establish control and minimize risk\")\n[Now available: GraphQL Query Generator (Preview Release)](https://www.flexera.com/blog/it-visibility/now-available-graphql-query-generator-preview-release/ \"Now available: GraphQL Query Generator (Preview Release)\")\n[The practical FinOps roadmap series: What to do before you start practicing FinOps\nSection Title: 8 best practices for choosing right Snowflake warehouse sizes (2026) > FAQs\nContent:\n(1/4)](https://www.flexera.com/blog/finops/the-practical-finops-roadmap-series-what-to-do-before-you-start-practicing-finops-1-4/ \"The practical FinOps roadmap series: What to do before you start practicing FinOps (1/4)\")\n[What’s new at Flexera: August 2025](https://www.flexera.com/blog/product/whats-new-at-flexera-august-2025/ \"What’s new at Flexera: August 2025\")\n[Kubernetes pods vs containers: 4 key differences and how they work together](https://www.flexera.com/blog/finops/kubernetes-architecture-kubernetes-pods-vs-containers-4-key-differences-and-how-they-work-together/ \"Kubernetes pods vs containers: 4 key differences and how they work together\")\nSection Title: 8 best practices for choosing right Snowflake warehouse sizes (2026) > FAQs > Want to know more?\nContent:\nTechnology is evolving rapidly—and it's important to stay on top of the latest trends and critical insights. Check out the latest blogs related to FinOps below.\nFinOps\n ... \nSection Title: ... > [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Req...\nContent:\nFebruary 22, 2023\nFinOps\n ... \nSection Title: ... > [Agentic FinOps for AI: autonomous optimization for Snowflake, Databricks and AI cloud cost...\nContent:\nFebruary 12, 2026\nFinOps"]},{"url":"https://motherduck.com/learn-more/reduce-snowflake-costs-duckdb/","title":"Why Your Snowflake Bill is High and How to Fix It with a Hybrid Approach - MotherDuck","publish_date":"2026-02-23","excerpts":["BACK TO LEARN\nSection Title: Why Your Snowflake Bill is High and How to Fix It with a Hybrid Approach\nContent:\n17 min read BY Manveer Chawla\nYour Snowflake bill is high primarily because of its compute billing model, which enforces a [60-second minimum charge](https://docs.snowflake.com/en/user-guide/cost-understanding-compute) each time a warehouse resumes. This creates a significant \"idle tax\" on the frequent, short-running queries common in BI dashboards and ad-hoc analysis. You're often paying for compute you don't actually use.\nA surprisingly high bill for a modest amount of data is frustrating. We see it all the time. The immediate question is, \"Why is my bill so high when my data isn't that big?\" The cost isn't driven by data at rest, it's driven by data in motion, specifically by compute patterns. For many modern analytical workflows, the bill inflates from thousands of frequent queries accumulating disproportionately high compute charges.\nSection Title: Why Your Snowflake Bill is High and How to Fix It with a Hybrid Approach\nContent:\nIf you don't address this, you'll face budget overruns, throttled innovation, or pressure to undertake a costly and risky platform migration. The solution isn't always abandoning a powerful platform like Snowflake. You can augment it intelligently instead.\nThis guide provides a practical playbook for understanding the root causes of high Snowflake costs and a strategy for reducing them using internal optimizations and a [modern hybrid architecture](https://motherduck.com/docs/concepts/architecture-and-capabilities/) .\n ... \nSection Title: ... > The Real Reason Your Snowflake Bill is So High\nContent:\nWhen a BI dashboard executes 20 quick queries upon loading, each taking three seconds, this single page view could trigger 1,200 seconds (20 minutes) of billed compute time. The actual work took only one minute.\nThis problem gets worse with warehouse sizing. Each incremental size increase in a Snowflake warehouse [doubles its credit consumption rate](https://docs.snowflake.com/en/user-guide/cost-understanding-compute) . We often see teams defaulting to 'Medium' or 'Large' warehouses for all tasks. That creates a 4x to 8x cost premium for workloads that could easily run on an 'X-Small' warehouse.\n ... \nSection Title: ... > The Real Reason Your Snowflake Bill is So High\nContent:\n| Warehouse Size | Credits per Hour | Relative Cost |\n| X-Small | 1 | 1x |\n| Small | 2 | 2x |\n| Medium | 4 | 4x |\n| Large | 8 | 8x |\n| X-Large | 16 | 16x |\n ... \nSection Title: ... > 1. Master Warehouse Management (Set AUTO_SUSPEND to 60s)\nContent:\nSet aggressive yet intelligent warehouse timeouts. For most workloads, set the [`AUTO_SUSPEND` parameter](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) to exactly 60 seconds. This ensures the warehouse suspends after one minute of inactivity, stopping credit consumption. Setting it lower than 60 seconds is counterproductive. A new query arriving within that first minute could trigger a second 60-second minimum charge.\nRight-size warehouses by defaulting to smaller configurations. Use 'X-Small' warehouses by default and only scale up when a specific workload fails to meet its performance SLA. Consolidate workloads onto fewer, appropriately sized warehouses to prevent warehouse sprawl. Multiple underutilized compute clusters add up on your bill.\nSection Title: ... > 1. Master Warehouse Management (Set AUTO_SUSPEND to 60s)\nContent:\nWe helped one analytics team save approximately $38,000 annually by moving its BI queries from a Medium to a Small warehouse. They accepted a marginal 4-second increase in query time.\nSection Title: ... > 2. Leverage Snowflake's Caching Layers (Result & Warehouse)\nContent:\nSnowflake's multi-layered cache is one of its most powerful cost-saving features. Not using it leaves money on the table.\n**Result Cache:** If you run the exact same query as one run previously (by anyone in the account) and the underlying data hasn't changed, Snowflake returns the results instantly from a global result cache. No warehouse starts. That's free compute. It's especially effective for BI dashboards where multiple users view the same default state.\n**Warehouse Cache (Local Disk Cache):** When a query runs, the required data from storage gets cached on the SSDs of the active virtual warehouse. Subsequent queries that need the same data read it from this much faster local cache instead of remote storage. This dramatically speeds up queries and reduces I/O. Keeping a warehouse warm for related analytical queries can be beneficial.\nDesign workloads to maximize cache hits through consistent query patterns.\nSection Title: ... > 3. Optimize Inefficient Queries (Prune Partitions & Avoid SELECT *)\nContent:\nPoorly written queries burn credits unnecessarily. While comprehensive query tuning is a deep topic, these practices provide immediate savings:\n**Avoid `SELECT *` :** Select only the columns you need. This reduces the amount of data processed and moved, improving caching and query performance.\n**Filter Early and Prune Partitions:** Apply `WHERE` clauses that filter on a table's clustering key as early as possible. This lets Snowflake prune massive amounts of data from being scanned. It's the single most effective way to speed up queries on large tables.\n**Use `QUALIFY` for Complex Window Function Filtering:** Instead of using a subquery or CTE to filter window function results, use the [`QUALIFY` clause](https://docs.snowflake.com/en/sql-reference/constructs/qualify) . It's more readable and often more performant.\n ... \nSection Title: ... > Go Local: Slashing Dev & Test Costs with DuckDB\nContent:\nThis workflow saves money and dramatically improves developer velocity. You shorten the feedback loop from minutes (waiting for a cloud warehouse to provision and run) to seconds.\nA typical local development pattern in Python is straightforward. You can prototype rapidly without any cloud interaction.\n```\nimport  duckdb\n import  pandas  as  pd\n\n # Analyze a local Parquet file instantly # No cloud warehouse, no compute credits consumed \ndf = duckdb.sql( \"\"\"\n    SELECT\n        product_category,\n        COUNT(DISTINCT order_id) as total_orders,\n        AVG(order_value) as average_value\n    FROM 'local_ecommerce_data.parquet'\n    WHERE order_date >= '2024-01-01'\n    GROUP BY ALL\n    ORDER BY total_orders DESC;\n\"\"\" ).df()\n print (df)\n```\nRunning analytics locally is powerful for development. For sharing insights and powering production dashboards, this local-first approach extends into a hybrid architecture.\nSection Title: ... > The Hybrid Solution: MotherDuck for Cost-Effective Interactive Analytics\nContent:\nMotherDuck is a serverless data warehouse as a service built on DuckDB. It provides a simpler, [Scale-up solution](https://motherduck.com/learn-more/select-olap-solution-postgres) for workloads that are inefficient on traditional distributed data warehouses. It directly solves the idle tax problem by replacing the provisioned warehouse model with per-query, usage-based compute that bills in [one-second increments](https://motherduck.com/product/pricing/) .\nThis billing model profoundly impacts the cost of interactive analytics. Let's quantify the savings with a realistic scenario.\n ... \nSection Title: ... > Breaking Down the Costs: A Tale of Two Queries\nContent:\nEven assuming a more complex query consuming 4 Compute Units, the cost would only be **$23.48** . This example shows how a usage-based model eliminates waste for bursty workloads, reducing costs by over 95% in this scenario.\nThis calculation focuses on compute cost, the primary driver. While negligible for this interactive pattern, a full TCO analysis would include data storage and egress, where MotherDuck also offers competitive pricing.\nMotherDuck's architecture introduces [**\"dual execution.\"**](https://motherduck.com/docs/concepts/architecture-and-capabilities/) Its query planner intelligently splits work between the local DuckDB client and the MotherDuck cloud service. This minimizes data transfer and latency by performing filters and aggregations locally before sending smaller, pre-processed datasets to the cloud. This hybrid model works ideal for interactive analytics, BI dashboards, and ad-hoc queries on sub-terabyte hot data.\n ... \nSection Title: ... > A Framework for Workload Triage\nContent:\n**Low Execution Time, High Frequency:** Short, bursty queries that run often.\n**Low Execution Time, Low Frequency:** Quick, sporadic, ad-hoc queries.\n**High Execution Time, Low Frequency:** Long-running, scheduled batch jobs.\n**High Execution Time, High Frequency:** Often an anti-pattern indicating a need for data modeling or architectural redesign. It can occur in complex, near-real-time operational analytics.\nYou can analyze Snowflake's [`query_history`](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) using SQL to categorize your workloads. This query provides a starting point. We use `MEDIAN` instead of `AVG` for execution time because it's more robust to outliers and gives a better sense of typical query duration.\n ... \nSection Title: ... > A Framework for Workload Triage\nContent:\n**Interactive BI / High Frequency (Short & Bursty):** Prime candidates for migration to **MotherDuck** . The per-second, usage-based billing model eliminates the idle tax, offering dramatic cost savings for dashboards and [embedded analytics](https://motherduck.com/learn-more/customer-facing-analytics-database/) . **Ad-Hoc Exploration (Short & Sporadic):** This category fits well with **MotherDuck** or local **DuckDB** . For queries on smaller datasets or local files, DuckDB provides instant, free execution. For shared datasets, MotherDuck offers a cost-effective cloud backend. **Batch ETL / Heavy Analytics (Long & Scheduled):** These large, resource-intensive jobs often work best on **Snowflake** . Its provisioned warehouses provide predictable performance for multi-terabyte transformations. Its mature ecosystem simplifies complex data pipelines.\n ... \nSection Title: ... > When the Hybrid Approach Isn't the Right Fit: Sticking with Snowflake\nContent:\nTo build an effective architecture, you need to know a tool's limitations. The hybrid approach isn't a universal solution. Certain workloads are best suited for a mature, large-scale data warehouse like Snowflake. Acknowledging this builds trust and leads to better technical decisions.\n**Massive Batch ETL/ELT:** For scheduled jobs processing many terabytes of data, Snowflake's provisioned compute model provides predictable power and performance. The 60-second minimum doesn't matter for jobs that run for hours.\n**[Enterprise-Grade Governance and Security](https://trust.snowflake.com/) :** Organizations with complex data masking requirements, deep Active Directory integrations, or strict regional data residency rules often rely on Snowflake's mature and comprehensive features.\nSection Title: ... > When the Hybrid Approach Isn't the Right Fit: Sticking with Snowflake\nContent:\n**Highly Optimized, Long-Running Workloads:** If you have a workload that already runs consistently on a warehouse and maximizes its uptime (like a data science cluster running for 8 hours straight), the idle tax isn't a problem. There's little cost benefit to moving it.\nThe goal of a hybrid architecture is using the right tool for the right job, not replacing a tool that's already performing efficiently.\n ... \nSection Title: ... > The Modern Alternatives Landscape: Where Does MotherDuck Fit?\nContent:\nWithin classic cloud data warehouses, [Google BigQuery](https://cloud.google.com/bigquery/) presents a different pricing model. Its on-demand, per-terabyte-scanned pricing can be cost-effective for sporadic forensic queries. But it carries the risk of a runaway query where a single mistake leads to a massive bill.\nMotherDuck carves a unique niche. It combines the serverless simplicity of BigQuery with the efficiency of a local-first workflow powered by DuckDB. This makes it highly cost-effective and productive for teams focused on speed, iteration, and interactive analytics. You don't get the cost penalty of a traditional warehouse or the operational complexity of a data lake.\nSection Title: ... > The Modern Alternatives Landscape: Where Does MotherDuck Fit?\nContent:\n| Workload Type | Recommended Primary Tool | Rationale |\n| **Local Dev/Testing** | DuckDB | Eliminates cloud compute cost for non-production work. |\n| **Interactive Dashboards (<5TB)** | MotherDuck | Per-second billing avoids idle tax on bursty query patterns. |\n| **Large Batch ETL (>10TB)** | Snowflake | Predictable performance and mature features for heavy jobs. |\n| **Real-Time Observability (High QPS)** | ClickHouse | Optimized architecture for sub-second latency at high concurrency. |\n| **Sporadic Forensic Queries** | BigQuery (On-Demand) / MotherDuck | Pay-per-use model is efficient for unpredictable, infrequent queries. |\nSection Title: ... > Conclusion and Path Forward\nContent:\nThe path to a more efficient and cost-effective analytics stack doesn't require abandoning existing investments. You augment them intelligently. By adopting a three-tiered strategy, organizations gain control over their cloud data warehouse spending while empowering teams with better tools.\nThe strategy is simple:\n**Tune:** Implement Snowflake-native optimizations like 60-second auto-suspend timers, right-sized warehouses, and resource monitors to immediately reduce waste.\n**Go Local:** Shift all development and testing workloads to a local-first workflow with DuckDB. This eliminates an entire category of cloud compute spend.\n**Go Hybrid:** Use the workload triage framework to identify bursty, interactive workloads. Offload them to MotherDuck, replacing the idle tax with fair, usage-based billing.\n ... \nSection Title: ... > How does a hybrid local-cloud analytics model optimize costs?\nContent:\nIt shifts the bulk of analytics work—especially development and ad-hoc queries—from an expensive, minute-metered cloud warehouse to your local machine, where it's free. You only use the serverless cloud backend (like MotherDuck) for collaboration or larger queries, paying only for the actual seconds of compute used.\nSection Title: ... > How can we optimize Snowflake resource allocation without hurting performance?\nContent:\nStart by right-sizing warehouses (default to XS/S), setting aggressive auto-suspend policies (30-120 seconds), and consolidating workloads. For a bigger impact, offload development and BI workloads to a hybrid DuckDB/MotherDuck architecture to isolate and reduce the most inefficient costs.\n ... \nSection Title: Why Your Snowflake Bill is High and How to Fix It with a Hybrid Approach > Additional Resources\nContent:\n[Blog The Data Warehouse TCO: A Guide to the True Costs of Snowflake, BigQuery, and Redshift](https://motherduck.com/learn-more/data-warehouse-tco/) [Blog MotherDuck: A Faster, Cost-Effective BigQuery Alternative](https://motherduck.com/learn-more/bigquery-alternative-motherduck/)"]},{"url":"https://ternary.app/blog/snowflake-cost-optimization/","title":"Snowflake cost optimization: 8 proven strategies for reducing costs - Ternary","publish_date":"2025-09-22","excerpts":["Section Title: Snowflake cost optimization: 8 proven strategies for reducing costs\nContent:\n**Last updated:** September 22, 2025\nTernary Team\nSnowflake has quickly become a favorite as a data warehousing platform, and for good reasons. This makes many teams jump in, only to realize later that their [Snowflake spend](https://ternary.app/blog/manage-your-snowflake-costs/) is climbing faster than expected.\nThis is because managing Snowflake costs often becomes a sticking point despite the platform being impressive from a tech POV. That’s where Snowflake cost optimization comes in.\nIn this guide, we’ll explore how Snowflake pricing works and how you can stay in control without giving up performance or flexibility.\nSection Title: ... > Snowflake cost components\nContent:\nSnowflake’s architecture has 3 layers:\nStorage\nCompute (which Snowflake calls virtual warehouses)\nCloud services\nThese layers are billed separately, and Snowflake pricing is usage-based, meaning you get charged for what you actually use.\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Compute layer\nContent:\nVirtual warehouses are basically compute clusters that run your queries and handle your data loads. They scale independently, and you can have more than one running at a time. The key thing to know here: compute is paid using Snowflake credits. You spin up a warehouse, and you start spending credits. You pause it, it stops costing you. Sounds fair, but it also means you’ve got to stay on top of usage.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Cloud services layer\nContent:\nThis layer handles all the coordination across the platform, such as authentication, metadata management, and query optimization. It also runs on Snowflake credits, but the cost here is usually a smaller percentage compared to compute. Still, it adds up if you’ve got a lot going on, especially with serverless features in play.\nSpeaking of credits, let’s clarify this: a Snowflake credit is a unit that measures usage.\nOne credit = one unit of usage. Simple. You’re charged credits whenever you’re running a virtual warehouse, leveraging cloud services, or tapping into Snowflake’s serverless features.\nOne more component that Snowflake charges is data transfer between cloud regions or providers. This applies if you’re using features like external tables or exporting data from Snowflake to a data lake.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Cloud services layer\nContent:\nThese Snowflake cost components vary depending on whether you’re on Amazon Web Services (AWS), Microsoft Azure, or Google Cloud (GCP), and the pricing structure for that is a bit more granular.\nLook at the tables below for Snowflake data transfer charges for AWS, Azure, and GCP:\n[AWS pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ] [Azure pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ] [GCP pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ]\nSection Title: ... > An example of how Snowflake calculates cost\nContent:\n**Note:** This example is courtesy of Snowflake.\nSuppose we have a customer using Snowflake Capacity Standard Service with Premier Support in the U.S.\nThey do 3 main things:\nLoad data nightly using a small virtual warehouse.\nSupport 8 users working 10 hours a day, 5 days a week, using a medium virtual warehouse.\nStore 4 TB of compressed data on Snowflake.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Data loading costs\nContent:\n| **Warehouse used** | Small Standard Virtual Warehouse |\n| **Rate** | 2 credits per hour |\n| **Usage** | 2.5 hours daily for 31 days/month |\n| **Monthly Credits** | 2 credits/hour × 2.5 hours/day × 31 days = 155 credits/month |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > User activity costs\nContent:\n| **Users** | 8 users |\n| **Warehouse used** | Medium Standard Virtual Warehouse |\n| **Rate** | 4 credits per hour |\n| **Usage** | 10 hours/day, 20 workdays/month |\n| **Monthly credits for users** | 4 credits/hour × 10 hours/day × 20 days = 800 credits/month |\n| **Total monthly credits (users + loading)** | 800 + 155 = 955 credits/month |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Storage costs\nContent:\n| **Data stored** | 4TB compressed |\n| **Rate** | $23 per TB/month |\n| **Annual storage cost** | 4 TB × $23 × 12 months = $1,104/year |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Virtual warehouse cost\nContent:\n| **Credits used per year** | 955 credits/month × 12 = 11,460 credits/year |\n| **Rate per credit** | $2 (with 5% discount: × 0.95) |\n| **Annual compute cost** | 11,460 × $2 × 0.95 = $21,774/year |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Total annual cost\nContent:\n| **Storage** | $1,104 |\n| **Virtual warehouse** | $21,774 |\n| **Grand Total** | $22,878 per year |\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\nWhile automatic clustering can improve query performance, it runs on serverless compute. This means it racks up Snowflake credits whether anyone’s actually using the table or not.\nIf the table is only getting hit a few times a week, that background compute activity is just silently chipping away at your budget. This is where smart Snowflake cost optimization begins.\nLook for tables with automatic clustering enabled that barely get queried, say, fewer than 100 times per week. Ask yourself if these tables are part of a disaster recovery setup or being shared with another account. If not, it’s probably safe to hit pause.\nTo suspend automatic clustering, run:\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\n| **ALTER** **TABLE** your_table_name **SUSPEND** RECLUSTER; |\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\nThis one step alone can help reduce Snowflake costs tied to unnecessary background compute.\n ... \nSection Title: ... > 4. Clean out large tables that haven’t been touched in a week\nContent:\nThe massive tables that sit there eating up storage and haven’t been queried at all in the past week not only inflate your Snowflake storage costs, but they also clutter up your environment and slow down everything from data discovery to data warehouse optimization and management.\nIf a table isn’t serving any purpose (besides reminding you of a project from six months ago), drop it. But again, always check if it’s being used for recovery or data sharing before swinging the axe.\nTo delete a table, run:\n ... \nSection Title: ... > 6. Allow multi-cluster warehouses to scale down\nContent:\nMulti-cluster warehouses can be incredibly powerful, especially when you’ve got a high volume of concurrent queries.\nBut if you’ve locked the cluster count at a fixed number, say, 3 minimum and 3 maximum, you’re forcing Snowflake to keep all clusters running at all times, even when demand doesn’t justify it.\nThat’s wasted compute and wasted credits. Snowflake is built to scale, so let it.\nLower the minimum cluster count so the warehouse can scale down during slower periods. It won’t affect performance during peak hours, but it’ll quietly reduce credit consumption when traffic drops.\nTo adjust the scaling behavior, run:\nSection Title: ... > 6. Allow multi-cluster warehouses to scale down\nContent:\n| ALTER WAREHOUSE your_warehouse_name **SET** MIN_CLUSTER_COUNT = 1; |\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > What is a KPI in Snowflake?\nContent:\nFor Snowflake cost optimization, KPIs are your best friend.\nSnowflake offers a bunch of performance metrics that, when tracked together, paint a full picture. These include basically anything that has a noticeable impact on credit usage, query speed, or system efficiency.\nSection Title: ... > Snowflake performance index (SPI)\nContent:\n[Snowflake Performance Index (SPI)](https://www.snowflake.com/en/pricing-options/performance-index/) is a macro-level view of how much performance has improved over time across typical customer workloads.\nIt tracks millions of jobs every month to give a reliable baseline for measuring how well Snowflake is optimizing things under the hood.\nThis tracking gradually surfaces improvements like query improvements, data ingestion speed, replication efficiency, and more.\nThe best part is that many of these performance gains happen automatically, so you benefit without needing to change your code or reconfigure anything.\n ... \nSection Title: ... > Decide how much automation you want\nContent:\nSome tools give you suggestions. Others take action like auto-suspending idle warehouses, resizing compute, or flagging inefficient queries in real time.\nAsk yourself: are you comfortable letting the tool make changes, or do you prefer having the final say? The answer will help you zero in on a solution that fits your workflow.\nSection Title: ... > Make sure the tool supports granular cost allocation\nContent:\nYou want to be able to [break down costs](https://ternary.app/blog/cloud-cost-analysis/) by user, warehouse, role, or even by specific workloads.\nThe more detail you have, the easier it is to hold teams accountable and find areas where spend can be trimmed.\nSection Title: ... > Check for query performance tuning support\nContent:\nLook for tools that not only track slow or costly queries but also help you understand why they’re inefficient, whether it’s due to joins, filters, or warehouse sizing.\nSection Title: ... > Prioritize customizable dashboards and reporting\nContent:\nEvery stakeholder needs different data. Finance might want a monthly credit burn summary, while engineering needs real-time warehouse spikes.\nThe tool should let you build dashboards and reports that speak to your team’s needs without having to export everything to spreadsheets every week.\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > Final thoughts\nContent:\nAt the end of the day, Snowflake cost optimization comes down to how much visibility you have.\nThe more visibility you have into your Snowflake usage, the better decisions you can make to keep costs in check.\nThat’s exactly where [Ternary](https://ternary.app/blog/manage-your-snowflake-costs/) can help.\nTernary gives your team the insights they need to manage Snowflake spend with confidence.\n**Get a clearer view of your Snowflake costs.**\n[Request a demo](https://ternary.app/demo/)\n ... \nSection Title: ... > What is the biggest contributor to Snowflake costs?\nContent:\nCompute is usually the biggest cost driver in Snowflake. Virtual warehouses charge based on per-second usage, and costs vary with warehouse size and workload."]},{"url":"https://blog.altimate.ai/does-size-matter-what-else-is-important-in-choosing-a-snowflake-warehouse","title":"Choosing the Right Snowflake Warehouse: It's not Rocket Science","publish_date":"2024-11-02","excerpts":["Section Title: Does Size Matter? What else is important in choosing a Snowflake Warehouse?\nContent:\nChoosing the Best Warehouse Size in Snowflake\nUpdated November 2, 2024\n•\n12 min read\n[J](https://hashnode.com/@John-ryan-uk)\n[John Ryan](https://hashnode.com/@John-ryan-uk)\nAfter 30 years of experience building multi-terabyte data warehouse systems, I spent five incredible years at Snowflake as a Senior Solution Architect, helping customers across Europe and the Middle East deliver lightning-fast insights from their data.\nIn 2023, I joined Altimate.AI, which uses generative artificial intelligence to provide Snowflake performance and cost optimization insights and maximize customer return on investment.\nOn this page\n ... \nSection Title: Does Size Matter? What else is important in choosing a Snowflake Warehouse? > TL;DR Take-Aways\nContent:\n**Size Isn't everything:** Warehouse Size is not that important. Get over it.\n**Don't Scale Up:** Although remarkably easy, it's a great way to burn through cash (and eventually lose your job).\n**Warehouse Count Matters:** If you have more than 20 warehouses - you're probably wasting hundreds of thousands of dollars.\n**Never give a user what they want:** You need to find out what they need!\n ... \nSection Title: ... > Separate Workloads to Avoid Contention\nContent:\n**Data Loading:** Using COPY commands to load data into Snowflake. [Data Loading](https://articles.analytics.today/best-practices-for-using-bulk-copy-to-load-data-into-snowflake) tends to favor an `XSMALL` warehouse as each file loads on a single CPU and in most cases cannot benefit from scaling up the warehouse. Equally, a second dedicated warehouse for [large data loads](https://articles.analytics.today/best-practices-for-using-bulk-copy-to-load-data-into-snowflake) is also recommended where sensible. **Huge Batch Transformations:** Which tend to process large data volumes and require more complex SQL operations including sort operations. These prioritize throughput over performance of individual queries, and tend to be submitted on a regular schedule. Finally, unlike end-user queries, individual query response time is not as critical as getting the overall job done as quickly as possible.\n ... \nSection Title: ... > Avoid Workload Separation by Team\nContent:\nThe diagram below illustrates a situation I've seen in almost every large Snowflake deployment I've worked on.\nThe above diagram shows how each team is given a set of warehouses, one for Sales, another for Marketing and yet another for Finance workloads. This separation is often driven by the need to chargeback warehouse costs to different teams, but it's both unnecessary and hugely inefficient.\nThe challenges with this deployment include:\nSection Title: ... > Avoid Workload Separation by Team\nContent:\n**Poor Infrastructure Management:** As each team manages it's own warehouses, the size, configuration and infrastructure management is distributed across the entire business. As it's not feasible for each team to have a Snowflake expert, it often leads to wasteful deployment and poor throughput and query performance.\n**Workload Contention:** Data Engineering teams focus on code and delivery schedules and often lack the skills and experience to manage conflicting workloads. This leads to the same machine being used for mixed purposes including end-user queries and batch processing on the same warehouse.\n**Significant Waste:** It's not unusual to find over a hundred virtual warehouses with multiple same-size machines executing similar workloads. This leads to significant waste as there's often many warehouses running at low utilization with similar workloads executed by different teams.\nSection Title: ... > Focus on the Workload not Warehouse Size\nContent:\nThe most common question I'm asked by Data Engineers is:\n*\"What warehouse size should I run this query on\"* .\nI would describe this sort of thinking as: *\"Looking down the wrong end of a telescope\"* . There are several critical mistakes implied by the above question:\n ... \nSection Title: ... > Focus on the Workload not Warehouse Size\nContent:\n**Ignoring Configuration Options:** [Virtual Warehouses](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse) can be configured with an `AUTO_SUSPEND` time, `WAREHOUSE TYPE` (memory size), `CLUSTER_COUNT` and `SCALING POLICY` in addition to warehouse size. You should be aware of these options when deciding [which warehouse to use](https://articles.analytics.today/snowflake-virtual-warehouses-what-you-need-to-know) . **Ignoring Performance Options:** Snowflake provides several performance tuning options including, [Query Acceleration Service](https://articles.analytics.today/how-snowflake-query-acceleration-service-boosts-performance) , [Cluster Keys](https://articles.analytics.today/snowflake-cluster-keys-and-micro-partition-elimination-best-practices) and [Search Optimization Service](https://articles.analytics.today/best-practices-snowflake-search-optimisation-services) to help maximize query performance.\nSection Title: ... > Focus on the Workload not Warehouse Size\nContent:\nA focus on warehouse size ignores these options. **Ignoring the Cost:** The question about warehouse size implies the need to maximize query performance, but this hyper-focus on performance leads to ignoring query cost. The diagram below illustrates the real balance every Data Engineer needs to be aware of, balancing the conflicting requirements of Throughput, Performance and Cost.\nSection Title: ... > Focus on the Workload not Warehouse Size\nContent:\nThe biggest single difference between an on-premises databases and Snowflake is that on-premises systems have a fixed up-front cost and limited resources, whereas on Snowflake the cost depends upon usage and machine resources are unlimited.\nWe therefore need to be aware of all three potential priorities, and (for example, with over-night batch processing), if neither throughput nor performance is critical, we must focus on cost.\nSection Title: ... > Workload Frequency and Warehouse Size\nContent:\nAnother common warehouse deployment mistake I see is high frequency tasks on an inappropriate (too large) warehouse. This is often driven by the need to quickly deliver near real-time incremental results to end-users. It typically involves a short, repeatedly executed scheduled job which is run every few minutes, running 24x7 on a MEDIUM or LARGE warehouse.\nTo understand why this is a bad mistake, consider how Snowflake bills for warehouse time.\nWhen a warehouse is resumed, there is a minimum 60 second charge, and a minimum `AUTO_SUSPEND` of 60 seconds (although the default time is 10 minutes).\nEven assuming the minimum 60 second auto-suspend time, a job which runs every five minutes and executes for just two minutes has an actual charge of three minutes as a result of the `AUTO_SUSPEND` time.\nSection Title: ... > Workload Frequency and Warehouse Size\nContent:\nAny job executed every five minutes executes 105,120 times per year, and assuming $3.00 per credit costs $15,768. If however, the job were executed on a `MEDIUM` size warehouse, the costs increases to over $63,000.\nWorst still, since each query in the job is short and processes relatively small data volumes, there's little benefit in increasing warehouse size as the queries won't run much faster, but the cost of the `AUTO_SUSPEND` time has increased to over $21,000 - more than the original cost on an `XSMALL` warehouse.\nSection Title: ... > Avoid Resizing the Warehouse\nContent:\nSnowflake recommends experimenting with different warehouse sizes and because it's easy to resize, most customers starting with a SMALL virtual warehouse and then increase the size as performance demands.\nThe diagram above illustrates a common scenario whereby users start with a `SMALL` warehouse, then increase size to a `MEDIUM` and then `LARGE` . As additional workloads are added, they find query performance suffers, but they also discover that increasing warehouse size again to an `XLARGE` leads to a significant increase in cost, so the warehouse is resized back down again.\nAs a Snowflake Solution Architect and Instructor to hundreds of Snowflake Data Engineers, and I've consistently advised:\n\"Don't increase the Virtual Warehouse Size\"\nSection Title: ... > Avoid Resizing the Warehouse\nContent:\nTo understand why, consider the \"Workload Frequency\" section above. It's likely that any virtual warehouse will include a combination of workloads, from huge batch jobs run once per day, to short, fast jobs run every few minutes.\nAssuming a virtual warehouse suspends after 60 seconds, a job which runs for an hour spends just 1.6% of the time waiting to suspend, whereas a job which runs for two minutes spends an additional 50% of the time waiting to suspend.\nWhen you increase warehouse size, you double the cost per hour, but you're also doubling the cost of the idle time.\n ... \nSection Title: ... > Mixed Workload: A Case Study\nContent:\nWith 70% of queries completing in under five seconds on a warehouse with a small number of queries (just 1%), taking up to an hour to complete, this is clearly a warehouse with a mixed workload.\nQueries taking under 5 seconds are considerably more cost effective on a smaller warehouse. My personal benchmark tests demonstrate that queries taking around 5 seconds run around 30% faster when executed on a `MEDIUM` rather than `XSMALL` warehouse, but the charge rate is 400% higher.\nFurther investigation revealed that the warehouse had been increased to a `LARGE` size, but was quickly reversed as the cost increased significantly and the customer simply accepted the poor performance.\nIn conclusion:\nSection Title: ... > Mixed Workload: A Case Study\nContent:\n70% of the queries complete in under 5 seconds are running these on a MEDIUM size warehouse is hugely wasteful.\nIt's likely the warehouse size was increased to `MEDIUM` size to ensure the longer running queries (the 1% of queries taking 10-60 minutes) finished quickly.\nIt would be sensible to identify the longer running queries and move them to a `MEDIUM` size warehouse, and then resize the warehouse to a `SMALL` or `XSMALL` to save money.\nSection Title: ... > Snowflake Virtual Warehouse Sizing: What are we doing wrong?\nContent:\nLet's quickly summarize the mistakes being made:\n ... \nSection Title: ... > Virtual Warehouse Sizing: Best Practices\nContent:\n\"Never give a customer what they asks for! Give them what they NEED! And remember, it's your job to understand what they need\" - Dr David. Pearson\nSection Title: ... > Virtual Warehouse Sizing: Best Practices\nContent:\n**Understand System Requirements:** Be aware of the trade-off of Cost, Performance and Throughput. If you're aware of the the conflicting requirements you might deliver a solution that gives the best value to the business - and keep yourself in a job.\n**Understand the Business Requirements:** Keeping in mind the overall requirements, agree the need for both data refresh frequency and query performance. Make business users aware of the real cost of their request. For example a job executing every five minutes on an `XSMALL` warehouse costs over $15,000 per year whereas the same job executed once per hour costs just $1,300.\n**Deploy by Workload not Team:** The diagram below illustrates an optimum warehouse deployment whereby workloads deployed to reduce contention between workloads rather than by different teams.\nSection Title: ... > Virtual Warehouse Sizing: Best Practices\nContent:\nThe above diagram illustrates how a single `XSMALL` warehouse is used for data loading, while there's a range of different size transformation warehouses for ELT processing. Finally, users with similar workload sizes share warehouses while Data Scientists use a much larger dedicated warehouse based upon workload size and performance needs.\nSection Title: ... > Virtual Warehouse Sizing: Best Practices\nContent:\n**Focus on the JOB not the Query:** Around 80% of Snowflake compute costs are the result of scheduled jobs, and it's easy to identify the steps in a job using the [QUERY_TAG](https://docs.snowflake.com/en/sql-reference/sql/alter-session) . Using this technique, we can quickly summarize costs and execution time by JOB, and ensure each job is assigned to the correct warehouse size. It can also be used to quickly identify frequently executed short jobs on large warehouses. Moving these from a `MEDIUM` to an `XSMALL` will reduce costs by 75%. **Move the JOB instead of resizing the warehouse:** To avoid (or resolve) a mixed workload, having identified the poorly deployed jobs, move them to an appropriate warehouse size.\nSection Title: ... > Virtual Warehouse Sizing: Best Practices\nContent:\nThis means moving jobs that [spill to storage](https://articles.analytics.today/improve-snowflake-query-speed-by-preventing-spilling-to-storage) to a larger warehouse, while short, frequently executed jobs are moved to a smaller (less expensive) warehouse.\nSection Title: ... > Virtual Warehouse Sizing: Best Practices\nContent:\n**Understand Snowflake Performance Features:** In addition to a range of features to [tune query performance](https://articles.analytics.today/boost-your-snowflake-query-performance-with-these-10-tips) , Snowflake supports a number of [warehouse parameters](https://articles.analytics.today/snowflake-virtual-warehouses-what-you-need-to-know) , along with advanced features including [Cluster Keys](https://articles.analytics.today/snowflake-cluster-keys-and-micro-partition-elimination-best-practices) , [Query Acceleration Service](https://articles.analytics.today/how-snowflake-query-acceleration-service-boosts-performance) and [Search Optimization Service](https://articles.analytics.today/best-practices-snowflake-search-optimisation-services) to help improve query performance. While increasing warehouse size might improve performance, in many cases it won't help, in which case alternative techniques need to be considered.\nSection Title: ... > Virtual Warehouse Sizing: Best Practices\nContent:\n**Control Warehouse Deployment:** Avoid a sprawl of hundreds of virtual warehouses with inappropriate (and undocumented) configuration and mixed workloads. Provide a centrally coordinated centre of excellence to oversee warehouse deployment and [monitor key warehouse metrics](https://articles.analytics.today/monitor-snowflake-usage-cost) to maximize business value.\nSection Title: ... > Virtual Warehouse Sizing: Best Practices\nContent:\nDeliver more performance and cost savings with Altimate AI's cutting-edge AI teammates. These intelligent teammates come pre-loaded with the insights discussed in this article, enabling you to implement our recommendations across millions of queries and tables effortlessly. Ready to see it in action? Request a recorded demo by just sending a chat message ( [here](https://app.myaltimate.com/contactus) ) and discover how AI teammates can transform your data team\n[](https://app.myaltimate.com/contactus)\n\\ \\"]}],"usage":[{"name":"sku_search","count":1}]}
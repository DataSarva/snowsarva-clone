{"extract_id":"extract_21e194268ae14a29839253a3e68ac048","results":[{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization","publish_date":null,"excerpts":["Section Title: Cost Optimization > Recommendations\nContent:\n**Establish a consistent and granular cost attribution strategy:** Implement robust and organizationally consistent tagging and\nlabeling strategies across all resources (storage objects,\nwarehouses, accounts, queries) to accurately allocate costs to\nspecific teams, products, or initiatives. **Embed cost accountability into your organization's DNA:** Implement a feedback system to transparently show or charge back\ncloud costs to relevant business units or teams, increasing\naccountability. **Deliver clear, historical consumption insights:** Utilize\nconsistent in-tool visualizations or custom dashboards to monitor\nconsumption and contextualize spend on the platform with unit\neconomics. **Investigate anomalous consumption activity:** Review anomaly\ndetection to identify unforeseen cost anomalies and investigate\ncause and effect trends. **Control**\n**Proactively monitor all platform usage:** Define and enforce\nbudgets for projects and services, setting soft quotas to limit\nresource consumption and prevent runaway spending. **Forecast consumption based on business needs:** Establish a\nforecast process to project future spend needs based on business and\ntechnical needs. **Enforce cost guardrails for organizational resources:** Set up\nautomated checks (e.g., Tasks, query insights) and resource\nguardrails (e.g., warehouse timeout, storage policies, resource\nmonitors) to identify unusual usage patterns and potential\noverspending as they occur.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Recommendations\nContent:\nImplementing a robust FinOps visibility framework in Snowflake,\nsupported by cross-functional collaboration, enables each business\nfunction to access timely and relevant usage and cost data. This\nempowers them to understand the business impact of their consumption and\ntake prompt action when anomalies arise. To meet this vision, consider\nthe following recommendations based on industry best practices and\nSnowflake's capabilities:\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\nThe most mature FinOps customers are those who programmatically and\nstrategically drive consumption insights across the business. This\ninvolves three core elements:\n**Platform cost tracking:** Pinpoint specific Snowflake credit\nconsumption (compute, storage, serverless, AI, and data transfer),\nusage patterns, and efficiency opportunities to deconstruct credit\nusage, understand drivers, identify anomalies, and (eventually) drive\nforecasting operations.\n**Normalization of consumption:** Once consumption has been attributed\nand aggregated to meaningful levels, normalizing it against relevant\nbusiness and technical metrics contextualizes it in relation to\norganizational goals. It allows for the natural growth and seasonality\nof platform usage to be put into context with business and technical\ndemand drivers.\n**Clear reporting:** Presenting Snowflake cost data in an\nunderstandable format for various stakeholders is vital. This enables\nbudgeting, forecasting, KPIs, and business value metrics directly tied\nto Snowflake credit consumption.\n**Track usage data for all platform resources**\n ... \nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nCost Anomaly Detection is a critical component of visibility that\nleverages machine learning to continuously monitor credit consumption\nagainst historical spending patterns, automatically flagging significant\ndeviations from the established baseline. This proactive monitoring is\nessential for preventing budget overruns and identifying inefficiencies,\nshifting the organization from a reactive to a proactive cost management\nposture to mitigate financial risk. As a best practice, you should\ninitially review anomaly detection on the entire account to gain a broad\nview, then dive deeper into a more granular review for individual\nhigh-spend warehouses. [This approach](https://docs.snowflake.com/en/user-guide/cost-anomalies) allows for more targeted analysis and assigns clear ownership for\ninvestigating any flagged anomalies. There are several methods for\nanomaly detection supported by Snowflake:\n**Cost Anomalies in Snowsight**\nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nSnowsight, Snowflake's primary web interface, offers a dedicated Cost\nManagement UI that allows users to visually identify and analyze the\ndetails of any detected cost anomaly. The importance of this intuitive\nvisual interface lies in its ability to make complex cost data\naccessible to a wide range of stakeholders, enabling rapid root cause\nanalysis by correlating a cost spike with specific query history or user\nactivity. One of the tabs in this UI is the Cost Anomaly Detection tab,\nwhich enables you to view cost anomalies at the organization or account\nlevel and explore the top warehouses or accounts driving this change. To\nfoster a culture of cost awareness and accountability, it is a best\npractice to ensure there is an owner for an anomaly detected in the\naccount and set up a [notification (via email)](https://docs.snowflake.com/en/user-guide/cost-anomalies-ui) in the UI itself to ensure that cost anomalies are quickly and\naccurately investigated.\n**Programmatic Cost Anomaly Detection**\nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nFor deeper integration and automation, organizations can review [anomalies programmatically](https://docs.snowflake.com/en/user-guide/cost-anomalies-class) using the SQL functions and views available within the SNOWFLAKE.LOCAL\nschema. This approach is important for enabling automation and\nscalability, allowing cost governance to be embedded directly into\noperational workflows, such as feeding anomaly data into third-party\nobservability tools or triggering automated incident response playbooks.\nA key best practice is to utilize this programmatic access to build\ncustom reports and dashboards that align with specific financial\nreporting needs and to create advanced, automated alerting mechanisms\nthat pipe anomaly data into established operational channels, such as\nSlack or PagerDuty.\n**Custom Anomaly Detection & Notification**\nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nAlthough anomalies are detected at the account and organization level,\nif you desire to detect anomalies at lower levels (e.g. warehouse or\ntable), it is recommended to leverage Snowflake’s [Anomaly Detection](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) ML class and pair it with a Snowflake [alert](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) to notify owners of more granular anomalies that occur within the\necosystem. This ensures all levels of Snowflake cost can be monitored in\na proactive and effective way. As a best practice, [notifications](https://docs.snowflake.com/en/user-guide/notifications/about-notifications) should be configured for a targeted distribution list that includes the\nbudget owner, the FinOps team, and the technical lead responsible for\nthe associated Snowflake resources, ensuring all stakeholders are\nimmediately aware of a potential cost overrun and can coordinate a swift\nresponse.\n ... \nSection Title: Cost Optimization > Control > Overview > Proactively monitor all platform usage\nContent:\nEffective budget management relies on timely communication. Setting up\nalerting through emails or webhooks to collaboration tools like Slack\nand Microsoft Teams provides proactive [notification](https://docs.snowflake.com/en/user-guide/budgets/notifications) to key stakeholders when spending approaches or exceeds a defined\nthreshold. These alerts provide teams with an opportunity to review and\nadjust their usage before it leads to significant cost overruns. This\ncapability positions organizations for security success by mitigating\npotential threats through comprehensive monitoring and detection.\nNotifications are not limited to just budgets; [Snowflake alerts](https://docs.snowflake.com/en/user-guide/alerts) can also be\nconfigured to systematically notify administrators of unusual or costly\npatterns, such as those listed in the Control and Optimize sections of\nthe Cost Pillar. This ensures that key drivers of Snowflake consumption\ncan be tracked and remediated proactively, even as the platform’s usage\ngrows.\nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\nForecasting Snowflake consumption should be a strategic business\nfunction, not a mere technical prediction. The goal is to establish a\ntransparent basis for budgeting and optimizing ROI by linking\nconsumption directly to measurable business outcomes. In a dynamic,\nusage-based environment where compute costs are the most volatile\nelement of the bill, a robust framework must integrate quantitative\nanalysis of historical usage with qualitative insights into future\nbusiness drivers. The following framework outlines how to build and\nmaintain a comprehensive consumption forecast.\n**Establish the Baseline**\nThis phase focuses on understanding the source of spend and establishing\ngranular cost accountability.\n**Identify demand drivers and unit economics:** To understand what\ndrives Snowflake spend, correlate historical credit, storage, and data\ntransfer usage with key business metrics like cost per customer or per\ntransaction. Use Snowflake's [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) schema, including the WAREHOUSE_METERING_HISTORY and QUERY_HISTORY\nviews, as the primary data sources for this analysis.\n**Granular cost attribution:** Accurately tie costs back to business\nteams or workloads by implementing a mandatory tagging strategy for\nall warehouses and queries. Align these tags with your organization's\nfinancial structure to provide clear cost segmentation.\nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\n**Build the predictive model**\nThis phase integrates historical trends with strategic business inputs\nto create forward-looking projections.\n**Historical trend analysis:** Analyze past usage for trends,\nseasonality, and outliers to inform future projections. Start with\nsimple trend-based forecasting and progressively move to more\nsophisticated models, leveraging Snowflake’s built-in [SNOWFLAKE.ML.FORECAST function](https://docs.snowflake.com/en/user-guide/ml-functions/forecasting) for time-series forecasting.\n**Driver-based forecasting:** Integrate planned business initiatives\nand new projects directly into the model. Collaborate with business\nleaders to gather strategic inputs such as projected customer growth,\nnew product launches, or increased data ingestion from marketing\ncampaigns.\n**Scenario modeling:** Develop multiple forecast scenarios (e.g.,\n\"conservative,\" \"base case,\" \"aggressive\") by applying varied growth\nfactors to key business drivers. This enables flexible planning and\nhelps mitigate financial risk.\n**Operationalize and optimize**\nThis phase links the forecast to continuous monitoring, governance, and\nproactive cost controls.\nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\n**Continuous monitoring and variance analysis:** Regularly compare\nactual consumption against the forecast to identify and investigate\nsignificant variances. This feedback loop is crucial for refining the\nunderlying model and adapting to evolving business needs.\n**Collaborative governance:** Ensure a single source of truth for\nconsumption data by establishing a regular FinOps review session with\nFinance, Engineering, and Business teams. Use customized dashboards to\npresent data in business-friendly terms.\n**Implement predictive budget controls:** Shift from reactive spending\nto a proactive model. Utilize Snowflake Resource Monitors and Budgets,\nwhich employ monthly-level time-series forecasting, to define credit\nquotas and trigger automated alerts or suspensions to prevent cost\noverruns.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\n**Programmatically deconstruct queries for automated analysis**\nFor advanced use cases and automated monitoring, you can\nprogrammatically access query performance data. The [GET_QUERY_OPERATOR_STATS](https://docs.snowflake.com/en/sql-reference/functions/get_query_operator_stats) function can be used to retrieve the granular, operator-level statistics\nfor a given query ID, showing many of the steps and attributes available\nin the query profile view. This allows you to build automated checks\nthat, for instance, flag any query where a full table scan accounts for\nmore than 90% of the execution time or where data spillage exceeds a\ncertain threshold. This approach helps scale performance visibility\nbeyond manual checks.\n**Pipeline optimization**\nSnowflake pipeline optimization is about designing and managing data\ningestion and transformation processes that are efficient,\ncost-effective, scalable, and low-maintenance, while balancing business\nvalue and SLAs (service levels for freshness and responsiveness). Key\nlevers include architecture patterns (truncate & load versus incremental\nloads), use of serverless managed services (e.g., Snowpipe, Dynamic\nTables), and auditing loading practices to maximize cost and performance\nbenefits.\n**Batch loading**\n ... \nSection Title: Cost Optimization > Optimize > Overview > Improve continually\nContent:\nOptimization is a continuous process that ensures all workloads not only\ndrive maximum business value but also do so in an optimal manner. By\nregularly reviewing, analyzing, and refining your Snowflake environment,\nyou can identify inefficiencies, implement improvements, and adapt your\nplatform to the ever-evolving business needs. The following set of steps\nwill help you continue to improve your environment as you grow:\n**Step 1: Identify & investigate workloads to improve**\nBegin by regularly reviewing (usually on a weekly, bi-weekly, or monthly\ncadence) workloads that could benefit from optimization, using\nSnowflake's [Cost Insights](https://docs.snowflake.com/en/user-guide/cost-insights) ,\ndeviations in unit economics or health metrics (from the Visibility\nprinciple), or objects hitting control limits (e.g., queries hitting\nwarehouse timeouts from the Control principle). Once identified,\ninvestigate these findings through the Cost Management UI, Cost Anomaly\ndetection, Query History, or custom dashboards with Account Usage Views\nto pinpoint the root cause. Then, using the recommendations in the\nOptimize Pillar, make improvements to the workload or object.\n**Step 2: Estimate & test**"],"full_content":null},{"url":"https://www.snowflake.com/en/developers/guides/ml-forecasting-ad/","title":"Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting","publish_date":null,"excerpts":["Section Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting\nContent:\nInteractive Analytics\nHarsh Patel\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/ml-forecasting-ad)\nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > Overview\nContent:\nOne of the most critical activities that a Data/Business Analyst has to perform is to produce recommendations to their business stakeholders based upon the insights they have gleaned from their data. In practice, this means that they are often required to build models to: make forecasts, identify long running trends, and identify abnormalities within their data. However, Analysts are often impeded from creating the best models possible due to the depth of statistical and machine learning knowledge required to implement them in practice. Further, python or other programming frameworks may be unfamiliar to Analysts who write SQL, and the nuances of fine-tuning a model may require expert knowledge that may be out of reach.\nFor these use cases, Snowflake has developed a set of SQL based ML Functions, that implement machine learning models on the user's behalf. As of December 2023, three ML Functions are available for time-series based data:\nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > Overview\nContent:\nForecasting: which enables users to forecast a metric based on past values. Common use-cases for forecasting including predicting future sales, demand for particular sku's of an item, or volume of traffic into a website over a period of time.\nAnomaly Detection: which flags anomalous values using both unsupervised and supervised learning methods. This may be useful in use-cases where you want to identify spikes in your cloud spend, identifying abnormal data points in logs, and more.\nContribution Explorer: which enables users to perform root cause analysis to determine the most significant drivers to a particular metric of interest.\nFor further details on ML Functions, please refer to the [snowflake documentation](https://docs.snowflake.com/guides-overview-analysis) .\n ... \nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > Overview > What You’ll Learn\nContent:\nHow to make use of Anomaly Detection & Forecasting ML Functions to create models and produce predictions\nUse Tasks to retrain models on a regular cadence\nUse the [email notfication integration](https://docs.snowflake.com/en/user-guide/email-stored-procedures) to send email reports of the model results after completion\naside positive\nNote: You can now run this entire quickstart as a Snowflake Notebook in your Snowsight UI. Download this [ipynb file](https://github.com/Snowflake-Labs/snowflake-demo-notebooks/blob/main/Getting%20Started%20with%20Snowflake%20Cortex%20ML-Based%20Functions/Getting%20Started%20with%20Snowflake%20Cortex%20ML-Based%20Functions.ipynb) and import it to your Snowflake account as shown below.\n ... \nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > ... > Step 3: Feature Importance & Evaluation Metrics\nContent:\nAn important part of the model building process is understanding how the individual columns or features that you put into the model weigh in on the final predictions made. This can help provide intuition into what the most significant drivers are, and allow us to iterate by either including other columns that may be predictive or removing those that don't provide much value. The forecasting ML Function gives you the ability to calculate [feature importance](https://docs.snowflake.com/en/user-guide/analysis-forecasting) , using the `explain_feature_importance` method as shown below.\n-- get Feature Importance\nCALL VANCOUVER_FORECAST!explain_feature_importance();\n```\n\nCopy\n```\nThe output of this call for our multi-series forecast model is shown below, which you can explore further on snowsight. One thing to notice here is that, for this particular dataset, including holidays as an exogenous variable didn't dramatically impact our predictions. We may consider dropping this altogether, and only rely on the daily sales themselves. **Note** , based on the version of the ML Function, the outputted feature importances may be different compared to what is shown below due how features are generated by the model.\n ... \nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > Identifying Anomalous Sales with the Anomaly Detection ML Function\nContent:\nIn the past couple of sections we have built forecasting models for the items sold in Vancouver to plan ahead to meet demand. As an analyst, another question we might be interested in understanding further are anomalous sales. If there is a consistent trend across a particular food item, this may constitute a recent trend, and we can use this information to better understand the customer experience and optimize it.\nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > ... > Step 1: Building the Anomaly Detection Model\nContent:\nIn this section, we will make use of the [anomaly detection ML Function](https://docs.snowflake.com/en/user-guide/analysis-anomaly-detection) to build a model for anamolous sales for all items sold in Vancouver. Since we had found that holidays were not impacting the model, we have dropped that as a column for our anomaly model.\n-- Create a view containing our training data\nCREATE OR REPLACE VIEW vancouver_anomaly_training_set AS (\nSELECT *\nFROM vancouver_sales\nWHERE timestamp < (SELECT MAX(timestamp) FROM vancouver_sales) - interval '1 Month'\n);\n-- Create a view containing the data we want to make inferences on\nCREATE OR REPLACE VIEW vancouver_anomaly_analysis_set AS (\nSELECT *\nFROM vancouver_sales\nWHERE timestamp > (SELECT MAX(timestamp) FROM vancouver_anomaly_training_set)\n);\n-- Create the model: UNSUPERVISED method, however can pass labels as well; this could take ~15-25 secs; please be patient\nCREATE OR REPLACE snowflake.ml.anomaly_detection vancouver_anomaly_model(\nINPUT_DATA => SYSTEM$REFERENCE('VIEW', 'vancouver_anomaly_training_set'),\nSERIES_COLNAME => 'MENU_ITEM_NAME',\nTIMESTAMP_COLNAME => 'TIMESTAMP',\nTARGET_COLNAME => 'TOTAL_SOLD',\nLABEL_COLNAME => ''\n);\nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > ... > Step 1: Building the Anomaly Detection Model\nContent:\n-- Call the model and store the results into table; this could take ~10-20 secs; please be patient\nCALL vancouver_anomaly_model!DETECT_ANOMALIES(\nINPUT_DATA => SYSTEM$REFERENCE('VIEW', 'vancouver_anomaly_analysis_set'),\nSERIES_COLNAME => 'MENU_ITEM_NAME',\nTIMESTAMP_COLNAME => 'TIMESTAMP',\nTARGET_COLNAME => 'TOTAL_SOLD',\nCONFIG_OBJECT => {'prediction_interval': 0.95}\n);\n-- Create a table from the results\nCREATE OR REPLACE TABLE vancouver_anomalies AS (\nSELECT *\nFROM TABLE(RESULT_SCAN(-1))\n);\n-- Review the results\nSELECT * FROM vancouver_anomalies;\n```\n\nCopy\n```\nA few comments on the code above:\nAnomaly detection is able work in both a supervised and unsupervised manner. In this case, we trained it in the unsupervised fashion. If you have a column that specifies labels for whether something was anomalous, you can use the `LABEL_COLNAME` argument to specify that column.\nSimilar to the forecasting ML Function, you also have the option to specify the `prediction_interval` . In this context, this is used to control how 'agressive' the model is in identifying an anomaly. A value closer to 1 means that fewer observations will be marked anomalous, whereas a lower value would mark more instances as anomalous. See [documentation](https://docs.snowflake.com/en/user-guide/analysis-anomaly-detection) for further details.\nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > ... > Step 1: Building the Anomaly Detection Model\nContent:\nThe output of the model should look similar to that found in the image below. Refer to the [output documentation](https://docs.snowflake.com/sql-reference/classes/anomaly_detection) for further details on what all the columns specify.\nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > ... > Step 2: Identifying Trends\nContent:\nWith our model output, we are now in a position to see how many times an anomalous sale occured for each of the items in our most recent month's worth of sales data. Using the sql below:\n-- Query to identify trends\nSELECT series, is_anomaly, count(is_anomaly) AS num_records\nFROM vancouver_anomalies\nWHERE is_anomaly =1\nGROUP BY ALL\nORDER BY num_records DESC\nLIMIT 5;\n```\n\nCopy\n```\nFrom the image above, it seems as if Hot Ham & Cheese, Pastrami, and Italian have had the most number of anomalous sales in the month of May!\n ... \nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > Productionizing Your Workflow Using Tasks & Stored Procedures\nContent:\n-- Creates a Stored Procedure to extract the anomalies from our freshly trained model:\nCREATE OR REPLACE PROCEDURE extract_anomalies()\nRETURNS TABLE ()\nLANGUAGE sql\nAS\nBEGIN\nCALL vancouver_anomaly_model!DETECT_ANOMALIES(\nINPUT_DATA => SYSTEM$REFERENCE('VIEW', 'vancouver_anomaly_analysis_set'),\nSERIES_COLNAME => 'MENU_ITEM_NAME',\nTIMESTAMP_COLNAME => 'TIMESTAMP',\nTARGET_COLNAME => 'TOTAL_SOLD',\nCONFIG_OBJECT => {'prediction_interval': 0.95});\nDECLARE res RESULTSET DEFAULT (\nSELECT series, is_anomaly, count(is_anomaly) as num_records\nFROM TABLE(result_scan(-1))\nWHERE is_anomaly = 1\nGROUP BY ALL\nHAVING num_records > 5\nORDER BY num_records DESC);\nBEGIN\nRETURN table(res);\nEND;\nEND;\n-- Create an email integration:\nCREATE OR REPLACE NOTIFICATION INTEGRATION my_email_int\nTYPE = EMAIL\nENABLED = TRUE\nALLOWED_RECIPIENTS = ('');  -- update the recipient's email here\n ... \nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > Conclusion & Resources > Resources:\nContent:\nThis guide contained code patterns that you can leverage to get quickly started with Snowflake ML Functions. For further details, here are some useful resources:\n[Anomaly Detection](https://docs.snowflake.com/en/user-guide/analysis-anomaly-detection) Product Docs, alongside the [anomaly syntax](https://docs.snowflake.com/en/sql-reference/classes/anomaly_detection)\n[Forecasting](https://docs.snowflake.com/en/user-guide/analysis-forecasting) Product Docs, alongside the [forecasting syntax](https://docs.snowflake.com/sql-reference/classes/forecast)\n[Fork Repo on GitHub](https://github.com/Snowflake-Labs/notebook-demo/tree/main/Getting%20Started%20with%20Snowflake%20Cortex%20ML-Based%20Functions)\n[Download Reference Architecture](/content/dam/snowflake-site/developers/2024/04/Forecasting-Restaurant-Sales-and-Identifying-Sales-Anomalies-using-Cortex-ML-functions.pdf)\n[Read the Blog](https://medium.com/snowflake/detecting-anomalies-with-snowflake-cortex-ml-based-functions-869c209d152d)\n[Watch the Demo](https://youtu.be/xgxycDLg07U?list=TLGGW3mZkg03mM0yNDA5MjAyNQ)\nUpdated Oct 7, 2024\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > Conclusion & Resources > Resources: > On this page\nContent:\n[Overview]()\n[Setting Up Data in Snowflake]()\n[Forecasting Demand for Lobster Mac & Cheese]()\n[Building Multiple Forecasts & Adding Holiday Information]()\n[Identifying Anomalous Sales with the Anomaly Detection ML Function]()\n[Productionizing Your Workflow Using Tasks & Stored Procedures]()\n[Conclusion & Resources]()\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n*\n*\n ... \nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > Conclusion & Resources > Resources: > On this page\nContent:\n*\nAdd me to the list to receive dedicated product updates and general availability emails.\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\nSubscribe Now\n[Industries](/en/solutions/industries/) * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n[Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n[Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n[Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n[Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n[Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n[Technology](https://www.snowflake.com/en/solutions/industries/technology/)\nLearn * [Resource Library](https://snowflake.com/en/resources/)\nSection Title: Getting Started with Snowflake ML Functions: Anomaly Detection & Forecasting > Conclusion & Resources > Resources: > On this page\nContent:\n[Live Demos](/en/webinars/demo/)\n[Fundamentals](https://www.snowflake.com/en/fundamentals/)\n[Training](https://www.snowflake.com/en/resources/learn/training/)\n[Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n[Snowflake University](https://learn.snowflake.com/en/)\n[Developer Guides](https://www.snowflake.com/en/developers/guides)\n[Documentation](https://docs.snowflake.com/)\n[](/en/)\n© 2026 Snowflake Inc. All Rights Reserved\n[Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n[Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\nCookie Settings\n[Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Legal](https://www.snowflake.com/en/legal/)\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n[](https://www.facebook.com/Snowflake-Computing-709171695819345/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"],"full_content":null}],"errors":[],"warnings":null,"usage":[{"name":"sku_extract_excerpts","count":2}]}

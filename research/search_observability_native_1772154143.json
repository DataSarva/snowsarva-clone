{
  "search_id": "search_d460ac93018d4a4292eed31d5261f552",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/developer-guide/builders/observability",
      "title": "Observability in Snowflake apps | Snowflake Documentation",
      "excerpts": [
        "Section Title: Observability in Snowflake apps \u00b6\nContent:\nThrough observability built into Snowflake, you can ensure that your applications are running as efficiently as possible.\nUsing the practices and features described in this topic, you can make the most of observability features that show you where you\ncan improve your code.\n ... \nSection Title: Observability in Snowflake apps \u00b6 > Observability in Snowflake \u00b6\nContent:\nSnowflake supports a model that provides built-in observable data while also giving you ways to add more instrumentation where you need it.\nWhile Snowflake provides support for telemetry data such as logs, metrics, and traces (which are typical of observability), it also\nincludes other features you can use to keep track of system usage and performance.\nThe following lists features you can use to receive and analyze system performance and usage.\n|Collected telemetry data |As your application generates logs, metrics, and traces, Snowflake collects that telemetry data in an event table. Using\nSnowsight, you can explore the data, looking for patterns.\nYou can emit custom telemetry into the event table to provide contextual, domain-specific information to expedite debugging. |\n| --- | --- |\n|History Tables |Use the following views and their associated tables to monitor all usage in your account.\nSection Title: Observability in Snowflake apps \u00b6 > Observability in Snowflake \u00b6\nContent:\nQuery History\nCopy History\nTasks |\n|Alerts and notifications |Alerts allow for customizable triggering conditions, actions, and a schedule, in combination with notification integrations for proactive monitoring. |\n|Extensibility with third-party tools |The Snowflake event table adopts [OpenTelemetry](https://opentelemetry.io/docs/) standards, so your\nSnowflake telemetry can easily be consumed by other ecosystem tools. |\nSection Title: Observability in Snowflake apps \u00b6 > Telemetry data collected for analysis \u00b6\nContent:\nAs code in your application executes, you can have Snowflake collect data from the code that tells you about the application\u2019s internal\nstate. Using this telemetry data\u2014collected in a Snowflake event table (your account has one by default )\u2014you can look for bottlenecks and other opportunities to optimize.\nTelemetry data must be emitted as your code executes. Snowflake emits some of this data on your code\u2019s behalf without\nyou needing to instrument your code. You can use also APIs included with Snowflake to emit telemetry data from specific parts of your code.\nAs described below, you can analyze the collected data by querying the event table or using the visualizations that capture the data\nin Snowsight.\nSection Title: Observability in Snowflake apps \u00b6 > ... > Types of telemetry data \u00b6\nContent:\nTo ensure that the telemetry data you collect is broadly useful, Snowflake telemetry is built on the standard [OpenTelemetry](https://opentelemetry.io/docs/) (sometimes called OTel) framework, an incubating project of the Cloud Native Compute Foundation. Through this framework (and APIs and\ntools designed for it), you can reuse collected data with tools besides Snowflake .\nThrough OpenTelemetry, you can instrument application code to add observability where you want it.\nSnowflake event tables collect log, span, and metrics data in the OpenTelemetry data model. The following describes each type of telemetry\ndata collected in an event table.\n|Logs |Logs record individual operations performed by code. Each log message is generated at\na discrete point during the execution of the code.\nSection Title: Observability in Snowflake apps \u00b6 > ... > Types of telemetry data \u00b6\nContent:\n**Instrumenting code** You can log from your code using libraries standard for the language you\u2019re using, as listed in Logging from handler code .\n**Viewing data** You can view log messages for analysis\neither by querying the event table or looking at the visualizations provided in Snowsight.\nThe following image from Snowsight shows a list of collected log messages for a two-hour period in a single database.\n|\n| --- | --- |\n|Metrics |Metrics are measurements calculated over a time period. These values include CPU and memory measurements.\n**Instrumenting code** Snowflake emits metrics data automatically as your code executes, so you don\u2019t need to instrument your code.\n**Viewing data** You can view metrics data for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\nSection Title: Observability in Snowflake apps \u00b6 > ... > Types of telemetry data \u00b6\nContent:\nThe following image from Snowsight shows changes in collected metrics data for the execution of a user-defined function.\n|\n|Traces |Traces show distributed events as data flows through a system. In a trace, you can see where time is spent as processing flows\nfrom component to component.\nYou can emit trace events\u2014both within the default span Snowflake creates or from a custom span you create\u2014using libraries\nstandard for the language you\u2019re using, as listed in Logging from handler code .\n**Instrumenting code** You can emit trace events from your code using libraries standard for the language you\u2019re using, as listed in Event tracing from handler code .\n**Viewing data** You can view trace events for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\nThe following image from Snowsight shows the spans resulting as a UDF executes.\n|\n ... \nSection Title: ... > Set up your environment to capture telemetry data before you need it \u00b6\nContent:\nEnable telemetry data collection for your Snowflake environment.To collect the data you\u2019ll need, ensure that you have an active event table. To ensure you\u2019re collecting telemetry data you want, set telemetry levels to\nuseful thresholds.At first, you\u2019ll want to set these levels to ensure that you\u2019re collecting data. For example, set log levels to at least WARN for any\nproduction or business critical jobs. Over time, you might adjust these levels to meet changing needs.Organize your production stored procedures, UDFs, and other objects under a database or schema so you can simply enable warning logs\nfor that database or schema. This saves the trouble of managing settings for separate objects.\nSection Title: ... > Set up your environment to capture telemetry data before you need it \u00b6\nContent:\nTo generate data for troubleshooting, add log statements or trace events to your production jobs.When you use standard logging libraries such as Java\u2019s SLF4J or Python\u2019s logging libraries, Snowflake routes logs from those packages to\nyour event table automatically.For tracing, you can use telemetry libraries included with Snowflake. To include in trace data parts of the handler\u2019s processing that you want to measure, add custom spans to your stored procedure handler code.Along with the built-in spans from Snowflake objects, Snowflake represents custom spans you create in the trace diagram. With custom\nspans, you can capture data about arbitrary parts of your code\u2019s processing to see how long those parts take to execute. You can also\nattach arbitrary metadata to custom spans to add descriptions to the data for troubleshooting and optimizing.\n ... \nSection Title: Observability in Snowflake apps \u00b6 > ... > Manage the amount of telemetry data received for UDFs \u00b6\nContent:\nWhen adding code to collect telemetry data with UDFs, remember that the UDF execution model can mean many more rows in the event table\nthan for a procedure.\nWhen a UDF is called on every input row, your handler code emits logging statements or span events for every row of the input dataset.\nFor example, a dataset of 10 million rows passed to a UDF would emit 10 million log entries.\nConsider using the following patterns when adding logs and span events to UDFs:\nSection Title: Observability in Snowflake apps \u00b6 > ... > Manage the amount of telemetry data received for UDFs \u00b6\nContent:\nInitially, use logging levels designed to reduce the number of entries recorded.Use DEBUG- or INFO-level logging statements and set the logging level to WARN in production. If an issue is found, you can lower the\nlogging level to DEBUG or INFO for the duration of the debugging session.\nUse try/catch blocks to isolate the code from which you want to emit logging data.Using try/catch can be useful to catch any unexpected UDF input, log it as a WARN-level log for awareness, and return a default value.\nUse condition statements to log only for scenarios that are meaningful to you.With if/else statements or other constraints, you can control the volume of logging output.\n ... \nSection Title: Observability in Snowflake apps \u00b6 > ... > Alerts and notifications best practices \u00b6\nContent:\nAvoid duplicating event evaluation.You can avoid duplicating evaluation on events by accounting for the latency between the alert schedule and execution. To do this,\nspecify alert timestamps using SCHEDULED_TIME and LAST_SUCCESSFUL_SCHEDULED_TIME instead of using CURRENT_TIMESTAMP .For more information, see Specifying timestamps based on alert schedules . Enrich an alert action or notification with query results.You can check the results from the SQL statement specified by an alert condition. To obtain the query results, do the following:\nRetrieve the query ID for the alert condition\u2019s SQL statement by calling GET_CONDITION_QUERY_UUID . Pass the query ID to RESULT_SCAN to obtain the query results. Log a result or take automated action in addition to sending a notification.You can specify that an alert action runs a task or logs a new row to a table whenever an alert\ncondition is met.\n ... \nSection Title: Observability in Snowflake apps \u00b6 > Tools for analysis and visualization \u00b6\nContent:\nYou can use the telemetry data collected in your event table with other tools that support the OpenTelemetry data model.\nThrough Snowflake support of OpenTelemetry, you can use APIs, SDKs, and other tools to instrument, generate, collect, and export telemetry\ndata. Using these tools, you can more thoroughly analyze software performance and behavior. Because a Snowflake event table uses this\nwidely-adopted standard, you might also be able to integrate your organization\u2019s observability tools with event tables with little overhead.\nConsider integrating your external tools in one of the following ways:\nIf your observability tools can read from external sources, point them to the event table.\nIf your tools use a push model\u2014in which telemetry data must be sent to the tool\u2014consider using a stored procedure with external access to regularly read telemetry data from\nthe event table and emit it to your tool.\nSection Title: Observability in Snowflake apps \u00b6 > Tools for analysis and visualization \u00b6\nContent:\nThe following lists tools you might integrate with Snowflake event tables:\n[Snowflake integration for Datadog](https://docs.datadoghq.com/integrations/snowflake_web/)\nSnowflake integration for Grafana dashboardFor an introduction to using Grafana with Snowflake, see [How to monitor Snowflake with Grafana Cloud](https://grafana.com/blog/2023/05/24/how-to-monitor-snowflake-with-grafana-cloud/) .\n[Snowflake data source for Grafana](https://grafana.com/docs/plugins/grafana-snowflake-datasource/latest/)\n[Snowflake integration for Grafana Cloud](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/integrations/integration-reference/integration-snowflake/)\n[Observe for Snowflake](https://app.snowflake.com/marketplace/listing/GZTYZY3AR0U/observe-inc-observe-for-snowflake) , Observe\u2019s native app for observability\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/native-apps/ui-consumer-enable-logging",
      "title": "Set up event tracing for an app | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Listings Snowflake Marketplace listings Use applications as a consumer Set up event tracing for an app\nSection Title: Set up event tracing for an app \u00b6\nContent:\nFeature \u2014 Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nThis topic describes how to set up use event tracing to capture the log messages and trace events\nemitted by an app. It also describes how to enable event sharing to share log messages and trace events\nwith providers.\nSection Title: Set up event tracing for an app \u00b6 > About event tracing in the Snowflake Native App Framework \u00b6\nContent:\nEvent tracing allows an app to emit information related to its performance and behavior. The Snowflake Native App Framework\nsupports using the Snowflake logging and tracing .\nfunctionality to gather this information. An app can emit the following:\nLog messages that are independent, detailed messages with information about the state of a specific\nfeature within the app.\nTrace events with structured data you can use to get information spanning and grouping multiple\nparts of an app.\nMetrics data that includes the CPU and memory metrics that Snowflake generates.\n ... \nSection Title: Set up event tracing for an app \u00b6 > About event sharing \u00b6 > Supported event definitions \u00b6\nContent:\nThe following table lists the event definitions that are currently supported:\nTypeNameDescriptionFilterAllSNOWFLAKE$ALLShares all log messages and trace events that the app emits.`*`Errors and warningsSNOWFLAKE$ERRORS_AND_WARNINGSShares logs related to errors, warnings, and fatal events.`RECORD_TYPE = \u2018LOG\u2019 AND RECORD:severity_text in (\u2018FATAL\u2019, \u2018ERROR\u2019, \u2018WARN\u2019)`MetricsSNOWFLAKE$METRICSShares the CPU and memory metrics that Snowflake generates.`RECORD_TYPE = in ('METRIC')`TracesSNOWFLAKE$TRACESShares detailed traces of user activities and journeys in the application.`RECORD_TYPE in (\u2018SPAN\u2019, \u2018SPAN_EVENT\u2019)`Usage logsSNOWFLAKE$USAGE_LOGSShares high-level logs related to user actions and app events.`RECORD_TYPE = LOG AND RECORD:severity_text = \u2018INFO\u2019`Debug logsSNOWFLAKE$DEBUG_LOGSShares technical logs used to troubleshoot the app.`RECORD_TYPE = \u2018LOG\u2019 AND RECORD:severity_text in (\u2018DEBUG\u2019, \u2018TRACE\u2019)`\nNote\n ... \nSection Title: Set up event tracing for an app \u00b6 > Set up an event table \u00b6\nContent:\nTo collect the log messages and trace events emitted by the app, consumers must create an event table to\nstore the information.\nNote\nIF the consumer does not set up an event table and make it the active event table before installing the\napp, trace event and log data is discarded.\nIf a provider includes required event definitions in the app, they are enabled by default during\ninstallation. However, if the consumer does not have an active event table, the log messages and\ntrace events emitted by the app are discarded.\nAn account can have multiple event tables, but only one of them can be set as the active event table in a\nSnowflake account at a time. Without an active event table, log messages and trace events that the app emits\nare not captured. This is true even if the functions and procedures in an app call the logging and trace\nevent APIs directly.\nSection Title: Set up event tracing for an app \u00b6 > Set up an event table \u00b6\nContent:\nTo create an event table, run the CREATE EVENT TABLE command as shown in the following example:\n```\nCREATE EVENT TABLE event_db . event_schema . my_event_table ;\n```\nCopy\nNote that this command specifies the database and schema that contain the event table.\nAfter creating the event table, use the ALTER ACCOUNT command to\nspecify that the event table is the active table for the account:\n```\nALTER ACCOUNT SET EVENT_TABLE = event_db . event_schema . my_event_table ;\n```\nCopy\n ... \nSection Title: Set up event tracing for an app \u00b6 > ... > Enable event sharing using Snowsight \u00b6\nContent:\nIn the navigation menu, select Catalog \u00bb Apps .\nSelect the app.\nSelect the Settings icon in the toolbar.\nSelect the Events and logs tab.\nUnder the Events and logs sharing area, move the slider for the events you want to capture.\nIf the provider has defined event definitions for the app:\nUse the slider to enable optional event definitions. By default, all event types are enabled.\nSelect Save .\nIf no event table is currently selected, select the event table from the list\nunder Event table location .CautionUse caution when changing the event table in Snowsight. Each Snowflake account\nuses a single event table for all events generated within the account. Changing the event\ntable causes all events generated in the account to be stored in the new location.\nSection Title: Set up event tracing for an app \u00b6 > ... > Enable event sharing by using SQL \u00b6\nContent:\nUse the SHOW TELEMETRY EVENT DEFINITIONS command to determine the event definitions for the app:CopyIf the provider did not configure the app to use event definitions, the `type` column\ndisplays `ALL` . Otherwise, this command lists the optional event definitions specified\nfor the app. If the app contains required event definitions, use the ALTER APPLICATION command to\nenable them:CopyThis command enables all of the require event definitions, but does not enable optional event\ndefinitions.NoteAfter enabling the required event definitions for an app, event sharing cannot be disabled. If the app contains options event definitions, use the use the ALTER APPLICATION to enable them as shown in the following example:CopyThis example enables the `SNOWFLAKE$TRACES` and `SNOWFLAKE$DEBUG_LOGS` based on the output of the SHOW TELEMETRY EVENT DEFINITIONS command.\nSection Title: Set up event tracing for an app \u00b6 > ... > Enable event sharing by using SQL \u00b6\nContent:\nTo verify that event tracing and logging is enabled, use the DESCRIBE APPLICATION command:CopyThe `authorize_telemetry_event_sharing` and `share_events_with_provider` rows of the output\nindicate if event sharing is enabled.\nSection Title: ... > Enable event sharing using SQL (deprecated functionality) \u00b6\nContent:\nCaution\nThe method of enabling event sharing using SQL described in this section will be deprecated\nin a future release. Snowflake recommends using the method described in Enable log and event sharing using SQL to enable event sharing using SQL.\nTo enable event sharing for an app, run the ALTER APPLICATION command to set\nSHARE_EVENTS_WITH_PROVIDER to `TRUE` . For example:\n```\nALTER APPLICATION HelloSnowflake SET SHARE_EVENTS_WITH_PROVIDER = TRUE ;\n```\nCopy\nTo show the event sharing status for an app, use the DESCRIBE APPLICATION command as shown in the following example:\n```\nDESC APPLICATION HelloSnowflake ;\n```\nCopy\n`SHARE_EVENTS_WITH_PROVIDER` shows the status of event sharing for the app.\n ... \nSection Title: Set up event tracing for an app \u00b6 > View the log messages and trace events in the event table \u00b6\nContent:\nWhen an event table is enabled, consumers can query the event table to see the log messages and\ntrace events emitted by the app.\n ... \nSection Title: ... > View the event log messages and trace events by using SQL \u00b6\nContent:\nUse the SELECT command to query the\nevent log messages and trace events, as shown in the following example:\n```\nSELECT \n  TIMESTAMP as time , \n  RESOURCE_ATTRIBUTES [ 'snow.executable.name' ] as executable , \n  RECORD [ 'severity_text' ] as severity , \n  VALUE as message \n FROM \n  \"EVENT_LOG\" . \"PUBLIC\" . \"CONSUMER_EVENT_TABLE\" \n WHERE RESOURCE_ATTRIBUTES [ 'snow.application.name' ] = 'YOUR_APP_NAME'\n```\nCopy\nThis command returns all of the log messages and trace events stored in the event table `CONSUMER_EVENT_TABLE` for an app named `YOUR_APP_NAME` .\nSection Title: ... > Determine if a log message or trace event is shared with the provider \u00b6\nContent:\nThe RECORD_ATTRIBUTES column contains the `snow.application.shared` field. If the value of\nthis field is TRUE, the log message or trace event is shared with the provider. Otherwise,\nthe log message or event is not shared.\n ... \nSection Title: Set up event tracing for an app \u00b6 > Privacy Preference Center > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details\u200e"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/native-apps/event-about",
      "title": "Use logging and event tracing for an app - Snowflake Documentation",
      "excerpts": [
        "Developer Snowflake Native App Framework Configure logging and event tracing for an app\nSection Title: Use logging and event tracing for an app \u00b6\nContent:\nFeature \u2014 Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nThis topic describes how providers can configure a Snowflake Native App to record log messages and trace events.\nSection Title: Use logging and event tracing for an app \u00b6 > About log messages and trace events in an app \u00b6\nContent:\nThe Snowflake Native App Framework supports using the Snowflake logging and tracing functionality to gather information about an app. Providers can configure an app to record and analyze the following:\nLog messages \u2014 Independent, detailed messages with information about\nthe state of a specific piece app code.\nTrace events \u2014 Structured data that providers can\nuse to get information spanning and grouping multiple parts of your code. Trace events allows an app to emit information related\nto its performance and behavior.\nMetrics - Information about stored procedure and UDF resource consumption\nbased on the CPU and memory metrics that Snowflake generates.\nTo configure an app to emit log messages and trace events, providers set the log and trace levels in the manifest file.\nSee Set the log and trace levels for an app .\nSection Title: Use logging and event tracing for an app \u00b6 > About log messages and trace events in an app \u00b6\nContent:\nProviders can also configure an app to use event sharing to allow the consumer to share the log messages\nand trace events with the provider. See About event sharing for\nmore information.\nSection Title: Use logging and event tracing for an app \u00b6 > About event sharing \u00b6\nContent:\nEvent sharing allows the provider to collect information about an app\u2019s performance and behavior.\nA provider can configure an app to request that the consumers share the log messages\nand trace events with the provider. Event sharing requires that the provider and consumer configure an\nevent table in their account to store the log messages and trace events emitted by the app.\nWhen event sharing is enabled, the log messages and trace events that are inserted into the event table in the\nconsumer account are also inserted into the event table in provider account.\nSection Title: Use logging and event tracing for an app \u00b6 > Considerations when using event sharing \u00b6\nContent:\nBefore configuring logging and event sharing for an app, providers must consider the following:\nProviders are responsible for all costs associated with event sharing on the provider side, including data\ningestion and storage.\nProviders must have an account to store shared events in each region where you want to support event sharing.\nProviders must define the default log level and trace level for an app in the manifest file.\nSection Title: ... > Considerations when migrating from the previous event sharing functionality \u00b6\nContent:\nWhen migrating from the existing event sharing functionality to use event definitions, providers\nshould consider the following.\nThe previous event sharing functionality is equivalent to the OPTIONAL ALL event definition.\nPublished versions and patches of an app that used the previous functionality will have the\nOPTIONAL ALL event definition by default. Providers do not need to add this event definition\nto the manifest file.\nTo begin using event definitions, providers can add supported event definitions to the manifest\nfile. This is applicable to new apps as well as new versions and patches of existing apps.\nNote\nTo being begin requesting more granular log and event sharing, providers only have to add\nevent definitions to the manifest file. No other actions are required for providers.\nSection Title: Use logging and event tracing for an app \u00b6 > Workflow - Set up event sharing for an app \u00b6\nContent:\nEvent sharing allows consumers to share log messages and trace events with the provider.\nThe following workflow shows how to set up and enable event sharing for an app:\nThe provider sets the log and trace levels for the app.\nThe provider adds event definitions to the manifest file.Event definitions act as filters on the log messages and trace events emitted by the app.\nProviders can configure event definitions to be required or optional.\nThe provider sets up an event table in their organization.\nThe provider publishes the app.\nWhen a consumer installs an app, they can set up an event table and enable event sharing.\nSee [Enable logging and event sharing for an app](https://other-docs.snowflake.com/en/native-apps/consumer-enable-logging) for more information on the consumer requirements for event sharing.\nSection Title: Use logging and event tracing for an app \u00b6 > Monitor consumer application health \u00b6\nContent:\nYou can use the `LAST_HEALTH_STATUS` and `LAST_HEALTH_STATUS_UPDATED_ON` columns\nof the APPLICATION_STATE view to monitor the health of consumer instances of your\napp. The `LAST_HEALTH_STATUS` column has the following possible values:\n`OK` : The consumer instance is healthy.\n`FAILED` : The consumer instance is in an error state.\n`PAUSED` : The consumer manually paused the app.\nThe following code sample demonstrates using the `APPLICATION_STATE` view\nto retrieve the health status of all consumer instances of your app:\n```\nSELECT \n    CONSUMER_ORGANIZATION_NAME , \n    CONSUMER_ACCOUNT_NAME , \n    LAST_HEALTH_STATUS , \n    LAST_HEALTH_STATUS_UPDATE_TIME \n FROM \n    SNOWFLAKE . ACCOUNT_USAGE . APPLICATION_STATE \n WHERE \n    PROVIDER_ORG_NAME = '<your_provider_org_name>' \n    AND APPLICATION_NAME = '<your_app_name>' \n ORDER BY \n    LAST_HEALTH_STATUS_UPDATE_TIME DESC ;\n```\nCopy\nSection Title: Use logging and event tracing for an app \u00b6 > Monitor consumer application health \u00b6\nContent:\nThe preceding query may return results similar to the following:\n```\nCONSUMER_ORG_NAME    CONSUMER_ACCOUNT_NAME    LAST_HEALTH_STATUS    LAST_HEALTH_STATUS_UPDATE_TIME \n ------------------   ---------------------    ------------------    ------------------------------- \n consumer_org_1      consumer_account_1       OK                    2024-01-15 10:30:00.000 \n consumer_org_2      consumer_account_2       FAILED                2024-01-15 09:45:00.000 \n consumer_org_3      consumer_account_3       PAUSED                2024-01-14 16:20:00.000\n```\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nSection Title: Use logging and event tracing for an app \u00b6 > Monitor consumer application health \u00b6\nContent:\nAbout log messages and trace events in an app\nAbout event sharing\nConsiderations when using event sharing\nConsiderations when migrating from the previous event sharing functionality\nWorkflow - Set up event sharing for an app\nMonitor consumer application health\nRelated content\nLogging, tracing, and metrics"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/dynamic-tables-monitor-event-table-alerts",
      "title": "Event table monitoring and alerts for dynamic tables | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Data engineering Dynamic Tables Monitoring and observability Event table monitoring and alerts for dynamic tables\nSection Title: Event table monitoring and alerts for dynamic tables \u00b6\nContent:\nThis topic discusses how to query an event table that provides information about your refresh status and how to set up alerts on new data in\nan event table.\nSection Title: Event table monitoring and alerts for dynamic tables \u00b6 > Query an event table to monitor refreshes \u00b6\nContent:\nWhen a dynamic table is refreshed, you can configure Snowflake to record an event that provides information about the status of the refresh\noperation. The event is recorded in the active event table associated with\nthe dynamic table.\nFor example, suppose that you have associated an event table with a database . When a\ndynamic table in that database is refreshed, Snowflake records an event to that event table.\nYou can query the events logged in this active event table to monitor your dynamic table refreshes.\nFor example, to get the timestamp, dynamic table name, query ID, and error message for errors with dynamic tables in the database `my_db` ,\ndo the following:\n ... \nSection Title: Event table monitoring and alerts for dynamic tables \u00b6 > Query an event table to monitor refreshes \u00b6\nContent:\n|                                      | Object 'MY_DB.MY_SCHEMA.MY_BASE_TABLE' does not exist or not authorized. | \n +-------------------------+------------------+--------------------------------------+---------------------------------------------------------------------------------+\n```\n ... \nSection Title: Event table monitoring and alerts for dynamic tables \u00b6 > Query an event table to monitor refreshes \u00b6\nContent:\n|                         |                 |                         |       |          |   \"snow.schema.name\": \"MY_SCHEMA\"               |       |                  |             |                             |                   |                               |           | \n |                         |                 |                         |       |          | }                                               |       |                  |             |                             |                   |                               |           | \n +-------------------------+-----------------+-------------------------+-------+----------+-------------------------------------------------+-------+------------------+-------------+-----------------------------+-------------------+-------------------------------+-----------+\n```\n ... \nSection Title: ... > Set up alerts on new data to monitor refreshes \u00b6\nContent:\nAs mentioned earlier , when a dynamic table is refreshed, an event is logged in the\nevent table to indicate whether the refresh succeeded or failed. You can set up an alert on new data to\nmonitor the event table. You can configure the alert to send a notification when a\nrefresh fails.\nThe next sections explain how to set up the event logging to capture the events, how to set up the alert, and how to interpret\nthe events recorded in the event table:\nSet the severity level of the events to capture\nSet up an alert on new data\nInformation logged for dynamic table events\nNote\nLogging events for dynamic tables incurs costs. See Costs of telemetry data collection .\n ... \nSection Title: Event table monitoring and alerts for dynamic tables \u00b6 > ... > Set up an alert on new data \u00b6\nContent:\nAfter you set the severity level for logging events, you can set up an alert on new data to monitor the event table for new events\nthat indicate a failure in a dynamic table refresh. An alert on new data is triggered when new rows in the event table are\ninserted and meet the condition specified in the alert.\nNote\nTo create the alert on new data, you must use a role that has been granted the required privileges to query the event table.\nSection Title: Event table monitoring and alerts for dynamic tables \u00b6 > ... > Set up an alert on new data \u00b6\nContent:\nIf the alert condition queries the default event table ( SNOWFLAKE.TELEMETRY.EVENTS )\nor the predefined view ( SNOWFLAKE.TELEMETRY.EVENTS_VIEW view ),\nsee Roles for access to the default event table and EVENTS_VIEW .To manage access to the EVENTS_VIEW view, see Manage access to the EVENTS_VIEW view .\nIf the alert condition queries a custom event table, see Access control privileges for event tables .To manage access to a custom event table, see Managing access to event table data .\nIn the alert condition, to query for dynamic table events, select rows where `resource_attributes:\"snow.executable.type\" = 'DYNAMIC_TABLE'` . To narrow down the list of events, you can filter on the\nfollowing columns:\nSection Title: Event table monitoring and alerts for dynamic tables \u00b6 > ... > Set up an alert on new data \u00b6\nContent:\nTo restrict the results to dynamic tables in a specific database, use `resource_attributes:\"snow.database.name\"` .\nTo return events where the refresh failed due to an error with the dynamic table, use `value:state = 'FAILED'` .\nTo return events where the refresh failed due to an error with an upstream dynamic table, use `value:state = 'UPSTREAM_FAILURE'` .\nFor information on the values logged for a dynamic table event, see Information logged for dynamic table events .\nFor example, the following statement creates an alert on new data that performs an action when refreshes fail for dynamic tables\nin the database `my_db` . The example assumes that:\nYour active event table is the default event table (SNOWFLAKE.TELEMETRY.EVENTS).\nYou have set up a webhook notification integration for that Slack\nchannel.\nSection Title: Event table monitoring and alerts for dynamic tables \u00b6 > ... > Set up an alert on new data \u00b6\nContent:\n```\nCREATE ALERT my_alert_on_dt_refreshes \n  IF ( EXISTS ( \n    SELECT * FROM SNOWFLAKE . TELEMETRY .\n ... \nSection Title: ... > Information logged for dynamic table events \u00b6\nContent:\nWhen a dynamic table refreshes, an event is logged to the event table. The following sections describe the event table row that\nrepresents the event:\nEvent table column values\nKey-value pairs in the resource_attributes column\nKey-value pairs in the record column\n ... \nSection Title: Event table monitoring and alerts for dynamic tables \u00b6 > ... > Event table column values \u00b6\nContent:\n| Column | Data type | Description |\n| `timestamp` | TIMESTAMP_NTZ | The UTC timestamp when an event was created. |\n| `observed_timestamp` | TIMESTAMP_NTZ | A UTC time used for logs. Currently, this is the same value that is in the `timestamp` column. |\n| `resource_attributes` | OBJECT | Attributes that identify the dynamic table that |\n| was refreshed. |  |  |\n| `record_type` | STRING | The event type, which is `EVENT` for dynamic table refreshes. |\n| `record` | OBJECT | Details about the status of the dynamic table refresh. |\n| `value` | VARIANT | The status of the dynamic table refresh and, if the refresh |\n| failed, the error message for the failure. |  |  |\n ... \nSection Title: ... > Key-value pairs in the `resource_attributes` column \u00b6\nContent:\n| Attribute name | Attribute type | Description | Example |\n| `snow.database.id` | INTEGER | The internal/system-generated identifier of the database containing the dynamic table. | `12345` |\n| `snow.database.name` | VARCHAR | The name of the database containing the dynamic table. | `MY_DATABASE` |\n| `snow.executable.id` | INTEGER | The internal/system-generated identifier of the dynamic table that was refreshed. | `12345` |\n| `snow.executable.name` | VARCHAR | The name of the dynamic table that was refreshed. | `MY_DYNAMIC_TABLE` |\n| `snow.owner.type` | VARCHAR | The type of role that owns the object, for example `ROLE` . . If a Snowflake Native App owns the object, the value is `APPLICATION` . . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role. | `ROLE` |\n ... \nSection Title: ... > Key-value pairs in the `value` column \u00b6\nContent:\nQuery an event table to monitor refreshes\nSet up alerts on new data to monitor refreshes\nSet the severity level of the events to capture\nSet up an alert on new data\nInformation logged for dynamic table events\nEvent table column values\nKey-value pairs in the resource_attributes column\nKey-value pairs in the record column\nKey-value pairs in the value column\nRelated content\nCREATE DYNAMIC TABLE\nALTER DYNAMIC TABLE\nDESCRIBE DYNAMIC TABLE\nDROP DYNAMIC TABLE\nSHOW DYNAMIC TABLES\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/blog/collect-logs-traces-snowflake-apps/",
      "title": "Collect Logs and Traces From Your Snowflake Applications",
      "publish_date": "2024-08-21",
      "excerpts": [
        "blog\nSection Title: Category\nContent:\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\nData Engineering\nOCT 30, 2023 | 4 min read\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables\nContent:\nWe are excited to announce the general availability of Snowflake\nEvent Tables\nfor logging and tracing, an essential feature to boost application observability and supportability for Snowflake developers.\nIn our conversations with developers over the last year, we\u2019ve heard that monitoring and observability are paramount to effectively develop and monitor applications. But previously, developers didn\u2019t have a centralized, straightforward way to capture application logs and traces.\nEnter the new Event Tables feature, which helps developers and data engineers easily instrument their code to capture and analyze logs and traces for all languages: Java, Scala, JavaScript, Python and Snowflake Scripting.\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables\nContent:\nWith Event Tables, developers can instrument logs and traces from their UDFs, UDTFs, stored procedures, Snowflake Native Apps and Snowpark Container Services, then seamlessly route them to a secure, customer-owned Event Table. Developers can then query Event Tables to troubleshoot their applications or gain insights into performance and code behavior.\nLogs and traces are collected and propagated via Snowflake\u2019s telemetry APIs, then automatically ingested into your Snowflake Event Table.\nSection Title: ... > Simplify troubleshooting in Snowflake Native Apps\nContent:\nEvent Tables are also supported for Snowflake Native Apps. When a Snowflake Native App runs, it is running in the consumer\u2019s account, generating telemetry data that\u2019s ingested into their active Event Table.\nOnce the consumer enables event sharing, new telemetry data will be ingested into both the consumer and provider Event Tables. Now the provider has the ability to debug the application that\u2019s running in the consumer\u2019s account. The provider only sees the telemetry data that is being shared from this data application\u2014nothing else.\nSection Title: ... > Improve reliability across a variety of use cases\nContent:\nYou can use Event Tables to capture and analyze logs for various use cases: * As a data engineer building UDFs and stored procedures within queries and tasks, you can instrument your code to analyze its behavior based on input data.\nAs a Snowpark developer, you can instrument logs and traces for your Snowflake applications to troubleshoot and improve their performance and reliability.\nAs a Snowflake Native App provider, you can analyze logs and traces from various consumers of your applications to troubleshoot and improve performance.\nSection Title: ... > Improve reliability across a variety of use cases\nContent:\nSnowflake customers ranging from Capital One to phData are already using Event Tables to unlock value in their organization. \u201cThe Event Tables feature simplifies capturing logs in the observability solution we built to monitor the quality and performance of Snowflake data pipelines in Capital One Slingshot,\u201d says Yudhish Batra, Distinguished Engineer, Capital One Software. \u201cEvent Tables has abstracted the complexity associated with logging from our data pipelines\u2014specifically, the central Event Table gives us the ability to monitor and alert from a single location.\u201d\nAs phData migrates its Spark and Hadoop applications to Snowpark, the Event Tables feature has helped architects save time and hassle.\n\u201cWhen working with Snowpark UDFs, some of the logic can become quite complex. In some instances, we had thousands of lines of Java code that needed to be monitored and debugged,\u201d says Nick Pileggi,\nPrincipal Solutions Architect at phData\nSection Title: ... > Improve reliability across a variety of use cases\nContent:\n. \u201cBefore Event Tables, we had almost no way to see what was happening inside the UDF and correct issues. Once we rolled out Event Tables, the amount of time we spent testing dropped significantly and allowed us to have debug and info-level access to the logs we were generating in Java.\u201d\nOne large communications service provider also uses logs in Event Tables to capture and analyze failed records during data ingestion from various external services to Snowflake. And a Snowflake Native App provider offering geolocation data services uses Event Tables to capture logs and traces from their UDFs to improve application reliability and performance.\nWith Event Tables, you now have a built-in place to easily and consistently manage logging and tracing for your Snowflake applications. And in conjunction with other features such as Snowflake Alerts and Email Notifications, you can be notified of new events and errors in your applications.\nSection Title: ... > Try Event Tables today\nContent:\nTo learn more about Event Tables, join us at [BUILD](https://www.snowflake.com/build/)\n,\nSnowflake\u2019s developer conference. Or get started with Event Tables today with a [tutorial](https://docs.snowflake.com/en/developer-guide/logging-tracing/tutorials/logging-tracing-getting-started)\nand quickstarts for [logging](https://quickstarts.snowflake.com/guide/alert_on_events/index.html)\nand [tracing](https://quickstarts.snowflake.com/guide/java_trace_events/index.html)\n. For further information about how Event Tables work, visit Snowflake product [documentation](https://docs.snowflake.com/en/developer-guide/logging-tracing/logging-tracing-overview)\n.\n ... \nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables > ... > Author\nContent:\nAshwin Kamath\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables > ... > Share Article\nContent:\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps&title=Collect+Logs+and+Traces+From+Your+Snowflake+Applications+With+Event+Tables)\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps&text=Collect+Logs+and+Traces+From+Your+Snowflake+Applications+With+Event+Tables)\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps)\nSubscribe to our blog newsletter\nGet the best, coolest and latest delivered to your inbox each week\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\n[start for free](https://signup.snowflake.com/)\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables > Where Data Does More\nContent:\n[Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n[Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n[Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n[Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n[Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n[Technology](https://www.snowflake.com/en/solutions/industries/technology/)\nLearn * [Resource Library](https://snowflake.com/en/resources/)\n ... \nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables > Where Data Does More\nContent:\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/product/features/snowflake-trail/",
      "title": "Snowflake Trail for Observability",
      "excerpts": [
        "Data for Breakfast Around the World\nDrive impact across your organization with data and agentic intelligence.\nregister now\nSection Title: Snowflake Trail for Observability\nContent:\nIntroducing Snowflake Trail: a set of Snowflake capabilities for developers to better monitor, troubleshoot, debug and take actions on pipelines, apps, user code and compute utilizations.\nstart for free\nSection Title: Snowflake Trail for Observability > NEW VIDEOS > Data Engineering School\nContent:\nLearn about the latest trends and skill sets in data engineering by completing 3 free instructional tracks presented by industry experts.\nget started\nBlog ##### Observability in Snowflake Read now\n[Blog ##### Enhanced Tracing, Log, and Metrics Read now](https://medium.com/snowflake/new-in-snowflake-trail-enhanced-logs-tracing-and-metrics-for-snowpark-a2476198e14e)\nQuickstart ##### Snowpark Observability (Public Preview) Get started\nBlog ##### Observability in Snowflake Read now\n[Blog ##### Enhanced Tracing, Log, and Metrics Read now](https://medium.com/snowflake/new-in-snowflake-trail-enhanced-logs-tracing-and-metrics-for-snowpark-a2476198e14e)\nQuickstart ##### Snowpark Observability (Public Preview) Get started\nBlog ##### Observability in Snowflake Read now\nWhat's new\nSection Title: Snowflake Trail for Observability > Observability in Snowflake\nContent:\nObservability shouldn\u2019t be an afterthought. Snowflake Trail allows you to monitor, diagnose and troubleshoot, and gain insight into your apps, pipelines and compute.\ntry snowpark observability\nFast Insights\nSection Title: Snowflake Trail for Observability > Effortless telemetry with one simple setting\nContent:\nGetting started with telemetry traditionally is a tedious process. Snowflake Trail eliminates the need for any agent installation, time-intensive setup or data export tasks, providing fast insights into application and pipeline performance. With just one simple setting, you can gain visibility into the performance of your Snowpark code and its resource usage, so you can quickly diagnose and debug your apps and pipeline development. Events are all within Snowflake with no need for additional data transfer.\nSection Title: Snowflake Trail for Observability > Reduce time to detect (TTD) and time to resolution (TTR)\nContent:\nSnowflake Trail provides a comprehensive set of telemetry signals, including metrics, logs and span events, to help developers better understand their applications and pipelines. These signals unite in Snowsight, Snowflake\u2019s user interface, to help developers debug and detect issues quickly.\nstart for free\nSection Title: Snowflake Trail for Observability > Bring Your Own Tools (BYOT) or use Snowsight\nContent:\nBuilt with OpenTelemetry standards, schema and open ecosystem integrations in mind, Snowflake telemetry and notification capabilities integrate with some of the most favored developer tools, including Datadog, Grafana, Metaplane, Monte Carlo, PagerDuty and Slack. Or simply use Snowsight, where developers can monitor and trace their pipelines, apps and runtime usage directly within Snowflake.\nSection Title: Snowflake Trail for Observability > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\nstart for free\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n[Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n[Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n[Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n[Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n[Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n[Technology](https://www.snowflake.com/en/solutions/industries/technology/)\nSection Title: Snowflake Trail for Observability > Where Data Does More\nContent:\nLearn * [Resource Library](https://snowflake.com/en/resources/)\nLive Demos\n[Fundamentals](https://www.snowflake.com/en/fundamentals/)\n[Training](https://www.snowflake.com/en/resources/learn/training/)\n[Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n[Snowflake University](https://learn.snowflake.com/en/)\n[Developer Guides](https://www.snowflake.com/en/developers/guides)\n[Documentation](https://docs.snowflake.com/)\n[Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n[Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\nCookie Settings\n[Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Legal](https://www.snowflake.com/en/legal/)\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\nSection Title: Snowflake Trail for Observability > Where Data Does More\nContent:\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/blog/observability-new-era-with-snowflake-trail/",
      "title": "Observability in Snowflake: A New Era with Snowflake Trail",
      "publish_date": "2024-08-15",
      "excerpts": [
        "blog\nSection Title: Category\nContent:\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\nProduct and Technology\nJun 10, 2024 | 5 min read\nSection Title: Observability in Snowflake: A New Era with Snowflake Trail\nContent:\nDiscovering and surfacing telemetry traditionally can be a tedious and challenging process, especially when it comes to pinpointing specific issues for debugging. However, as applications and pipelines grow in complexity, understanding what's happening beneath the surface becomes increasingly crucial. A lack of visibility hinders the development and maintenance of high-quality applications and pipelines, ultimately impacting customer experience. Comprehensive observability tools are a must to empower developers and data engineers to quickly identify and resolve issues.\nSection Title: Observability in Snowflake: A New Era with Snowflake Trail > Introducing Snowflake Trail\nContent:\nSnowflake Trail is a rich set of Snowflake capabilities that allows developers and data engineers to observe and act on their applications and data pipelines through Snowsight or third-party tools. Leveraging Snowflake's Query History, Event Tables , Alerts and Notifications as the telemetry foundation, Snowflake Trail provides enhanced visibility into data quality, pipelines and applications. Each of these signals empowers developers to monitor, troubleshoot and optimize their workflows with ease. Snowflake Trail builds upon the observability foundation already in Snowflake. You are probably familiar with the built-in observability, with capabilities such as Task History and Dynamic Tables observability . With this launch, we are expanding the scope of what and how you can observe with Snowflake.\nSection Title: ... > Effortless telemetry with one simple setting\nContent:\nSnowflake Trail is built with automated telemetry; no agent or setup tasks are needed. A default Event Table (public preview soon) is in the Snowflake database of every account, removing the need to create and manage your own custom event table. Snowflake Trail eliminates the need for any agent installation, tedious setup or data export tasks, providing fast insights into application and pipeline performance. With just one simple setting, you can gain visibility into the performance of your Snowpark code and its resource usage, so you can quickly diagnose and debug your apps and pipeline development. Events are all within Snowflake with no need for additional data transfer.\nSection Title: Observability in Snowflake: A New Era with Snowflake Trail > ... > Customer stories\nContent:\nOur customers have seen significant improvements in their application and pipeline development with Snowflake Trail\u2019s capabilities. \" *When working with Snowpark UDFs, some of the logic can become quite complex. In some instances, we had thousands of lines of Java code that needed to be monitored and debugged. With the new logging and tracing capabilities, we are able to investigate issues in our code or data much quicker and find performance issues much faster* ,\u201d said Nick Pileggi, Principal Solutions Architect at **phData** **Inc** ., in regards to migrating Spark and Hadoop applications to Snowpark. *\u201cEvent Tables have been invaluable, as we take our Snowflake Native App to market. By choosing to share events with us, our customers benefit from us being able to assist them without needing to manually extract diagnostic data and send it to us.\nSection Title: Observability in Snowflake: A New Era with Snowflake Trail > ... > Customer stories\nContent:\nWe are excited to see new Snowflake Trail features like the Log Explorer, which will help us hone in on the relevant information even faster, and Trace Viewer, which will help us remove performance bottlenecks in our code,\u201d* said James Weakley, Snowflake Data Superhero and co-founder at **Omnata,** a Snowflake Native App available in Snowflake Marketplace.\nSection Title: ... > Built-in observability experiences reduce time to detect (TTD) and time to resolution (TTR)\nContent:\nSnowflake Trail provides a comprehensive set of telemetry signals, including metrics, logs and span events, to give developers a deeper understanding of their applications and pipelines. Snowsight is where these signals are brought together to help developers debug and detect issues near instantly, thereby reducing TTD and TTR. Key capabilities include: * **Snowpark metrics (private preview):** Understand the CPU and memory consumption of your code in Snowpark (Python) stored procedures and functions, using the new Snowpark metrics. Support for other languages coming soon.\nSection Title: ... > Built-in observability experiences reduce time to detect (TTD) and time to resolution (TTR)\nContent:\n**Automatic Python DataFrame tracing (private preview):** Snowpark DataFrames allow developers to write queries in native Python. When you use DataFrames on Snowflake, those operations will now also appear on the trace view, allowing you to see the full execution of your pipeline. **User code profiler for Python (private preview):** Developers can attach a profiler to their stored procedure to understand where the most compute time is spent and better optimize their Python execution. **Log attributes (public preview):** Filter logs further; available for Java and JavaScript, Python support coming soon. **Serverless Alerts** **(public preview):** The power and evaluation logic of alerts are now available with the cost and warehouse optimization of serverless capabilities.\nSection Title: ... > Built-in observability experiences reduce time to detect (TTD) and time to resolution (TTR)\nContent:\nDevelopers can visualize what\u2019s going on with their pipelines and apps, and interact with logs, metrics and tracing directly within Snowsight using features like Log Explorer (Public Preview) to easily view and filter logs from Snowpark code.\n ... \nSection Title: ... > Built-in observability experiences reduce time to detect (TTD) and time to resolution (TTR)\nContent:\nLast but not least, Snowflake Trail also provides data-quality monitoring (general availability soon) as part of Snowflake Horizon. Customers get built-in data-quality solutions with out-of-the-box system metrics (such as null count) or custom metrics that they can define to monitor data quality. Data engineers and stewards can effectively monitor and report on degradation in data quality across their organization. **Simply use Snowsight or Bring Your Own Tools:** You can use Snowsight to monitor and trace pipelines, apps and resource usage directly in Snowflake.\nSection Title: ... > Built-in observability experiences reduce time to detect (TTD) and time to resolution (TTR)\nContent:\nEven better, Snowflake Trail adheres to the industry-standard [OpenTelemetry](https://opentelemetry.io/) specification and notification destinations, allowing easy integration with your favorite observability and customizable notification tools, including [Datadog](https://www.datadoghq.com/blog/snowflake-snowpark-monitoring-datadog/) , [Grafana](https://grafana.com/blog/2024/06/06/snowflake-data-visualization-all-the-latest-features-to-monitor-metrics-enhance-security-and-more/) , Observe, Metaplane, PagerDuty, Slack and Microsoft Teams.\nSection Title: Observability in Snowflake: A New Era with Snowflake Trail > ... > Get started with Snowflake Trail\nContent:\nSnowflake Trail marks a significant milestone in Snowflake's observability journey, addressing the long-standing observability challenges faced by our users. With its rich telemetry, built-in observability experiences and easy integration with third-party tools, Snowflake Trail is poised to revolutionize the way developers build, deploy and maintain applications and pipelines in Snowflake. Learn more about Snowflake Trail and take it for a spin .\n ... \nSection Title: ... > Reimagine Batch and Streaming Data Pipelines with Dynamic Tables, Now Generally Available\nContent:\nSaras Nowak | Shiyi Gu\nMay 6, 2024 | 7 min read\nProduct and Technology"
      ]
    },
    {
      "url": "https://www.metaplane.dev/snowflake-native-app",
      "title": "Snowflake Native App",
      "excerpts": [
        "New Metaplane and Datadog join forces to bring software and data observability together.\nLearn More\nPlatform\nPlatform Overview Learn all about Metaplane\nData CI/CD Prevent data quality issues in PRs\nMonitoring and anomaly detection Be the first to know when something breaks\nSchema change alerts Increase awareness for the entire data team\nLineage and impact analysis Data pipeline visibility from source to usage\nJob monitoring Eliminate pipeline latency issues\nData Insights Explore how your data is being used\nAlerting One rally point for all of your stakeholders\nStart monitoring today Sign up, connect your tool(s) in minutes, and start monitoring right away.\nCustomers Pricing Integrations\nIntegrations\nIntegrations overview Learn all the tools Metaplane integrates with\n ... \nSection Title: DATA WAREHOUSES > Ingestion\nContent:\nAirbyte Fivetran\n ... \nSection Title: DATA WAREHOUSES > Ingestion > Transformations > Business intelligence\nContent:\nLooker Metabase Mode PowerBI Sigma Tableau Hex\nResources\n[Docs Get started with Metaplane in under 15 minutes](https://docs.metaplane.dev/) Resource Library Webinars, podcasts, live events, and white paper downloads Changelog See the latest Metaplane product updates Data Observability Guides How to get started with the fundamentals of data observability Blog \u2014 In Data We Trust Guides and perspectives from data leaders\nBook a demo\nSnowflake Native App\nSection Title: Use your existing credits with our Snowflake Native App\nContent:\nAll of Metaplane\u2019s robust data observability features, brought directly into your Snowflake environment.\nTalk to sales\nPay with credits\nSkip the procurement process\nHelping the world\u2019s best data teams TRUST THEIR DATA\nSection Title: ... > Get our Snowflake native app and use your existing credits\nContent:\n[Setup now, for free](https://app.snowflake.com/marketplace/listing/GZTSZ7NSX7E/metaplane-metaplane-data-observability-platform)\nSection Title: ... > End-to-end observability inside Snowflake\nContent:\nAll of Metaplane\u2019s robust data observability features brought directly into your Snowflake environment.\nSection Title: ... > Your data never leaves your warehouse\nContent:\nWith our native app, you can install monitors directly within your warehouse, helping keep your data all the more secure.\nSection Title: Use your existing credits with our Snowflake Native App > ... > Use your Snowflake credits\nContent:\nPay for monitors with your existing Snowflake credits and skip the extra procurement step.\nSection Title: ... > Without Metaplane, we wouldn\u2019t be as proactive with data quality. There would be a lot of u...\nContent:\nAdam Smith\nAnalytics Engineer @ Imperfect Foods\nSection Title: Book a Demo\nContent:\nWe\u2019ll show you the product (yes, really) and talk through how to use Metaplane to achieve your data observability goals.\nIf you cannot find a time that works for you, please email team@metaplane.dev and we'll try to accommodate another time.\nSection Title: Book a Demo > Marion Pavillet\nContent:\nSenior Analytics Engineer\nSenior Analytics Engineer\nSection Title: Book a Demo > Book a Demo\nContent:\nWe\u2019ll show you the product (yes, really) and talk through how to use Metaplane to achieve your data observability goals.\nIf you cannot find a time that works for you, please email team@metaplane.dev and we'll try to accommodate another time.\nBuild trust in your data\nDetect and fix problems quickly\nMonitor what matters most to you\nSection Title: ... > 95% test coverage. In just a few clicks, Mux got full visibility into the quality and perfo...\nContent:\nMarion Pavillet\nSenior Analytics Engineer\nData observability for high-leverage data teams. Save time and preserve trust by being the first to know of data quality issues.\n ... \nSection Title: Book a Demo > Book a Demo > Platform\nContent:\nMonitoring and anomaly detection\nUsage analytics\nData CI/CD\nSchema change alerts\nLineage and impact analysis\nJob monitoring\nAlerts\nIntegrations\nSection Title: Book a Demo > Book a Demo > RESOURCES\nContent:\n[Docs](https://docs.metaplane.dev)\nChangelog\nBlog\nResources\nData observability guide\ndbt Alerting tool\nInteractive query builder"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up",
      "title": "Event table overview | Snowflake Documentation",
      "excerpts": [
        "Developer Logging, Tracing, and Metrics Event table overview\nSection Title: Event table overview \u00b6\nContent:\nAs your Snowflake objects\u2014including procedures and UDFs\u2014emit telemetry data, Snowflake collects the data in an event table whose\ndata is available for queries. Snowflake includes an event table by default, but you can also create a new one.\nTo collect telemetry data, you must have an active event table and have set telemetry levels to allow data collection. If you don\u2019t already have an active event table,\nSnowflake makes the default event table the active event table.\nWhen collecting telemetry data, you incur costs. To understand these costs\u2014or to reduce or avoid these costs\u2014see Costs of telemetry data collection .\nSection Title: Event table overview \u00b6 > What is an event table? \u00b6\nContent:\nAn event table is a special kind of database table with a predefined set of columns. The table\u2019s structure supports the data model for [OpenTelemetry](https://opentelemetry.io/) , a framework for handling telemetry data. When an\nevent table is active, Snowflake collects telemetry data in the table\u2014including data that\nSnowflake itself generates and data that you emit by instrumenting your handler code using certain APIs. You can view the collected data\nby executing SQL queries.\nAfter installation, Snowflake includes a default event table called\nSNOWFLAKE.TELEMETRY.EVENTS. This event table is active and collects data until you deactivate it. You can also create your own .\nSection Title: Event table overview \u00b6 > Use the default event table \u00b6\nContent:\nIf you do not set an active event table, Snowflake uses as the active event table a default event table named SNOWFLAKE.TELEMETRY.EVENTS.\nYou can also create your own event tables for specific uses.\nBy default, Snowflake also includes a predefined view called SNOWFLAKE.TELEMETRY.EVENTS_VIEW view ,\nwith which you more securely make event table data available to a range of users. You can manage access to the view with a row access policy .\nNote\nThe default event table supports only a subset of DDL commands supported for event tables you create or for regular tables. For more\ninformation, see Working with event tables .\nSection Title: Event table overview \u00b6 > ... > Roles for access to the default event table and EVENTS_ VIEW \u00b6\nContent:\nSnowflake includes the following predefined application roles you can use to manage access to the default event table and EVENTS_VIEW view:\nA person with the ACCOUNTADMIN role can access the default event table and EVENTS_VIEW view and can grant the roles described here\nto other roles for access to them.\nYou must grant these roles to other roles, rather than to a user. For example, you might grant the EVENTS_ADMIN role to another admin\nrole you\u2019ve created for broader administrative use.\n```\nGRANT APPLICATION ROLE SNOWFLAKE . EVENTS_ADMIN TO ROLE my_admin_role ; \n\n GRANT APPLICATION ROLE SNOWFLAKE . EVENTS_VIEWER TO ROLE my_analysis_role ;\n```\nCopy\nEVENTS_VIEWER :\nRole with privileges to execute a SELECT statement on the EVENTS_VIEW view .\nEVENTS_ADMIN :\nRole with the following privileges:\n ... \nSection Title: Event table overview \u00b6 > Use a custom event table \u00b6\nContent:\nTo create a new event table, execute the CREATE EVENT TABLE command and specify a name for the event table.\nNote\nIf you don\u2019t create an event table, Snowflake uses the default event table to collect telemetry data.\nCreate an event table by executing the CREATE EVENT TABLE command,\nspecifying a name for the event table.\nAssociate the event table with an object by executing the ALTER  command on the object, setting the EVENT_TABLE parameter to the name of your event table.This sets the scope of data captured by the event table to the object with which you\u2019re associating the table.\n ... \nSection Title: Event table overview \u00b6 > Use a custom event table \u00b6 > Associate an event table with an object \u00b6\nContent:\nTo specify the object for which an event table is active, execute the ALTER  command on the object.\nAssociating an event table with a database is an Enterprise Edition feature.\nEnsure that you\u2019re using a role that has the required privileges .\nExecute the ALTER  command on the object, setting the EVENT_TABLE parameter to the name of\nyour event table.Setting this parameter sets the object as the scope within which events will be collected in the specified event table.For example, to associate the event table with a database, use ALTER DATABASE, as in the following example:CopyIn this example, Snowflake\u2014depending on how you\u2019ve specified telemetry levels \u2014captures\ntelemetry data for procedures and UDFs in `my_database` in the `telemetry_database.telemetry_schema.my_events` event table.\nSection Title: Event table overview \u00b6 > ... > Associate an event table with an object \u00b6 > Supported objects \u00b6\nContent:\nThe following table lists the objects with which you can associate an event, along with the privileges required to make the association.\nSection Title: Event table overview \u00b6 > ... > Associate an event table with an object \u00b6 > Supported objects \u00b6\nContent:\n| Object | Privileges required | Scope of objects whose data is collected |\n| Account | * ACCOUNTADMIN role. |  |\nSection Title: Event table overview \u00b6 > ... > Associate an event table with an object \u00b6 > Supported objects \u00b6\nContent:\nOWNERSHIP privilege for the account.\nOWNERSHIP or INSERT privileges for the event table . |Procedures and UDFs in the account. Use this for the broadest scope. |\n|Database |* ACCOUNTADMIN role.\nOWNERSHIP or INSERT privileges for the event table . |Procedures and UDFs in the specified database. |\nAn order of precedence determines which event table is used to collect telemetry data for an object. In that precedence order, an event\ntable associated with a database takes precedence over an event table associated with an account.\nAccount \u00bb Database\nSection Title: Event table overview \u00b6 > ... > Associate an event table with an object \u00b6 > Supported objects \u00b6\nContent:\nIn other words, if you have event tables associated with both your account and a database `my_database` , telemetry data generated by\nobjects in `my_database` will be collected in the database\u2019s event table. For other databases in the account that don\u2019t have an\nassociated event table, telemetry data will be collected in the event table associated with the account.\n ... \nSection Title: Event table overview \u00b6 > Use a custom event table \u00b6 > Set the event table for a database \u00b6\nContent:\nTo set up the event table named `my_events` in the schema `my_schema` in the database `my_database` as\nthe active event table for the database `my_database` , execute the following statement:\n```\nALTER DATABASE my_database SET EVENT_TABLE = my_database . my_schema . my_events ;\n```\nCopy\nTo disassociate an event table from a database, execute the ALTER DATABASE command and unset the EVENT_TABLE parameter. For example:\n```\nALTER DATABASE my_database UNSET EVENT_TABLE ;\n```\nCopy\nYou can confirm the EVENT_TABLE value with the SHOW PARAMETERS command:\n```\nSHOW PARAMETERS LIKE 'event_table' IN DATABASE my_database ;\n```\nCopy\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\nSection Title: Event table overview \u00b6 > Use a custom event table \u00b6 > Set the event table for a database \u00b6\nContent:\n[Get your own certification](https://learn.snowflake.com)\n ... \nSection Title: Event table overview \u00b6 > Privacy Preference Center > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Details\u200e"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/getting-started-with-snowflake-trail-for-observability/",
      "title": "Getting Started with Snowflake Trail for Observability",
      "excerpts": [
        "Section Title: Getting Started with Snowflake Trail for Observability > Overview\nContent:\nSnowflake Trail is Snowflake's suite of observability capabilities that enable its users to better monitor, troubleshoot, debug and take actions on pipelines, apps, user code and compute utilizations.\nAs you can see above, Snowflake Trail utilizes core observability data \u2014 logs, metrics, traces, events, alerts, and notifications \u2014 to provide comprehensive workload monitoring across AI, applications, pipelines, and infrastructure.\nThis quickstart is intended to help tie together all the components of Snowflake Trail, and in turn, help you get started with Observability in Snowflake. While this quickstart will walk you through the basics of enabling and viewing telemetry, you will need to dive deeper into each area in order to fully understand Snowflake Trail. So wherever possible, links will be provided to additional quickstarts, documentation, and resources.\n ... \nSection Title: Getting Started with Snowflake Trail for Observability > Overview > What You'll Need:\nContent:\nA Snowflake account. If you do not have a Snowflake account, you can register for a [free trial account](https://signup.snowflake.com/?utm_source=snowflake-devrel&utm_medium=developer-guides&utm_cta=developer-guides) .\nA Snowflake account login with the `ACCOUNTADMIN` role, or a custom role with privileges to:\nSet account-level parameters ( `ALTER ACCOUNT` )\nView and query event tables ( `SELECT` on `SNOWFLAKE.TELEMETRY.*` )\nAccess monitoring features in Snowsight\nBasic familiarity with SQL and Snowflake concepts\nAbout 45 minutes to complete this quickstart\nSection Title: Getting Started with Snowflake Trail for Observability > Overview > What You'll Build:\nContent:\nA Snowflake account with telemetry collection enabled at the account level\nUnderstanding of how to navigate and use Snowflake's built-in monitoring tools\nFoundation for implementing comprehensive observability across your Snowflake workloads\nSection Title: Getting Started with Snowflake Trail for Observability > Data Source Overview\nContent:\nObservability in Snowflake comes in two main categories: System Views and Telemetry.\n**System Views** provide historical data about your Snowflake account through views and table functions in the following schemas:\nThe [Snowflake Information Schema](https://docs.snowflake.com/en/sql-reference/info-schema) ( `INFORMATION_SCHEMA` ) in every Snowflake database\nThe [Account Usage](https://docs.snowflake.com/en/sql-reference/account-usage) ( `ACCOUNT_USAGE` and `READER_ACCOUNT_USAGE` ) in the Snowflake database\nThe [Organization Usage](https://docs.snowflake.com/en/sql-reference/organization-usage) ( `ORGANIZATION_USAGE` ) in the Snowflake database\nSection Title: Getting Started with Snowflake Trail for Observability > Data Source Overview\nContent:\n**Telemetry data** , on the other hand, is delivered exclusively through [event tables](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up) . An event table is a special kind of database table with a [predefined set of columns](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-columns) that follows the data model for [OpenTelemetry](https://opentelemetry.io/) - a leading the industry standard for collecting and structuring observability data across systems.\nThis distinction is important because of the default behavior of each:\nSystem Views are always available and automatically populated with data about your Snowflake usage\nTelemetry data requires you to **explicitly enable collection** by setting levels for logging, metrics, and tracing\nSection Title: Getting Started with Snowflake Trail for Observability > Enabling Telemetry\nContent:\nBy default, Snowflake includes a [predefined event table](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up) ( `SNOWFLAKE.TELEMETRY.EVENTS` ) that is used if you don't specify an active event table. You can also [create your own event tables](https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up) for specific uses.\nMost telemetry levels can be set at the account, object, or session level. And while many of the telemetry levels can be set via Snowsight, some require using SQL commands for full flexibility.\nFor this quickstart, we will focus on enabling telemetry at the account level. We will show you both the SQL and Snowsight methods, and use the default table.\nComplete **one** of the following:\nSection Title: ... > (Option 1): Setting Telemetry Levels via Snowsight\nContent:\nYou can use Snowsight to set telemetry levels at the account level.\nIn the navigation menu, select **Monitoring** \u00bb **Traces and Logs** .\nOn the Traces & Logs page, select **Set Event Level** .\nFor **Set logging & tracing for** , ensure **Account** is selected.\nSet your desired levels:\nFor **All Events** , select **On**\nFor **Logs** , select **INFO**\nEnsure all other fields show as **On** .\nClick **Save** .\nYou can see the **Set Event Level** dialog box below.\nSection Title: ... > (Option 2): Setting Telemetry Levels via SQL\nContent:\nOpen a new SQL worksheet or a [workspace](https://app.snowflake.com/_deeplink/#/workspaces?utm_source=snowflake-devrel&utm_medium=developer-guides&utm_content=getting-started-with-snowflake-trail-for-observability&utm_cta=developer-guides-deeplink) .\nRun the following:\n-- Switch to ACCOUNTADMIN\nUSE ROLE ACCOUNTADMIN;\n-- Set account level values\nALTER ACCOUNT SET LOG_LEVEL = 'INFO';\nALTER ACCOUNT SET METRIC_LEVEL = 'ALL';\nALTER ACCOUNT SET TRACE_LEVEL = 'ALWAYS';\n```\n\nCopy\n```\nNote that valid and default values are as follows:\n ... \nSection Title: Getting Started with Snowflake Trail for Observability > Logs\nContent:\n**Logs** are structured records of events that occur during the execution of your Snowflake workloads. They provide detailed information about what happened during code execution, including informational messages, warnings, errors, and debug information. Logs are essential for troubleshooting issues, understanding application behavior, and monitoring the health of your systems.\nSection Title: Getting Started with Snowflake Trail for Observability > Logs > Why Logs Are Useful\nContent:\nLogs help you:\nDebug issues by providing detailed error messages and stack traces\nMonitor application behavior and performance\nAudit operations and track important events\nUnderstand the flow of execution in complex procedures\nIdentify patterns in application usage or errors\nSection Title: Getting Started with Snowflake Trail for Observability > Logs > Accessing Logs in Snowsight\nContent:\nTo view logs in Snowsight:\nNavigate to **Monitoring** \u00bb **Traces & Logs**\nClick on the **Logs** tab to switch from the default traces view.\n(Optional) Use the filters to find specific logs. For example:\n**Time Range** can be set either by using the drop-down or by clicking on the graph.\n**Severity** can be used to select specific log levels (DEBUG, WARN, etc).\n**Languages** allows filtering by handler code language (Python, Java, etc).\n**Database** allows filtering by specific procedures, functions, or applications.\n**Record** allows selecting Logs, Events, or All.\n ... \nSection Title: Getting Started with Snowflake Trail for Observability > AI Observability\nContent:\n**Evaluations** : Use AI Observability to systematically evaluate the performance of your generative AI applications and agents using the LLM-as-a-judge technique. You can use metrics, such as accuracy, latency, usage, and cost, to quickly iterate on your application configurations and optimize performance.\n**Comparison** : Compare multiple evaluations side by side and assess the quality and accuracy of responses. You can analyze the responses across different LLMs, prompts, and inference configurations to identify the best configuration for production deployments.\n**Tracing** : Trace every step of application executions across input prompts, retrieved context, tool use, and LLM inference. Use it to debug individual records and refine the app for accuracy, latency, and cost.\n ... \nSection Title: Getting Started with Snowflake Trail for Observability > Conclusion And Resources > What You Learned\nContent:\n**Enabled telemetry collection** at the account level to start capturing logs, metrics, and traces\n**Know the difference** between System Views (historical data) and Telemetry data (event-driven observability)\n**Utilize traces** to gain end-to-end insight of execution flows and identify performance bottlenecks\n**Analyze logs** to debug issues and monitor application behavior\n**Leverage Query History** to optimize query performance and troubleshoot database operations\n**Monitor data loading** activities through Copy History for all ingestion methods\n**Track automated operations** using Task History for pipeline monitoring\n**Observe Dynamic Tables** refresh patterns and data freshness\n**Explore AI observability** concepts for monitoring AI/ML workloads and Cortex AI functions\n**Build a foundational understanding** of the tools available for observability across your Snowflake workloads"
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}

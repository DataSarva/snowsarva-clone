{
  "extract_id": "extract_7ce532b572bb4fc7be9cc9bbdc6adef6",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/developer-guide/builders/observability",
      "title": "Observability in Snowflake apps | Snowflake Documentation",
      "publish_date": "2016-09-18",
      "excerpts": [
        "Developer Observability\n\n# Observability in Snowflake apps \u00b6\n\nThrough observability built into Snowflake, you can ensure that your applications are running as efficiently as possible.\nUsing the practices and features described in this topic, you can make the most of observability features that show you where you\ncan improve your code.\n\n## What is observability? \u00b6\n\nIn an observable system, you can understand what\u2019s happening internally through external evidence generated by the system\u2014evidence\nthat includes telemetry data, alerts, and notifications.\n\nThrough the evidence of internal functioning it provides, observability makes it easier for you to troubleshoot hard-to-understand behaviors\non a production system. This is especially true in a distributed system, where evidence collected from observation provides a view of\nbehavior across multiple components. Rather than disrupting a production environment to diagnose issues, you can analyze the collected\ndata from it.\n\nWith an observable system, you can start to answer questions such as the following:\n\n* How well is the system performing?\n* Where is there latency and what\u2019s causing it?\n* Why is a particular component or process not working as it should?\n* What improvements can be made?\n\n## Observability in Snowflake \u00b6\n\nSnowflake supports a model that provides built-in observable data while also giving you ways to add more instrumentation where you need it.\nWhile Snowflake provides support for telemetry data such as logs, metrics, and traces (which are typical of observability), it also\nincludes other features you can use to keep track of system usage and performance.\n\nThe following lists features you can use to receive and analyze system performance and usage.\n\n|Collected telemetry data |As your application generates logs, metrics, and traces, Snowflake collects that telemetry data in an event table. Using\nSnowsight, you can explore the data, looking for patterns.\n\nYou can emit custom telemetry into the event table to provide contextual, domain-specific information to expedite debugging. |\n| --- | --- |\n|History Tables |Use the following views and their associated tables to monitor all usage in your account.\n\n* Query History\n* Copy History\n* Tasks |\n|Alerts and notifications |Alerts allow for customizable triggering conditions, actions, and a schedule, in combination with notification integrations for proactive monitoring. |\n|Extensibility with third-party tools |The Snowflake event table adopts [OpenTelemetry](https://opentelemetry.io/docs/) standards, so your\nSnowflake telemetry can easily be consumed by other ecosystem tools. |\n\n## Telemetry data collected for analysis \u00b6\n\nAs code in your application executes, you can have Snowflake collect data from the code that tells you about the application\u2019s internal\nstate. Using this telemetry data\u2014collected in a Snowflake event table (your account has one by default )\u2014you can look for bottlenecks and other opportunities to optimize.\n\nTelemetry data must be emitted as your code executes. Snowflake emits some of this data on your code\u2019s behalf without\nyou needing to instrument your code. You can use also APIs included with Snowflake to emit telemetry data from specific parts of your code.\n\nAs described below, you can analyze the collected data by querying the event table or using the visualizations that capture the data\nin Snowsight.\n\n### Types of telemetry data \u00b6\n\nTo ensure that the telemetry data you collect is broadly useful, Snowflake telemetry is built on the standard [OpenTelemetry](https://opentelemetry.io/docs/) (sometimes called OTel) framework, an incubating project of the Cloud Native Compute Foundation. Through this framework (and APIs and\ntools designed for it), you can reuse collected data with tools besides Snowflake .\nThrough OpenTelemetry, you can instrument application code to add observability where you want it.\n\nSnowflake event tables collect log, span, and metrics data in the OpenTelemetry data model. The following describes each type of telemetry\ndata collected in an event table.\n\n|Logs |Logs record individual operations performed by code. Each log message is generated at\na discrete point during the execution of the code.\n\n**Instrumenting code** You can log from your code using libraries standard for the language you\u2019re using, as listed in Logging from handler code .\n\n**Viewing data** You can view log messages for analysis\neither by querying the event table or looking at the visualizations provided in Snowsight.\n\nThe following image from Snowsight shows a list of collected log messages for a two-hour period in a single database.\n\n |\n| --- | --- |\n|Metrics |Metrics are measurements calculated over a time period. These values include CPU and memory measurements.\n\n**Instrumenting code** Snowflake emits metrics data automatically as your code executes, so you don\u2019t need to instrument your code.\n\n**Viewing data** You can view metrics data for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\n\nThe following image from Snowsight shows changes in collected metrics data for the execution of a user-defined function.\n\n |\n|Traces |Traces show distributed events as data flows through a system. In a trace, you can see where time is spent as processing flows\nfrom component to component.\n\nYou can emit trace events\u2014both within the default span Snowflake creates or from a custom span you create\u2014using libraries\nstandard for the language you\u2019re using, as listed in Logging from handler code .\n\n**Instrumenting code** You can emit trace events from your code using libraries standard for the language you\u2019re using, as listed in Event tracing from handler code .\n\n**Viewing data** You can view trace events for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\n\nThe following image from Snowsight shows the spans resulting as a UDF executes.\n\n |\n\n## Telemetry best practices \u00b6\n\nUse the following best practices to get the most out of observablity in Snowflake.\n\n* Set up your environment to capture telemetry data before you need it\n* Optimize procedures with query telemetry\n* Cache redundant DataFrame operations\n* Manage the amount of telemetry data received for UDFs\n* Optimize user-defined functions with query telemetry\n\n### Set up your environment to capture telemetry data before you need it \u00b6\n\nYou can\u2019t analyze data that you haven\u2019t collected, so it\u2019s best to start collecting telemetry data so you\u2019ll have it when you need it.\nAs your deployment grows, your need to understand how your code is performing grows.\n\nUse the following best practices:\n\n* Enable telemetry data collection for your Snowflake environment.\n  \n  To collect the data you\u2019ll need, ensure that you have an active event table.\n* To ensure you\u2019re collecting telemetry data you want, set telemetry levels to\n  useful thresholds.\n  \n  At first, you\u2019ll want to set these levels to ensure that you\u2019re collecting data. For example, set log levels to at least WARN for any\n  production or business critical jobs. Over time, you might adjust these levels to meet changing needs.\n  \n  Organize your production stored procedures, UDFs, and other objects under a database or schema so you can simply enable warning logs\n  for that database or schema. This saves the trouble of managing settings for separate objects.\n* To generate data for troubleshooting, add log statements or trace events to your production jobs.\n  \n  When you use standard logging libraries such as Java\u2019s SLF4J or Python\u2019s logging libraries, Snowflake routes logs from those packages to\n  your event table automatically.\n  \n  For tracing, you can use telemetry libraries included with Snowflake.\n* To include in trace data parts of the handler\u2019s processing that you want to measure, add custom spans to your stored procedure handler code.\n  \n  Along with the built-in spans from Snowflake objects, Snowflake represents custom spans you create in the trace diagram. With custom\n  spans, you can capture data about arbitrary parts of your code\u2019s processing to see how long those parts take to execute. You can also\n  attach arbitrary metadata to custom spans to add descriptions to the data for troubleshooting and optimizing.\n\n### Optimize procedures with query telemetry \u00b6\n\nIn the Query Telemetry trace diagram, you\u2019ll find data about all the spans emitted from a query.\n\n* The horizontal axis displays duration. A span that appears longer horizontally took longer to complete than a shorter\n  span.\n* The vertical axis displays the call hierarchy. In that hierarchy, any span that is directly under another span is a \u201cchild\u201d of\n  the \u201cparent\u201d span above it.\n\nYou can use this diagram to find opportunities for optimization in stored procedures. Using what you see in the diagram as a starting\nplace, you can take steps to optimize your code.\n\nFor example, you might organize sequential operations so they execute in parallel using libraries like joblib. [Joblib](https://joblib.readthedocs.io/en/stable/) is a set of\ntools for adding pipelining to Python code. With it, you can more easily write parallel code.\n\n### Cache redundant DataFrame operations \u00b6\n\nWhen you have a chain of DataFrame operations that is used multiple times, you\u2019ll see them in the trace diagram as a span for each\nDataFrame action. Depending on the query\u2019s complexity, this span can be quite long.\n\nFor example, in the code below the same DataFrame chain is called in multiple contexts:\n\n```\ncount = session . table ( ... ) . select () . filter () . join () . count () \n\n if count > 0 : \n  session . table ( ... ) . select () . filter () . join () . write . save_as_table ( ... ) # same query as the count, this will execute again \n else : \n  session . table ( ... ) . select () . filter ( 'other criteria' ) . join () # nearly same query as the count\n```\n\nCopy\n\nUsing caching improves performance by caching the intermediate DataFrame as a temporary table, reducing redundant queries:\n\n```\ncached_df = session . table ( ... ) . select () . filter () . join () . cache_result () \n count = cached_df . count () \n\n if count > 0 : \n  cached_df . write . save_as_table () # reuses the cached DF \n else : \n  cached_df\n```\n\nCopy\n\n### Manage the amount of telemetry data received for UDFs \u00b6\n\nWhen adding code to collect telemetry data with UDFs, remember that the UDF execution model can mean many more rows in the event table\nthan for a procedure.\n\nWhen a UDF is called on every input row, your handler code emits logging statements or span events for every row of the input dataset.\nFor example, a dataset of 10 million rows passed to a UDF would emit 10 million log entries.\n\nConsider using the following patterns when adding logs and span events to UDFs:\n\n* Initially, use logging levels designed to reduce the number of entries recorded.\n  \n  Use DEBUG- or INFO-level logging statements and set the logging level to WARN in production. If an issue is found, you can lower the\n  logging level to DEBUG or INFO for the duration of the debugging session.\n* Use try/catch blocks to isolate the code from which you want to emit logging data.\n  \n  Using try/catch can be useful to catch any unexpected UDF input, log it as a WARN-level log for awareness, and return a default value.\n* Use condition statements to log only for scenarios that are meaningful to you.\n  \n  With if/else statements or other constraints, you can control the volume of logging output.\n\n### Optimize user-defined functions with query telemetry \u00b6\n\nWhen a UDF is called, Snowflake executes it in parallel by creating an instance of the handler code for each input row. You\u2019ll see each of\nthese instances represented as its own span in a trace diagram.\n\nYou can use these spans to troubleshoot slow queries and find opportunities to improve performance. For example, you might see scenarios\nsuch as the following:\n\n* One or more instances of your UDF code might receive a row with data that is significantly larger or otherwise unlike the rest of your\n  data. When this happens, that instance might take much longer to complete, and therefore its span is much longer.\n* Depending on your query\u2019s input partitioning and preceding clauses, a minority of the instances might receive an outsized amount of\n  input data.\n\nThe following image shows a span for each row passed to a UDF, where one span\u2019s longer duration suggests that the row might have larger\ndata than the others.\n\n## Alerts and notifications for time-sensitive response \u00b6\n\nYou can use Snowflake alerts and notifications to have your system reveal what\u2019s going on inside, then take action or send information\nabout system state. Unlike telemetry data, which you collect and analyze later, alerts and notifications are useful when you want an\nimmediate response to what\u2019s happening in the system.\n\n* With an alert , you can specify a condition, action, and schedule, then specify that the action should take\n  place when the condition and schedule details are met.\n  \n  For example, you might use an alert to monitor complex conditions that you specify in SQL. The most common action after an alert\n  condition is met is to send a notification. Snowflake supports sending notifications to email, cloud service provider queues, Slack,\n  PagerDuty, and Microsoft Teams.\n* With a notification , you can use included stored procedures to send messages to\n  destinations such as email addresses , webhooks (for client tool integrations such as a chat tool), or to a queue hosted by a cloud service .\n\n### Alerts and notifications best practices \u00b6\n\nUse the following practices to improve observability by refining and increasing the amount of information you receive from the system.\n\n* Avoid duplicating event evaluation.\n  \n  You can avoid duplicating evaluation on events by accounting for the latency between the alert schedule and execution. To do this,\n  specify alert timestamps using SCHEDULED\\_TIME and LAST\\_SUCCESSFUL\\_SCHEDULED\\_TIME instead of using CURRENT\\_TIMESTAMP .\n  \n  For more information, see Specifying timestamps based on alert schedules .\n* Enrich an alert action or notification with query results.\n  \n  You can check the results from the SQL statement specified by an alert condition. To obtain the query results, do the following:\n  \n    1. Retrieve the query ID for the alert condition\u2019s SQL statement by calling GET\\_CONDITION\\_QUERY\\_UUID .\n    2. Pass the query ID to RESULT\\_SCAN to obtain the query results.\n* Log a result or take automated action in addition to sending a notification.\n  \n  You can specify that an alert action runs a task or logs a new row to a table whenever an alert\n  condition is met. For example, you might do this if you\u2019ll take an action in Snowflake each time the alert condition is met.\n  \n  If you intend to perform a complex action after a condition is met, ensure that your warehouse is an appropriate size.\n\n## Tools for analysis and visualization \u00b6\n\nYou can use the telemetry data collected in your event table with other tools that support the OpenTelemetry data model.\n\nThrough Snowflake support of OpenTelemetry, you can use APIs, SDKs, and other tools to instrument, generate, collect, and export telemetry\ndata. Using these tools, you can more thoroughly analyze software performance and behavior. Because a Snowflake event table uses this\nwidely-adopted standard, you might also be able to integrate your organization\u2019s observability tools with event tables with little overhead.\n\nConsider integrating your external tools in one of the following ways:\n\n* If your observability tools can read from external sources, point them to the event table.\n* If your tools use a push model\u2014in which telemetry data must be sent to the tool\u2014consider using a stored procedure with external access to regularly read telemetry data from\n  the event table and emit it to your tool.\n\nThe following lists tools you might integrate with Snowflake event tables:\n\n* [Snowflake integration for Datadog](https://docs.datadoghq.com/integrations/snowflake_web/)\n* Snowflake integration for Grafana dashboard\n  \n    + [Snowflake data source for Grafana](https://grafana.com/docs/plugins/grafana-snowflake-datasource/latest/)\n    + [Snowflake integration for Grafana Cloud](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/integrations/integration-reference/integration-snowflake/)\n  \n  For an introduction to using Grafana with Snowflake, see [How to monitor Snowflake with Grafana Cloud](https://grafana.com/blog/2023/05/24/how-to-monitor-snowflake-with-grafana-cloud/) .\n* [Observe for Snowflake](https://app.snowflake.com/marketplace/listing/GZTYZY3AR0U/observe-inc-observe-for-snowflake) , Observe\u2019s native app for observability\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/native-apps/event-about",
      "title": "Use logging and event tracing for an app | Snowflake Documentation",
      "publish_date": "2024-01-15",
      "excerpts": [
        "Developer Snowflake Native App Framework Configure logging and event tracing for an app\n\n# Use logging and event tracing for an app \u00b6\n\n Feature \u2014 Generally Available\n\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\n\nThis topic describes how providers can configure a Snowflake Native App to record log messages and trace events.\n\n## About log messages and trace events in an app \u00b6\n\nThe Snowflake Native App Framework supports using the Snowflake logging and tracing functionality to gather information about an app. Providers can configure an app to record and analyze the following:\n\n* Log messages \u2014 Independent, detailed messages with information about\n  the state of a specific piece app code.\n* Trace events \u2014 Structured data that providers can\n  use to get information spanning and grouping multiple parts of your code. Trace events allows an app to emit information related\n  to its performance and behavior.\n* Metrics \\- Information about stored procedure and UDF resource consumption\n  based on the CPU and memory metrics that Snowflake generates.\n\nTo configure an app to emit log messages and trace events, providers set the log and trace levels in the manifest file.\nSee Set the log and trace levels for an app .\n\nProviders can also configure an app to use event sharing to allow the consumer to share the log messages\nand trace events with the provider. See About event sharing for\nmore information.\n\n## About event sharing \u00b6\n\nEvent sharing allows the provider to collect information about an app\u2019s performance and behavior.\nA provider can configure an app to request that the consumers share the log messages\nand trace events with the provider. Event sharing requires that the provider and consumer configure an\nevent table in their account to store the log messages and trace events emitted by the app.\n\nWhen event sharing is enabled, the log messages and trace events that are inserted into the event table in the\nconsumer account are also inserted into the event table in provider account.\n\n## Considerations when using event sharing \u00b6\n\nBefore configuring logging and event sharing for an app, providers must consider the following:\n\n* Providers are responsible for all costs associated with event sharing on the provider side, including data\n  ingestion and storage.\n* Providers must have an account to store shared events in each region where you want to support event sharing.\n* Providers must define the default log level and trace level for an app in the manifest file.\n\n## Considerations when migrating from the previous event sharing functionality \u00b6\n\nWhen migrating from the existing event sharing functionality to use event definitions, providers\nshould consider the following.\n\n* The previous event sharing functionality is equivalent to the OPTIONAL ALL event definition.\n* Published versions and patches of an app that used the previous functionality will have the\n  OPTIONAL ALL event definition by default. Providers do not need to add this event definition\n  to the manifest file.\n\nTo begin using event definitions, providers can add supported event definitions to the manifest\nfile. This is applicable to new apps as well as new versions and patches of existing apps.\n\nNote\n\nTo being begin requesting more granular log and event sharing, providers only have to add\nevent definitions to the manifest file. No other actions are required for providers.\n\n## Workflow - Set up event sharing for an app \u00b6\n\nEvent sharing allows consumers to share log messages and trace events with the provider.\n\nThe following workflow shows how to set up and enable event sharing for an app:\n\n1. The provider sets the log and trace levels for the app.\n2. The provider adds event definitions to the manifest file.\n   \n   Event definitions act as filters on the log messages and trace events emitted by the app.\n   Providers can configure event definitions to be required or optional.\n3. The provider sets up an event table in their organization.\n4. The provider publishes the app.\n\nWhen a consumer installs an app, they can set up an event table and enable event sharing.\nSee [Enable logging and event sharing for an app](https://other-docs.snowflake.com/en/native-apps/consumer-enable-logging) for more information on the consumer requirements for event sharing.\n\n## Monitor consumer application health \u00b6\n\nYou can use the `LAST_HEALTH_STATUS` and `LAST_HEALTH_STATUS_UPDATED_ON` columns\nof the APPLICATION\\_STATE view to monitor the health of consumer instances of your\napp. The `LAST_HEALTH_STATUS` column has the following possible values:\n\n* `OK` : The consumer instance is healthy.\n* `FAILED` : The consumer instance is in an error state.\n* `PAUSED` : The consumer manually paused the app.\n\nThe following code sample demonstrates using the `APPLICATION_STATE` view\nto retrieve the health status of all consumer instances of your app:\n\n```\nSELECT \n    CONSUMER_ORGANIZATION_NAME , \n    CONSUMER_ACCOUNT_NAME , \n    LAST_HEALTH_STATUS , \n    LAST_HEALTH_STATUS_UPDATE_TIME \n FROM \n    SNOWFLAKE . ACCOUNT_USAGE . APPLICATION_STATE \n WHERE \n    PROVIDER_ORG_NAME = '<your_provider_org_name>' \n    AND APPLICATION_NAME = '<your_app_name>' \n ORDER BY \n    LAST_HEALTH_STATUS_UPDATE_TIME DESC ;\n```\n\nCopy\n\nThe preceding query may return results similar to the following:\n\n```\nCONSUMER_ORG_NAME    CONSUMER_ACCOUNT_NAME    LAST_HEALTH_STATUS    LAST_HEALTH_STATUS_UPDATE_TIME \n ------------------   ---------------------    ------------------    ------------------------------- \n consumer_org_1      consumer_account_1       OK                    2024-01-15 10:30:00.000 \n consumer_org_2      consumer_account_2       FAILED                2024-01-15 09:45:00.000 \n consumer_org_3      consumer_account_3       PAUSED                2024-01-14 16:20:00.000\n```\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. About log messages and trace events in an app\n2. About event sharing\n3. Considerations when using event sharing\n4. Considerations when migrating from the previous event sharing functionality\n5. Workflow - Set up event sharing for an app\n6. Monitor consumer application health\n\nRelated content\n\n1. Logging, tracing, and metrics\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ],
      "full_content": null
    },
    {
      "url": "https://www.snowflake.com/en/blog/observability-new-era-with-snowflake-trail/",
      "title": "Observability in Snowflake: A New Era with Snowflake Trail",
      "publish_date": "2024-08-15",
      "excerpts": [
        "blog\n\n##### Category\n\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\n\nProduct and Technology\n\nJun 10, 2024 | 5 min read\n\n# Observability in Snowflake: A New Era with Snowflake Trail\n\nDiscovering and surfacing telemetry traditionally can be a tedious and challenging process, especially when it comes to pinpointing specific issues for debugging. However, as applications and pipelines grow in complexity, understanding what's happening beneath the surface becomes increasingly crucial. A lack of visibility hinders the development and maintenance of high-quality applications and pipelines, ultimately impacting customer experience. Comprehensive observability tools are a must to empower developers and data engineers to quickly identify and resolve issues.\n\n### Introducing Snowflake Trail\n\nSnowflake Trail is a rich set of Snowflake capabilities that allows developers and data engineers to observe and act on their applications and data pipelines through Snowsight or third-party tools. Leveraging Snowflake's Query History, Event Tables , Alerts and Notifications as the telemetry foundation, Snowflake Trail provides enhanced visibility into data quality, pipelines and applications. Each of these signals empowers developers to monitor, troubleshoot and optimize their workflows with ease. Snowflake Trail builds upon the observability foundation already in Snowflake. You are probably familiar with the built-in observability, with capabilities such as Task History and Dynamic Tables observability . With this launch, we are expanding the scope of what and how you can observe with Snowflake.\n\n### Effortless telemetry with one simple setting\n\nSnowflake Trail is built with automated telemetry; no agent or setup tasks are needed. A default Event Table (public preview soon) is in the Snowflake database of every account, removing the need to create and manage your own custom event table. Snowflake Trail eliminates the need for any agent installation, tedious setup or data export tasks, providing fast insights into application and pipeline performance. With just one simple setting, you can gain visibility into the performance of your Snowpark code and its resource usage, so you can quickly diagnose and debug your apps and pipeline development. Events are all within Snowflake with no need for additional data transfer.\n\n### Customer stories\n\nOur customers have seen significant improvements in their application and pipeline development with Snowflake Trail\u2019s capabilities. \" _When working with Snowpark UDFs, some of the logic can become quite complex. In some instances, we had thousands of lines of Java code that needed to be monitored and debugged. With the new logging and tracing capabilities, we are able to investigate issues in our code or data much quicker and find performance issues much faster_ ,\u201d said Nick Pileggi, Principal Solutions Architect at **phData** **Inc** ., in regards to migrating Spark and Hadoop applications to Snowpark. _\u201cEvent Tables have been invaluable, as we take our Snowflake Native App to market. By choosing to share events with us, our customers benefit from us being able to assist them without needing to manually extract diagnostic data and send it to us. We are excited to see new Snowflake Trail features like the Log Explorer, which will help us hone in on the relevant information even faster, and Trace Viewer, which will help us remove performance bottlenecks in our code,\u201d_ said James Weakley, Snowflake Data Superhero and co-founder at **Omnata,** a Snowflake Native App available in Snowflake Marketplace.\n\n### Built-in observability experiences reduce time to detect (TTD) and time to resolution (TTR)\n\nSnowflake Trail provides a comprehensive set of telemetry signals, including metrics, logs and span events, to give developers a deeper understanding of their applications and pipelines. Snowsight is where these signals are brought together to help developers debug and detect issues near instantly, thereby reducing TTD and TTR. Key capabilities include: * **Snowpark metrics (private preview):** Understand the CPU and memory consumption of your code in Snowpark (Python) stored procedures and functions, using the new Snowpark metrics. Support for other languages coming soon.\n* **Automatic Python DataFrame tracing (private preview):** Snowpark DataFrames allow developers to write queries in native Python. When you use DataFrames on Snowflake, those operations will now also appear on the trace view, allowing you to see the full execution of your pipeline.\n* **User code profiler for Python (private preview):** Developers can attach a profiler to their stored procedure to understand where the most compute time is spent and better optimize their Python execution.\n* **Log attributes (public preview):** Filter logs further; available for Java and JavaScript, Python support coming soon.\n* **Serverless Alerts** **(public preview):** The power and evaluation logic of alerts are now available with the cost and warehouse optimization of serverless capabilities.\n Developers can visualize what\u2019s going on with their pipelines and apps, and interact with logs, metrics and tracing directly within Snowsight using features like Log Explorer (Public Preview) to easily view and filter logs from Snowpark code.\n\nAnd distributed tracing experience (private preview) for Snowpark makes it easier to visualize and troubleshoot calls across objects.\n\nLast but not least, Snowflake Trail also provides data-quality monitoring (general availability soon) as part of Snowflake Horizon. Customers get built-in data-quality solutions with out-of-the-box system metrics (such as null count) or custom metrics that they can define to monitor data quality. Data engineers and stewards can effectively monitor and report on degradation in data quality across their organization. **Simply use Snowsight or Bring Your Own Tools:** You can use Snowsight to monitor and trace pipelines, apps and resource usage directly in Snowflake. Even better, Snowflake Trail adheres to the industry-standard [OpenTelemetry](https://opentelemetry.io/) specification and notification destinations, allowing easy integration with your favorite observability and customizable notification tools, including [Datadog](https://www.datadoghq.com/blog/snowflake-snowpark-monitoring-datadog/) , [Grafana](https://grafana.com/blog/2024/06/06/snowflake-data-visualization-all-the-latest-features-to-monitor-metrics-enhance-security-and-more/) , Observe, Metaplane, PagerDuty, Slack and Microsoft Teams.\n\n### Get started with Snowflake Trail\n\nSnowflake Trail marks a significant milestone in Snowflake's observability journey, addressing the long-standing observability challenges faced by our users. With its rich telemetry, built-in observability experiences and easy integration with third-party tools, Snowflake Trail is poised to revolutionize the way developers build, deploy and maintain applications and pipelines in Snowflake. Learn more about Snowflake Trail and take it for a spin .\n\n#### Save the date! We look forward to welcoming you back in 2025 for Snowflake Summit!\n\n[save your spot](https://snowflake.com/summit/save-the-date/?utm_cta=website-blog-observability-summit)\n\n###### Author\n\nAshwin Kamath\n\n###### Share Article\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fobservability-new-era-with-snowflake-trail&title=Observability+in+Snowflake%3A+A+New+Era+with+Snowflake+Trail)\n\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fobservability-new-era-with-snowflake-trail&text=Observability+in+Snowflake%3A+A+New+Era+with+Snowflake+Trail)\n\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fobservability-new-era-with-snowflake-trail)\n\n#### Just For You\n\nProduct and Technology\n\n##### Reimagine Batch and Streaming Data Pipelines with Dynamic Tables, Now Generally Available\n\nSaras Nowak | Shiyi Gu\n\nMay 6, 2024 | 7 min read\n\nProduct and Technology\n\n##### Snowflake\u2019s New Python API Empowers Data Engineers to Build Modern Data Pipelines with Ease\n\nAnurag Gupta | Cheryl Manalo\n\nApr 17, 2024 | 5 min read\n\nSubscribe to our blog newsletter\n\nGet the best, coolest and latest delivered to your inbox each week\n\n\\*\n\nSubscribe Now\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\n[start for free](https://signup.snowflake.com/)\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\n\n\\*\n\n\\*\n\n\\* Country United States Canada United Kingdom Germany France Australia Japan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo The Democratic Republic of The Cook Islands Costa Rica Cote D'Ivoire (Ivory Coast) Croatia (Hrvatska) Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Heard and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jordan Kazakhstan Kenya Kiribati Korea Republic of (South) Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia Federated States of Moldova Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Occupied Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Reunion Romania Russian Federation Saint Helena Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and The South Sandwich Island Spain Sri Lanka Suriname Svalbard and Jan Mayen Islands Swaziland Sweden Switzerland Taiwan Tajikistan Tanzania United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Venezuela Viet Nam Virgin Islands (British) Virgin Islands (U.S.) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe\n\n\\*\n\nAdd me to the list to receive dedicated product updates and general availability emails.\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\n\nSubscribe Now\n\nLearn * Resource Library\n* Live Demos\n* Fundamentals\n* Training\n* Certifications\n* Snowflake University\n* Developer Guides\n* Documentation\n\n* Privacy Policy\n* Site Terms\n* Communication Preferences\n* Cookie Settings\n* Do Not Share My Personal Information\n* Legal\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ],
      "full_content": null
    },
    {
      "url": "https://www.snowflake.com/en/blog/collect-logs-traces-snowflake-apps/",
      "title": "Collect Logs and Traces From Your Snowflake Applications",
      "publish_date": "2024-08-21",
      "excerpts": [
        "blog\n\n##### Category\n\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\n\nData Engineering\n\nOct 30, 2023 | 4 min read\n\n# Collect Logs and Traces From Your Snowflake Applications With Event Tables\n\nWe are excited to announce the general availability of Snowflake\n\nEvent Tables\n\nfor logging and tracing, an essential feature to boost application observability and supportability for Snowflake developers.\n\nIn our conversations with developers over the last year, we\u2019ve heard that monitoring and observability are paramount to effectively develop and monitor applications. But previously, developers didn\u2019t have a centralized, straightforward way to capture application logs and traces.\n\nEnter the new Event Tables feature, which helps developers and data engineers easily instrument their code to capture and analyze logs and traces for all languages: Java, Scala, JavaScript, Python and Snowflake Scripting.\n\nWith Event Tables, developers can instrument logs and traces from their UDFs, UDTFs, stored procedures, Snowflake Native Apps and Snowpark Container Services, then seamlessly route them to a secure, customer-owned Event Table. Developers can then query Event Tables to troubleshoot their applications or gain insights into performance and code behavior.\n\nLogs and traces are collected and propagated via Snowflake\u2019s telemetry APIs, then automatically ingested into your Snowflake Event Table.\n\n#### Simplify troubleshooting in Snowflake Native Apps\n\nEvent Tables are also supported for Snowflake Native Apps. When a Snowflake Native App runs, it is running in the consumer\u2019s account, generating telemetry data that\u2019s ingested into their active Event Table.\n\nOnce the consumer enables event sharing, new telemetry data will be ingested into both the consumer and provider Event Tables. Now the provider has the ability to debug the application that\u2019s running in the consumer\u2019s account. The provider only sees the telemetry data that is being shared from this data application\u2014nothing else.\n\n#### Improve reliability across a variety of use cases\n\nYou can use Event Tables to capture and analyze logs for various use cases: * As a data engineer building UDFs and stored procedures within queries and tasks, you can instrument your code to analyze its behavior based on input data.\n* As a Snowpark developer, you can instrument logs and traces for your Snowflake applications to troubleshoot and improve their performance and reliability.\n* As a Snowflake Native App provider, you can analyze logs and traces from various consumers of your applications to troubleshoot and improve performance.\n\nSnowflake customers ranging from Capital One to phData are already using Event Tables to unlock value in their organization. \u201cThe Event Tables feature simplifies capturing logs in the observability solution we built to monitor the quality and performance of Snowflake data pipelines in Capital One Slingshot,\u201d says Yudhish Batra, Distinguished Engineer, Capital One Software. \u201cEvent Tables has abstracted the complexity associated with logging from our data pipelines\u2014specifically, the central Event Table gives us the ability to monitor and alert from a single location.\u201d\n\nAs phData migrates its Spark and Hadoop applications to Snowpark, the Event Tables feature has helped architects save time and hassle.\n\n\u201cWhen working with Snowpark UDFs, some of the logic can become quite complex. In some instances, we had thousands of lines of Java code that needed to be monitored and debugged,\u201d says Nick Pileggi,\n\nPrincipal Solutions Architect at phData\n\n. \u201cBefore Event Tables, we had almost no way to see what was happening inside the UDF and correct issues. Once we rolled out Event Tables, the amount of time we spent testing dropped significantly and allowed us to have debug and info-level access to the logs we were generating in Java.\u201d\n\nOne large communications service provider also uses logs in Event Tables to capture and analyze failed records during data ingestion from various external services to Snowflake. And a Snowflake Native App provider offering geolocation data services uses Event Tables to capture logs and traces from their UDFs to improve application reliability and performance.\n\nWith Event Tables, you now have a built-in place to easily and consistently manage logging and tracing for your Snowflake applications. And in conjunction with other features such as Snowflake Alerts and Email Notifications, you can be notified of new events and errors in your applications.\n\n#### Try Event Tables today\n\nTo learn more about Event Tables, join us at BUILD\n\n,\n\nSnowflake\u2019s developer conference. Or get started with Event Tables today with a tutorial\n\nand quickstarts for logging\n\nand tracing\n\n. For further information about how Event Tables work, visit Snowflake product documentation\n\n.\n\n#### The Data Engineer\u2019s Guide to Python for Snowflake\n\n[download now](https://www.snowflake.com/resource/the-data-engineers-guide-to-python-for-snowflake/)\n\n###### Author\n\nAshwin Kamath\n\n###### Share Article\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps&title=Collect+Logs+and+Traces+From+Your+Snowflake+Applications+With+Event+Tables)\n\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps&text=Collect+Logs+and+Traces+From+Your+Snowflake+Applications+With+Event+Tables)\n\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps)\n\n#### Just For You\n\nProduct and Technology\n\n##### Alerts and Observability for Pipeline Monitoring and Cost Management\n\nShiyi Gu\n\nNov 23, 2022 | 5 min read\n\nSubscribe to our blog newsletter\n\nGet the best, coolest and latest delivered to your inbox each week\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\n[start for free](https://signup.snowflake.com/)\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake\u2019s latest products, expert insights and resources\u2014right in your inbox!\n\nLearn * Resource Library\n* Live Demos\n* Fundamentals\n* Training\n* Certifications\n* Snowflake University\n* Developer Guides\n* Documentation\n\n* Privacy Policy\n* Site Terms\n* Communication Preferences\n* Cookie Settings\n* Do Not Share My Personal Information\n* Legal\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 4
    }
  ]
}

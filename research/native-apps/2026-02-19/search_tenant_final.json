{"search_id":"search_b856541f892e443a91908be28d89f34f","results":[{"url":"https://medium.com/snowflake/snowflake-cortex-and-multitenancy-e6509fd1eb5f","title":"Snowflake Cortex and Multitenancy | by Brian Hess | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2025-10-29","excerpts":["Sitemap\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nWrite\n\nSearch\n\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-e6509fd1eb5f---------------------------------------)\n\n·\n\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-e6509fd1eb5f---------------------------------------)\n\nBest practices, tips & tricks from Snowflake experts and community\n\n# Snowflake Cortex and Multitenancy\n\nBrian Hess\n\n5 min read\n\n·\n\nOct 29, 2025\n\n\\--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nMultitenancy is a crucial concept in software architecture, especially when designing applications that serve multiple customers or users efficiently. While often discussed, the nuances between multi-tenant and multi-user systems, and the various approaches to implementing them, can be complex and even more complicated when used with generative AI, such as Snowflake Cortex. Let’s break it down.\n\n## Multi-tenant vs. Multi-user: What’s the Difference?\n\nAt first glance, these two concepts seem very similar, and they do address a common problem: managing data access for different entities within a single system. In a multi-tenant system, you host data for multiple distinct “tenants,” which could be different organizations or large groups of users. The key principle here is strict data isolation: each tenant should only be able to access their own data. Think of it like an apartment building where each tenant has their own separate apartment.\n\nA multi-user system, on the other hand, hosts data for all users, but individual users should only be able to access a _subset_ of that data that is relevant to them. This is more like a shared office space where everyone has access to certain common areas but also has their own private desk or office, while managers have access to the private desks of all their employees.\n\n## Approaches to Multitenancy\n\nThere are two primary architectural approaches to implementing multitenancy.\n\n### Multi-Tenant Tables (MTT)\n\nThe first approach is **Multi-Tenant Tables (MTT)** , which works by mixing data from all tenants together within the same tables. A specific column in these tables (e.g., tenant\\_id) indicates which tenant each row of data belongs to. For implementation, you need a mechanism to map each user to their corresponding tenant\\_id value, ensuring that queries always include a predicate to filter data for the correct tenant.\n\n### Object-Per-Tenant (OPT)\n\nPress enter or click to view image in full size\n\nThe second approach is **Object-Per-Tenant (OPT)** (or Table-Per-Tenant). With this method, each tenant’s data is stored in separate tables, or even separate schemas or databases, providing a higher degree of physical isolation. Implementation requires creating new Snowflake objects (tables, schemas, etc.) every time a new tenant is onboarded. You also need a way to map each tenant to their specific tables (often through database, schema, or table naming conventions) and map users to their respective tenants. A key consideration for OPT is that it is generally more reasonable for small to medium numbers of tenants/users (typically less than a few hundred) due to the overhead of managing individual objects per tenant.\n\n## Multitenancy in Applications\n\nWhen building applications, how do you handle multitenancy? Many applications construct their own SQL queries. In this scenario, the application explicitly handles tenancy logic: for **MTT** , the application adds the tenant\\_id predicate to every query where appropriate, and for **OPT** , the application dynamically chooses the correct tenant-specific table or schema to query.\n\nApplications can also leverage database context for multitenancy. For **MTT** , the application can set a session context (e.g., a session variable or role) and rely on Row Access Policies (RAP) or Views to automatically filter data based on that context. For example, the View or RAP could have a predicate that matched the tenant\\_id column with the current role.\n\nFor **OPT** , tenants can be placed in their own schema (or optionally, their own database) and common table names can be used within each schema. The application then sets the current schema for the session and issues queries using relative table names, ensuring it only accesses the data within that tenant’s schema.\n\n## Multitenancy in Snowflake Cortex\n\nSnowflake Cortex, and other “text-to-SQL” technologies, presents unique challenges for multitenancy because the application doesn’t directly control SQL creation, making it difficult to add tenant predicates directly to the generated SQL. Snowflake Cortex Analyst returns SQL as a string, so it’s hard to edit this string to inject tenancy logic (you would need to parse the returned SQL statement, which is not trivial).\n\nSome higher-level generative AI solutions, such as Snowflake Cortex Agent, will generate the SQL and then execute the generated query on behalf of the caller. This fully-automated solution is powerful, but does present some challenges for multitenant situations. Specifically, these solutions need to provide some way to set a context (e.g., session variable or role for MTT architectures, or the schema for OPT architectures) before executing the generated SQL.\n\nOther systems are more semi-automatic, where the generated SQL is returned to the caller, at which point the caller will execute the SQL itself and re-call the generative AI with a pointer to the results of the SQL query. This semi-automatic approach is helpful for multitenant use cases as the caller is responsible for executing the SQL, so the caller can set the appropriate session context before executing the query. The v1 API for Snowflake Cortex Agent follows this semi-automatic approach.\n\n### Snowflake Cortex and MTT\n\nPress enter or click to view image in full size\n\nFor Multi-Tenant Table (MTT) in Snowflake Cortex, you can use session context with Row Access Policies (RAP) or Views. This can be based on the current role, which is good for a small number of tenants and requires creating a Snowflake object (like a role) for each tenant during onboarding. Alternatively, it can be based on a session variable, where you set a session variable to indicate the tenant. It’s important to also restrict access to the current user being the app service user or app service role to maintain security. An entitlement table can also be used to map roles or session variable values to the allowed tenant column (e.g., tenant\\_id) values, providing a flexible way to manage access.\n\n### Snowflake Cortex and OPT\n\nFor Object-Per-Tenant (OPT) in Cortex, consider using a Semantic View, Semantic Search Service, or even an Agent per tenant. If you’re already creating a table per tenant, the additional overhead of managing an Agent per tenant (or similar structures) might not be a significant concern. Then, when calling the Snowflake Cortex Agent API, choose the appropriate Agent, Semantic View(s), and/or Semantic Search Service(s) for the tenant of interest.\n\n## Conclusion\n\nUnderstanding these approaches is key to designing robust and scalable multi-tenant solutions, especially within specialized environments like Snowflake Cortex where direct SQL manipulation might be limited.\n\nSnowflake\n\nSnowflake Cortex\n\nText To Sql\n\nMultitenancy\n\n\\-- \n\n\\--\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--e6509fd1eb5f---------------------------------------)\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--e6509fd1eb5f---------------------------------------)\n\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--e6509fd1eb5f---------------------------------------)\n\n10\\.9K followers\n\n· Last published 2 hours ago\n\nBest practices, tips & tricks from Snowflake experts and community\n\n## Written by Brian Hess\n\n239 followers\n\n· 1 following\n\nI’ve been in the data and analytics space for over 25 years in a variety of roles. Essentially, I like doing stuff with data and making data work.\n\n## No responses yet\n\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--e6509fd1eb5f---------------------------------------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----e6509fd1eb5f---------------------------------------)\n\n[Status](https://status.medium.com/?source=post_page-----e6509fd1eb5f---------------------------------------)\n\nAbout\n\nCareers\n\nPress\n\n[Blog](https://blog.medium.com/?source=post_page-----e6509fd1eb5f---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e6509fd1eb5f---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----e6509fd1eb5f---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e6509fd1eb5f---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----e6509fd1eb5f---------------------------------------)"]},{"url":"https://blog.infostrux.com/unlocking-the-power-of-snowflakes-multi-tenant-architecture-proven-best-practices-for-success-5690290804a6","title":"Unlocking the Power of Snowflake's Multi-Tenant Architecture: Proven Best Practices for Success | by Fabian Hernandez | Infostrux Engineering Blog","publish_date":"2023-10-15","excerpts":["Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n[](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n[## Infostrux Engineering Blog](https://blog.infostrux.com/?source=post_page---publication_nav-d8fca5ddf2c6-5690290804a6---------------------------------------)\n·\nFollow publication\n[](https://blog.infostrux.com/?source=post_page---post_publication_sidebar-d8fca5ddf2c6-5690290804a6---------------------------------------)\n ... \nSection Title: ... > After some consultation and discovery, it was clear we needed to focus on:\nContent:\nStandardizing onboarding processes to bring in data from the newly acquired companies\nFlexible Role-Based Access Control System design to ensure compliance with unique data-related regulations\nFull chargeback and granular understanding of cost per business unit\nHave resource isolation for tenants, as performance should not be impacted by resource contention\nFlexible data sharing was needed between different business units, without duplicating data or physically moving it in some cases.\nMonitoring and auditing to ensure each acquisition was using resources efficiently and maintaining compliance with an enterprise-wide governance model\nUnified Data Catalog to help understand the available data sets across all businesses and to enable an enterprise Master Data Management strategy.\nDisaster Recovery strategy with strict Recovery Point Objectives and Recovery Time objectives\nSection Title: ... > After some consultation and discovery, it was clear we needed to focus on:\nContent:\nWhile this is an abbreviated list of requirements, these are an example of some of the aspects one needs to consider when designing a multi-tenant architecture for your data platform. As you may have guessed, it’s no surprise that the client saw Snowflake Cloud Data Warehouse as a major enabler of this kind of architecture.\nSnowflake has a rich ecosystem of 3rd party data integration technologies and native features such as Snowpipe to integrate data. Additionally, if the data cannot be brought in for compliance reasons, Snowflake can integrate with external data lakes.\nSnowflake governance and security model is based on both Role Based Access Control and Discretionary Access control principles. It allows for a flexible governance framework within Snowflake accounts. However, you can scale your Access Control model across multiple snowflake accounts in a federated model.\nSection Title: ... > After some consultation and discovery, it was clear we needed to focus on:\nContent:\nSnowflake native separation of storage and compute and cost control mechanisms allow dedicated computing for workloads and the ability to track those compute and storage costs granularly.\nSnowflake data-sharing features allow different accounts to securely share data without physically moving the data. This will enable data to stay federated yet accessible for querying with proper governance and security controls.\nUnderstanding your technical, business, and compliance requirements is key to designing a successful multi-tenant architecture. Luckily, Snowflake has many features necessary to develop it securely, efficiently, and cost-effectively.\nLet’s look at Snowflake’s recommended architectural patterns, best practices, and suggestions for designing your own Snowflake Multi-Tenant data warehouse solution.\nSection Title: ... > After some consultation and discovery, it was clear we needed to focus on:\nContent:\nDisclaimer: This article refers to Snowflake’s May 2021 publication “Design Patterns for Building Multi-Tenant Applications on Snowflake.” The original version is available at the following [link](https://developers.snowflake.com/wp-content/uploads/2021/05/Design-Patterns-for-Building-Multi-Tenant-Applications-on-Snowflake.pdf) .\nSection Title: ... > Multi-Tenant Design Patterns\nContent:\nMulti-tenant Snowflake applications typically conform to one of three design patterns:\nMulti-Tenant Table (MTT): (MTT) consolidates tenants (data owner) within a shared table or warehouse. This centralizes tenants in single, shared objects to enable them to share compute efficiently and other resources.\nObject Per Tenant (OPT): OPT separates tenants (data owner) into individual tables, schemas, databases, and warehouses within a single Snowflake account.\nAccount Per Tenant (APT): APT isolates tenants (data owners) into separate Snowflake accounts. Each tenant has its account.\nLet’s take a closer look at each one.\n ... \nSection Title: ... > Best practices\nContent:\nCreate a customizable role hierarchy based on functional access and privilege per tenant. Create two schemas: one for security views and one for base tables and entitlements. This separates access privileges. Create sandbox areas for individual customers to do more sophisticated operations. You can use a schema per customer to preserve isolation. Use the Snowflake’s current_role() function on the secure views to filter the base table. This will require joining the entitlements table to get the proper access for a particular tenant session. To secure your entitlement tables, it is recommended to implement restrictive permissions. Adding new customers by running single INSERT/UPDATE statements against the entitlement tables is not advisable. To minimize human error, wrapping processing in automated procedures with controls in place is best. These procedures can be executed either inside or outside of Snowflake.\nSection Title: ... > Best practices\nContent:\nAdditionally, to ensure there are no issues, it is recommended to run regular regression tests after entitlement table updates. These tests will help you to verify that the secure view results match the expected outcomes. Tenants should have a unique numeric identifier. This identifier works as the tenant_id. Cluster all transaction tables by tenant_id and a meaningful date field, at minimum. (The reverse, date then tenant_id, is also fine.) Despite the small size, cluster the entitlement table if there are a lot of users or roles per tenant; otherwise, sort the load. For easier application administration, separate the serving and working databases used to ingest or transform external data. Developers should have their warehouse for development work, while application users can be pooled in a typical multi-cluster warehouse or isolated into dedicated warehouses based on application requirements.\nSection Title: ... > Best practices\nContent:\nMulti-region data sharing can be challenging due to the single-object nature of this design pattern. MERGE, UPDATE, and auto-clustering operations can be a challenge due to the size of the base tables. Determining per-tenant storage cost becomes difficult.\nSection Title: ... > Object per tenant (OPT)\nContent:\nPress enter or click to view image in full size\nSnowflake’s Object per ternant design pattern. Image authored by Snowflake’s article Design Patterns for Building Multi-Tenant Applications on Snowflake.\nThe OPT architectural pattern isolates tenants at the database level, as shown by the red box in the image. Dedicated warehouses can also be assigned to tenants to separate their data at the processing level.\n ... \nSection Title: ... > Pros\nContent:\nOPT is an ideal solution when each tenant has a unique data model.\nSome customers prefer the OPT model to avoid managing entitlement tables, secure views, or row-level security requiring solid processes.\nThe OPT design pattern considers security as a critical factor. Some apps utilizing the OPT model provide customers with dedicated computing resources to meet contractual, security, or regulatory requirements.\nSection Title: ... > Cons\nContent:\nOPT does not scale as quickly as MTT. Scaling OPT beyond hundreds of tenants is challenging due to the unwieldiness of thousands of tenant databases.\nCreating/Maintaining a consistent data modeling state across all databases becomes hard.\nCompute per tenant can increase costs because you lose the ability to pool compute across tenants.\nAutomation becomes almost mandatory to create and manage objects.\nSection Title: ... > Best practices\nContent:\nIt is common practice to separate customers into their databases as it provides the most straightforward and cleanest isolation level. However, some app builders choose to separate customers into dedicated tables, for instance, in embedded analytics use cases where data applications create a report table per tenant.\nImplement OPT authentication and authorization similar to MTT. However, it is advisable to implement a user lookup process within the secrets manager and application tier to route the user to the proper database automatically and the correct warehouse.\nTenant data can be ingested and transformed through a single working database and then distributed to tenant-serving databases to optimize efficiency and cost-effectiveness. However, Keeping the same tenant-per-database separation at this level is also possible.\nSection Title: ... > Account per tenant (APT)\nContent:\nPress enter or click to view image in full size\nSnowflake’s Account per tenant design pattern. Image authored by Snowflake’s article Design Patterns for Building Multi-Tenant Applications on Snowflake.\nTypically, each tenant using the APT model has one Snowflake account, database, and warehouse, as shown in the red square above. The isolation between tenants is kept at the account level. This patter\nSection Title: ... > Pros\nContent:\nIsolating tenants reduces the risk of mismanaging security for all clients.\nIsolate tenants per account to allow for stricter security measures such as encryption keys, IP-allows lists, and better-than-RBAC controls.\nAllows strict network measures, including BYOT, Snowflake UI login, and dedicated connection string per tenant.\nSection Title: ... > Cons\nContent:\nTypical tenant counts range from tens to low hundreds, introducing more scaling limitations for APT.\nManaging thousands of tenant accounts using APT can be challenging due to its complexity and potential to become unwieldy.\nCreating/Maintaining a consistent data modeling state across all accounts becomes hard.\nCompute per tenant can increase costs because you lose the ability to pool compute across tenants.\nAutomation has become almost mandatory to create and manage accounts and objects.\n ... \nSection Title: ... > Best practices\nContent:\nSnowflake network policies (IP allow lists) can be applied only at the account or user level. Snowflake virtual warehouses temporarily cache data from object stores for whole or partial reuse in subsequent queries. RBAC and secure view rules still apply. Snowflake's compute billing is at the virtual warehouse level, which can lead to inexact calculations for per-query, per-user, or per-tenant costs when tenants share compute. Snowflake virtual warehouses do not provide the option to set resource limits for individual users or tenants.\nSection Title: ... > Implementation Considerations\nContent:\nRegardless of the pattern you choose to implement, you must also think of the ongoing maintainability of the solution. The solution’s technical development, maintainability, and governance will come at a greater cost the higher the isolation level you require per tenant. That is why implementing Data Ops best practices in all these proposed patterns is vital to maintaining the platform and driving the most value for your business."]},{"url":"https://www.snowflake.com/en/blog/snowflake-native-apps-security/","title":"Snowflake Native Apps: Secure By Design | Blog","publish_date":"2025-06-27","excerpts":["Section Title: Category\nContent:\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\nProduct and Technology\nSEP 26, 2023 | 8 min read\nSection Title: How Snowflake Native Apps Deliver Security for App Builders and Consumers\nContent:\nThe [Snowflake Native App Framework](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/) , which leverages Snowflake’s advanced architecture, allows for a new level of security for applications. This security spans not just the application consumer, but also the application provider. Controlling all software and infrastructure in the Snowflake Data Cloud, Snowflake can protect the application code, reducing risk for app providers. For consumers, protection comes via extensive capabilities in [role-based access control](https://www.snowflake.com/en/fundamentals/rbac/) (RBAC), which is used to control application access.\nSection Title: ... > Challenges for applications\nContent:\nThe application market has been growing at a fast pace. This creates several challenges that prevent greater application adoption: * Data protection and governance\n ... \nSection Title: ... > Challenges for applications\nContent:\nApplication consumers, on the other hand, really liked this model for their data protection and governance because the solutions operated on the data in its existing location and, in most cases, a third party didn’t need access. For the application providers, not having control over the code/executables not only puts the license at risk, but also puts their business at risk should their software be reverse engineered. Any time an application is packaged and given to a customer to run on their own hardware, reverse engineering those packages becomes much simpler. The code can be debugged and memory can be easily inspected. App builders can build in protection to try to prevent reverse engineering, such as by obfuscating executables, but this is complicated, error prone, and still doesn’t provide complete protection. The emergence of SaaS applications and managed services alleviated some of these challenges, but also introduced new ones.\nSection Title: ... > Challenges for applications\nContent:\nWith the SaaS model, when application builders control all hardware and software, they maintain control over their code at all times. The builder can also push any urgent updates and security patches as needed. The challenge in this shift in the industry is that these applications often require the application consumer to provide access to or a copy of their data to the third-party provider. The builder has, or can have, direct access to this information, which puts more risk on the application consumer. The data that is being stored is outside the consumer’s systems, so the third party has to be evaluated for trust and security risk, and processes have to be carefully checked for compliance.\nSection Title: ... > Compute and data isolation\nContent:\nSnowflake Native Apps (currently in public preview on AWS and private preview on GCP and Azure) are deployed into the application consumer’s Snowflake account. This offers an advantage in that the application is hosted in isolation from other consumers and their data. Multi-tenant applications are often difficult to secure and require a significant amount of engineering and testing to verify data cannot be accessed by unauthorized or malicious users. For example, if a [managed application](https://www.snowflake.com/blog/powered-by-snowflake-building-a-connected-application-for-growth-and-scale/) is vulnerable to a SQL Injection attack, it’s very possible the attacker could gain access to other users’ data. In the case of a Snowflake Native App, the application has only been granted access to the application consumer’s data in the consumer’s own account.\nSection Title: ... > Compute and data isolation\nContent:\nIf an exploit is found, thanks to the way Snowflake Native Apps isolates each tenant’s data, an application user would only gain access to their own data that they provided to the application.\nSection Title: ... > Compute and data isolation\nContent:\nSnowflake Native Apps are deployed into the consumer’s account. In most application models, this would also allow the consumer access to the builder’s binaries and the servers hosting them. Snowflake’s architecture, in which the servers are administered by Snowflake and not by the customer, helps providers protect the application code from prying eyes. Here you can see the objects and code when the application is installed in the provider’s account in debug mode:\nWhen installed in the consumer account, the views and functions were configured to not be accessible. The SHOWME_VIEW and SUPERSECRETSAUCE can still be used by the SHOWME_PROC during execution, but the existence of and the code backing all these objects is protected.\nSection Title: ... > Compute and data isolation\nContent:\nSnowflake can allow the consumer to access parts of the application and data, which are granted to the application without giving application consumers, even ACCOUNTADMIN, access to the code/data/logs. With Snowflake Native Apps, providers that had concerns about protecting their code are no longer limited to SaaS options. A whole new market is opening up for consumers and providers that want more control over their data.\nSection Title: How Snowflake Native Apps Deliver Security for App Builders and Consumers > ... > Package security\nContent:\n[Snowpark for Python](https://www.snowflake.com/en/data-cloud/snowpark/) is commonly used in Snowflake Native Apps to provide features which were not possible using Snowflake’s SQL engine. While Python is extremely powerful, it could be a vector for additional exploits—and our customers spoke loudly that package security and governance was of utmost concern. Any developer on the internet can publish a Python package, or in some cases hijack an existing project. To mitigate these concerns, Snowflake teamed up with [Anaconda](https://anaconda.com) to deliver a vast number of packages to run directly in Snowflake. Because Anaconda maintains its own security practices around curation, patches, scanning and release security, customers can optionally rely on Anaconda to provide up-to-date packages.\n ... \nSection Title: ... > Data control and sovereignty\nContent:\nApplication consumers install a Snowflake Native App from a provider in their own account. Access to any data that application needs must be granted to the application by the consumer administrator, so it is well known what the application can access and use. In Snowflake Native Apps, the data doesn’t need to be moved or sent to a third party, which makes it much easier to be compliant with security requirements and data sovereignty rules.\nSection Title: ... > Application permissions\nContent:\nWhen a provider publishes an application, it includes a [manifest](https://docs.snowflake.com/en/developer-guide/native-apps/creating-manifest) file. This manifest contains the permissions to be granted to the application, which can be reviewed by the application consumer during installation. By default, a Snowflake Native App has no permissions outside of the application itself, including data access or the ability to create database objects (tables, tasks, warehouses, etc). Additional rights can be granted to the application, but that must be done by the application consumer administrators through explicit grants or [references](https://docs.snowflake.com/en/developer-guide/native-apps/requesting-refs) . Application code runs as a new application role, with owner’s rights, such that it can only access what that role has been granted to do.\nSection Title: ... > Application permissions\nContent:\nThis means that the application can’t access data that the caller has access to—it must be explicitly granted permission.\n ... \nSection Title: ... > A secure framework for applications\nContent:\nSnowflake Native Apps were developed to fulfill needs that were previously unmet in the industry, and Snowflake is uniquely positioned to provide applications from providers to consumers while protecting each party. Snowflake Native Apps leverage some of the biggest strengths in managed applications, provider-managed code and upgrades, and protecting the provider's code, along with the biggest strengths of traditional applications, which keep control and sovereignty of data in the consumers' hands. Check out the [Snowflake Native App Framework](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/) or use our [Quickstart tutorial](https://quickstarts.snowflake.com/guide/getting_started_with_native_apps) to develop a Snowflake Native app for yourself. We look forward to seeing what you build!\n ... \nSection Title: How Snowflake Native Apps Deliver Security for App Builders and Consumers > ... > Share Article\nContent:\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fsnowflake-native-apps-security&title=How+Snowflake+Native+Apps+Deliver+Security+for+App+Builders+and+Consumers)\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fsnowflake-native-apps-security&text=How+Snowflake+Native+Apps+Deliver+Security+for+App+Builders+and+Consumers)\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fsnowflake-native-apps-security)\nSubscribe to our blog newsletter\nGet the best, coolest and latest delivered to your inbox each week\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\nSection Title: How Snowflake Native Apps Deliver Security for App Builders and Consumers > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\n[start for free](https://signup.snowflake.com/)\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\nSection Title: How Snowflake Native Apps Deliver Security for App Builders and Consumers > Where Data Does More\nContent:\n[Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n[Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n[Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n[Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n[Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n[Technology](https://www.snowflake.com/en/solutions/industries/technology/)\nLearn * [Resource Library](https://snowflake.com/en/resources/)\n ... \nSection Title: How Snowflake Native Apps Deliver Security for App Builders and Consumers > Where Data Does More\nContent:\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"]},{"url":"https://developers.snowflake.com/wp-content/uploads/2021/05/Design-Patterns-for-Building-Multi-Tenant-Applications-on-Snowflake.pdf","title":"[PDF] Design Patterns for Building Multi-Tenant Applications on Snowflake","excerpts":["MTT's chief characteristics are **scalability** and **architectural simplicity** .\n●\nMTT is the most scalable design pattern in terms of the number of tenants an application\ncan support. This approach supports apps with millions of tenants.\nPage 2\n●\nIt has a simpler architecture within Snowflake. Simplicity matters because object\nproliferation makes managing myriad objects increasingly difficult over time. With MTT,\nadding tenants does not cause the number of objects to grow, but adding tenants to OPT\nand APT can result in hundreds or thousands of objects being created within Snowflake.\nFrom a cost standpoint, MTT is usually more cost-efficient because multiple customers utilize\nshared compute and other resources more efficiently.\nBut MTT has a somewhat rigid requirement: To use MTT, an app's data model has to have the same\ngeneral shape across all tenants. Application builders can achieve slight variances using custom\ncolumns that only apply to certain types of tenants, but this approach introduces sparsity into the\ndata.\nObject per tenant (OPT)\nOPT is a great fit if each tenant has a different data model. Unlike MTT, the tenant data shape can\nbe unique for each tenant. OPT does not scale as easily as MTT, however. OPT typically scales well\nfrom tens to hundreds of tenants, but starts to become unwieldy when it includes thousands of\ntenant databases.\nSecurity can factor into the decision to use an OPT design pattern. Some customers prefer the\nOPT model because they don't want to manage an entitlement table, secure views, or row-level\nsecurity with strong processes behind it. They are, however, comfortable using RBAC to control\nwho has specific access to a database.\nSome apps that use the OPT model give customers their own dedicated compute resources to\nsatisfy contractual, security, or regulatory requirements.\nAccount per tenant (APT)\n ... \nauto-clustering\noperations can be a\nchallenge on very large\ntables.\n● It's hard to determine\nper-tenant storage costs\nin a multi-tenant table.\n● Creating objects within\nSnowflake is easy, but\nmaintaining a consistent\nstate across many similar\nobjects is hard. As\nnumbers increase, keeping\nobjects in sync becomes\ndifficult.\n● Compute per tenant can\nincrease costs because\nyou lose the ability to\npool compute across\ntenants.\n● Increased automation is\nrequired to maintain and\nversion objects.\n● Creating an account\nwithin Snowflake is\neasy, but\nmaintaining a\nconsistent state\nacross accounts is\nhard.\n● Compute per tenant\ncan increase costs\nbecause you lose\nthe ability to pool\ncompute across\ntenants.\n● Increased\nautomation is\nrequired to create\nand manage\naccounts and\nobjects.\n**Part Two: Exploring Each Design Pattern**\nMTT design notes\nThe logical diagram depicted in Figure 1 represents a fairly common application setup:\n●\nApplication users access tenant data via secure views in a serving database (highlighted in\nred)\n●\nAn *entitlements table* controls which Snowflake users or roles have access to which tenants\n●\n*Secure* *views* ensure application users only see their tenant rows\n●\nAll tables are clustered by a tenant_id type column\nTo enforce that users can see only their tenant rows, tenants query through secure views, which\nJOIN base tables to the entitlements table on tenant_id . Common tables, where everybody\ngets to see all the rows, use *regular views* pointed to the base tables.\nFigure 1: A serving database with secure views ensures application users see only their tenant rows.\nPage 6\nNote : Many of the concepts covered in this section apply to the OPT and APT models as well.\nSnowflake recommends creating a hierarchy of roles based on privilege and functional access,\nwith a role and user defined per tenant. Set the privileges for dedicated tenant roles by following\nrole-based hierarchy best practices.\nFigure 2 depicts application setup at the schema level. Secure views occupy one schema, and base\ntables and the entitlements table occupy a second schema to separate the privileges that\ndetermine who can access what data. Secure tables and common tables help segregate developer\nusers and application users. There may also be instances where you want to create sandbox areas\nfor individual customers to do more sophisticated things, and you can use a schema per customer\nto separate that as well. Users can be given default namespaces ( database.schema ) to further\ndirect access.\nFigure 2: Schema-level view of database objects based on RBAC\nSecure views use the current_role() parameter to filter the base table using a JOIN to the\nentitlements table where the value of current_role() matches one or more rows in the\nentitlements table.\nPage 7\nIn a data application you can implement secure views by user or by role.\nSecure views based on CURRENT_USER (see Figure 3) make sense if you have one database user\nper tenant and you don't need fine-grained control of different users within the tenant.\nFigure 3: Secure views based on CURRENT_USER()\nSecure views based on CURRENT_ROLE (see Figure 4) allow fine-grained entitlements between\napplication users. You can have multiple sets of privileges within a given tenant and selectively\nassign privileges to tenant users, for example users who can write data into the sandbox versus\nusers who cannot.\nPage 8\nFigure 4: Secure views based on CURRENT_ROLE()\n**Maintaining the entitlement table**\nApplication data security depends on the entitlements table working correctly, so managing the\nentitlement table is a major priority for data application builders. Snowflake recommends starting\nwith the following best practices.\nRegarding security:\n●\nLock down entitlement tables with restrictive permissions.\n●\n ... \nSnowflake user and session access token. Network policies control access to Snowflake from the\napplication tier over Private Link, which is optional. And, finally, role-based access controls (RBAC)\nroute users to default warehouses and databases, and allow users to access only the data that they\nare permitted to see within the application.\nPage 12\nFigure 7: Obtaining and storing a user session via key-pair authentication\n**Isolating working databases if needed**\nIsolating working databases is optional. Some application builders directly load data into the\nserving database and Snowflake points to the initial landing tables. But other builders need to run\ntransformations in Snowflake before serving data, in which case a best practice is to separate the\nserving database from the working databases used for transformation or ingestion from outside\nsources. The application can be configured to write data to both the serving database or the\nworking databases as appropriate for the application functionality.\nSnowflake recommends separating databases to simplify application administration. For example,\nit's easier to configure RBAC to control \"what should be done where *\"* and \"who has access to\nwhat *\"* if databases are separate.\nRegarding workload processing, you can do some of these processes offline if that makes sense for\nthe application, and then apply them to the serving database as appropriate.\nPage 13\nFigure 8: Separate the working databases used for transformation or ingestion from the service\ndatabase as needed for your application.\n**Isolating workloads as needed or pooling to save costs**\nSimilar to database separation being a general best practice, workload separation based on the\ntype of workload is a good idea. Specific recommendations include:\n●\nGive developers their own warehouse for development work\n●\nPool application users on a common multi-cluster warehouse or isolate them onto\ndedicated warehouses based on application requirements\n●\nUse different warehouses for different application purposes\n●\nIsolate other workloads to their own warehouses\nWhen it comes to tenants, app builders need to make decisions around whether to give tenants a\ndedicated warehouse, versus pooling them on common warehouses or multi-cluster warehouses.\nCost will be a factor. You can pool dashboard queries more easily than ad hoc queries because\nthey're predictable. Ad hoc usage can introduce unexpected and unplanned expenses. Strict COGS\nper tenant calculations are a reason to separate tenants into dedicated warehouses because\npooled heuristics are less precise. Some applications pool users by default but offer the option to\npay extra to get a dedicated warehouse.\nPage 14\nFigure 9: Separating workloads based on the type of workload is a best practice.\n**Routing users to warehouses**\nTo make application management easier, it's essential to configure RBAC and default warehouses\n ... \nthe most common practice (see Figure 11), because it is the easiest, cleanest isolation level, but\nsome app builders separate customers into dedicated tables, for example, in embedded analytics\nuse cases in which data applications create a report table per tenant.\nWhich objects to use for isolation depends on factors such as your data pipeline design, your\nsoftware development life cycle process, the consistency of your data shape, and more. How many\ntotal tenants do you expect to have? How many tables will you use? Think through the features\nyou plan to use, such as replication and zero-copy cloning. (Replication can only be done at the\ndatabase level. And, while zero-copy cloning can take place at all three levels, it's cleaner to clone a\ndatabase.) All of these factors and more come into play when you implement OPT.\nFigure 11: Isolating customers into their own databases is the most common OPT pattern.\n**Using automation to create new tenants**\n ... \ncertain tenant slices from a multi-tenant database somewhere else. While it's possible to replicate\nan entire multi-tenant table to all clouds and regions where it's required, over time this design will\nbecome unmanageable as data sizes and the number of tenants grow. For example, Figure 15\nshows a multi-tenant, multi-CSP app design. Customer D shares data on GCP, but it does not make\nsense to replicate Customer D's data on Azure if no one accesses it there.\nPage 20\nNote : If you have a data sharing use case, consider using Snowflake Data Marketplace to take\nadvantage of the latest features.\nFigure 15: If you do not want to replicate all tenant data to all cloud/region pairs, consider\nincorporating OPT into your MTT design.\nPage 21\nAPT design notes\nWith the APT model, there is typically one Snowflake account, one warehouse, and one database\nper tenant.\nThere can be exceptions. For example:\n●\nMultiple tenants can share an account to form a hybrid of APT and MTT (see Figure 16).\n●\n ... \nmore complex requirements that require account-based policies?\n●\nIf virtual warehouse cache constitutes data that must be isolated.\n●\nHow COGS are managed per tenant or billed back to the customer.\n(Calculating COGS per tenant is more straightforward when each tenant has its own\ncompute resources. If tenants share compute resources, you can use a heuristic to\nPage 26\ncalculate COGS per tenant, but it's not as precise. Some apps need the precision, and some\nare fine with a reasonable approximation.)\n●\nHow customers access the application:\n○\nThrough an application UI?\n○\nThrough the Snowflake UI?\n○\nThrough a BYOT solution?\n●\nHow many tenants could use a single virtual warehouse concurrently.\nBilling, resource utilization, and network policy considerations\n●\nSnowflake network policies (IP allow lists) can be applied only at the account or user level.\n●\nSnowflake virtual warehouses cache data from object stores temporarily for whole or"]},{"url":"https://medium.com/snowflake/mastering-snowflake-cost-optimization-at-scale-optimoves-real-world-multi-tenant-saas-strategies-96f8ac9ca883","title":"Mastering Snowflake Cost Optimization at Scale: Optimove’s Real-World Multi-Tenant SaaS Strategies | by Dor Harchol | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2025-06-09","excerpts":["Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nSearch\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-96f8ac9ca883---------------------------------------)\n·\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-96f8ac9ca883---------------------------------------)\nBest practices, tips & tricks from Snowflake experts and community\nSection Title: **Mastering Snowflake Cost Optimization at Scale: Optimove’s Real-World Multi-Tenant SaaS Strateg...\nContent:\nDor Harchol\n8 min read\n·\nJun 9, 2025\n--\nListen\nShare\nSection Title: ... > Introduction\nContent:\nSnowflake is a powerhouse for modern data-driven SaaS companies — but as data loads and complexity scale, so do the costs. At Optimove, we have been laser-focused on migrating workloads from legacy systems like MSSQL into Snowflake over the past few years. However, as part of our ongoing commitment to optimizing our processes and technology, just as we help our customers optimize their marketing campaigns, it was time to turn our attention to cost visibility, efficiency, and long-term sustainability.\nWe’re still in the middle of this transformation. This post shares our journey so far — how we identified inefficiencies, rethought warehouse allocation, and introduced real-time observability. The work isn’t over yet, and we anticipate sharing more discoveries and tooling in future posts.\nSection Title: ... > Introduction\nContent:\nThis initiative was a true cross-functional effort, with close collaboration between **SREs, DBAs, Data Engineers, and Developers** . If you’re operating a multi-tenant SaaS business and feel your Snowflake bill is growing faster than your insights, this story is for you.\nSection Title: ... > Multi-Tenant SaaS and the Snowflake Challenge\nContent:\nIn a multi-tenant SaaS environment, many core processes run across all customers- but each tenant differs in usage patterns, data volume, and product interaction. A query that’s quick for one tenant can choke resources for another.\nThis creates a challenge: how do you allocate Snowflake resources efficiently while balancing fairness, performance, and cost?\nWe approached this challenge by identifying waste, rethinking warehouse strategies, and instrumenting our platform with fine-grained observability.\nSection Title: ... > Snowflake System Views: Building Observability for Cost Optimization\nContent:\nA key enabler of our optimization journey was the ability to deeply understand Snowflake’s internal operations through the system views and metadata it exposes.\nStrong observability is essential for any team aiming to achieve cost efficiency, and Snowflake provides rich data sources that help analyze usage patterns, resource scaling, and credit consumption.\nHere are the main system views and tables we relied on:\nSection Title: ... > Snowflake System Views: Building Observability for Cost Optimization\nContent:\n**QUERY_HISTORY\n** Track all executed queries, their runtimes, queued times, warehouse usage, and cluster scaling behavior.\n**WAREHOUSE_UTILIZATION\n** Analyze warehouse efficiency metrics such as utilization percentages and concurrency.\n**WAREHOUSE_METERING_HISTORY\n** Understand credit consumption per warehouse, broken down by compute usage, cloud services usage, and more.\n**QUERY_ATTRIBUTION_HISTORY\n** Attribute credit usage to specific queries, users, and workloads, allowing precise visibility into who or what is driving costs.\n**WAREHOUSE_CLUSTER_UTILIZATION\n** Dive deeper into multi-cluster warehouse behavior, observing when and how many clusters were used during query execution.\nFamiliarizing yourself with these views allows you to build strong internal monitoring, make informed decisions about warehouse sizing and query design, and ultimately control your Snowflake costs more effectively.\n ... \nSection Title: ... > The Result\nContent:\nWe decommissioned around five low-efficiency warehouses and merged their workloads into a small number of **multi-cluster, highly utilized warehouses** . After rollout, we closely monitored cost attribution per user and workload, and observed a **~50% reduction in compute credit usage** from those workloads.\nSection Title: ... > A Strategic Shift\nContent:\nUniform warehouse allocation across all tenants in a shared process proved inefficient. Some tenants needed significantly more compute, while others barely scratched the surface.\nSection Title: ... > The Process\nContent:\nWe developed a structured methodology for optimizing cost-heavy processes:\nSection Title: ... > The Process\nContent:\n**Identify the Problem Process:** **Use user-level cost observability to find which process was consuming disproportionate Snowflake credits** . To achieve this, we integrated **Datadog** with our Snowflake accounts and used Datadog’s **custom metrics** feature to pull live data directly from Snowflake’s internal views. Specifically, we query the SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY view, which gives us detailed attribution of compute credit usage per user, query, and time window. **Collaborate on Query Optimization:** Work with Data Engineers, Developers, and DBAs to improve query plans and reduce unnecessary compute. In this case, we had an internal query that performed a **PIVOT** operation to generate a structured output for **inserting Snowflake data into a CSV file** . Initially, the PIVOT operated over raw, semi-aggregated data, making the operation slow and compute-intensive.\nSection Title: ... > The Process\nContent:\nTo optimize it, we **moved the MAX() aggregation directly into the source query** feeding the PIVOT. By pre-aggregating the necessary data before pivoting, we minimized the amount of data the PIVOT had to scan and transform, resulting in **significantly faster execution times** and more efficient use of resources. **Design WH-per-Tenant Strategy:**\nSection Title: ... > The Process\nContent:\nDefine WH tiers for the process (e.g., XS, M, XXL).\nIdentify **indicators** that signal which tenant should use which tier — this might include: Data volume in key tables, Frequency or depth of usage of that feature\n**Optimove Approach:**\nAt Optimove, we recognized that **larger tenants typically have both a higher number of users and more attributes per user** .\nTo create an effective scaling indicator, we **multiplied the number of users by the number of attributes** for each tenant, generating a **“tenant score”** .\nThis score determined which warehouse tier (XS, M, XXL) the tenant would be routed to, ensuring better performance and cost efficiency.\n**4. Simulate the Change:** Compare credit usage and WH behavior before and after the change.\n**5. Deploy with Guardrails:** Apply changes gradually in production.\nEnsure production teams have config-level override access to force specific WHs for troubleshooting or performance needs.\nSection Title: ... > The Process\nContent:\n**6. Monitor, Measure, Document:** Track costs post-rollout, document the decision logic, and ensure easy configurability for future updates.\n ... \nSection Title: ... > The Problem\nContent:\nAt Optimove, we are transitioning legacy workloads from **MSSQL** to **Snowflake** using a **Change Data Capture (CDC)** approach.\nWe utilize a CDC tool to **continuously replicate changes** from MSSQL into Snowflake, allowing our production applications to gradually move from reading and updating data on MSSQL to operating directly on Snowflake.\nThis approach enables a **smooth migration path** without needing to freeze development or halt operations during the transition.\nOne of the challenges we faced during this migration was around the **cost challenges around our CDC-driven data sync process:**\nSection Title: ... > The Problem\nContent:\nA recurring process was running **every 10 minutes for every tenant** , pulling fresh changes into Snowflake.\nAlthough each individual query was relatively fast, they all ran **simultaneously** , causing a **massive burst in resource demand** at very short intervals.\nAs a result, the Snowflake warehouse would **scale from 0 to 19 clusters almost instantly** to handle the surge.\nHowever, Snowflake’s **scale-down delay** meant that clusters remained active for a while even after queries finished, leading to **significant unnecessary compute costs** .\n ... \nSection Title: ... > The Result\nContent:\nCluster usage stabilized to **4–5 concurrent clusters** .\n**Credit usage dropped by ~150 per day** , amounting to savings of ~$10.5K–$12K monthly, depending on contract pricing.\nPress enter or click to view image in full size\nSection Title: ... > 4. Building Cost and Performance Observability\nContent:\nMonitoring and observability are baked into everything we do as SREs — and Snowflake is no exception. We recently integrated Snowflake with **Datadog** to centralize metrics and alerts.\n ... \nSection Title: ... > 🔹 Cost\nContent:\n**Anomaly Detection:\n** Detects unusual cost spikes **per user** , based on a rolling 7-day baseline.\n**Overall Usage Alerts:\n** Monitors daily **warehouse-level and account-level credit usage** against historical patterns and thresholds.\n ... \nSection Title: ... > 🔍 Monitoring Examples\nContent:\nAND (QUEUED_PROVISIONING_TIME + QUEUED_REPAIR_TIME + QUEUED_OVERLOAD_TIME) > 5000\nGROUP BY\nWAREHOUSE_NAME, MINUTE_START, MINUTE_END\nORDER BY\nWAREHOUSE_NAME, MINUTE_START;\nSection Title: ... > Final Thoughts\nContent:\nThis journey is ongoing. We’re still refining, experimenting, and improving — but what we’ve done so far has already delivered measurable value.\nOur top takeaways?\n**Start with visibility:** You can’t optimize what you can’t see.\n**Collaborate deeply across roles:** The biggest wins came from SRE, DBA, Data Engineering, and Development working in sync.\n**Be surgical, not blunt:** Wholesale changes risk breakage — simulate, roll out gradually, and keep observability close.\n**Instrument everything:** Monitoring isn’t an afterthought — it’s the enabler of sustained cost control.\nWe’ll continue to evolve our practices and share more as we go. If you’re navigating a similar journey, we’d love to hear your stories.\n**By Dor Harchol**\nSection Title: ... > Final Thoughts\nContent:\nDor Harchol is a Site Reliability Engineer at Optimove, a software company that is the leader in Positionless Marketing. Dor specializes in building resilient infrastructure, automating operations, and ensuring high system reliability across cloud environments. With a strong background in production monitoring, DevOps tooling, and data-driven operational improvements, he plays a key role in optimizing the performance of mission-critical systems. Dor is passionate about identifying opportunities to reduce cloud and infrastructure costs without compromising scalability or performance. He holds a B.Sc. in Industrial Engineering & Management from Shenkar College.\nPress enter or click to view image in full size\nPerformance\nCost Optimization\nObservability\n--\n--\n[](https://medium.com/snowflake?source=post_page---post_publication_info--96f8ac9ca883---------------------------------------)\nSection Title: ... > Final Thoughts\nContent:\n[](https://medium.com/snowflake?source=post_page---post_publication_info--96f8ac9ca883---------------------------------------)\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--96f8ac9ca883---------------------------------------)\n10.9K followers\n· Last published just now\nBest practices, tips & tricks from Snowflake experts and community\n ... \nSection Title: ... > No responses yet\nContent:\n[Text to speech](https://speechify.com/medium?source=post_page-----96f8ac9ca883---------------------------------------)"]},{"url":"https://ternary.app/blog/top-snowflake-cost-management-tools/","title":"Top 4 Snowflake Cost Management & Optimization Tools | Ternary","publish_date":"2025-12-11","excerpts":["Section Title: Top 4 Snowflake cost management tools > Can I just use Snowflake native cost tools?\nContent:\nTake Resource Monitors: they can tell you a warehouse exceeded its quota, but they can’t explain **why** . They don’t surface inefficient queries, runaway workloads, or which teams and users are driving the spike.\nSnowflake also doesn’t automatically map warehouse or query usage to business units, cost centers, or environments. This makes accurate cost allocation slow, manual, and often incomplete.\nIf you want to track Snowflake alongside the rest of your cloud ecosystem such as AWS, Google Cloud, Databricks, or other SaaS data tools, native Snowflake features cannot provide unified visibility.\nFor organizations with multiple teams, fast-growing workloads, or high Snowflake usage, native tools cannot deliver the visibility, automation, or intelligence required to proactively manage spend.\nA third-party Snowflake cost management platform fills those gaps by offering:\nSection Title: Top 4 Snowflake cost management tools > Can I just use Snowflake native cost tools?\nContent:\n**Full cost visibility and attribution** across teams and workloads\n**Intelligent forecasting** that predicts spend before it spikes\n**Automated alerts and anomaly detection** that highlight issues in real time\nIn short, Snowflake shows you what you spent. A third-party Snowflake management tool explains why it happened and how to reduce it.\n ... \nSection Title: Top 4 Snowflake cost management tools > ... > 1. Ternary\nContent:\nThe platform provides industry-leading capabilities across anomaly detection, budget management, cost allocation, and forecasting, making it a great tool for organizations that require cost management and optimization across their entire infrastructure.\nSection Title: Top 4 Snowflake cost management tools > ... > 2. Select.dev\nContent:\n[ [Source](https://select.dev/) ]\nSelect.dev is a dedicated Snowflake cost management platform. Its two main functions are Snowflake observability and optimization. Like Ternary, it also gives you a detailed visibility into credits down to individual queries, workloads, users, and teams.\nIn addition to providing visibility, it also continuously analyzes your account to inform you about optimization opportunities.\nThere’s also support for automatic warehouse adjustments. For example, it can scale virtual warehouses up or down based on actual usage patterns with zero intervention from you.\nYou can also allocate costs to specific teams or projects, then set budget thresholds for them so they feel accountable for their spending.\nSelect.dev has built-in [anomaly detection](https://ternary.app/platform/anomaly-detection/) too, which informs you about spikes through channels like Slack and email.\nSection Title: Top 4 Snowflake cost management tools > ... > 3. Datadog\nContent:\n[ [Source](https://www.datadoghq.com/blog/snowflake-monitoring-datadog/) ]\nDatadog isn’t exclusively a Snowflake cost management tool, but like Ternary, it does provide visibility into Snowflake.\nThe platform shows you metrics on query performance, warehouse activity, credit consumption, storage usage, and more.\nYou have access to pre-built dashboards that show things like top warehouses by cost, query throughput, failed tasks, and login history. If these dashboards aren’t the right fit for you, you can also build custom views for specific teams or business units.\nWith Datadog Cloud Cost Management, you can filter costs by service, tag, or team, then track how spending trends over time. You can also view cost-per-query to identify expensive queries and trace down the code or application that triggers them."]},{"url":"https://dev.to/qvfagundes/building-multi-tenant-analytics-with-snowflake-rbac-and-sigma-computing-part-1-foundation-and-co8","title":"Building Multi-Tenant Analytics with Snowflake RBAC and Sigma Computing: Part 1 - Foundation and Data Modeling - DEV Community","publish_date":"2025-06-24","excerpts":["Section Title: Forem Feed > DEV Community > HMPL.js Forem\nContent:\nFollow\nFor developers using HMPL.js to build fast, lightweight web apps. A space to share projects, ask questions, and discuss server-driven templating\nSkip to content\n[Powered by Algolia](https://www.algolia.com/developers/?utm_source=devto&utm_medium=referral)\n ... \nSection Title: Building Multi-Tenant Analytics with Snowflake RBAC and Sigma Computing: Part 1 - Foundation and ...\nContent:\n# analytics # snowflake # dataengineering # datascience\nSection Title: Introduction\nContent:\nAs a data architect, I've seen countless teams struggle with building custom analytics dashboards that scale across multiple clients while maintaining strict data isolation. After implementing this architecture for several enterprise clients, I want to share the complete technical approach that eliminates 80% of custom dashboard complexity.\nThis post covers the end-to-end implementation of a multi-tenant analytics platform using `Snowflake's RBAC system` with Sigma Computing, including production-ready code samples and best practices.\nSection Title: Architecture Overview\nContent:\n```\n┌─────────────┐    ┌──────────────┐    ┌─────────────┐    ┌─────────────┐\n│   MySQL     │───▶│  Snowflake   │───▶│    DBT      │───▶│   Sigma     │\n│(Production) │    │  (Raw Layer) │    │(Transform)  │    │(Dashboards) │\n└─────────────┘    └──────────────┘    └─────────────┘    └─────────────┘\n                            │\n                            ▼\n                   ┌─────────────────┐\n                   │   Gold Layer    │\n                   │ (Business Ready)│\n                   └─────────────────┘\n```\n ... \nSection Title: 1. Snowflake RBAC Foundation > ... > 2. Row-Level Security Implementation\nContent:\n*Assumption: Your silver layer already has a client_code column for filtering.*\n```\nCREATE TABLE silver . user_client_mapping ( username STRING , client_code STRING , access_level STRING ); CREATE OR REPLACE SECURE VIEW gold . customer_metrics AS SELECT cm . * FROM silver . customer_metrics cm JOIN silver . user_client_mapping ucm ON cm . client_code = ucm . client_code WHERE ucm . username = CURRENT_USER ();\n```\nThis automatically filters data based on who's logged in. No application-level security needed!\n ... \nSection Title: 1. Snowflake RBAC Foundation > ... > Key Takeaways for Part 1\nContent:\nAt this point, you've established:\n**Secure Foundation** : Client-specific roles with proper inheritance;\n**Automated Security** : Row-level security that works without application logic;\n**Scalable Data Models** : DBT transformations that handle multiple clients efficiently;\n**Flexible Filtering** : Macros that make client-specific data processing simple.\nWhat's Next?\nIn **Part 2** , we'll dive into the advanced features that make this architecture production-ready:\nAdvanced Snowflake security features and cost controls\nSigma Computing integration and dynamic dashboards\nMonitoring, observability, and performance optimization\nProduction deployment strategies and automated client onboarding\nThe foundation we've built here ensures that your data is secure and your transformations are maintainable. Part 2 will show you how to operationalize this at scale.\nSection Title: 1. Snowflake RBAC Foundation > Top comments (0)\nContent:\nSubscribe\nPersonal Trusted User\nCreate template\nTemplates let you quickly answer FAQs or store snippets for re-use.\nSubmit Preview Dismiss\nCode of Conduct • Report abuse\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's permalink .\nHide child comments as well\nConfirm\nFor further actions, you may consider blocking this person and/or reporting abuse\nVinicius Fagundes\nFollow\nData Architect building scalable cloud-native platforms AWS Snowflake Python. Transform fragmented data into AI-ready pipelines; integrate LLMs; optimize cost & performance.\nhttps://x.com/dfagundesv\nJoinedJun 19, 2025"]},{"url":"https://docs.snowflake.com/pt/user-guide/cost-exploring-compute","title":"Exploração do custo de computação - Snowflake Documentation","excerpts":["No menu de navegação, selecione Admin » Cost management. ; Selecione Consumption. ; Selecione Compute no menu suspenso Tipo de uso. ; No menu suspenso Tags, ..."]},{"url":"https://www.snowflake.com/en/developers/guides/snowflake-build-secure-multitenant-data-applications-snowflake-sigma/","title":"Build and Secure Multi-Tenant Data Applications with Snowflake and Sigma","excerpts":["Section Title: Build and Secure Multi-Tenant Data Applications with Snowflake and Sigma > Setting up the Workbook\nContent:\n3.1 From the home page select `Create New` then select `Workbook` :\n3.2 In the left-hand pane, select `Add New` then click `Table.` :\n3.3 Next, select `Tables and Datasets.` :\n3.4 For your source, select `Connections` , then choose the connection we created earlier. If you did not choose to rename it, it should be labeled something like `Snowflake PC_SIGMA_WH.` :\n3.5 When you have found the connection, expand the `FROSTBYTE_TASTY_BYTES` database, and select the `Orders` table from the Harmonized Schema.:\n**NOTE:**\nThis is different than the Orders_V view.\nSection Title: Build and Secure Multi-Tenant Data Applications with Snowflake and Sigma > Setting up the Workbook\nContent:\n3.6 You will notice that Sigma populates a preview of the table you have selected. On this interface, you have the ability to select which columns from the table you would like to bring into the workbook table. De-select `Customer Id, First Name, Last Name, Email, Phone Number, Children Count, Marital Status, Order Tax Amount, and Order Discount Amount` . You will not need these columns for the purpose of this lab.\nThen Click `Select` in the bottom right corner:\n3.7 Rename the table by clicking on the title `Orders` , and rename it to `Orders Base Table` :\n**IMPORTANT:**\nIn Sigma, it is best practice to begin with base elements, and build child elements like visualizations from there.\n3.8 Select the drop down arrow to the right of the `Order Ts` Column. Select `Truncate Date` , then choose `Day` .\nSection Title: Build and Secure Multi-Tenant Data Applications with Snowflake and Sigma > Setting up the Workbook\nContent:\nThis will change the formatting to show the timestamp as just the year, month, and day of the timestamp:\n3.9 Double click the column name and rename `Purchase Date` :\n3.10 Scroll through the Orders Base Table to the right until you locate the `Unit Price, Price, Order Amount, and Order Total` columns. Use `Shift + Click` to select all 4 columns, then hit the `Format As Currency` (or Dollar Sign) button on the top formula bar to format these columns as currency:\n3.11 Now we will add another table to the `Workbook Page` .\nClick the `plus sign` in the left hand pane to add a new element, and select `Table` . Click the tab for `New` then select `Tables and Datasets` .\nThis time, you are looking for the `Menu` table in the `RAW_POS` schema within the `FROSTBYTE_TASTY_BYTES` database. We only want the columns `Menu Item ID and Cost of Goods Usd` . Click `Select` when done.\nSection Title: Build and Secure Multi-Tenant Data Applications with Snowflake and Sigma > Setting up the Workbook\nContent:\n3.12 We now have two tables on the page and we want to create a relationship between them.\nOn your `Orders Base Table` , scroll all the way to the far right column `Order Total` , then click the drop down arrow. Select `Add column via lookup` .\n3.13 From the drop down, select `MENU` as the element, and `Cost of Goods Usd` as the column to add. Map the elements on `Menu Item Id` , then click `Done` :\n3.14 With the `Cost of Goods Usd (Menu)` column still selected, click the `Format as Currency` button once again to format the cost as a currency.\n3.15 Click the arrow next to `Cost of Goods Used (MENU)` and select `Add new column` :\n3.16 In the formula bar, type `[Quantity] * [Cost of Goods Usd (MENU)]` and hit enter.\nThis will create a new calculated column. Name this column `COGS` (Cost of Goods Sold)"]},{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/container-cost-governance","title":"Costs associated with apps with containers | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nDeveloper Snowflake Native App Framework Costs associated with apps with containers\nSection Title: Costs associated with apps with containers ¶\nContent:\nFeature — Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nThis topic describes the costs associated with developing, publishing and using a\nSnowflake Native App with Snowpark Container Services. It contains information for both providers and consumers.\nSection Title: Costs associated with apps with containers ¶ > Costs to consumers ¶\nContent:\nA Snowflake Native App may incur costs in the consumer account. The total cost of running\na Snowflake Native App with Snowpark Container Services is determined by the following:\nCosts determined by the provider\nInfrastructure costs\nSection Title: Costs associated with apps with containers ¶ > ... > Costs determined by the provider ¶\nContent:\nA provider may monetize a Snowflake Native App using any of the paid listing pricing models that are available in the Snowflake Marketplace. These models include subscription based and usage based plans.\nThis cost to the consumer is determined by the provider. Consumers pay for provider software via the Snowflake Marketplace in addition to costs associated with running Snowflake\ninfrastructure, including warehouses and compute pools.\nSection Title: Costs associated with apps with containers ¶ > Costs to consumers ¶ > Infrastructure costs ¶\nContent:\nAll infrastructure costs, including those related to compute pools, warehouse compute, storage, and\ndata transfer are the responsibility of the consumer of a Snowflake Native App.\nA consumer can use the IN ACCOUNT clause of the SHOW COMPUTE POOLS command to see all compute pools in their account\nand the current state of the compute pool. Costs are not incurred when a compute pool is suspended.\nA Snowflake Native App with Snowpark Container Services requires at least one compute pool and might require multiple compute pools to run as\nintended. A consumer has full control over the compute resources that the app requires, and may suspend a\ncompute pool or drop an application at any time.\nSection Title: Costs associated with apps with containers ¶ > Costs to consumers ¶ > Infrastructure costs ¶\nContent:\nSeparate charges for compute pool compute related to the Snowflake Native App with Snowpark Container Services appear on the customer billing\nstatement. A consumer can determine the compute pool billing charges for a Snowflake Native App with Snowpark Container Services using the ACCOUNT USAGE views provided by\nSnowpark Container Services.\nFor more details, such as the consumption table for compute pools, contact your account representative.\nSection Title: Costs associated with apps with containers ¶ > Costs to providers ¶\nContent:\nProviders can also incur costs when developing and maintaining a Snowflake Native App with Snowpark Container Services, including the\nfollowing:\n ... \nSection Title: Costs associated with apps with containers ¶ > Costs to providers ¶\nContent:\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nCosts to consumers\nCosts to providers\nRelated content\nAbout the Snowflake Native App Framework\nLanguage: **English**\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês"]}],"usage":[{"name":"sku_search","count":1}]}
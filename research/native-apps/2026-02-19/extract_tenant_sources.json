{"extract_id":"extract_bbd11bacd1a0467e927afd0c55bc5c5c","results":[{"url":"https://medium.com/snowflake/snowflake-cortex-and-multitenancy-e6509fd1eb5f","title":"Snowflake Cortex and Multitenancy | by Brian Hess | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2025-10-29","excerpts":["Sitemap\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nWrite\n\nSearch\n\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-e6509fd1eb5f---------------------------------------)\n\n·\n\nFollow publication\n\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-e6509fd1eb5f---------------------------------------)\n\nBest practices, tips & tricks from Snowflake experts and community\n\nFollow publication\n\n# Snowflake Cortex and Multitenancy\n\nBrian Hess\n\nFollow\n\n5 min read\n\n·\n\nOct 29, 2025\n\n25\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nMultitenancy is a crucial concept in software architecture, especially when designing applications that serve multiple customers or users efficiently. While often discussed, the nuances between multi-tenant and multi-user systems, and the various approaches to implementing them, can be complex and even more complicated when used with generative AI, such as Snowflake Cortex. Let’s break it down.\n\n## Multi-tenant vs. Multi-user: What’s the Difference?\n\nAt first glance, these two concepts seem very similar, and they do address a common problem: managing data access for different entities within a single system. In a multi-tenant system, you host data for multiple distinct “tenants,” which could be different organizations or large groups of users. The key principle here is strict data isolation: each tenant should only be able to access their own data. Think of it like an apartment building where each tenant has their own separate apartment.\n\nA multi-user system, on the other hand, hosts data for all users, but individual users should only be able to access a _subset_ of that data that is relevant to them. This is more like a shared office space where everyone has access to certain common areas but also has their own private desk or office, while managers have access to the private desks of all their employees.\n\n## Approaches to Multitenancy\n\nThere are two primary architectural approaches to implementing multitenancy.\n\n### Multi-Tenant Tables (MTT)\n\nThe first approach is **Multi-Tenant Tables (MTT)** , which works by mixing data from all tenants together within the same tables. A specific column in these tables (e.g., tenant\\_id) indicates which tenant each row of data belongs to. For implementation, you need a mechanism to map each user to their corresponding tenant\\_id value, ensuring that queries always include a predicate to filter data for the correct tenant.\n\n### Object-Per-Tenant (OPT)\n\nPress enter or click to view image in full size\n\nThe second approach is **Object-Per-Tenant (OPT)** (or Table-Per-Tenant). With this method, each tenant’s data is stored in separate tables, or even separate schemas or databases, providing a higher degree of physical isolation. Implementation requires creating new Snowflake objects (tables, schemas, etc.) every time a new tenant is onboarded. You also need a way to map each tenant to their specific tables (often through database, schema, or table naming conventions) and map users to their respective tenants. A key consideration for OPT is that it is generally more reasonable for small to medium numbers of tenants/users (typically less than a few hundred) due to the overhead of managing individual objects per tenant.\n\n## Multitenancy in Applications\n\nWhen building applications, how do you handle multitenancy? Many applications construct their own SQL queries. In this scenario, the application explicitly handles tenancy logic: for **MTT** , the application adds the tenant\\_id predicate to every query where appropriate, and for **OPT** , the application dynamically chooses the correct tenant-specific table or schema to query.\n\n## Get Brian Hess’s stories in your inbox\n\nJoin Medium for free to get updates from this writer.\n\nSubscribe\n\nSubscribe\n\nApplications can also leverage database context for multitenancy. For **MTT** , the application can set a session context (e.g., a session variable or role) and rely on Row Access Policies (RAP) or Views to automatically filter data based on that context. For example, the View or RAP could have a predicate that matched the tenant\\_id column with the current role.\n\nFor **OPT** , tenants can be placed in their own schema (or optionally, their own database) and common table names can be used within each schema. The application then sets the current schema for the session and issues queries using relative table names, ensuring it only accesses the data within that tenant’s schema.\n\n## Multitenancy in Snowflake Cortex\n\nSnowflake Cortex, and other “text-to-SQL” technologies, presents unique challenges for multitenancy because the application doesn’t directly control SQL creation, making it difficult to add tenant predicates directly to the generated SQL. Snowflake Cortex Analyst returns SQL as a string, so it’s hard to edit this string to inject tenancy logic (you would need to parse the returned SQL statement, which is not trivial).\n\nSome higher-level generative AI solutions, such as Snowflake Cortex Agent, will generate the SQL and then execute the generated query on behalf of the caller. This fully-automated solution is powerful, but does present some challenges for multitenant situations. Specifically, these solutions need to provide some way to set a context (e.g., session variable or role for MTT architectures, or the schema for OPT architectures) before executing the generated SQL.\n\nOther systems are more semi-automatic, where the generated SQL is returned to the caller, at which point the caller will execute the SQL itself and re-call the generative AI with a pointer to the results of the SQL query. This semi-automatic approach is helpful for multitenant use cases as the caller is responsible for executing the SQL, so the caller can set the appropriate session context before executing the query. The v1 API for Snowflake Cortex Agent follows this semi-automatic approach.\n\n### Snowflake Cortex and MTT\n\nPress enter or click to view image in full size\n\nFor Multi-Tenant Table (MTT) in Snowflake Cortex, you can use session context with Row Access Policies (RAP) or Views. This can be based on the current role, which is good for a small number of tenants and requires creating a Snowflake object (like a role) for each tenant during onboarding. Alternatively, it can be based on a session variable, where you set a session variable to indicate the tenant. It’s important to also restrict access to the current user being the app service user or app service role to maintain security. An entitlement table can also be used to map roles or session variable values to the allowed tenant column (e.g., tenant\\_id) values, providing a flexible way to manage access.\n\n### Snowflake Cortex and OPT\n\nFor Object-Per-Tenant (OPT) in Cortex, consider using a Semantic View, Semantic Search Service, or even an Agent per tenant. If you’re already creating a table per tenant, the additional overhead of managing an Agent per tenant (or similar structures) might not be a significant concern. Then, when calling the Snowflake Cortex Agent API, choose the appropriate Agent, Semantic View(s), and/or Semantic Search Service(s) for the tenant of interest.\n\n## Conclusion\n\nUnderstanding these approaches is key to designing robust and scalable multi-tenant solutions, especially within specialized environments like Snowflake Cortex where direct SQL manipulation might be limited.\n\nSnowflake\n\nSnowflake Cortex\n\nText To Sql\n\nMultitenancy\n\n25 \n\n25\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--e6509fd1eb5f---------------------------------------)\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--e6509fd1eb5f---------------------------------------)\n\nFollow\n\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--e6509fd1eb5f---------------------------------------)\n\n11K followers\n\n· Last published 18 hours ago\n\nBest practices, tips & tricks from Snowflake experts and community\n\nFollow\n\nFollow\n\n## Written by Brian Hess\n\n240 followers\n\n· 1 following\n\nI’ve been in the data and analytics space for over 25 years in a variety of roles. Essentially, I like doing stuff with data and making data work.\n\nFollow\n\n## No responses yet\n\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--e6509fd1eb5f---------------------------------------)\n\nWrite a response\n\nWhat are your thoughts?\n\nCancel\n\nRespond"],"full_content":null},{"url":"https://developers.snowflake.com/wp-content/uploads/2021/05/Design-Patterns-for-Building-Multi-Tenant-Applications-on-Snowflake.pdf","title":"","publish_date":null,"excerpts":["Design Patterns for Building\n\nMulti-Tenant Applications\n\non Snowflake\n\n**Contents**\n\n**Introduction**\n\n**2**\n\n**Part One: Comparing Multi-Tenant Design Patterns**\n\n**2**\n\nMulti-tenant table (MTT)\n\n2\n\nObject per tenant (OPT)\n\n3\n\nAccount per tenant (APT)\n\n3\n\nSummarizing the three patterns\n\n4\n\n**Part Two: Exploring Each Design Pattern**\n\n**6**\n\n**MTT design notes**\n\n**6**\n\nMaintaining the entitlement table\n\n9\n\nAuthenticating to Snowflake\n\n11\n\nIsolating working databases if needed\n\n13\n\nIsolating workloads as needed or pooling to save costs\n\n14\n\nRouting users to warehouses\n\n15\n\n**OPT design notes**\n\n**17**\n\nUsing automation to create new tenants\n\n17\n\nOPT authentication and authorization considerations\n\n18\n\nIsolating ingestion/transformation databases in the OPT model\n\n19\n\nIncorporating OPT to facilitate multi-region data sharing\n\n20\n\n**APT design notes**\n\n**22**\n\nAPT authentication considerations\n\n23\n\nAPT ingestion/transformation considerations\n\n24\n\n**Part Three: Evaluating Tenancy Models**\n\n**25**\n\nStorage and security considerations\n\n25\n\nEncryption, isolation, and data protection considerations\n\n26\n\nBilling, resource utilization, and network policy considerations\n\n27\n\nPage 1\n\n**Introduction**\n\nMulti-tenant Snowflake applications typically conform to one of three design patterns:\n\n●\n\n**Multi-Tenant Table (MTT)** : MTT consolidates tenants within a shared table or warehouse.\n\nCentralizing tenants in single, shared objects enables tenants to share compute and other\n\nresources efficiently.\n\n●\n\n**Object Per Tenant (OPT):** OPT isolates tenants into separate tables, schemas, databases,\n\nand warehouses. Although this approach allocates individual objects to tenants, the\n\napplication still operates within a single Snowflake account.\n\n●\n\n**Account Per Tenant (APT):** APT isolates tenants into separate Snowflake accounts. Unlike\n\nOPT, each tenant within the application has its own dedicated Snowflake account.\n\nTenancy models have different advantages regarding security, storage, compute, and connectivity,\n\nand a hybrid approach may be needed to properly address these considerations. Hybrids between\n\ntenancy models are common. For example, a design might use a multi-tenant table to consolidate\n\nstorage but allocate dedicated compute resources to each tenant, thereby forming an MTT/OPT\n\nhybrid design.\n\nThis white paper has three parts:\n\n●\n\nPart One helps data application builders understand the pros-and-cons and\n\ncosts-and-benefits of the various patterns\n\n●\n\nPart Two describes each pattern in greater detail and offers guidance on the Snowflake\n\nfeatures required for proper implementation\n\n●\n\nPart Three provides additional information to help you evaluate tenancy models based on\n\nsecurity, storage, and compute requirements\n\n**Part One: Comparing Multi-Tenant**\n\n**Design Patterns**\n\nThis section compares and contrasts the three patterns and describes the requirements that tend\n\nto favor one design approach over the others.\n\nMulti-tenant table (MTT)\n\nMTT's chief characteristics are **scalability** and **architectural simplicity** .\n\n●\n\nMTT is the most scalable design pattern in terms of the number of tenants an application\n\ncan support. This approach supports apps with millions of tenants.\n\nPage 2\n\n●\n\nIt has a simpler architecture within Snowflake. Simplicity matters because object\n\nproliferation makes managing myriad objects increasingly difficult over time. With MTT,\n\nadding tenants does not cause the number of objects to grow, but adding tenants to OPT\n\nand APT can result in hundreds or thousands of objects being created within Snowflake.\n\nFrom a cost standpoint, MTT is usually more cost-efficient because multiple customers utilize\n\nshared compute and other resources more efficiently.\n\nBut MTT has a somewhat rigid requirement: To use MTT, an app's data model has to have the same\n\ngeneral shape across all tenants. Application builders can achieve slight variances using custom\n\ncolumns that only apply to certain types of tenants, but this approach introduces sparsity into the\n\ndata.\n\nObject per tenant (OPT)\n\nOPT is a great fit if each tenant has a different data model. Unlike MTT, the tenant data shape can\n\nbe unique for each tenant. OPT does not scale as easily as MTT, however. OPT typically scales well\n\nfrom tens to hundreds of tenants, but starts to become unwieldy when it includes thousands of\n\ntenant databases.\n\nSecurity can factor into the decision to use an OPT design pattern. Some customers prefer the\n\nOPT model because they don't want to manage an entitlement table, secure views, or row-level\n\nsecurity with strong processes behind it. They are, however, comfortable using RBAC to control\n\nwho has specific access to a database.\n\nSome apps that use the OPT model give customers their own dedicated compute resources to\n\nsatisfy contractual, security, or regulatory requirements.\n\nAccount per tenant (APT)\n\nAPT isolates tenants at the account level. Typically, customers have a strong security reason for\n\nchoosing this approach. For example, organizations bound by strict regulatory mandates may\n\nchoose this option if:\n\n●\n\nThey need to implement a dedicated connection string per tenant\n\n●\n\nThey require security measures such as Bring Your Own Tool (BYOT)\n\n●\n\nThey want to use per-tenant IP restrictions at the account level\n\nAPT requires the customer to also implement OPT, which can support a huge variety of tenant\n\ndata shapes. In addition, APT introduces more scaling limitations—tenant counts in the tens to low\n\nhundreds are typical, however, customers with higher tenant numbers exist. APT can become\n\nunwieldy when managing thousands of tenant accounts.\n\nPage 3\n\nSummarizing the three patterns\n\nTable 1 summarizes the similarities and differences among the three design patterns.\n\nPage 4\n\nTable 1: Design pattern similarities and differences\n\nMTT\n\nOPT\n\nAPT\n\n**Data model**\n\n**characteristics**\n\n● Tenant data needs to\n\nfollow the same general\n\nshape.\n\n● Data is stamped with a\n\ntenant\\_id , so within a\n\nrow it's easy to tell what\n\ntenant the data belongs\n\nto.\n\n● Tenant data shape can\n\nbe unique to each tenant\n\nor similar across multiple\n\ntenants.\n\n● Tenant data shape can\n\nbe unique to each\n\ntenant or similar\n\nacross multiple\n\ntenants.\n\n**Scalability**\n\n● Scales from tens to\n\nmillions of tenants and\n\nbeyond, although upper\n\nscale limits are\n\nunknown.\n\n● Scales from tens to\n\nhundreds of tenants in\n\ntypical deployments.\n\n● Scales from tens to\n\nlow hundreds of\n\ntenants in typical\n\ndeployments.\n\n**Security**\n\n**concerns**\n\n● Requires developers to\n\nmanage security, such as\n\nan entitlement table,\n\nsecure views, or\n\nrow-level security\n\nsettings.\n\n● Requires application\n\nowner to be proficient in\n\nRBAC and row-level\n\nsecurity.\n\n● Enables customers who\n\nare comfortable using\n\nRBAC to isolate tenants\n\nwithout requiring them\n\nto manage entitlement\n\ntables with strong\n\nprocesses.\n\n● Isolates tenants,\n\nthereby reducing the\n\nrisk of mismanaging\n\nsecurity.\n\n● Allows for strict\n\nsecurity measures\n\n(encryption keys, IP\n\nallow lists,\n\nbetter-than-RBAC\n\ncontrols) by isolating\n\ntenants by account.\n\n● Allows for strict\n\nnetwork measures,\n\nsuch as BYOT,\n\nSnowflake UI login,\n\nand dedicated\n\nconnection string per\n\ntenant.\n\nTable 2 lists notes and drawbacks to consider when evaluating design patterns.\n\nPage 5\n\nTable 2: Design pattern notes and drawbacks\n\nMTT\n\nOPT\n\nAPT\n\n**Notes**\n\n● Pooling customers on\n\nshared, scalable compute\n\nsaves money and is\n\nsimpler to operationalize.\n\n● Compute can be pooled or\n\nisolated per tenant based\n\non customer goals. Pooled\n\ncompute frequently saves\n\nmoney but increases the\n\npossibility of contention\n\nbetween tenants.\n\n● Using this design\n\nfeels familiar for\n\ncustomers who are\n\nre-platforming from\n\na legacy database\n\nplatform.\n\n**Drawbacks**\n\n● Multi-region data sharing\n\ncan be a challenge, but\n\nsee _Incorporating OPT to_\n\n_facilitate multi-region data_\n\n_sharing_ .\n\n● To improve performance\n\nyou might need to shard\n\nlarge tables .\n\n●\n\nMERGE , UPDATE , and\n\nauto-clustering\n\noperations can be a\n\nchallenge on very large\n\ntables.\n\n● It's hard to determine\n\nper-tenant storage costs\n\nin a multi-tenant table.\n\n● Creating objects within\n\nSnowflake is easy, but\n\nmaintaining a consistent\n\nstate across many similar\n\nobjects is hard. As\n\nnumbers increase, keeping\n\nobjects in sync becomes\n\ndifficult.\n\n● Compute per tenant can\n\nincrease costs because\n\nyou lose the ability to\n\npool compute across\n\ntenants.\n\n● Increased automation is\n\nrequired to maintain and\n\nversion objects.\n\n● Creating an account\n\nwithin Snowflake is\n\neasy, but\n\nmaintaining a\n\nconsistent state\n\nacross accounts is\n\nhard.\n\n● Compute per tenant\n\ncan increase costs\n\nbecause you lose\n\nthe ability to pool\n\ncompute across\n\ntenants.\n\n● Increased\n\nautomation is\n\nrequired to create\n\nand manage\n\naccounts and\n\nobjects.\n\n**Part Two: Exploring Each Design Pattern**\n\nMTT design notes\n\nThe logical diagram depicted in Figure 1 represents a fairly common application setup:\n\n●\n\nApplication users access tenant data via secure views in a serving database (highlighted in\n\nred)\n\n●\n\nAn _entitlements table_ controls which Snowflake users or roles have access to which tenants\n\n●\n\n_Secure_ _views_ ensure application users only see their tenant rows\n\n●\n\nAll tables are clustered by a tenant\\_id type column\n\nTo enforce that users can see only their tenant rows, tenants query through secure views, which\n\nJOIN base tables to the entitlements table on tenant\\_id . Common tables, where everybody\n\ngets to see all the rows, use _regular views_ pointed to the base tables.\n\nFigure 1: A serving database with secure views ensures application users see only their tenant rows.\n\nPage 6\n\nNote : Many of the concepts covered in this section apply to the OPT and APT models as well.\n\nSnowflake recommends creating a hierarchy of roles based on privilege and functional access,\n\nwith a role and user defined per tenant. Set the privileges for dedicated tenant roles by following\n\nrole-based hierarchy best practices.\n\nFigure 2 depicts application setup at the schema level. Secure views occupy one schema, and base\n\ntables and the entitlements table occupy a second schema to separate the privileges that\n\ndetermine who can access what data. Secure tables and common tables help segregate developer\n\nusers and application users. There may also be instances where you want to create sandbox areas\n\nfor individual customers to do more sophisticated things, and you can use a schema per customer\n\nto separate that as well. Users can be given default namespaces ( database.schema ) to further\n\ndirect access.\n\nFigure 2: Schema-level view of database objects based on RBAC\n\nSecure views use the current\\_role() parameter to filter the base table using a JOIN to the\n\nentitlements table where the value of current\\_role() matches one or more rows in the\n\nentitlements table.\n\nPage 7\n\nIn a data application you can implement secure views by user or by role.\n\nSecure views based on CURRENT\\_USER (see Figure 3) make sense if you have one database user\n\nper tenant and you don't need fine-grained control of different users within the tenant.\n\nFigure 3: Secure views based on CURRENT\\_USER()\n\nSecure views based on CURRENT\\_ROLE (see Figure 4) allow fine-grained entitlements between\n\napplication users. You can have multiple sets of privileges within a given tenant and selectively\n\nassign privileges to tenant users, for example users who can write data into the sandbox versus\n\nusers who cannot.\n\nPage 8\n\nFigure 4: Secure views based on CURRENT\\_ROLE()\n\n**Maintaining the entitlement table**\n\nApplication data security depends on the entitlements table working correctly, so managing the\n\nentitlement table is a major priority for data application builders. Snowflake recommends starting\n\nwith the following best practices.\n\nRegarding security:\n\n●\n\nLock down entitlement tables with restrictive permissions.\n\n●\n\nManage the entitlement table with a systematic process. Avoid poor practices such as\n\nadding new customers by running single INSERT/UPDATE statements against the\n\nentitlement tables.\n\n●\n\nEliminate human error by wrapping processing in procedures that are automated and have\n\ncontrols in place. Procedures can execute either inside or outside of Snowflake.\n\n●\n\nTo find issues, run regular regression tests after entitlement table updates to test secure\n\nview results against expected outcomes.\n\nRegarding optimization:\n\n●\n\nTenants should have a unique numeric identifier (that is, a tenant\\_id ).\n\nPage 9\n\n●\n\nCluster all transaction tables by tenant\\_id and a meaningful date field, at minimum.\n\n(The reverse, date then tenant\\_id , is also fine.)\n\n●\n\nSort load dimension tables representing tenants initially, and use incrementing identifiers\n\nfor tenants.\n\n●\n\nDespite the small size, cluster the entitlement table if there are a lot of users or roles per\n\ntenant; otherwise sort load.\n\nTable clustering is common in a multi-tenant model because each tenant typically can access only\n\nits own slice of the data. The type of table and the data model (such as star schema or highly\n\ndenormalized) also play a role in determining which tables you need to cluster.\n\nSometimes you can do simple sort ordering when the table loads to make the data easy to access\n\nand to help with partition pruning. But be aware that auto-clustering runs as a background service\n\nand is not instantaneous. Depending on how frequently the data is updated and loaded within the\n\napplication, auto-clustering may not be enough and may require additional workarounds, such as\n\nchanging how data pipelines are structured.\n\nPage 10\n\nFigure 5: Cluster tables by tenant\\_id and DATE\n\n**Authenticating to Snowflake**\n\nThe way application users connect to Snowflake is a little different from most other Snowflake\n\nusers. Because application users come through an application tier, users are typically unaware that\n\nSnowflake exists, as shown in Figure 6.\n\nPage 11\n\nFigure 6: Users authenticate to Snowflake through the application tier.\n\nApplications need to handle authentication to Snowflake on behalf of the user. There are multiple\n\nways to do authentication, but the following principles generally apply:\n\n●\n\nApplication users authenticate to the application as they normally would.\n\n●\n\nThere is typically a secrets manager at the application-tier level that stores credentials for\n\nthe corresponding Snowflake user. The application is programmed to obtain a Snowflake\n\nsession using one of several supported authentication methods.\n\n●\n\nUsers are only authorized to query secure views and are only authorized to see their\n\ntenant based on the user/role link to tenant\\_id .\n\nFigure 7 shows an application that establishes a Snowflake session based on a secrets manager\n\nlookup. Note that the application manages the key-pair user authentication flow and stores the\n\nSnowflake user and session access token. Network policies control access to Snowflake from the\n\napplication tier over Private Link, which is optional. And, finally, role-based access controls (RBAC)\n\nroute users to default warehouses and databases, and allow users to access only the data that they\n\nare permitted to see within the application.\n\nPage 12\n\nFigure 7: Obtaining and storing a user session via key-pair authentication\n\n**Isolating working databases if needed**\n\nIsolating working databases is optional. Some application builders directly load data into the\n\nserving database and Snowflake points to the initial landing tables. But other builders need to run\n\ntransformations in Snowflake before serving data, in which case a best practice is to separate the\n\nserving database from the working databases used for transformation or ingestion from outside\n\nsources. The application can be configured to write data to both the serving database or the\n\nworking databases as appropriate for the application functionality.\n\nSnowflake recommends separating databases to simplify application administration. For example,\n\nit's easier to configure RBAC to control \"what should be done where _\"_ and \"who has access to\n\nwhat _\"_ if databases are separate.\n\nRegarding workload processing, you can do some of these processes offline if that makes sense for\n\nthe application, and then apply them to the serving database as appropriate.\n\nPage 13\n\nFigure 8: Separate the working databases used for transformation or ingestion from the service\n\ndatabase as needed for your application.\n\n**Isolating workloads as needed or pooling to save costs**\n\nSimilar to database separation being a general best practice, workload separation based on the\n\ntype of workload is a good idea. Specific recommendations include:\n\n●\n\nGive developers their own warehouse for development work\n\n●\n\nPool application users on a common multi-cluster warehouse or isolate them onto\n\ndedicated warehouses based on application requirements\n\n●\n\nUse different warehouses for different application purposes\n\n●\n\nIsolate other workloads to their own warehouses\n\nWhen it comes to tenants, app builders need to make decisions around whether to give tenants a\n\ndedicated warehouse, versus pooling them on common warehouses or multi-cluster warehouses.\n\nCost will be a factor. You can pool dashboard queries more easily than ad hoc queries because\n\nthey're predictable. Ad hoc usage can introduce unexpected and unplanned expenses. Strict COGS\n\nper tenant calculations are a reason to separate tenants into dedicated warehouses because\n\npooled heuristics are less precise. Some applications pool users by default but offer the option to\n\npay extra to get a dedicated warehouse.\n\nPage 14\n\nFigure 9: Separating workloads based on the type of workload is a best practice.\n\n**Routing users to warehouses**\n\nTo make application management easier, it's essential to configure RBAC and default warehouses\n\nin Snowflake to route users to the correct warehouses. Proper planning and up front configuration\n\nwill ensure that user lookups within the secrets manager and the application tier will\n\nautomatically route the user to the right database and the right warehouse.\n\nThe following guidance applies to routing users:\n\n●\n\nYou can grant roles the privilege to operate (modify) or use (run queries against) a given\n\nwarehouse\n\n●\n\nUsers can be configured to use a specific warehouse by default, but roles cannot\n\n●\n\nUsers with access to multiple warehouses can choose to use a warehouse upon\n\nestablishing a session or before query execution\n\nPage 15\n\nFigure 10: Configuring RBAC and default warehouses in Snowflake makes managing the application\n\ntier easier.\n\nPage 16\n\nOPT design notes\n\nOPT enables you to isolate tenant data by database, schema, and table, and use RBAC to control\n\nwhich user or role can see or query an object. Separating customers into their own databases is\n\nthe most common practice (see Figure 11), because it is the easiest, cleanest isolation level, but\n\nsome app builders separate customers into dedicated tables, for example, in embedded analytics\n\nuse cases in which data applications create a report table per tenant.\n\nWhich objects to use for isolation depends on factors such as your data pipeline design, your\n\nsoftware development life cycle process, the consistency of your data shape, and more. How many\n\ntotal tenants do you expect to have? How many tables will you use? Think through the features\n\nyou plan to use, such as replication and zero-copy cloning. (Replication can only be done at the\n\ndatabase level. And, while zero-copy cloning can take place at all three levels, it's cleaner to clone a\n\ndatabase.) All of these factors and more come into play when you implement OPT.\n\nFigure 11: Isolating customers into their own databases is the most common OPT pattern.\n\n**Using automation to create new tenants**\n\nIf you implement the OPT or APT pattern, use automation to create new tenants (see Figure 12).\n\nAutomation can be written inside or outside of Snowflake to create new tenants based on a\n\ntemplate. Your template should cover databases, schemas, tables, compute, security, and anything\n\nelse new tenants require. Automation is necessary because when you start to get into the\n\nPage 17\n\nhundreds and thousands of objects, tenant creation and ongoing enhancements become too\n\nunwieldy to manage any other way.\n\nThird-party products, such as Flyway and others, can help synchronize template updates with\n\nexisting tenants.\n\nFigure 12: Use automation to create and synchronize tenants when implementing either OPT or\n\nAPT.\n\n**Authenticating and authorizing**\n\nOPT authentication and authorization is similar to MTT (see _Authenticating to Snowflake_ ), but with\n\nOPT, routing users to the right databases becomes even more important.\n\nThe routing process is similar to what is described in _Routing users to warehouses_ (see Figure 13),\n\nbut users are routed to different objects because the context changes relative to MTT. When done\n\nproperly, user lookup within the secrets manager and application tier automatically routes the\n\nuser to the right database and the right warehouse.\n\nPage 18\n\nFigure 13: OPT authentication is unchanged from MTT, but routing users to the right databases\n\nbecomes even more important.\n\n**Isolating ingestion/transformation databases in the OPT model**\n\nWhen planning how to isolate your serving database from your working\n\n(ingestion/transformation) database, consider how your data will fan out to and fan in from tenant\n\ndatabases for common processing. Frequently, running separate workloads on a separate,\n\nper-tenant basis will cost more than consolidating the workloads into a single instance. For\n\nexample, if you settle on multiple tables per tenant and each has its own pipeline, the cost will\n\nlikely be higher than if you manage a single transformation process in a common data store that\n\napplication users cannot access. If necessary, after transformation you can distribute data into\n\nmultiple tenant-specific objects or store data in a single, shared, serving database.\n\nTo optimize efficiency and cost, consider hybrid models, such as the hybrid OPT/MPT model\n\ndescribed above.\n\nPage 19\n\nFigure 14: Tenant data can be ingested and transformed through one working database and fanned\n\nout to tenant serving databases.\n\n**Incorporating OPT to facilitate multi-region data sharing**\n\nAs noted previously, multi-region data sharing can be a challenge for the MTT model. If you need\n\nto share data in cloud/region pairs other than your primary one, and you do not want to replicate\n\nall tenant data to all cloud/region pairs, consider incorporating OPT into your MTT design.\n\nBecause Snowflake supports replication at the entire database level, it's not possible to send only\n\ncertain tenant slices from a multi-tenant database somewhere else. While it's possible to replicate\n\nan entire multi-tenant table to all clouds and regions where it's required, over time this design will\n\nbecome unmanageable as data sizes and the number of tenants grow. For example, Figure 15\n\nshows a multi-tenant, multi-CSP app design. Customer D shares data on GCP, but it does not make\n\nsense to replicate Customer D's data on Azure if no one accesses it there.\n\nPage 20\n\nNote : If you have a data sharing use case, consider using Snowflake Data Marketplace to take\n\nadvantage of the latest features.\n\nFigure 15: If you do not want to replicate all tenant data to all cloud/region pairs, consider\n\nincorporating OPT into your MTT design.\n\nPage 21\n\nAPT design notes\n\nWith the APT model, there is typically one Snowflake account, one warehouse, and one database\n\nper tenant.\n\nThere can be exceptions. For example:\n\n●\n\nMultiple tenants can share an account to form a hybrid of APT and MTT (see Figure 16).\n\n●\n\nThere might be an additional administrative warehouse for data loading or administrative\n\nactivities, depending on whether the data is going out to the account through data sharing,\n\nor if some form of ETL or ELT is used to do additional processing within the tenant account.\n\nFor example, some applications load the data and don't need to do anything further in the\n\ntenant account because it's done elsewhere.\n\n●\n\nMany APT designs can rely on single-cluster warehouses. A heavily used application may\n\nrequire many clusters, including multi-cluster warehouses.\n\nFigure 16: Multiple tenants can share an account to form a hybrid of APT and MTT.\n\nPage 22\n\n**Authenticating**\n\nWith APT, authenticating via the application tier largely works the same as with MTT and OPT.\n\nThe key difference is that the account URL changes per tenant. It's also possible for users to log in\n\nto their Snowflake account directly, either through the UI or a BYOT solution.\n\nFigure 17: Sometimes data application builders decide not to authenticate users in the application\n\nBYOT solution.\n\nPage 23\n\n**Ingesting and transforming data**\n\nAs mentioned previously, applications typically use a central account to manage the working\n\ndatabases used for ingestion from outside sources or transformation.\n\nYou can share tenant data with tenant accounts using Snowflake Secure Data Sharing. This can\n\nalso be done with an MTT or OPT approach.\n\nYou could also use ELT or Snowflake replication to materialize data in the tenant accounts.\n\nFigure 18: Data can pass to the account through an ETL/ELT process, or through Secure Data\n\nSharing in which the data is directly loaded into the accounts.\n\nPage 24\n\n**Part Three: Evaluating Tenancy Models**\n\nYou should evaluate all three tenancy models, but Snowflake recommends starting with the MTT\n\npattern. It's generally instructive to first evaluate if the MTT pattern will work—and, if not, why.\n\nStorage and security considerations\n\nFigure 19: Flowchart for evaluating storage and security requirements. OPT here refers to\n\ndatabases, schemas, and tables (not virtual warehouses and compute).\n\nDecisions hinge on:\n\n●\n\nContractual obligations that dictate how data should be stored and encrypted\n\n●\n\nRegulatory obligations that dictate how data should be stored and encrypted\n\n●\n\nInfoSec standards on how data should be stored and encrypted\n\n●\n\nApplication owner's perspective on the enforcement of database RBAC\n\n●\n\nApplication owner's perspective on the enforcement of row-level security through\n\nentitlement tables and views\n\n●\n\nHow customers access the application:\n\n○\n\nThrough an application UI?\n\n○\n\nThrough the Snowflake UI?\n\n○\n\nThrough a BYOT solution?\n\n●\n\nHow consistent data shapes (data models) are across customers\n\nPage 25\n\nEncryption, isolation, and data protection considerations\n\n●\n\nTri-secret Secure (Bring Your Own Key) is available at only the account level in Snowflake.\n\n●\n\nSnowflake uses a hierarchy of encryption keys at the account, table, and file level to\n\nencrypt data-at-rest and prevent data from being accessed between accounts (except data\n\nsharing).\n\n●\n\nSnowflake is a multi-tenant service, and the cloud object store is a multi-tenant service, so\n\ndata is not truly isolated at the public cloud level, but encryption creates the isolation.\n\n●\n\nDatabases and schemas are largely logical constructs; they don’t physically separate data.\n\nFigure 20: Flowchart for evaluating compute and security requirements. OPT means one virtual\n\nwarehouse per tenant where MTT refers to tenants on a pooled virtual warehouse.\n\nDecisions hinge on:\n\n●\n\nContractual obligations that dictate how tenants should be isolated on raw compute.\n\n●\n\nRegulatory obligations that dictate how tenants should be isolated on raw compute.\n\n●\n\nInfoSec standards on how tenants should be isolated on raw compute.\n\n●\n\nNetwork policy requirements. Will user-based network policies work? Or do you have\n\nmore complex requirements that require account-based policies?\n\n●\n\nIf virtual warehouse cache constitutes data that must be isolated.\n\n●\n\nHow COGS are managed per tenant or billed back to the customer.\n\n(Calculating COGS per tenant is more straightforward when each tenant has its own\n\ncompute resources. If tenants share compute resources, you can use a heuristic to\n\nPage 26\n\ncalculate COGS per tenant, but it's not as precise. Some apps need the precision, and some\n\nare fine with a reasonable approximation.)\n\n●\n\nHow customers access the application:\n\n○\n\nThrough an application UI?\n\n○\n\nThrough the Snowflake UI?\n\n○\n\nThrough a BYOT solution?\n\n●\n\nHow many tenants could use a single virtual warehouse concurrently.\n\nBilling, resource utilization, and network policy considerations\n\n●\n\nSnowflake network policies (IP allow lists) can be applied only at the account or user level.\n\n●\n\nSnowflake virtual warehouses cache data from object stores temporarily for whole or\n\npartial reuse in subsequent queries. RBAC and secure view rules still apply.\n\n●\n\nSnowflake compute billing is done at the virtual warehouse level. Calculating per-query,\n\nper-user, or per-tenant costs can be inexact if tenants share compute.\n\n●\n\nSnowflake virtual warehouses do not allow for resource limits per user or per tenant.\n\n**About Snowflake**\n\nSnowflake delivers the Data Cloud—a global network where thousands of organizations mobilize\n\ndata with near-unlimited scale, concurrency, and performance. Inside the Data Cloud,\n\norganizations unite their siloed data, easily discover and securely share governed data, and\n\nexecute diverse analytic workloads. Wherever data or users live, Snowflake delivers a single and\n\nseamless experience across multiple public clouds. Snowflake’s platform is the engine that powers\n\nand provides access to the Data Cloud, creating a solution for data warehousing, data lakes, data\n\nengineering, data science, data application development, and data sharing. Join Snowflake\n\ncustomers, partners, and data providers already taking their businesses to new frontiers in the\n\nData Cloud. Snowflake.com.\n\nPage 27"],"full_content":null},{"url":"https://www.snowflake.com/en/blog/snowflake-native-apps-security/","title":"Snowflake Native Apps: Secure By Design | Blog","publish_date":"2025-06-27","excerpts":["blog\n\n##### Category\n\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\n\nProduct and Technology\n\nSEP 26, 2023 | 8 min read\n\n# How Snowflake Native Apps Deliver Security for App Builders and Consumers\n\nThe [Snowflake Native App Framework](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/) , which leverages Snowflake’s advanced architecture, allows for a new level of security for applications. This security spans not just the application consumer, but also the application provider. Controlling all software and infrastructure in the Snowflake Data Cloud, Snowflake can protect the application code, reducing risk for app providers. For consumers, protection comes via extensive capabilities in [role-based access control](https://www.snowflake.com/en/fundamentals/rbac/) (RBAC), which is used to control application access.\n\n### Challenges for applications\n\nThe application market has been growing at a fast pace. This creates several challenges that prevent greater application adoption: * Data protection and governance\n* Data sovereignty\n* Third-party agreements\n* Security audits\n* Security patch and maintenance\n Application builders spend a considerable amount of time developing custom processes and algorithms. Before Snowflake Native Apps, application builders had to choose between a managed offering where the provider hosts the infrastructure, the code and the user experience, or a licensing model where the code/packages are given to the application consumer to host on their own hardware. Traditional software started with licensing agreements where application customers were expected to install and operate the software bought from an application builder. Unfortunately, the team that was expected to operate the application generally knew very little about the solution because it was built by a third party. Operations teams were concerned about introducing issues while patching and upgrading these systems, resulting in systems that were not actively patched. Application consumers, on the other hand, really liked this model for their data protection and governance because the solutions operated on the data in its existing location and, in most cases, a third party didn’t need access. For the application providers, not having control over the code/executables not only puts the license at risk, but also puts their business at risk should their software be reverse engineered. Any time an application is packaged and given to a customer to run on their own hardware, reverse engineering those packages becomes much simpler. The code can be debugged and memory can be easily inspected. App builders can build in protection to try to prevent reverse engineering, such as by obfuscating executables, but this is complicated, error prone, and still doesn’t provide complete protection. The emergence of SaaS applications and managed services alleviated some of these challenges, but also introduced new ones. With the SaaS model, when application builders control all hardware and software, they maintain control over their code at all times. The builder can also push any urgent updates and security patches as needed. The challenge in this shift in the industry is that these applications often require the application consumer to provide access to or a copy of their data to the third-party provider. The builder has, or can have, direct access to this information, which puts more risk on the application consumer. The data that is being stored is outside the consumer’s systems, so the third party has to be evaluated for trust and security risk, and processes have to be carefully checked for compliance.\n\n### Snowflake Native Apps: Secure for builders\n\n#### Compute and data isolation\n\nSnowflake Native Apps (currently in public preview on AWS and private preview on GCP and Azure) are deployed into the application consumer’s Snowflake account. This offers an advantage in that the application is hosted in isolation from other consumers and their data. Multi-tenant applications are often difficult to secure and require a significant amount of engineering and testing to verify data cannot be accessed by unauthorized or malicious users. For example, if a [managed application](https://www.snowflake.com/blog/powered-by-snowflake-building-a-connected-application-for-growth-and-scale/) is vulnerable to a SQL Injection attack, it’s very possible the attacker could gain access to other users’ data. In the case of a Snowflake Native App, the application has only been granted access to the application consumer’s data in the consumer’s own account. If an exploit is found, thanks to the way Snowflake Native Apps isolates each tenant’s data, an application user would only gain access to their own data that they provided to the application.\n\nSnowflake Native Apps are deployed into the consumer’s account. In most application models, this would also allow the consumer access to the builder’s binaries and the servers hosting them. Snowflake’s architecture, in which the servers are administered by Snowflake and not by the customer, helps providers protect the application code from prying eyes. Here you can see the objects and code when the application is installed in the provider’s account in debug mode:\n\nWhen installed in the consumer account, the views and functions were configured to not be accessible. The SHOWME\\_VIEW and SUPERSECRETSAUCE can still be used by the SHOWME\\_PROC during execution, but the existence of and the code backing all these objects is protected.\n\nSnowflake can allow the consumer to access parts of the application and data, which are granted to the application without giving application consumers, even ACCOUNTADMIN, access to the code/data/logs. With Snowflake Native Apps, providers that had concerns about protecting their code are no longer limited to SaaS options. A whole new market is opening up for consumers and providers that want more control over their data.\n\n#### Package security\n\n[Snowpark for Python](https://www.snowflake.com/en/data-cloud/snowpark/) is commonly used in Snowflake Native Apps to provide features which were not possible using Snowflake’s SQL engine. While Python is extremely powerful, it could be a vector for additional exploits—and our customers spoke loudly that package security and governance was of utmost concern. Any developer on the internet can publish a Python package, or in some cases hijack an existing project. To mitigate these concerns, Snowflake teamed up with [Anaconda](https://anaconda.com) to deliver a vast number of packages to run directly in Snowflake. Because Anaconda maintains its own security practices around curation, patches, scanning and release security, customers can optionally rely on Anaconda to provide up-to-date packages. When building Snowflake Native Apps, you can use any package in the [Snowflake conda channel](https://repo.anaconda.com/pkgs/snowflake/) . The packages in the conda channel are vetted and patched by Anaconda. Snowflake Native App publishing and upgrades are gated on a security scan, which checks for known security vulnerabilities in the application. Approval is only granted after the app passes the scan. Snowflake also scans packages and code included in Snowflake Native Apps for security concerns prior to making them available in [Snowflake Marketplace](https://app.snowflake.com/marketplace/) .\n\n#### Upgrading applications\n\nApplication consumers have always expected that the SaaS applications they purchase would be upgraded and kept secure by the application provider. Snowflake offers application providers a way to push updates to customers in an automated rollout, improving both user experience and security. Even though Snowflake Native Apps are installed and run in the application consumer’s account, the application provider still has control over the release cycle. To release a new version of the application, the builder can create a new [release directive](https://docs.snowflake.com/en/developer-guide/native-apps/versioning) to push that update to a select set of users or to all users. This capability gives the provider confidence in knowing that all applications are patched (or upgraded) even though the application is deployed in many accounts.\n\n### Snowflake Native Apps: Secure for consumers\n\n#### Data control and sovereignty\n\nApplication consumers install a Snowflake Native App from a provider in their own account. Access to any data that application needs must be granted to the application by the consumer administrator, so it is well known what the application can access and use. In Snowflake Native Apps, the data doesn’t need to be moved or sent to a third party, which makes it much easier to be compliant with security requirements and data sovereignty rules.\n\n#### Application permissions\n\nWhen a provider publishes an application, it includes a [manifest](https://docs.snowflake.com/en/developer-guide/native-apps/creating-manifest) file. This manifest contains the permissions to be granted to the application, which can be reviewed by the application consumer during installation. By default, a Snowflake Native App has no permissions outside of the application itself, including data access or the ability to create database objects (tables, tasks, warehouses, etc). Additional rights can be granted to the application, but that must be done by the application consumer administrators through explicit grants or [references](https://docs.snowflake.com/en/developer-guide/native-apps/requesting-refs) . Application code runs as a new application role, with owner’s rights, such that it can only access what that role has been granted to do. This means that the application can’t access data that the caller has access to—it must be explicitly granted permission.\n\n#### Upgrading applications\n\nSnowflake Native Apps will automatically be kept up to date by the provider. The Snowflake Native App Framework has all the capabilities needed to upgrade in place and push updates to customers. This empowers the builder to keep all their installations patched, and the application consumers can rely on them always being up to date with the latest changes without any effort.\n\n### A secure framework for applications\n\nSnowflake Native Apps were developed to fulfill needs that were previously unmet in the industry, and Snowflake is uniquely positioned to provide applications from providers to consumers while protecting each party. Snowflake Native Apps leverage some of the biggest strengths in managed applications, provider-managed code and upgrades, and protecting the provider's code, along with the biggest strengths of traditional applications, which keep control and sovereignty of data in the consumers' hands. Check out the [Snowflake Native App Framework](https://www.snowflake.com/en/data-cloud/workloads/applications/native-apps/) or use our [Quickstart tutorial](https://quickstarts.snowflake.com/guide/getting_started_with_native_apps) to develop a Snowflake Native app for yourself. We look forward to seeing what you build!\n\n#### Snowflake Native App Bootcamp\n\n[start now!](https://www.snowflake.com/snowflake-native-app-bootcamp/)\n\n###### Author\n\nBrad Culberson\n\n###### Share Article\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fsnowflake-native-apps-security&title=How+Snowflake+Native+Apps+Deliver+Security+for+App+Builders+and+Consumers)\n\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fsnowflake-native-apps-security&text=How+Snowflake+Native+Apps+Deliver+Security+for+App+Builders+and+Consumers)\n\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fsnowflake-native-apps-security)\n\nSubscribe to our blog newsletter\n\nGet the best, coolest and latest delivered to your inbox each week\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\n[start for free](https://signup.snowflake.com/)\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n* [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n* [Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n* [Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n* [Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n* [Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n* [Technology](https://www.snowflake.com/en/solutions/industries/technology/)\n\nLearn * [Resource Library](https://snowflake.com/en/resources/)\n* Live Demos\n* [Fundamentals](https://www.snowflake.com/en/fundamentals/)\n* [Training](https://www.snowflake.com/en/resources/learn/training/)\n* [Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n* [Snowflake University](https://learn.snowflake.com/en/)\n* [Developer Guides](https://www.snowflake.com/en/developers/guides)\n* [Documentation](https://docs.snowflake.com/)\n\n* [Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n* [Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\n* Cookie Settings\n* [Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Legal](https://www.snowflake.com/en/legal/)\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"],"full_content":null},{"url":"https://medium.com/snowflake/mastering-snowflake-cost-optimization-at-scale-optimoves-real-world-multi-tenant-saas-strategies-96f8ac9ca883","title":"Mastering Snowflake Cost Optimization at Scale: Optimove’s Real-World Multi-Tenant SaaS Strategies | by Dor Harchol | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2025-06-09","excerpts":["Sitemap\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nWrite\n\nSearch\n\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-96f8ac9ca883---------------------------------------)\n\n·\n\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-96f8ac9ca883---------------------------------------)\n\nBest practices, tips & tricks from Snowflake experts and community\n\n# **Mastering Snowflake Cost Optimization at Scale: Optimove’s Real-World Multi-Tenant SaaS Strategies**\n\nDor Harchol\n\n8 min read\n\n·\n\nJun 9, 2025\n\n\\--\n\nListen\n\nShare\n\n## Introduction\n\nSnowflake is a powerhouse for modern data-driven SaaS companies — but as data loads and complexity scale, so do the costs. At Optimove, we have been laser-focused on migrating workloads from legacy systems like MSSQL into Snowflake over the past few years. However, as part of our ongoing commitment to optimizing our processes and technology, just as we help our customers optimize their marketing campaigns, it was time to turn our attention to cost visibility, efficiency, and long-term sustainability.\n\nWe’re still in the middle of this transformation. This post shares our journey so far — how we identified inefficiencies, rethought warehouse allocation, and introduced real-time observability. The work isn’t over yet, and we anticipate sharing more discoveries and tooling in future posts.\n\nThis initiative was a true cross-functional effort, with close collaboration between **SREs, DBAs, Data Engineers, and Developers** . If you’re operating a multi-tenant SaaS business and feel your Snowflake bill is growing faster than your insights, this story is for you.\n\n## Multi-Tenant SaaS and the Snowflake Challenge\n\nIn a multi-tenant SaaS environment, many core processes run across all customers- but each tenant differs in usage patterns, data volume, and product interaction. A query that’s quick for one tenant can choke resources for another.\n\nThis creates a challenge: how do you allocate Snowflake resources efficiently while balancing fairness, performance, and cost?\n\nWe approached this challenge by identifying waste, rethinking warehouse strategies, and instrumenting our platform with fine-grained observability.\n\n## Snowflake System Views: Building Observability for Cost Optimization\n\nA key enabler of our optimization journey was the ability to deeply understand Snowflake’s internal operations through the system views and metadata it exposes.  \nStrong observability is essential for any team aiming to achieve cost efficiency, and Snowflake provides rich data sources that help analyze usage patterns, resource scaling, and credit consumption.\n\nHere are the main system views and tables we relied on:\n\n* **QUERY\\_HISTORY  \n  ** Track all executed queries, their runtimes, queued times, warehouse usage, and cluster scaling behavior.\n* **WAREHOUSE\\_UTILIZATION  \n  ** Analyze warehouse efficiency metrics such as utilization percentages and concurrency.\n* **WAREHOUSE\\_METERING\\_HISTORY  \n  ** Understand credit consumption per warehouse, broken down by compute usage, cloud services usage, and more.\n* **QUERY\\_ATTRIBUTION\\_HISTORY  \n  ** Attribute credit usage to specific queries, users, and workloads, allowing precise visibility into who or what is driving costs.\n* **WAREHOUSE\\_CLUSTER\\_UTILIZATION  \n  ** Dive deeper into multi-cluster warehouse behavior, observing when and how many clusters were used during query execution.\n\nFamiliarizing yourself with these views allows you to build strong internal monitoring, make informed decisions about warehouse sizing and query design, and ultimately control your Snowflake costs more effectively.\n\n## 1\\. Consolidating Underutilized Warehouses\n\n## The Starting Point\n\nOur first big win came from analyzing warehouse utilization. Snowflake makes it easy to spin up specialized warehouses, but without guardrails, these can proliferate, leading to waste and underuse.\n\n## The Solution\n\nWe began by identifying warehouses that were not utilized but still burning credits. To do this, we used a query that returns a **list of warehouses with p50 utilization (excluding idle time) below 30% over the past 7 days** , along with their compute credit usage.\n\n### 🔍 Example Query — Identify Underutilized Warehouses\n\n_WITH low\\_util\\_wh AS (SELECT warehouse\\_name,_\n\n_APPROX\\_PERCENTILE(utilization, .5) AS p50\\_utilization_\n\n_FROM SNOWFLAKE.ACCOUNT\\_USAGE.WAREHOUSE\\_UTILIZATION_\n\n_WHERE start\\_time::date >= DATEADD(day, -7, CURRENT\\_DATE)_\n\n_AND utilization > 0_\n\n_GROUP BY ALL_\n\n_HAVING p50\\_utilization < 30)_\n\n_SELECT l.warehouse\\_name,_\n\n_l.p50\\_utilization,_\n\n_SUM(a.credits\\_used\\_compute) AS total\\_credits_\n\n_FROM SNOWFLAKE.ACCOUNT\\_USAGE.WAREHOUSE\\_METERING\\_HISTORY a_\n\n_JOIN low\\_util\\_wh l_\n\n_ON a.warehouse\\_name = l.warehouse\\_name_\n\n_WHERE a.start\\_time::date >= DATEADD(day,-7,CURRENT\\_DATE)_\n\n_GROUP BY ALL_\n\n_ORDER BY total\\_credits DESC;_\n\n## The Result\n\nWe decommissioned around five low-efficiency warehouses and merged their workloads into a small number of **multi-cluster, highly utilized warehouses** . After rollout, we closely monitored cost attribution per user and workload, and observed a **~50% reduction in compute credit usage** from those workloads.\n\n## 2\\. Warehouse-Per-Tenant vs. Warehouse-Per-Process\n\n## A Strategic Shift\n\nUniform warehouse allocation across all tenants in a shared process proved inefficient. Some tenants needed significantly more compute, while others barely scratched the surface.\n\n## The Process\n\nWe developed a structured methodology for optimizing cost-heavy processes:\n\n1. **Identify the Problem Process:** **Use user-level cost observability to find which process was consuming disproportionate Snowflake credits** . To achieve this, we integrated **Datadog** with our Snowflake accounts and used Datadog’s **custom metrics** feature to pull live data directly from Snowflake’s internal views. Specifically, we query the SNOWFLAKE.ACCOUNT\\_USAGE.QUERY\\_ATTRIBUTION\\_HISTORY view, which gives us detailed attribution of compute credit usage per user, query, and time window.\n2. **Collaborate on Query Optimization:** Work with Data Engineers, Developers, and DBAs to improve query plans and reduce unnecessary compute. In this case, we had an internal query that performed a **PIVOT** operation to generate a structured output for **inserting Snowflake data into a CSV file** . Initially, the PIVOT operated over raw, semi-aggregated data, making the operation slow and compute-intensive. To optimize it, we **moved the MAX() aggregation directly into the source query** feeding the PIVOT. By pre-aggregating the necessary data before pivoting, we minimized the amount of data the PIVOT had to scan and transform, resulting in **significantly faster execution times** and more efficient use of resources.\n3. **Design WH-per-Tenant Strategy:**\n\n* Define WH tiers for the process (e.g., XS, M, XXL).\n* Identify **indicators** that signal which tenant should use which tier — this might include: Data volume in key tables, Frequency or depth of usage of that feature\n\n**Optimove Approach:**\n\nAt Optimove, we recognized that **larger tenants typically have both a higher number of users and more attributes per user** .\n\nTo create an effective scaling indicator, we **multiplied the number of users by the number of attributes** for each tenant, generating a **“tenant score”** .\n\nThis score determined which warehouse tier (XS, M, XXL) the tenant would be routed to, ensuring better performance and cost efficiency.\n\n**4\\. Simulate the Change:** Compare credit usage and WH behavior before and after the change.\n\n**5\\. Deploy with Guardrails:** Apply changes gradually in production.\n\n* Ensure production teams have config-level override access to force specific WHs for troubleshooting or performance needs.\n\n**6\\. Monitor, Measure, Document:** Track costs post-rollout, document the decision logic, and ensure easy configurability for future updates.\n\n## The Result\n\nOne such process saw compute usage drop from **244 to 35 credits/day** , while performance and reliability improved dramatically.\n\nPress enter or click to view image in full size\n\n## 3\\. Flattening Workload Spikes with Scheduled Offsets\n\n## The Problem\n\nAt Optimove, we are transitioning legacy workloads from **MSSQL** to **Snowflake** using a **Change Data Capture (CDC)** approach.  \nWe utilize a CDC tool to **continuously replicate changes** from MSSQL into Snowflake, allowing our production applications to gradually move from reading and updating data on MSSQL to operating directly on Snowflake.  \nThis approach enables a **smooth migration path** without needing to freeze development or halt operations during the transition.\n\nOne of the challenges we faced during this migration was around the **cost challenges around our CDC-driven data sync process:**\n\n* A recurring process was running **every 10 minutes for every tenant** , pulling fresh changes into Snowflake.\n* Although each individual query was relatively fast, they all ran **simultaneously** , causing a **massive burst in resource demand** at very short intervals.\n* As a result, the Snowflake warehouse would **scale from 0 to 19 clusters almost instantly** to handle the surge.\n\nHowever, Snowflake’s **scale-down delay** meant that clusters remained active for a while even after queries finished, leading to **significant unnecessary compute costs** .\n\n## The Solution\n\nWe introduced **randomized execution offsets** in the scheduler. Instead of firing all processes at 10:00, 10:10, etc., the load is now staggered (e.g., 10:03, 10:13…), smoothing the execution curve.\n\n## The Result\n\n* Cluster usage stabilized to **4–5 concurrent clusters** .\n* **Credit usage dropped by ~150 per day** , amounting to savings of ~$10.5K–$12K monthly, depending on contract pricing.\n\nPress enter or click to view image in full size\n\n## 4\\. Building Cost and Performance Observability\n\nMonitoring and observability are baked into everything we do as SREs — and Snowflake is no exception. We recently integrated Snowflake with **Datadog** to centralize metrics and alerts.\n\n## Key Monitoring Areas\n\n### 🔹 Performance\n\n* **Cluster Saturation:  \n  ** Alert if a warehouse is using its **maximum cluster count for X consecutive minutes** within a 24-hour window.\n* **Queued Queries:  \n  ** We only alert on queries that **waited more than 5 seconds** to reduce noise and focus on true pain points.\n* **Warehouse Configuration Drift:  \n  ** All warehouses are standardized to use **multi-cluster with auto-suspend/resume** . We alert on any drift from this configuration.\n\n### 🔹 Cost\n\n* **Anomaly Detection:  \n  ** Detects unusual cost spikes **per user** , based on a rolling 7-day baseline.\n* **Overall Usage Alerts:  \n  ** Monitors daily **warehouse-level and account-level credit usage** against historical patterns and thresholds.\n\n### 🔍 Monitoring Examples\n\n**PRODUCTION Cluster Usage Over Time**\n\nSELECT\n\nWAREHOUSE\\_NAME wh\\_name,\n\nDATE\\_TRUNC(‘minute’, start\\_time) start\\_minute,\n\nMAX(CLUSTER\\_NUMBER) clusters\n\nFROM SNOWFLAKE.ACCOUNT\\_USAGE.QUERY\\_HISTORY\n\nWHERE\n\nexecution\\_status = ‘SUCCESS’\n\nAND start\\_time >= DATEADD(hour, -12, CURRENT\\_TIMESTAMP())\n\nAND WAREHOUSE\\_NAME IS NOT NULL\n\nAND WAREHOUSE\\_NAME LIKE ‘%PRODUCTION%’\n\nAND WAREHOUSE\\_SIZE IS NOT NULL\n\nGROUP BY ALL\n\nORDER BY start\\_minute\n\nLIMIT 10000;\n\n**Queued Queries Over Time Buckets**\n\nSELECT\n\nWAREHOUSE\\_NAME,\n\nTO\\_CHAR(DATEADD(‘MINUTE’, 10 \\* FLOOR(DATE\\_PART(‘MINUTE’, START\\_TIME) / 10), DATE\\_TRUNC(‘HOUR’, START\\_TIME)), ‘YYYY-MM-DD HH24:MI’) AS MINUTE\\_START,\n\nTO\\_CHAR(DATEADD(‘MINUTE’, 10 \\* (FLOOR(DATE\\_PART(‘MINUTE’, START\\_TIME) / 10) + 1), DATE\\_TRUNC(‘HOUR’, START\\_TIME)), ‘YYYY-MM-DD HH24:MI’) AS MINUTE\\_END,\n\nCOUNT(\\*) AS QUEUED\\_QUERIES\n\nFROM\n\nSNOWFLAKE.ACCOUNT\\_USAGE.QUERY\\_HISTORY\n\nWHERE\n\nSTART\\_TIME >= DATEADD(‘HOUR’, -1, CURRENT\\_TIMESTAMP())\n\nAND (QUEUED\\_PROVISIONING\\_TIME + QUEUED\\_REPAIR\\_TIME + QUEUED\\_OVERLOAD\\_TIME) > 5000\n\nGROUP BY\n\nWAREHOUSE\\_NAME, MINUTE\\_START, MINUTE\\_END\n\nORDER BY\n\nWAREHOUSE\\_NAME, MINUTE\\_START;\n\n## Final Thoughts\n\nThis journey is ongoing. We’re still refining, experimenting, and improving — but what we’ve done so far has already delivered measurable value.\n\nOur top takeaways?\n\n* **Start with visibility:** You can’t optimize what you can’t see.\n* **Collaborate deeply across roles:** The biggest wins came from SRE, DBA, Data Engineering, and Development working in sync.\n* **Be surgical, not blunt:** Wholesale changes risk breakage — simulate, roll out gradually, and keep observability close.\n* **Instrument everything:** Monitoring isn’t an afterthought — it’s the enabler of sustained cost control.\n\nWe’ll continue to evolve our practices and share more as we go. If you’re navigating a similar journey, we’d love to hear your stories.\n\n**By Dor Harchol**\n\nDor Harchol is a Site Reliability Engineer at Optimove, a software company that is the leader in Positionless Marketing. Dor specializes in building resilient infrastructure, automating operations, and ensuring high system reliability across cloud environments. With a strong background in production monitoring, DevOps tooling, and data-driven operational improvements, he plays a key role in optimizing the performance of mission-critical systems. Dor is passionate about identifying opportunities to reduce cloud and infrastructure costs without compromising scalability or performance. He holds a B.Sc. in Industrial Engineering & Management from Shenkar College.\n\nPress enter or click to view image in full size\n\nPerformance\n\nCost Optimization\n\nObservability\n\n\\-- \n\n\\--\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--96f8ac9ca883---------------------------------------)\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--96f8ac9ca883---------------------------------------)\n\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--96f8ac9ca883---------------------------------------)\n\n11K followers\n\n· Last published 18 hours ago\n\nBest practices, tips & tricks from Snowflake experts and community\n\n## Written by Dor Harchol\n\n7 followers\n\n· 1 following\n\nSRE - Specializes in building resilient infrastructure, automating operations, and ensuring high system reliability across cloud environments.\n\n## No responses yet\n\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--96f8ac9ca883---------------------------------------)"],"full_content":null}],"errors":[],"warnings":[{"type":"warning","message":"Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.","detail":null}],"usage":[{"name":"sku_extract_excerpts","count":4}]}
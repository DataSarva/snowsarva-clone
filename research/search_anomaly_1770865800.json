{"search_id":"search_b7a53cc81ce2471ba89a8f5dbb618529","results":[{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization - Snowflake","excerpts":["Section Title: Cost Optimization > Recommendations\nContent:\n**Deliver clear, historical consumption insights:** Utilize\nconsistent in-tool visualizations or custom dashboards to monitor\nconsumption and contextualize spend on the platform with unit\neconomics. **Investigate anomalous consumption activity:** Review anomaly\ndetection to identify unforeseen cost anomalies and investigate\ncause and effect trends. **Control**\n**Proactively monitor all platform usage:** Define and enforce\nbudgets for projects and services, setting soft quotas to limit\nresource consumption and prevent runaway spending. **Forecast consumption based on business needs:** Establish a\nforecast process to project future spend needs based on business and\ntechnical needs. **Enforce cost guardrails for organizational resources:** Set up\nautomated checks (e.g., Tasks, query insights) and resource\nguardrails (e.g., warehouse timeout, storage policies, resource\nmonitors) to identify unusual usage patterns and potential\noverspending as they occur.\n ... \nSection Title: Cost Optimization > Visibility > Overview\nContent:\nThe Snowflake Visibility principle is designed to transform opaque cloud\nspending into actionable insights, fostering financial accountability\nand maximizing business value within your Snowflake environment. It is\nfoundational to the FinOps framework, as you cannot control, optimize,\nor attribute business value to what you cannot see. To effectively\nmanage and optimize cloud costs in Snowflake, it's crucial to align\norganizationally to an accountability structure of spend, gain deep and\ngranular insight into all aspects of your cloud spending, and\ntransparently display it to the appropriate stakeholders to take action.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nCost Anomaly Detection is a critical component of visibility that\nleverages machine learning to continuously monitor credit consumption\nagainst historical spending patterns, automatically flagging significant\ndeviations from the established baseline. This proactive monitoring is\nessential for preventing budget overruns and identifying inefficiencies,\nshifting the organization from a reactive to a proactive cost management\nposture to mitigate financial risk. As a best practice, you should\ninitially review anomaly detection on the entire account to gain a broad\nview, then dive deeper into a more granular review for individual\nhigh-spend warehouses. [This approach](https://docs.snowflake.com/en/user-guide/cost-anomalies) allows for more targeted analysis and assigns clear ownership for\ninvestigating any flagged anomalies. There are several methods for\nanomaly detection supported by Snowflake:\n ... \nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nSnowsight, Snowflake's primary web interface, offers a dedicated Cost\nManagement UI that allows users to visually identify and analyze the\ndetails of any detected cost anomaly. The importance of this intuitive\nvisual interface lies in its ability to make complex cost data\naccessible to a wide range of stakeholders, enabling rapid root cause\nanalysis by correlating a cost spike with specific query history or user\nactivity. One of the tabs in this UI is the Cost Anomaly Detection tab,\nwhich enables you to view cost anomalies at the organization or account\nlevel and explore the top warehouses or accounts driving this change.\nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nTo\nfoster a culture of cost awareness and accountability, it is a best\npractice to ensure there is an owner for an anomaly detected in the\naccount and set up a [notification (via email)](https://docs.snowflake.com/en/user-guide/cost-anomalies-ui) in the UI itself to ensure that cost anomalies are quickly and\naccurately investigated.\nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\n**Programmatic Cost Anomaly Detection**\nFor deeper integration and automation, organizations can review [anomalies programmatically](https://docs.snowflake.com/en/user-guide/cost-anomalies-class) using the SQL functions and views available within the SNOWFLAKE.LOCAL\nschema. This approach is important for enabling automation and\nscalability, allowing cost governance to be embedded directly into\noperational workflows, such as feeding anomaly data into third-party\nobservability tools or triggering automated incident response playbooks.\nA key best practice is to utilize this programmatic access to build\ncustom reports and dashboards that align with specific financial\nreporting needs and to create advanced, automated alerting mechanisms\nthat pipe anomaly data into established operational channels, such as\nSlack or PagerDuty.\n**Custom Anomaly Detection & Notification**\nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nAlthough anomalies are detected at the account and organization level,\nif you desire to detect anomalies at lower levels (e.g. warehouse or\ntable), it is recommended to leverage Snowflake’s [Anomaly Detection](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) ML class and pair it with a Snowflake [alert](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) to notify owners of more granular anomalies that occur within the\necosystem. This ensures all levels of Snowflake cost can be monitored in\na proactive and effective way.\n ... \nSection Title: Cost Optimization > Control > Overview > Proactively monitor all platform usage\nContent:\nTo effectively manage and [control Snowflake spend](https://docs.snowflake.com/en/user-guide/cost-controlling) ,\nit is essential to establish and enforce cost guardrails. Implementing a [budgeting system](https://docs.snowflake.com/en/user-guide/budgets) is a key\nFinOps practice that promotes cost accountability and optimizes resource\nusage by providing teams with visibility into their consumption and the\nability to set alerts and automated actions. Budgeting helps to prevent\nunexpected cost overruns and encourages a cost-conscious culture.\n**Set budgets permissions**\n ... \nSection Title: Cost Optimization > Control > Overview > Proactively monitor all platform usage\nContent:\n**Implement a notification strategy**\nEffective budget management relies on timely communication. Setting up\nalerting through emails or webhooks to collaboration tools like Slack\nand Microsoft Teams provides proactive [notification](https://docs.snowflake.com/en/user-guide/budgets/notifications) to key stakeholders when spending approaches or exceeds a defined\nthreshold. These alerts provide teams with an opportunity to review and\nadjust their usage before it leads to significant cost overruns. This\ncapability positions organizations for security success by mitigating\npotential threats through comprehensive monitoring and detection.\nSection Title: Cost Optimization > Control > Overview > Proactively monitor all platform usage\nContent:\nNotifications are not limited to just budgets; [Snowflake alerts](https://docs.snowflake.com/en/user-guide/alerts) can also be\nconfigured to systematically notify administrators of unusual or costly\npatterns, such as those listed in the Control and Optimize sections of\nthe Cost Pillar. This ensures that key drivers of Snowflake consumption\ncan be tracked and remediated proactively, even as the platform’s usage\ngrows.\n ... \nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\n**Continuous monitoring and variance analysis:** Regularly compare\nactual consumption against the forecast to identify and investigate\nsignificant variances. This feedback loop is crucial for refining the\nunderlying model and adapting to evolving business needs.\n**Collaborative governance:** Ensure a single source of truth for\nconsumption data by establishing a regular FinOps review session with\nFinance, Engineering, and Business teams. Use customized dashboards to\npresent data in business-friendly terms.\n**Implement predictive budget controls:** Shift from reactive spending\nto a proactive model. Utilize Snowflake Resource Monitors and Budgets,\nwhich employ monthly-level time-series forecasting, to define credit\nquotas and trigger automated alerts or suspensions to prevent cost\noverruns.\n ... \nSection Title: Cost Optimization > Control > Overview > Enforce cost guardrails for organizational resources\nContent:\n**Retained for clones:** This is data that is stored because it is\nreferenced by a clone, despite the data being deleted or outside the\nretention period of the base table that was cloned. To control costs\nrelated to these “stale” clones, it is recommended that you monitor [RETAINED_FOR_CLONE_BYTES](https://community.snowflake.com/s/article/How-to-manage-RETAINED-FOR-CLONE-BYTES) and drop clones that are no longer needed. You can leverage the [Alerts and Notifications features](https://docs.snowflake.com/en/guides-overview-alerts) to alert you when RETAINED_FOR_CLONE_BYTES exceeds a threshold, prompting you to take action.\n**Serverless Features**\n ... \nSection Title: Cost Optimization > Optimize > Overview > Limit data transfer\nContent:\nThis view provides detailed insights into data\nmovement between different regions and clouds. Establish dashboards\nand alerts to meticulously track this usage, enabling prompt detection\nof any unexpected cost increases.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Improve continually\nContent:\nBegin by regularly reviewing (usually on a weekly, bi-weekly, or monthly\ncadence) workloads that could benefit from optimization, using\nSnowflake's [Cost Insights](https://docs.snowflake.com/en/user-guide/cost-insights) ,\ndeviations in unit economics or health metrics (from the Visibility\nprinciple), or objects hitting control limits (e.g., queries hitting\nwarehouse timeouts from the Control principle). Once identified,\ninvestigate these findings through the Cost Management UI, Cost Anomaly\ndetection, Query History, or custom dashboards with Account Usage Views\nto pinpoint the root cause. Then, using the recommendations in the\nOptimize Pillar, make improvements to the workload or object.\n**Step 2: Estimate & test**\n ... \nSection Title: Cost Optimization > Optimize > Overview > Improve continually > On this page\nContent:\nLearn * [Resource Library](https://snowflake.com/en/resources/)\nLive Demos\n[Fundamentals](https://www.snowflake.com/en/fundamentals/)\n[Training](https://www.snowflake.com/en/resources/learn/training/)\n[Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n[Snowflake University](https://learn.snowflake.com/en/)\n[Developer Guides](https://www.snowflake.com/en/developers/guides)\n[Documentation](https://docs.snowflake.com/)\n[Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n[Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\nCookie Settings\n[Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Legal](https://www.snowflake.com/en/legal/)\n[](https://x.com/Snowflake \"X (Twitter)\")"]},{"url":"https://medium.com/snowflake/machine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee","title":"Machine Learning-Based Alerts for Snowflake FinOps | by Piotr Paczewski | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2023-12-04","excerpts":["Sitemap\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nWrite\n\nSearch\n\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-8ec640fb1cee---------------------------------------)\n\n·\n\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-8ec640fb1cee---------------------------------------)\n\nBest practices, tips & tricks from Snowflake experts and community\n\n# Machine Learning-Based Alerts for Snowflake FinOps\n\nPiotr Paczewski\n\n5 min read\n\n·\n\nDec 4, 2023\n\n\\--\n\nListen\n\nShare\n\nPress enter or click to view image in full size\n\nOverview of GenAI and LLM capabilities in Snowflake\n\n### Overview\n\nCost management is a key component of every successful cloud strategy. Snowflake provides a range of built-in tools to effectively manage cost, including:\n\n* [Cost exploration using Snowsight & account usage](https://docs.snowflake.com/en/user-guide/cost-exploring-overall)\n* [Resource monitors](https://docs.snowflake.com/en/user-guide/resource-monitors)\n* [Alerts & notifications](https://docs.snowflake.com/en/guides-overview-alerts)\n* [Budgets](https://docs.snowflake.com/en/user-guide/budgets)\n* [Object tagging](https://docs.snowflake.com/en/user-guide/object-tagging)\n\nThis article will demonstrate how you can leverage Snowflake alerts to automatically receive email notifications for anomalies detected in virtual warehouse compute usage using [Snowflake Cortex ML-Based Functions](https://docs.snowflake.com/en/guides-overview-ml-powered-functions) .\n\nThe cost of using Snowflake platform can be broken down into:\n\n* Compute resources (virtual warehouse compute, serverless compute, cloud service compute)\n* Storage\n* Data transfer\n\nVirtual warehouse compute usage can be monitored using [warehouse\\_metering\\_history](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history) view in [account usage schema](https://docs.snowflake.com/en/sql-reference/account-usage) . It shows hourly credit usage for virtual warehouses in your account within the last 365 days.\n\nPress enter or click to view image in full size\n\nSample result from querying [warehouse\\_metering\\_history](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history)\n\nTo automatically detect anomalies in virtual warehouse compute usage, Snowflake users can leverage machine learning. Snowflake Cortex ML-based functions simplify the complexities associated with using ML models and enable organizations to quickly gain insights into their data.\n\nTo train a machine learning using Snowflake Cortex ML-based functions, let’s start with splitting the data into a training dataset and a test dataset. Data from the last 12 months of virtual warehouse compute usage will be used, with the test dataset comprising the most recent 2 months and the remaining 10 months forming the training dataset.\n\n### Build ML model using Snowflake Cortex ML-based functions\n\nStep 1: Create the training dataset:\n\n```\ncreate or replace view warehouse_compute_usage_train as  \n  select  \n    to_timestamp_ntz(to_date(start_time)) as timestamp,  \n    sum(credits_used_compute) as credits_used  \n  from snowflake.account_usage.warehouse_metering_history  \n  where timestamp between dateadd(day,-365,current_date()) and dateadd(day,-61,current_date())  \n  group by all;\n```\n\nStep 2: Create the test dataset:\n\n```\ncreate or replace view warehouse_compute_usage_test as  \n  select  \n    to_timestamp_ntz(to_date(start_time)) as timestamp,  \n    sum(credits_used_compute) as credits_used  \n  from snowflake.account_usage.warehouse_metering_history  \n  where timestamp between dateadd(day,-60,current_date()) and current_date()  \n  group by all;\n```\n\nStep 3: Train an anomaly detection model using Snowflake Cortex ML-based functions:\n\n```\ncreate or replace snowflake.ml.anomaly_detection warehouse_usage_analysis(  \n  input_data => system$reference('view', 'warehouse_compute_usage_train'),  \n  timestamp_colname => 'timestamp',  \n  target_colname => 'credits_used',  \n  label_colname => ''  \n  );\n```\n\nStep 4: Run model inference using the trained model:\n\n```\ncall warehouse_usage_analysis!detect_anomalies(  \n  input_data => system$reference('view','warehouse_compute_usage_test')  \n  , timestamp_colname => 'timestamp'  \n  , target_colname => 'credits_used'  \n  );\n```\n\nPress enter or click to view image in full size\n\nResults of model inference\n\nStep 5: Visualize model results using Snowsight:\n\nPress enter or click to view image in full size\n\nVisualization of ML model inference using selected parameters.\n\nThe forecasted values are indicated by the red line, while the lower bounds and upper bounds of the forecasts are respectively represented by light blue and yellow lines.\n\nThe actual (observed) values are displayed using a dark blue line. Every point on this line that is not within the lower and upper bounds of the forecasts is marked an anomaly.\n\nBy running the SQL below, I am able to identify that they are 6anomalies detected in the test dataset.\n\n```\ncreate table warehouse_usage_anomalies   \n  as select * from table(result_scan(last_query_id()));  \n  \nselect * from warehouse_usage_anomalies   \n  where is_anomaly = true;\n```\n\nIn the code used to call the model inference, the _prediction\\_interval_ parameter value was not specified, therefore, the default value 0.99 was used. To mark more observations as anomalies, reduce the value of _prediction\\_interval_ and set it to, for instance, 0.9. On the other hand, to mark fewer observations as anomalies, the value of _prediction\\_interval_ parameter should be increased.\n\n### Create email notifications for newly detected anomalies using Snowflake Alerts and Tasks\n\nTo create automatic email notifications for new anomalies detected in virtual warehouse compute usage Snowflake Tasks & Alerts can be used.\n\nStep 1: Create a task to retrain ML model on a weekly basis at 5 AM every Sunday LA time:\n\n```\ncreate or replace task train_warehouse_usage_anomaly_task  \nwarehouse = demo_wh  \nschedule = 'USING CRON 0 5 * * 0 America/Los_Angeles'  \nas  \nexecute immediate  \n$$  \nbegin  \n  create or replace snowflake.ml.anomaly_detection warehouse_usage_analysis(  \n    input_data => system$reference('view', 'warehouse_compute_usage_train'),  \n    timestamp_colname => 'timestamp',  \n    target_colname => 'credits_used',  \n    label_colname => ''  \n    );  \nend;  \n$$;\n```\n\nStep 2: Create a task to call the anomaly detection model on a daily basis at 7 AM LA time and insert the result into warehouse\\_usage\\_anomalies table:\n\n```\ncreate or replace task inference_warehouse_usage_anomaly_task  \nwarehouse = demo_wh  \nschedule = 'USING CRON 0 7 * * * America/Los_Angeles'   \nas  \nexecute immediate  \n$$  \nbegin  \n  call warehouse_usage_analysis!detect_anomalies(  \n    input_data => system$reference('view','warehouse_compute_usage_test')  \n    , timestamp_colname => 'timestamp'  \n    , target_colname => 'credits_used'  \n    );  \ninsert into warehouse_usage_anomalies  \n  select * from table(result_scan(last_query_id()));  \nend;  \n$$;\n```\n\nStep 3: Set up an alert to check every day at 8 AM LA time if any new anomalies have been detected in warehouse compute usage:\n\n```\ncreate or replace alert warehouse_usage_anomaly_alert  \n  warehouse = demo_wh  \n  schedule = 'USING CRON 0 8 * * * America/Los_Angeles'  \n  if (exists (select * from warehouse_usage_anomalies where is_anomaly=True and ts > dateadd('day',-1,current_timestamp())))  \n  then  \n  call system$send_email(  \n    'warehouse_email_alert',  \n    'test@domain.com',  \n    'Warehouse compute usage anomaly detected',  \n    concat(  \n      'Anomaly detected in the warehouse compute usage. ',  \n      'Value outside of confidence interval detected.'  \n    )  \n  );\n```\n\nSimilarly to tasks, alerts are created in suspended state and need to be resumed in order to start running. After resuming the alert, this is the email I have received after some time:\n\nPress enter or click to view image in full size\n\nSample email notification received\n\n### Final note\n\nAnomaly detection using Snowflake Cortex ML-based functions works with both single-series and multi-series data. While this article focuses on single-series data, it’s important to note that it’s also possible to use anomaly detection functions with multi-series data to build separate, independent ML models, for example, for each virtual warehouse object.\n\nThere are various approaches that can be taken to anomaly detection in Snowflake. The decision to implement ML-based anomaly detection should be assessed on case-by-case basis. In certain scenarios, simple approaches, such as threshold-based anomaly detection, might be more practical than implementing ML-based methods.\n\n_I am currently a Snowflake Solutions Consultant at Snowflake. Opinions expressed in this post are solely my own and do not represent the views or opinions of my employer._\n\nSnowflake\n\nMlops\n\nFinops\n\nCost Optimization\n\nAI\n\n\\-- \n\n\\--\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--8ec640fb1cee---------------------------------------)\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--8ec640fb1cee---------------------------------------)\n\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--8ec640fb1cee---------------------------------------)\n\n10\\.9K followers\n\n· Last published 4 hours ago\n\nBest practices, tips & tricks from Snowflake experts and community\n\n## Written by Piotr Paczewski\n\n36 followers\n\n· 3 following\n\nSolutions Consultant at Snowflake. Opinions expressed are solely my own and do not represent views of my employer\n\n## No responses yet\n\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--8ec640fb1cee---------------------------------------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Status](https://status.medium.com/?source=post_page-----8ec640fb1cee---------------------------------------)\n\nAbout\n\nCareers\n\nPress\n\n[Blog](https://blog.medium.com/?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8ec640fb1cee---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----8ec640fb1cee---------------------------------------)"]},{"url":"https://docs.snowflake.com/en/user-guide/cost-insights","title":"Using cost insights to save | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\n1. Overview\n2. Snowflake Horizon Catalog\n3. Applications and tools for connecting to Snowflake\n4. Virtual warehouses\n5. Databases, Tables, & Views\n6. Data types\n7. Data Integration\n   \n    + Snowflake Openflow\n    + Apache Iceberg™\n         \n        - Apache Iceberg™ Tables\n        - Snowflake Open Catalog\n8. Data engineering\n   \n    + Data loading\n    + Dynamic Tables\n    + Streams and Tasks\n    + dbt Projects on Snowflake\n    + Data Unloading\n9. Storage Lifecycle Policies\n10. Migrations\n11. Queries\n12. Listings\n13. Collaboration\n14. Snowflake AI & ML\n15. Alerts & Notifications\n16. Security\n17. Data Governance\n18. Privacy\n19. Organizations & Accounts\n20. Business continuity & data recovery\n21. Performance optimization\n22. Cost & Billing\n    \n        - Cost Management\n        - Overview\n        - Understanding cost\n        - Access control\n        - Visibility\n              \n            * Exploring cost\n            * Cost anomalies\n            * Attributing cost\n        - Control\n        - Optimization\n              \n                + Cost insights\n                + Optimizing cloud services\n        - Billing Management\n        - Accessing usage statements\n        - Reconciling usage statements\n        - Updating billing contacts\n\nGuides Cost & Billing Optimization Cost insights\n\n# Using cost insights to save ¶\n\nSnowflake provides cost insights that identify opportunities to optimize Snowflake for cost within a particular account. These insights are\ncalculated and refreshed weekly.\n\nEach insight indicates how many credits or terabytes could be saved by optimizing Snowflake.\n\nTo access the Cost Insights tile:\n\n2. Switch to a role with access to cost-related features .\n3. In the navigation menu, select Admin » Cost management .\n4. Select the Account Overview tab.\n5. Find the Cost insights tile.\n\nEach of the following insights includes suggestions on how to optimize your spend.\n\n* Insight: Rarely used tables with automatic clustering\n* Insight: Rarely used materialized views\n* Insight: Rarely used search optimization paths\n* Insight: Large tables that are never queried\n* Insight: Tables over 100 GB from which data is written but not read\n* Insight: Short-lived permanent tables\n* Insight: Inefficient usage of multi-cluster warehouses\n\nInsight: Rarely used tables with automatic clustering\n    This insight identifies tables with automatic clustering that are queried fewer than 100\ntimes per week by this account.\n\nEnabling automatic clustering for a table can significantly improve the performance of queries against that table. However, as the table\nchanges, Snowflake must use serverless compute resources to keep it in a well-clustered state. If the number of queries executed against\nthe table is minimal, the cost incurred might not justify the performance improvements.\n\n**Recommendation:** Consider disabling automatic clustering on these tables. Before you turn off automatic clustering, determine whether the table exists\nsolely for disaster recovery purposes or for use by other Snowflake accounts through data sharing, which might explain why it isn’t\naccessed frequently.\n\nFor example, to disable automatic clustering for a table named `t1` , execute the following command:\n\n```\nALTER TABLE t1 SUSPEND RECLUSTER ;\n```\n\nCopy\n\nInsight: Rarely used materialized views\n    This insight identifies materialized views that are queried fewer than 10 times per week by this\naccount.\n\nCreating a materialized view can significantly improve performance for certain query patterns. However, materialized views incur\nadditional storage costs as well as serverless compute costs associated with keeping the materialized view up to date with new data. If\nthe number of queries executed against the materialized view is minimal, the cost incurred might not justify the performance improvements.\n\n**Recommendation:** Consider removing or suspending updates to the materialized views. Before you drop a materialized view, determine whether the table exists\nsolely for disaster recovery purposes or for use by other Snowflake accounts through data sharing, which might explain why it isn’t\naccessed frequently.\n\nFor example, to delete a materialized view named `mv1` , execute the following command:\n\n```\nDROP MATERIALIZED VIEW mv1 ;\n```\n\nCopy\n\nInsight: Rarely used search optimization paths\n    This insight identifies search optimization access paths that are used fewer than\n10 times per week by this account.\n\nSearch optimization uses search access paths to improve the performance of certain types of point lookup and analytical queries. Adding\nsearch optimization to a table can significantly improve performance for these queries. However, search optimization incurs additional\nstorage costs as well as serverless compute costs associated with keeping that storage up to date. If the number of queries that use the\nsearch access path created by search optimization is minimal, the cost incurred might not justify the performance improvements.\n\n**Recommendation:** Consider removing search optimization from the table. Before you remove search optimization, determine whether the table exists solely\nfor disaster recovery purposes or for use by other Snowflake accounts through data sharing, which might explain why it isn’t accessed\nfrequently.\n\nFor example, to completely remove search optimization from a table named `t1` , execute the following command:\n\n```\nALTER TABLE t1 DROP SEARCH OPTIMIZATION ;\n```\n\nCopy\n\nInsight: Large tables that are never queried\n    This insight identifies large tables that have not been queried in the last week by this account.\n\n**Recommendation:** Consider deleting unused tables, which can reduce storage costs without impacting any workloads. Before you drop the tables, determine\nwhether the table exists solely for disaster recovery purposes or for use by other Snowflake accounts through data sharing, which might\nexplain why it isn’t accessed frequently.\n\nFor example, to delete a table name `t1` , execute the following command:\n\n```\nDROP TABLE t1 ;\n```\n\nCopy\n\nInsight: Tables over 100 GB from which data is written but not read\n    This insight identifies tables where data is written but never read by this account.\n\n**Recommendation:** It might be wasteful to store data and ingest new data into Snowflake if the data is never read. Consider dropping these tables to save on\nstorage costs or stop writing new data to save on credits consumed by ingestion. Before you drop the tables, determine whether the table\nexists solely for disaster recovery purposes or for use by other Snowflake accounts through data sharing, which might explain why it\nisn’t being read.\n\nFor example, to drop a table name `t1` , execute the following command:\n\n```\nDROP TABLE t1 ;\n```\n\nCopy\n\nInsight: Short-lived permanent tables\n    This insight identifies tables over 100 GB that were deleted within 24 hours of their creation.\n\n**Recommendation:** If data needs to be persisted for only a short time, consider using a temporary table or transient table for future tables. Using a temporary table or transient\ntable might help you save on Fail-safe and Time Travel costs .\n\nFor example, to create a new transient table `t1` , execute the following command:\n\n```\nCREATE TRANSIENT TABLE t1 ;\n```\n\nCopy\n\nInsight: Inefficient usage of multi-cluster warehouses\n    This insight identifies when you have the minimum and maximum cluster count set to the same value for a multi-cluster warehouse, which\nprevents the warehouse from scaling up or down to respond to demand. If your multi-cluster warehouse can scale down during periods of\nlighter usage, it can save credits.\n\n**Recommendation:** Consider lowering the minimum cluster count to allow the multi-cluster warehouse to scale down during periods of\nlighter usage.\n\nFor example, to set the minimum cluster count to 1 for a warehouse named `wh1` , execute the following command:\n\n```\nALTER WAREHOUSE wh1 SET MIN_CLUSTER_COUNT = 1 ;\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nRelated content\n\n1. Managing cost in Snowflake\n2. Optimizing cost\n\nLanguage: **English**\n\n* English\n* Français\n* Deutsch\n* 日本語\n* 한국어\n* Português"]},{"url":"https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/","title":"FinOps on Snowflake: Built-In Cost and Performance Control","excerpts":["Data for Breakfast Around the World\n\nDrive impact across your organization with data and agentic intelligence.\n\n[register now](https://www.snowflake.com/en/data-for-breakfast/)\n\npricing options\n\noverview\n\ncost & performance optimization\n\npricing calculator\n\n###### Resources\n\n[Pricing calculator overview](https://www.snowflake.com/en/pricing-options/calculator/overview/) [Pricing calculator FAQs](https://www.snowflake.com/en/pricing-options/calculator/feedback-faqs/) [Snowflake Performance Index](https://www.snowflake.com/en/pricing-options/performance-index/)\n\n[_Image Source_](https://squadrondata.com/org-impact-comparison-spark-based-saas-vs-snowflake/)\n\n###### COST MANAGEMENT AND PERFORMANCE OPTIMIZATION\n\n# FinOps on Snowflake\n\nTime is money – save both with Snowflake.\n\n[explore the economic impact of snowflake](https://www.snowflake.com/resource/forrester-the-total-economic-impact-of-the-snowflake-ai-data-cloud/?utm_cta=website-cost-and-performance-forrester-tei)\n\n## Save time on platform management. Spend it on what matters more.\n\n## Go from painstaking configurations to a proven, fully-managed service\n\nSince its founding in 2012, Snowflake has provided automated cluster management, maintenance and upgrades — all without downtime — so you can spend time on valuable data projects\n\nGet **[out-of-the-box governance and security through Snowflake Horizon Catalog](https://www.snowflake.com/en/data-cloud/horizon/)** without extra configurations or protocols\n\n## Go from piecemeal dashboards to built-in cost & performance management\n\n* Get granular visibility, control and optimization of Snowflake spend through a unified [Cost Management Interface](https://docs.snowflake.com/en/guides-overview-cost) .\n* Check query performance easily to proactively save on costs.\n* Automatically benefit from [regular rollouts of performance improvements](https://docs.snowflake.com/en/release-notes/performance-improvements) across all workloads.\n\n## Maximize your Snowflake spend\n\n* Add flexibility in how you use funds committed in your Snowflake Capacity contract.\n* Deploy partner solutions faster by simplifying finance and procurement processes.\n* Bundle your spend to increase your buying power with Snowflake and partners.\n\nlearn more\n\n###### OUR CUSTOMERS\n\n## Saving time on platform admin. Getting to market faster.\n\nTravelpass CTC Natwest\n\n[Travel and Hospitality](https://www.snowflake.com/en/solutions/industries/travel-hospitality/) “Now, we aren’t so focused on how to build things. We are focused more on what to build.” Dan Shah  \nManager of Data Science [Read the story](https://www.snowflake.com/en/customers/all-customers/case-study/travelpass/) * **1 week** for 130 Dynamic Tables to be in production after migration\n* **65%** cost savings switching from Databricks to Snowflake\n\nRead the case study [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/) “Now with fewer ephemeral failures and higher visibility in Snowflake, we have a platform that’s much easier and cost-effective to operate than managed Spark.” David Trumbell  \nHead of Data Engineering, CTC [Read the story](https://www.snowflake.com/en/customers/all-customers/case-study/chicago-trading-company/) * **1st** data availability deadline was hit everyday for the 1st time\n* **54%** cost savings switching from managed Spark to Snowflake\n\nRead the case study [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/) “The speed at which we’ve delivered wouldn’t have been possible with other providers.” Kaushik Ghosh Dastidar  \nHead of ESG Cloud Solutions, NatWest [Read the story](https://www.snowflake.com/en/customers/all-customers/case-study/natwest/) * **6x** reduction in onboarding time from 3 months to 2 weeks\n* **$750K** saved in salaries & staff training costs\n\nRead the case study\n\n[Resource #### Snowflake Joins the FinOps Foundation Snowflake joins The FinOps Foundation as a Premier Enterprise Member to provide thought leadership and set industry financial best practices. Read more](https://www.finops.org/members/snowflake/)\n\nResource #### Snowflake Pricing Calculator Curious about Snowflake pricing? Our Snowflake pricing calculator shows credit usage, warehouse costs, and total expenses. Access calculator\n\n[Guide #### Definitive Guide to Managing Spend in Snowflake Learn about considerations for consumption models such as Snowflake's, frameworks for better managing spend, and more. Get the guide](https://www.snowflake.com/en/resources/white-paper/definitive-guide-to-managing-spend-in-snowflake/)\n\n## Even More To Explore\n\n#### Snowflake Documentation\n\nAccess documentation on Managing Costs and Optimizing Performance in Snowflake.\n\n[Read about Managing Costs](https://docs.snowflake.com/en/user-guide/cost-management-overview)\n\n[Read about Optimizing Performance](https://docs.snowflake.com/en/guides-overview-performance)\n\n#### On-Demand Cost Governance Training\n\nLearn how to successfully examine, control, and optimize Snowflake costs.\n\n[Register Now](https://learn.snowflake.com/en/courses/OD-FINOPS/)\n\n#### Professional Services\n\nEngage Snowflake’s Professional Services for expert advice on optimizing your use of Snowflake.\n\n[Discover Professional Services](https://www.snowflake.com/snowflake-professional-services/)\n\n#### Priority Support\n\nLearn more about how our Priority Support team can help you reduce consumption spend through performance monitoring, observability, and management.\n\n[Learn about Priority Support](https://www.snowflake.com/support/priority-support/)\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\n[start for free](https://signup.snowflake.com/)\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\n\\*\n\n\\*\n\n\\* Country United States Canada United Kingdom Germany France Australia Japan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands Central African Republic Chad Chile China Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo The Democratic Republic of The Cook Islands Costa Rica Cote D'Ivoire (Ivory Coast) Croatia (Hrvatska) Cyprus Czech Republic Denmark Djibouti Dominica Dominican Republic Ecuador Egypt El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe Guam Guatemala Guinea Guinea-Bissau Guyana Haiti Heard and McDonald Islands Holy See (Vatican City State) Honduras Hong Kong Hungary Iceland India Indonesia Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jordan Kazakhstan Kenya Kiribati Korea Republic of (South) Kuwait Kyrgyzstan Lao People's Democratic Republic Latvia Lebanon Lesotho Liberia Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico Micronesia Federated States of Moldova Republic of Monaco Mongolia Montenegro Montserrat Morocco Mozambique Namibia Nauru Nepal Netherlands Netherlands Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory Occupied Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto Rico Qatar Reunion Romania Russian Federation Saint Helena Saint Kitts and Nevis Saint Lucia Saint Pierre and Miquelon Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia Solomon Islands Somalia South Africa South Georgia and The South Sandwich Island Spain Sri Lanka Suriname Svalbard and Jan Mayen Islands Swaziland Sweden Switzerland Taiwan Tajikistan Tanzania United Republic of Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab Emirates United Kingdom United States Minor Outlying Islands Uruguay Uzbekistan Vanuatu Venezuela Viet Nam Virgin Islands (British) Virgin Islands (U.S.) Wallis and Futuna Islands Western Sahara Yemen Zambia Zimbabwe\n\n\\*\n\nAdd me to the list to receive dedicated product updates and general availability emails.\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\n\nSubscribe Now\n\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n* [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n* [Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n* [Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n* [Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n* [Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n* [Technology](https://www.snowflake.com/en/solutions/industries/technology/)\n\nLearn * [Resource Library](https://snowflake.com/en/resources/)\n* Live Demos\n* [Fundamentals](https://www.snowflake.com/en/fundamentals/)\n* [Training](https://www.snowflake.com/en/resources/learn/training/)\n* [Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n* [Snowflake University](https://learn.snowflake.com/en/)\n* [Developer Guides](https://www.snowflake.com/en/developers/guides)\n* [Documentation](https://docs.snowflake.com/)\n\n* [Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n* [Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\n* Cookie Settings\n* [Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Legal](https://www.snowflake.com/en/legal/)\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")\n\n\\* Private preview, <sup>†</sup> Public preview, <sup>‡</sup> Coming soon"]},{"url":"https://yukidata.com/snowflake-finops-guide/","title":"Snowflake FinOps: Complete Guide to Automated Cost Optimization | Yuki","publish_date":"2025-09-19","excerpts":["[](https://yukidata.com)\nSolutionsClose Solutions Open Solutions\nResourcesClose Resources Open ResourcesResources[Blog](https://yukidata.com/blog/) [Customers](https://yukidata.com/customers/) [News](https://yukidata.com/news/) [Documentation](https://docs.yukidata.com/)[](https://yukidata.com/nowflake-cybersecurity-guide/)[Snowflake Cybersecurity: What Changed After 2024 & How to Protect Your Data](https://yukidata.com/nowflake-cybersecurity-guide/)[Learn More >](https://yukidata.com/nowflake-cybersecurity-guide/)[](https://yukidata.com/qwilt/)[How Qwilt Cut 63% of Snowflake Costs in Days](https://yukidata.com/qwilt/)[Learn More >](https://yukidata.com/qwilt/)\n[About Us](https://yukidata.com/about/)\n[Contact Us](https://yukidata.com/contact/)\n[Get Started](https://yukidata.com/request-demo/)\n[Free Trial](https://yukidata.com/free-trial)\n[](https://yukidata.com)\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > What is FinOps for Snowflake?\nContent:\nFinOps is the marriage of engineering, finance, and data teams in the cloud. When it comes to Snowflake, that means balancing performance, cost, and quality across your data platform – all while maintaining business agility\nUnlike your traditional cloud FinOps that focus on infrastructure, Snowflake FinOps requires managing:\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > What is FinOps for Snowflake?\nContent:\n**Credit-based consumption:** At $2-4 per credit (depending on your region and edition) with cost scaling linearly from X-Small (1 credit/hour) to 6X-Large (512 credits/hour).\n**Query-level optimization:** Using QUERY_HISTORY and WAREHOUSE_METERING_HISTORY views to identify expensive queries before they ruin your budget.\n**Dynamic warehouse scaling:** Beyond basic auto-suspend, implementing intelligent scaling based on queue depth and query complexity\n**Storage and** [**data transfer**](https://yukidata.com/blog/snowflake-data-transfer-costs-complete-guide/) **:** Long-term storage and cross-regional replication can add significant costs if not managed with lifestyle policies and governance.\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > The Speed vs. Cost Dilemma\nContent:\nFast-growing companies always run into this growing pain: should they optimize for speed or for cost? Most choose speed, leaving them with:\n**Overprovisioned warehouses** to avoid performance bottlenecks\n**Warehouse sprawl** as teams created dedicated resources for SLAs\n**Idle compute** running 24/7 “just in case”\nThe result? Snowflake costs outgrowing revenue and eating into margin and ROI.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > The Compliance Complexity\nContent:\nEnterprise FinOps needs control, not just cost reduction. You’ll find manual approaches falling short of this:\n**Granular spend attribution** across cost centers and teams\n**Real-time budget enforcement** to prevent runaway costs\n**Audit trails** for compliance and chargeback scenarios\n**Role-based access** maintaining security while enabling autonomy\nThese challenges only continue to compound as you scale, making manual management not just expensive, but impossible to maintain.\n ... \nSection Title: ... > : Automated Chargeback and Showback Models\nContent:\n**The problem:** No clear cost attribution means that teams often treat Snowflake as if it were “free,” leading to wasteful usage and budget overruns.\n**The solution:** Automated cost attribution that gets you accountability without expensive administration overhead:\n**Tag-based attribution** that automatically assigns costs to projects and business units\n**Usage-based chargeback** for shared warehouses using per-query\n**Predictive showback** forecasting team spend based on current trends\n**ROI tracking** connecting data platform costs to business outcomes\nManual tagging doesn’t scale. You need systems that automatically attribute costs based on your usage patterns, user roles, and business context.\nSection Title: ... > : Proactive Budget Management and Alerts\nContent:\n**The problem:** Reactive alerts document overspend after it happens. They don’t prevent it.\n**The solution:** Intelligent budget management that actually prevents overruns before they occur:\n**Predictive alerting** based on usage trends and historical patterns\n**Automated spend controls** scaling resources down when budgets risk\n**Multi-tiered notifications** that escalates teams to finance as spending approaches limits\n**Business-context budgeting** adjusting limits based on revenue cycles\nEffective budget management isn’t about saying “no”. It’s saying “yes” to the right workloads while automating the rest.\n ... \nSection Title: ... > : Automated Warehouse Scaling\nContent:\nThis approach can help you [reduce warehouse costs](https://yukidata.com/blog/snowflake-warehouse-optimization-guide/) by 20-40% because it lets you match your compute size to actual workload requirements.\nSection Title: ... > : Strategic Workload Investment\nContent:\n**The problem:** All workloads aren’t equal. Fraud detection needs different allocation outside of monthly reports.\n**The solution:** ROI-device resource allocation aligning your spending with real business values:\n**Workload classification** that automatically identifies business-critical vs development queries\n**Priority-based allocation** so you can be certain critical workloads always get resources first\n**Cost-per-insight analysis** which lets you measure business value per dollar spent\n**Automated lifecycle management** that lets you retire low-value processes while scaling high-impact ones\nThis step lets you move past generic optimization into business-aware optimizations you can create the perfect system for varying workloads.\n ... \nSection Title: ... > Maintaining Fraud Detection & Risk Scoring at Scale\nContent:\nFinancial service teams need millisecond response times for fraud detection. Downtime is simply not an option. Status provisioning either wastes compute or fails under spikes.\n**The solution:** Invest in a third-party tool that detects anomalies in query volume and dynamically scales compute to meet SLAs, then scales back when that load reduces.\nSection Title: ... > Multi-Region Compliance & Data Residency\nContent:\nGlobal enterprises have to maintain data residency requirements while continuing to optimize costs across different regions. Doing this manually means having to carefully balance regulatory compliance and [cost optimization](https://yukidata.com/blog/snowflake-optimization-guide/) across multiple geographic regions and regulatory frameworks.\n**The solution:** Use automated, region-aware optimization. This lets you maintain compliance while minimizing cross-region data movement costs. Think intelligent query routing that processes all you need within boundaries, optimizing for cost and performance and getting you the best of both worlds.\nSection Title: ... > AI & Machine Learning Pipeline Economics\nContent:\nML workloads have completely different requirements than your standard BI queries. You’re often left with static provisioning that either wastes money during model training downtimes or fails during intensive periods.\n**The solution:** Workload-aware optimization that automatically provisioning specialized resources for training, switching to standard compute for interference, managing your entire pipeline lifecycle without manual intervention.\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > The Future of Snowflake FinOps\nContent:\nFinOps strategies have to continue to evolve alongside Snowflake. That means always being on  your toes and ready to adapt to the next new thing.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > AI-Native Cost Optimization\nContent:\nIt shouldn’t surprise you to see AI on this list. Next-generation systems will be using machine learning for everything. Not just analysis, but optimization, too:\n**Predictive configurations** for new workloads based on historical patterns\n[**Automated query tuning**](https://yukidata.com/blog/snowflake-query-optimization/) using reinforcement learning\n**Intelligent data placement** that minimizes storage and compute costs at the same time\nAI-native approaches take you another step further from reactive optimization to [predictive cost management](https://yukidata.com/blog/snowflake-cost-optimization-guide/) . It means you can prevent inefficiencies before they even occur.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > Multi-Cloud FinOps\nContent:\nAs cloud adoption only continues to grow, FinOps must be prepared to manage:\n**Cross-cloud cost optimization** considering data gravity and egress charges\n**Vendor-agnostic optimization** that maintains efficiency regardless of the cloud provider\n**Unified governance** across different platforms and pricing models\nMulti-cloud FinOps requires you to look beyond Snowflake and see the bigger total data platform economics picture across all cloud providers.\nSection Title: ... > Automation On a Whole New Level\nContent:\nManual FinOps worked when you had five warehouses and 100 daily queries. At an enterprise scale, it collapses under the weight of hundreds of queries, teams, and compliance requirements.\nThe organizations succeeding with Snowflake FinOps aren’t tracking costs in spreadsheets – they’re automating everything.\nThat’s where Yuki comes in. Our platform continuously analyzes your Snowflake usage, optimizes workloads in real time, and delivers up to 30% cost reduction in the first month. And it does all that without sacrificing performance?\nReady to see what automated FinOps looks like? [Book your personal demo with Yuki today.](https://yukidata.com/request-demo/)\nBy Perry Tapiero\nSection Title: ... > Automation On a Whole New Level\nContent:\nPerry Tapiero leads marketing at Yuki, driving demand generation and brand growth for B2B and B2C SaaS companies in FinTech, AdTech, and Cybersecurity. With 15+ years of experience, he specializes in go-to-market strategies, ICP refinement, and managing multi-million-dollar campaigns using HubSpot and Salesforce. Previously at other companies, he led ABM, PBM, and product marketing initiatives that drove ARR growth and helped achieve Gartner Magic Quadrant recognition. Perry was a regular contributor for marketers and now shares his insights on LinkedIn.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > Free cost analysis\nContent:\nTake 5 minutes to learn how much money you can save on your Snowflake account.\nBy clicking Submit you’re confirming that you agree with our Terms and Conditions.\n[### Follow us on LinkedIn](https://www.linkedin.com/company/yukidata)\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > Related posts\nContent:\n[](https://yukidata.com/nowflake-cybersecurity-guide/)\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > Related posts\nContent:\n[](https://yukidata.com/nowflake-cybersecurity-guide/)\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > Related posts > Free cost analysis\nContent:\nTake 5 minutes to learn how much money you can save on your Snowflake account.\nBy clicking Submit you’re confirming that you agree with our Terms and Conditions.\nSkip to content"]},{"url":"https://www.snowflake.com/en/developers/guides/query-cost-monitoring/","title":"Build a Query Cost Monitoring Tool with Snowflake and Streamlit","excerpts":["Section Title: Build a Query Cost Monitoring Tool with Snowflake and Streamlit > Overview\nContent:\nManaging compute costs is crucial for optimizing database performance and budgeting effectively. In this tutorial, you'll build a Query Cost Monitoring tool that breaks down compute costs by individual queries using Snowflake's account usage data. This tool will help your team identify high-cost operations and gain valuable insights through interactive visualizations.\nSection Title: Build a Query Cost Monitoring Tool with Snowflake and Streamlit > Overview > What You'll Learn\nContent:\nRetrieve and merge query cost data from Snowflake\nConvert SQL data to a Pandas DataFrame for analysis\nImplement interactive filtering with Streamlit widgets\nCreate insightful visualizations using Altair\n ... \nSection Title: Build a Query Cost Monitoring Tool with Snowflake and Streamlit > Setup\nContent:\nFirstly, to follow along with this quickstart, you can click on [query_cost_monitoring.ipynb](https://github.com/Snowflake-Labs/snowflake-demo-notebooks/blob/main/Query_Cost_Monitoring/Query_Cost_Monitoring.ipynb) to download the Notebook from GitHub.\nEnsure that your notebook environment has access to the necessary Python libraries. Notebooks come pre-installed with common Python libraries for data science and machine learning, such as numpy, pandas, matplotlib, and more! If you are looking to use other packages, click on the Packages dropdown on the top right to add additional packages to your notebook.\nSection Title: Build a Query Cost Monitoring Tool with Snowflake and Streamlit > ... > Write the SQL Query\nContent:\nTo gain insights into query costs, we'll write a SQL query to retrieve the `credits_used` data from the `snowflake.account_usage.metering_history` table and merge it with associated user, database, schema, and warehouse information from the `snowflake.account_usage.query_history` table.\nNote that the following SQL cell name is `sql_data` , which we'll use shortly for the data app.\n ... \nSection Title: State conditional > Conclusion And Resources > What You Learned\nContent:\nRetrieved and merged query cost data from Snowflake's account usage views\nConverted SQL data into a Pandas DataFrame for analysis\nImplemented interactive filtering using Streamlit widgets\nCreated heatmaps, stacked bar charts, and bubble plots with Altair for data visualization\n ... \nSection Title: State conditional > Conclusion And Resources > Related Resources > On this page\nContent:\nOverview\nSetup\nRetrieve Query Cost Data\nCreate App & Data Preparation\nAdd Data Visualizations\nConclusion And Resources\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n*\n*\n ... \nSection Title: State conditional > Conclusion And Resources > Related Resources > On this page\nContent:\nLive Demos\n[Fundamentals](https://www.snowflake.com/en/fundamentals/)\n[Training](https://www.snowflake.com/en/resources/learn/training/)\n[Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n[Snowflake University](https://learn.snowflake.com/en/)\n[Developer Guides](https://www.snowflake.com/en/developers/guides)\n[Documentation](https://docs.snowflake.com/)\n[Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n[Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\nCookie Settings\n[Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Legal](https://www.snowflake.com/en/legal/)\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")"]},{"url":"https://docs.snowflake.com/en/guides-overview-ml-functions","title":"ML Functions - Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nGuides Snowflake AI & ML ML Functions\n\n# ML Functions ¶\n\nThese powerful analysis functions give you automated predictions and insights into your data using machine learning.\nSnowflake provides an appropriate type of model for each feature, so you don’t have to be a machine learning expert\nto take advantage of them. All you need is your data.\n\n## Time-Series Functions ¶\n\nUse time-series functions to train a machine learning model on your time-series data to determine how a specified metric (for example,\nsales) varies over time and relative to other features of your data. The model then provides insights or predictions\nbased on the trends detected in the data.\n\n* Forecasting predicts future metric values from past trends in time-series data.\n* Anomaly Detection flags metric values that differ from typical expectations.\n\n## Other Analysis Functions ¶\n\nThese features don’t require time series data.\n\n* Classification sort rows into two or more classes based on\n  their most predictive features.\n* Top Insights helps you find dimensions and values that affect the metric in\n  surprising ways.\n\n## Cost Considerations ¶\n\nWhen you use ML functions, you incur storage and compute costs. These costs vary depending on the feature used and the\nquantity of data used in training and prediction.\n\nThe storage costs you incur reflect storage of the ML model instances created during the training step. To view the\nobjects associated with your model instance, navigate to your Account Usage views (ACCOUNT\\_USAGE.TABLES and ACCOUNT\\_USAGE.STAGES). These objects appear with null database and schema columns. The `instance_id` column, however, will be populated, indicating that these objects are contained in a model instance.\nThese objects are fully managed by the model instance, and you cannot access or delete them separately. To reduce\nstorage costs associated with your models, delete unused or obsolete models.\n\nSee Understanding compute cost for general information on Snowflake compute costs.\n\n## Limitations ¶\n\nBefore you use ML functions, you must ensure AUTOCOMMIT is enabled in your session. AUTOCOMMIT is\nenabled by default when you start a new Snowflake session.\n\n## Using ML functions in Snowpark ¶\n\n`session.call` is not yet compatible with models created by ML functions. To call such a model in Snowpark, use `session.sql` instead, as shown here.\n\n```\nsession . sql ( 'call my_model!FORECAST(...)' ) . collect ()\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Time-Series Functions\n2. Other Analysis Functions\n3. Cost Considerations\n4. Limitations\n5. Using ML functions in Snowpark\n\nLanguage: **English**\n\n* English\n* Français\n* Deutsch\n* 日本語\n* 한국어\n* Português"]},{"url":"https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection","title":"Anomaly Detection (Snowflake ML Functions) | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg™\nApache Iceberg™ Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Snowflake AI & ML ML Functions Anomaly Detection\nSection Title: Anomaly Detection (Snowflake ML Functions) ¶ > Overview ¶\nContent:\nAnomaly detection is the process of identifying outliers in data. The anomaly detection function lets you train a model\nto detect outliers in your time-series data. Outliers, which are data points that deviate from the expected range, can\nhave an outsized impact on statistics and models derived from your data. Spotting and removing outliers can therefore\nhelp improve the quality of your results.\nNote\nAnomaly Detection is part of Snowflake’s suite of business analysis tools powered by machine learning.\nDetecting outliers can also be useful in pinpointing the origin of problems or deviations in processes when there is no\nobvious cause. For example:\nDetermining when a problem started to occur with your logging pipeline.\nIdentifying the days when your Snowflake compute costs are higher than expected.\n ... \nSection Title: ... > Automate Anomaly Detection with Snowflake Tasks and Alerts ¶\nContent:\nYou can create an automated anomaly detection pipeline, both for retraining the model and for monitoring your data for anomalies, by using Anomaly Detection functions within Snowflake Tasks or Alerts.\nRecurring Training with a Snowflake Task\nMonitoring with a Snowflake Task\nMonitoring with a Snowflake Alert\n ... \nSection Title: Anomaly Detection (Snowflake ML Functions) ¶ > ... > Monitoring with a Snowflake Alert ¶\nContent:\nYou can also use Snowflake Alerts to monitor your data at a given frequency and send you\nemail with detected anomalies. The following statements create an alert that detects anomalies every minute. First you\ndefine a stored procedure to detect anomalies, then create an alert\nthat uses that stored procedure.\nNote\nYou must set up email integration to send mail from a stored procedure; see Notifications in Snowflake .\n ... \nSection Title: Anomaly Detection (Snowflake ML Functions) ¶ > Cost Considerations ¶\nContent:\nFor details on costs for using ML functions, see Cost Considerations in the ML functions overview.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nOverview\nAbout the Algorithm for Anomaly Detection\nPreparing for Anomaly Detection\nSetting Up the Data for the Examples\nTraining, Using, Viewing, Deleting, and Updating Models\nDetecting Anomalies\nVisualizing Anomalies and Interpreting the Results\nAutomate Anomaly Detection with Snowflake Tasks and Alerts\nUnderstanding Feature Importance\nInspecting Training Logs\nCost Considerations\nRelated content"]},{"url":"https://medium.com/snowflake/identify-easy-ways-to-save-with-snowflake-cost-insights-f9ef29bc3917","title":"Identify easy ways to save with Snowflake cost insights - Medium","excerpts":["May 21, 2024 · Snowflake continuously analyzes customer usage and surfaces cost insights through the new Cost Management Interface. By reviewing your account's cost insights,"]},{"url":"https://ternary.app/blog/cloud-cost-anomaly-detection/","title":"Cloud Cost Anomaly Detection: Concepts, Strategy, & More - Ternary","publish_date":"2026-01-30","excerpts":["Ternary named a Leader in the 2025 ISG Provider Lens® for FinOps Platforms . [Download the report](https://ternary.app/isg-names-ternary-a-leader/) .\n✕\nBlog\n ... \nSection Title: Cloud cost anomaly detection: Concepts, strategy, and more > Cloud cost anomaly definition\nContent:\nAccording to the [FinOps Foundation](https://www.finops.org/wg/managing-cloud-cost-anomalies/) , “Anomalies in the context of FinOps are unpredicted variations (resulting in increases) in cloud spending that are larger than would be expected given historical spending patterns.”\nNot every weird number on your bill is a red alert. A little spending fluctuation can be normal. But when something leaps off the chart compared to past usage trends? That’s when we have found an anomaly.\nNow, what qualifies as an anomaly totally depends on your cloud cost model. A startup’s idea of “unexpected” might be a rounding error to an enterprise.\nBut what causes these anomalies? Anomalies can sneak in through:\nMisconfigurations\nIdle resources\nRogue deployments\nThe problem is that even small glitches, if left unchecked, start snowballing.\nSection Title: ... > Technical foundation of cost anomaly detection\nContent:\nFrom a technical lens, anomaly detection works in layers.\nFirst, the data collection layer grabs everything: usage logs, cost metrics, all piped in through APIs from cloud providers.\nThen comes the analysis layer, where machine learning models are run and compare current behavior against historical trends.\nFinally, the response layer kicks in (alerts, auto-shutdowns, resource throttling), whatever it takes to contain the budget fire.\n ... \nSection Title: Cloud cost anomaly detection: Concepts, strategy, and more > Lifecycle of a cloud cost anomaly\nContent:\n[Managing Cloud Cost Anomalies](https://www.finops.org/wg/managing-cloud-cost-anomalies/) by [FinOps Foundation](https://www.finops.org/)\n ... \nSection Title: ... > Anomaly detection examples across FinOps maturity levels\nContent:\nThe FinOps Foundation Working Group has thoroughly [documented](https://www.finops.org/wg/managing-cloud-cost-anomalies/) examples of the anomaly lifecycle across various maturity levels. Their examples have been summarized into the following table:\nSection Title: ... > The responsibilities of key FinOps personas\nContent:\nEach core FinOps persona has a role to play when it comes to anomaly detection.\nFinOps Practitioners manage the anomaly lifecycle (detection → resolution → retrospection), ensuring cross-team accountability.\nProduct Owners collaborate with Engineering on root cause analysis and corrective actions.\nEngineering implements fixes, optimizes resources, and provides technical insights.\nFinance adjusts forecasts, tracks cost deviations, and ensures financial alignment.\nLeadership oversees anomaly management efficacy via KPIs and strategic alignment.\nView the [Managing Cloud Cost Anomalies](https://www.finops.org/wg/managing-cloud-cost-anomalies/) article by the FinOps Foundation for a full RACI.\n ... \nSection Title: Cloud cost anomaly detection: Concepts, strategy, and more > ... > Step by step > 1. Create an alert\nContent:\nStart by creating customizable alert rules based on absolute or percentage thresholds to monitor increases or decreases in cloud spending patterns over time. Optionally bypass the AI/ML algorithms to configure [prescriptive threshold alerting](https://ternary.app/blog/finops-roundup-january-2024/) rules.\n ... \nSection Title: ... > Future developments: emerging technologies\nContent:\nPredictive analytics is becoming a key player in helping teams forecast cloud costs before they spiral. On top of that, automated root cause analysis is evolving, so you get insights about the why of an alert.\nAdd to this the growing power of API integrations, seamless compatibility across platforms, and standardized reporting, and you’ve got a future where cloud anomalies are spotted, explained, and resolved faster than ever.\n**Want high-accuracy cloud anomaly detection? See what Ternary can do for your team.**\n[Book a demo](https://ternary.app/demo/)"]}],"usage":[{"name":"sku_search","count":1}]}
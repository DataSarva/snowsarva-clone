{"extract_id":"extract_409e8d8ac4b34d64bad79d6e0211df1e","results":[{"url":"https://docs.snowflake.com/en/user-guide/alerts","title":"Setting up alerts based on data in Snowflake | Snowflake Documentation","publish_date":null,"excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nGuides Alerts & Notifications Snowflake Alerts\n\n# Setting up alerts based on data in Snowflake ¶\n\nThis topic explains how to set up an alert that periodically performs an action under specific conditions, based on data within\nSnowflake.\n\n## Introduction ¶\n\nIn some cases, you might want to be notified or take action when data in Snowflake meets certain conditions. For example, you\nmight want to receive a notification when:\n\n* The warehouse credit usage increases by a specified percentage of your current quota.\n* The resource consumption for your pipelines, tasks, materialized views, etc. increases beyond a specified amount.\n* Your data fails to comply with a particular business rule that you have set up.\n\nTo do this, you can set up a Snowflake alert. A Snowflake alert is a schema-level object that specifies:\n\n* A condition that triggers the alert (e.g. the presence of queries that take longer than a second to complete).\n* The action to perform when the condition is met (e.g. send an email notification, capture some data in a table, etc.).\n* When and how often the condition should be evaluated (e.g. every 24 hours, every Sunday at midnight, etc.).\n\nFor example, suppose that you want to send an email notification when the credit consumption exceeds a certain limit for a\nwarehouse. Suppose that you want to check for this every 30 minutes. You can create an alert with the following properties:\n\n* Condition: The credit consumption for a warehouse (the sum of the `credits_used` column in the WAREHOUSE\\_METERING\\_HISTORY view in the ACCOUNT\\_USAGE ) schema exceeds a specified limit.\n* Action: Email the administrator.\n* Frequency / schedule: Check for this condition every 30 minutes.\n\n## Choosing the type of alert ¶\n\nYou can create the following types of alerts:\n\n* Alert on a schedule : Snowflake evaluates the condition against the existing data on a\n  scheduled basis.\n  \n  For example, you can set up a alert on a schedule to check if any of the existing rows in a table has a column value that\n  exceeds a specified amount.\n* Alert on new data : Snowflake evaluates the condition against any new rows in a specified\n  table or a view.\n  \n  For example, you can set up an alert on new data to notify you when new rows for error messages are inserted into the event table for your account. Because dynamic table refreshes\n  and task executions log events to the event table, you can set up an alert on new data to:\n  \n    + Monitor dynamic table refreshes .\n    + Monitor task executions .\n\n### Alerts on a schedule ¶\n\nWith an alert on a schedule, you can set up an alert to execute every `_n_` minutes or on a schedule specified by a cron\nexpression.\n\nThe condition of the alert is evaluated on all of the data (as opposed to alerts on new data, where conditions are evaluated\nagainst only the new rows that have been inserted).\n\n### Alerts on new data ¶\n\nWith an alert on new data, you can set up an alert to execute only when new rows are inserted in a table or are made available\nin a view.\n\nWhenever new rows are inserted, the alert executes, evaluating the condition against just the new rows, and performing the action\nif the condition evaluates to TRUE.\n\nIf you want to evaluate a condition on newly inserted rows, use an alert on new data, rather than setting up an alert on a\nschedule (which executes on a fixed schedule, regardless of whether or not data has been added).\n\nBecause the alert operates only on newly inserted rows in a table or view, there are restrictions on the condition that you can\nspecify:\n\n* In the SELECT statement, the FROM clause can specify only one regular table, view, or event table.\n* You must enable change tracking on that table or view.\n* You cannot use:\n  \n    + Common table expressions (CTEs)\n    + Data Manipulation Language (DML) commands\n    + Calls to stored procedures\n    + Joins\n\nNote\n\nYou cannot use the EXECUTE ALERT command to execute an alert on new data.\n\n## Choosing the warehouse for the alerts ¶\n\nAn alert requires a warehouse for execution. You can either use the serverless compute model or a virtual warehouse that you specify .\n\n### Using the serverless compute model (serverless alerts) ¶\n\nAlerts that use the serverless compute model called _serverless alerts_ . If you use the serverless compute model, Snowflake\nautomatically resizes and scales the compute resources required for the alert. Snowflake determines the ideal size of the compute\nresources for a given run based on a dynamic analysis of statistics for the most recent previous runs of the same alert. The\nmaximum size for a serverless alert run is equivalent to an XXLARGE warehouse. Multiple workloads in your account share a common\nset of compute resources.\n\nBilling is similar to other serverless features (such as serverless tasks). See Understanding the costs of alerts .\n\nNote\n\nIf you are creating an alert on new data that is added infrequently, consider\nconfiguring this as a serverless alert. If you configure the alert to use a warehouse instead, even a simple action that sends\nan email notification incurs at least one minute of warehouse cost.\n\n### Using a virtual warehouse that you specify ¶\n\nIf you want to specify a virtual warehouse, you must choose a warehouse that is sized appropriately for the SQL actions that\nare executed by the alert. For guidelines on choosing a warehouse, see Warehouse considerations .\n\n## Understanding the costs of alerts ¶\n\nThe costs associated with running an alert to execute SQL code differ depending on the compute resources used for the alert:\n\n* For serverless alerts, Snowflake bills your account based on compute resource usage. Charges are calculated based on your\n  total usage of the resources, including cloud service usage, measured in _compute-hours_ credit usage. The compute-hours cost\n  changes based on warehouse size and query runtime. For more information, see Serverless credit usage .\n  \n  To learn how many credits are consumed by alerts, refer to the “Serverless Feature Credit Table” in\n  the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\n  \n  To view the usage history of serverless alerts, you can:\n  \n    + Call the SERVERLESS\\_ALERT\\_HISTORY function.\n    + Query the SERVERLESS\\_ALERT\\_HISTORY view .\n* For alerts that use a virtual warehouse that you specify, Snowflake bills your account for credit usage based on the warehouse usage when an alert is running. This is\n  similar to the warehouse usage for executing the same SQL statements in a client or Snowsight. Per-second credit\n  billing and warehouse auto-suspend give you the flexibility to start with larger warehouse sizes and then adjust the size to\n  match your alert workloads.\n\nTip\n\nIf you want to set up an alert that evaluates new rows added to a table or view, use an alert on new data , rather than an alert on a schedule. An alert on a schedule will\nexecute at a scheduled time, regardless of whether or not new rows have been inserted.\n\n## Granting the privileges to create alerts ¶\n\nIn order to create an alert, you must use a role that has the following privileges:\n\n* The EXECUTE ALERT privilege on the account.\n  \n  Note\n  \n  This privilege can only be granted by a user with the ACCOUNTADMIN role.\n* One of the following privileges:\n  \n    + The EXECUTE MANAGED ALERT privilege on the account, if you are creating a serverless alert.\n    + The USAGE privilege on the warehouse used to execute the alert, if you are specifying a virtual warehouse for the alert.\n* The USAGE and CREATE ALERT privileges on the schema in which you want to create the alert.\n* The USAGE privilege on the database containing the schema.\n* The SELECT privilege on the table or view that you want to query in the alert condition (if you are creating an alert on new data ).\n\nTo grant these privileges to a role, use the GRANT <privileges> … TO ROLE command.\n\nFor example, suppose that you want to create a custom role named `my_alert_role` that has the privileges to create an alert in\nthe schema named `my_schema` . You want the alert to use the warehouse `my_warehouse` .\n\nTo do this:\n\n1. Have a user with the ACCOUNTADMIN role do the following:\n   \n    1. Create the custom role .\n          \n          For example:\n          \n          ```\n          USE ROLE ACCOUNTADMIN ; \n          \n           CREATE ROLE my_alert_role ;\n          ```\n          \n          Copy\n    2. Grant the EXECUTE ALERT global privilege to that custom role.\n          \n          For example:\n          \n          ```\n          GRANT EXECUTE ALERT ON ACCOUNT TO ROLE my_alert_role ;\n          ```\n          \n          Copy\n    3. If you want to create a serverless alert, grant the EXECUTE MANAGED ALERT global privilege to that custom role.\n          \n          For example:\n          \n          ```\n          GRANT EXECUTE MANAGED ALERT ON ACCOUNT TO ROLE my_alert_role ;\n          ```\n          \n          Copy\n    4. Grant the custom role to a user.\n          \n          For example:\n          \n          ```\n          GRANT ROLE my_alert_role TO USER my_user ;\n          ```\n          \n          Copy\n2. Have the owners of the database, schema, and warehouse grant the privileges needed for creating the alert to the custom role:\n   \n    + The owner of the schema must grant the CREATE ALERT and USAGE privileges on the schema:\n         \n         ```\n         GRANT CREATE ALERT ON SCHEMA my_schema TO ROLE my_alert_role ; \n          GRANT USAGE ON SCHEMA my_schema TO ROLE my_alert_role ;\n         ```\n         \n         Copy\n    + The owner of the database must grant the USAGE privilege on the database:\n         \n         ```\n         GRANT USAGE ON DATABASE my_database TO ROLE my_alert_role ;\n         ```\n         \n         Copy\n    + If you want to specify a warehouse for the alert, the owner of that warehouse must grant the USAGE privilege on the\n         warehouse:\n         \n         ```\n         GRANT USAGE ON WAREHOUSE my_warehouse TO ROLE my_alert_role ;\n         ```\n         \n         Copy\n\n## Creating an alert ¶\n\nThe following sections provide the basic steps and an example of creating different types of alerts:\n\n* Creating an alert on a schedule\n* Creating an alert on new data\n\n### Creating an alert on a schedule ¶\n\nSuppose that whenever one or more rows in a table named `gauge` has a value in the `gauge_value` column that exceeds 200,\nyou want to insert the current timestamp into a table named `gauge_value_exceeded_history` .\n\nYou can create an alert that:\n\n* Evaluates the condition that `gauge_value` exceeds 200.\n* Inserts the timestamp into `gauge_value_exceeded_history` if this condition evaluates to true.\n\nTo create an alert named `my_alert` that does this:\n\n1. Verify that you are using a role that has the privileges to create an alert .\n   \n   If you are not using that role, execute the USE ROLE command to use that role.\n2. Verify that you are using the database and schema in which you plan to create the alert.\n   \n   If you are not using that database and schema, execute the USE DATABASE and USE SCHEMA commands to use that database and schema.\n3. Execute the CREATE ALERT command to create the alert:\n   \n   ```\n   CREATE OR REPLACE ALERT my_alert \n     WAREHOUSE = mywarehouse \n     SCHEDULE = '1 minute' \n     IF ( EXISTS ( \n       SELECT gauge_value FROM gauge WHERE gauge_value > 200 )) \n     THEN \n       INSERT INTO gauge_value_exceeded_history VALUES ( current_timestamp ());\n   ```\n   \n   Copy\n   \n   If you want to create a serverless alert, omit the WAREHOUSE parameter:\n   \n   ```\n   CREATE OR REPLACE ALERT my_alert \n     SCHEDULE = '1 minute' \n     IF ( EXISTS ( \n       SELECT gauge_value FROM gauge WHERE gauge_value > 200 )) \n     THEN \n       INSERT INTO gauge_value_exceeded_history VALUES ( current_timestamp ());\n   ```\n   \n   Copy\n   \n   For the full description of the CREATE ALERT command, refer to CREATE ALERT .\n   \n   Note\n   \n   When you create an alert, the alert is suspended by default. You must resume the newly created alert in order for the alert\n   to execute.\n4. Resume the alert by executing the ALTER ALERT … RESUME command. For example:\n   \n   ```\n   ALTER ALERT my_alert RESUME ;\n   ```\n   \n   Copy\n\n### Creating an alert on new data ¶\n\nSuppose that you want to receive an email notification when a stored procedure named `my_stored_proc` in the database and\nschema `my_db.my_schema` logs a FATAL message to the active event table for your account .\n\nTo create an alert named `my_alert` that does this:\n\n1. Find the name of the active event table for your account:\n   \n   ```\n   SHOW PARAMETERS LIKE 'EVENT_TABLE' IN ACCOUNT ;\n   ```\n   \n   Copy\n   \n   ```\n   +-------------+---------------------------+----------------------------+---------+-----------------------------------------+--------+ \n    | key         | value                     | default                    | level   | description                             | type   | \n    |-------------+---------------------------+----------------------------+---------+-----------------------------------------+--------| \n    | EVENT_TABLE | my_db.my_schema.my_events | snowflake.telemetry.events | ACCOUNT | Event destination for the given target. | STRING | \n    +-------------+---------------------------+----------------------------+---------+-----------------------------------------+--------+\n   ```\n2. Enable change tracking on the table or view that you plan to query in the alert\n   condition.\n   \n   ```\n   ALTER TABLE my_db . my_schema . my_events SET CHANGE_TRACKING = TRUE ;\n   ```\n   \n   Copy\n3. Set up a notification integration for sending email .\n4. Verify that you are using a role that has the privileges to create an alert .\n   \n   If you are not using that role, execute the USE ROLE command to use that role.\n5. Verify that you are using database and schema in which you plan to create the alert.\n   \n   If you are not using that database and schema, execute the USE DATABASE and USE SCHEMA commands to use that database and schema.\n6. Execute the CREATE ALERT command to create the alert, and omit the SCHEDULE parameter.\n   \n   For example, the following example creates an alert on new data that monitors the event table for errors in dynamic table\n   refreshes and sends a notification to a Slack channel. The example assumes the following:\n   \n    + Your active event table is the default event table (SNOWFLAKE.TELEMETRY.EVENTS).\n    + You have set the severity level to capture events for your dynamic\n         table.\n    + You have set up a webhook notification integration for that Slack\n         channel.\n   \n   ```\n   CREATE OR REPLACE ALERT my_alert \n     WAREHOUSE = mywarehouse \n     IF ( EXISTS ( \n       SELECT * FROM SNOWFLAKE . TELEMETRY . EVENTS \n         WHERE \n           resource_attributes : \"snow.executable.type\" = 'DYNAMIC_TABLE' AND \n           record_type = 'EVENT' AND \n           value : \"state\" = 'ERROR' \n     )) \n     THEN \n       BEGIN \n         LET result_str VARCHAR ; \n         ( SELECT ARRAY_TO_STRING ( ARRAY_AGG ( name ) ::ARRAY , ',' ) INTO :result_str \n           FROM ( \n             SELECT resource_attributes : \"snow.executable.name\" ::VARCHAR name \n               FROM TABLE ( RESULT_SCAN ( SNOWFLAKE.ALERT.GET_CONDITION_QUERY_UUID ())) \n               LIMIT 10 \n           ) \n         ); \n         CALL SYSTEM$SEND_SNOWFLAKE_NOTIFICATION ( \n           SNOWFLAKE.NOTIFICATION.TEXT_PLAIN ( :result_str ), \n           '{\"my_slack_integration\": {}}' \n         ); \n       END ;\n   ```\n   \n   Copy\n   \n   If you want to create a serverless alert, omit the WAREHOUSE parameter:\n   \n   ```\n   CREATE OR REPLACE ALERT my_alert \n     IF ( EXISTS ( \n       SELECT * FROM SNOWFLAKE . TELEMETRY . EVENTS \n         WHERE \n           resource_attributes : \"snow.executable.type\" = 'DYNAMIC_TABLE' AND \n           record_type = 'EVENT' AND \n           value : \"state\" = 'ERROR' \n     )) \n     THEN \n       BEGIN \n         LET result_str VARCHAR ; \n         ( SELECT ARRAY_TO_STRING ( ARRAY_AGG ( name ) ::ARRAY , ',' ) INTO :result_str \n           FROM ( \n             SELECT resource_attributes : \"snow.executable.name\" ::VARCHAR name \n               FROM TABLE ( RESULT_SCAN ( SNOWFLAKE.ALERT.GET_CONDITION_QUERY_UUID ())) \n               LIMIT 10 \n           ) \n         ); \n         CALL SYSTEM$SEND_SNOWFLAKE_NOTIFICATION ( \n           SNOWFLAKE.NOTIFICATION.TEXT_PLAIN ( :result_str ), \n           '{\"my_slack_integration\": {}}' \n         ); \n       END ;\n   ```\n   \n   Copy\n   \n   For the full description of the CREATE ALERT command, refer to CREATE ALERT .\n   \n   Note\n   \n   When you create an alert, the alert is suspended by default. You must resume the newly created alert in order for the alert\n   to execute.\n7. Resume the alert by executing the ALTER ALERT … RESUME command. For example:\n   \n   ```\n   ALTER ALERT my_alert RESUME ;\n   ```\n   \n   Copy\n\n## Specifying timestamps based on alert schedules ¶\n\nIn some cases, you might need to define a condition or action based on the alert schedule.\n\nFor example, suppose that a table has a timestamp column that represents when a row was added, and you want to send an alert\nif any new rows were added between the last alert that was successfully evaluated and the current scheduled alert. In other\nwords, you want to evaluate:\n\n```\n<now> - <last_execution_of_the_alert>\n```\n\nCopy\n\nIf you use CURRENT\\_TIMESTAMP and the scheduled time of the alert to calculate this range of\ntime, the calculated range does not account for latency between the time that the alert is scheduled and the time when the\nalert condition is actually evaluated.\n\nInstead, when you need the timestamps of the current schedule alert and the last alert that was successfully evaluated, use the\nfollowing functions:\n\n* SCHEDULED\\_TIME returns the timestamp representing when the current alert was scheduled.\n* LAST\\_SUCCESSFUL\\_SCHEDULED\\_TIME returns the timestamp representing when the last successfully\n  evaluated alert was scheduled.\n\nThese functions are defined in the SNOWFLAKE.ALERT schema . To call these functions, you need\nto use a role that has been granted the SNOWFLAKE.ALERT\\_VIEWER database role . To\ngrant this role to another role, use the GRANT DATABASE ROLE command. For example, to grant this role\nto the custom role `alert_role` , execute:\n\n```\nGRANT DATABASE ROLE SNOWFLAKE . ALERT_VIEWER TO ROLE alert_role ;\n```\n\nCopy\n\nThe following example sends an email message if any new rows were added to `my_table` between the time that the last\nsuccessfully evaluated alert was scheduled and the time when the current alert has been scheduled:\n\n```\nCREATE OR REPLACE ALERT alert_new_rows \n  WAREHOUSE = my_warehouse \n  SCHEDULE = '1 MINUTE' \n  IF ( EXISTS ( \n      SELECT * \n      FROM my_table \n      WHERE row_timestamp BETWEEN SNOWFLAKE.ALERT.LAST_SUCCESSFUL_SCHEDULED_TIME () \n       AND SNOWFLAKE.ALERT.SCHEDULED_TIME () \n  )) \n  THEN CALL SYSTEM$SEND_EMAIL (...);\n```\n\nCopy\n\n## Checking the results of the SQL statement for the condition in the alert action ¶\n\nWithin the action of an alert, if you need to check the results of the SQL statement for the condition:\n\n1. Call the GET\\_CONDITION\\_QUERY\\_UUID function to get the query ID for the SQL statement for the\n   condition.\n2. Pass the query ID to the RESULT\\_SCAN function to get the results of the execution of that SQL\n   statement.\n\nFor example:\n\n```\nCREATE ALERT my_alert \n  WAREHOUSE = my_warehouse \n  SCHEDULE = '1 MINUTE' \n  IF ( EXISTS ( \n    SELECT * FROM my_source_table )) \n  THEN \n    BEGIN \n      LET condition_result_set RESULTSET := \n        ( SELECT * FROM TABLE ( RESULT_SCAN ( SNOWFLAKE.ALERT.GET_CONDITION_QUERY_UUID ()))); \n      ... \n    END ;\n```\n\nCopy\n\n## Manually executing alerts ¶\n\nIn some cases, you might need to execute an alert manually. For example:\n\n* If you are creating a new alert, you might want to verify that the alert works as you would expect.\n* You might want to execute the alert at a specific point in your data pipeline. For example, you might want to execute the\n  alert at the end of a stored procedure call.\n\nTo execute an alert manually, run the EXECUTE ALERT command:\n\n```\nEXECUTE ALERT my_alert ;\n```\n\nCopy\n\nNote\n\nYou cannot use EXECUTE ALERT to execute an alert on new data .\n\nThe EXECUTE ALERT command manually triggers a single run of an alert, independent of the schedule defined for the alert.\n\nYou can execute this command interactively. You can also execute this command from within a stored procedure or a Snowflake\nScripting block.\n\nFor details on the privileges required to run this command and the effect of this command on suspended, running, and scheduled\nalerts, see EXECUTE ALERT .\n\n## Suspending and resuming an alert ¶\n\nIf you need to prevent an alert from executing temporarily, you can suspend the alert by executing the ALTER ALERT … SUSPEND command. For example:\n\n```\nALTER ALERT my_alert SUSPEND ;\n```\n\nCopy\n\nTo resume a suspended alert, execute the ALTER ALERT … RESUME command. For example:\n\n```\nALTER ALERT my_alert RESUME ;\n```\n\nCopy\n\nNote\n\nIf you are not the owner of the alert, you must have the OPERATE privilege on the alert to suspend or resume the alert.\n\n## Modifying an alert ¶\n\nTo modify the properties of an alert, execute the ALTER ALERT command.\n\nNote\n\n* You must be the owner of the alert to modify the properties of the alert.\n* You cannot change an alert on new data to an alert on a schedule . Similarly, you cannot change an alert on a schedule to an alert\n  on new data.\n\nFor example:\n\n* To change the warehouse for the alert named `my_alert` to `my_other_warehouse` , execute:\n  \n  ```\n  ALTER ALERT my_alert SET WAREHOUSE = my_other_warehouse ;\n  ```\n  \n  Copy\n* To change the schedule for the alert named `my_alert` to be evaluated every 2 minutes, execute:\n  \n  ```\n  ALTER ALERT my_alert SET SCHEDULE = '2 minutes' ;\n  ```\n  \n  Copy\n* To change the condition for the alert named `my_alert` so that you are alerted if any rows in the table named `gauge` have\n  values greater than `300` in the `gauge_value` column, execute:\n  \n  ```\n  ALTER ALERT my_alert MODIFY CONDITION EXISTS ( SELECT gauge_value FROM gauge WHERE gauge_value > 300 );\n  ```\n  \n  Copy\n* To change the action for the alert named `my_alert` to `CALL my_procedure()` , execute:\n  \n  ```\n  ALTER ALERT my_alert MODIFY ACTION CALL my_procedure ();\n  ```\n  \n  Copy\n\n## Dropping an alert ¶\n\nTo drop an alert, execute the DROP ALERT command. For example:\n\n```\nDROP ALERT my_alert ;\n```\n\nCopy\n\nTo drop an alert without raising an error if the alert does not exist, execute:\n\n```\nDROP ALERT IF EXISTS my_alert ;\n```\n\nCopy\n\nNote\n\nYou must be the owner of the alert to drop the alert.\n\n## Viewing details about an alert ¶\n\nTo list the alerts that have been created in an account, database, or schema, execute the SHOW ALERTS command. For example, to list the alerts that were created in the current schema, run the following command:\n\n```\nSHOW ALERTS ;\n```\n\nCopy\n\nThis command lists the alerts that you own and the alerts that you have the MONITOR or OPERATE privilege on.\n\nTo view the details about a specific alert, execute the DESCRIBE ALERT command. For example:\n\n```\nDESC ALERT my_alert ;\n```\n\nCopy\n\nNote\n\nIf you are not the owner of the alert, you must have the MONITOR or OPERATE privilege on the alert to view the details of the\nalert.\n\n## Cloning an alert ¶\n\nYou can clone an alert (either by using CREATE ALERT … CLONE or by cloning the\ndatabase or schema containing the alert).\n\nIf you are cloning a serverless alert, you don’t need to use a role that has the global EXECUTE MANAGED ALERT privilege. However,\nyou will not be able to resume that alert until the role that owns the alert has been granted the EXECUTE MANAGED ALERT privilege.\n\n## Monitoring the execution of alerts ¶\n\nTo monitor the execution of the alerts, you can:\n\n* Check the results of the action that was specified for the alert. For example, if the action inserted rows into a table, you can\n  check the table for new rows.\n* View the history of alert executions by using one of the following:\n  \n    + The ALERT\\_HISTORY table function in the INFORMATION\\_SCHEMA schema.\n        \n        For example, to view the executions of alerts over the past hour, execute the following statement:\n        \n        ```\n        SELECT * \n         FROM \n          TABLE ( INFORMATION_SCHEMA . ALERT_HISTORY ( \n            SCHEDULED_TIME_RANGE_START \n              => dateadd ( 'hour' ,- 1 , current_timestamp ()))) \n         ORDER BY SCHEDULED_TIME DESC ;\n        ```\n        \n        Copy\n    + The ALERT\\_HISTORY view in the ACCOUNT\\_USAGE schema in the shared\n        SNOWFLAKE database.\n\nIn the query history, the name of the user who executed the query will be SYSTEM. (The alerts are run by the system service .)\n\n## Viewing the query history of a serverless alert ¶\n\nTo view the query history of a serverless alert, you must be the owner of the alert, or you must use a role that has the\nMONITOR or OPERATE privilege on the alert itself. (This differs from alerts that use one your warehouses, which require the\nMONITOR or OPERATOR privilege on the warehouse.)\n\nFor example, suppose that you want to use the `my_alert_role` role when viewing the query history of the alert `my_alert` .\nIf `my_alert_role` is not the owner of `my_alert` , you must grant that role the\nMONITOR or OPERATE privilege on the alert:\n\n```\nGRANT MONITOR ON ALERT my_alert TO ROLE my_alert_role ;\n```\n\nCopy\n\nAfter the role is granted this privilege, you can use the role to view the query history of the alert:\n\n```\nUSE ROLE my_alert_role ;\n```\n\nCopy\n\n```\nSELECT query_text FROM TABLE ( INFORMATION_SCHEMA . QUERY_HISTORY ()) \n  WHERE query_text LIKE '%Some condition%' \n    OR query_text LIKE '%Some action%' \n  ORDER BY start_time DESC ;\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nLanguage: **English**\n\n* English\n* Français\n* Deutsch\n* 日本語\n* 한국어\n* Português"],"full_content":null},{"url":"https://www.rst.software/blog/snowflake-cost-monitoring","title":"Snowflake cost monitoring: how to avoid skyrocketing bills? | RST Software","publish_date":"2024-06-20","excerpts":["## Expertise\n\n#### Location-based Services\n\nWant to build an app based on maps or optimize your current solution?\n\nMobility (MaaS) Development Logistics Software Development GIS Services Geospatial Mapping Spatial Data Visualization OpenStreetMap Consulting Mapbox Consulting\n\n#### Chat app development\n\nWant to build scalable chat and messaging apps?\n\nChat App Development\n\n#### Video streaming app development\n\nWant to build scalable video streaming apps or systems?\n\nVideo Streaming Development\n\nCase Studies Resources Blog About Us Career Contact Us\n\nEstimate my project\n\nMagdalena Jackiewicz\n\nEditorial Expert\n\nReviewed by a tech expert\n\n# Snowflake cost monitoring: how to avoid skyrocketing bills?\n\n#### Home #### Blog #### Data\n\n#### Snowflake cost monitoring: how to avoid skyrocketing bills?\n\n\\\n\n\\\n\n\\\n\n\\\n\nRead this articles in:\n\nEN PL\n\nTable of contents\n\nExample H2\n\nThe [Snowflake Data Cloud](https://www.rst.software/blog/snowflake-data-platform-what-is-it-how-much-does-it-cost-and-how-to-get-started) is a robust platform that empowers businesses to extract greater value from their data. However, if businesses do not diligently monitor their usage costs, the overall expense of this leading data cloud solution can escalate rapidly. Fortunately, there are a number of Snowflake cost monitoring mechanisms you can implement to avoid this grim scenario.\n\nIn this blog post, I will help you understand the Snowflake pricing model, establish an effective budgeting strategy, explore the creation of resource monitors, and leverage Snowflake's alert system to ensure that your utilization of the platform remains efficient and cost-effective.\n\nBy following these best practices, you can optimize your cloud warehouse investment and unlock the full potential of your data.\n\n## Understanding the Snowflake pricing model\n\nUsing any cloud computing platform, including Snowflake, can lead to rapidly escalating costs if not closely monitored. The overall Snowflake price is the cumulative sum of the expenses associated with data transfer, storage, and computing resources. Snowflake's unique cloud architecture segregates the cost of each task into one of these distinct usage categories. For an effective Snowflake cost monitoring strategy, you first need to fully grasp the specific components that will make up the monthly bill.\n\nThe Snowflake pricing model is based on a usage-based approach, where the costs are divided into storage and compute components.\n\nThere are three main benefits of this architecture:\n\n* You can **scale storage and compute resources separately** to meet your specific performance requirements, without over-provisioning unneeded resources.\n* Its inherent flexibility helps you **optimize costs,** as it allows you to provision only the resources you need, without paying for unused capacity.\n* The ability to scale storage and compute independently ensures that you can achieve **high performance levels** , as you can allocate resources based on your workload demands.\n\nAlthough taking a full grasp of the Snowflake pricing model may be intimidating, it offers granular visibility into your expenditures, which in turn helps to identify optimization opportunities. Before we delve into those in detail, let’s see what affects the overall Snowflake cost, so that you know what to optimize.\n\n### Snowflake account type\n\nSnowflake offers two options when it comes to its service: On Demand and Pre-Purchase.\n\nThe On-Demand storage option provides businesses with the flexibility to use as much or as little storage capacity as needed, without any long-term commitments. This flexibility, however, comes at a premium, as the On-Demand storage is the most expensive way to utilize Snowflake's data storage capabilities. For instance, Snowflake On-Demand storage costs $40 per TB per month when deployed within the AWS US East (Northern Virginia) Region.\n\nIn contrast, the Pre-Purchase storage option operates similar to AWS Reserved Instances, where commitment is made to a certain amount of storage capacity upfront in exchange for a discounted rate. With this option, 1 TB costs $23 per month when deployed in the AWS US East (Northern Virginia) Region.\n\nDetermining your storage needs is thus the first step towards minimizing the overall cloud data platform expenses.\n\n### Snowflake editions pricing\n\nSnowflake offers four distinct product editions, each catering to different business needs. These editions vary in terms of features, capabilities, and Snowflake pricing models and each has a different impact on Snowflake cost management.\n\n#### Standard\n\nThis is the most basic option, providing access to Snowflake's core features, including essential security features like end-to-end data encryption, federated authentication, and database replication. However, there are limitations in setting up and using certain advanced features, such as multi-cluster virtual warehouses, extended time travel, and row/column-level security policies.\n\n#### Enterprise\n\nThis edition builds upon the Standard Edition, offering additional capabilities. It allows for multi-clustering of virtual warehouses to handle unpredictable compute usage, provides 90-day time travel, column and row-level observability policies, and access to account usage history. Additionally, it includes advanced security features like private connectivity, failover/failback for disaster recovery, and Tri-Secret Security for customer-managed encryption keys. This edition can be beneficial for Snowflake cost management and monitoring.\n\n#### Business Critical\n\nThis is the most comprehensive edition for hosting Snowflake in a public cloud region (non-VPC). Designed for mission-critical data and applications, it offers support for specific regional regulations like HIPAA HITRUST, CSF, FedRamp, and IRAP. It also provides failover capabilities between Snowflake accounts and client connection redirection.\n\n#### Virtual Private Snowflake\n\nThis offering provides a dedicated and managed instance of Snowflake within an AWS Virtual Private Cloud (VPC). This edition offers the highest level of security, with a separate metadata store for your data. It is ideal for industries like finance, healthcare, and government, where cloud-based data hosting requires enhanced security. Data sharing outside the VPS is not permitted, and this edition is only available with capacity pricing (the other three also offer On Demand options).\n\n### Snowflake compute costs\n\nThe utilization of Snowflake's compute resources is metered through the consumption of Snowflake credits, with the billed cost calculated by multiplying the number of credits used by Snowflake credit pricing.\n\nThere are several distinct types of compute resources within Snowflake that consume these credits:\n\n* **Virtual warehouse compute:** the compute power allocated to run your Snowflake workloads. It’s the primary cost driver in Snowflake. Snowflake warehouse pricing comes in a T-shirt size range (X-Small, Small, Medium, Large, X-Large, 2X-Large, 3X-Large, 4X-Large, 5XL and 6XL) that determines the compute power. It also supports clustering multiple virtual warehouses, up to 10, in a single cluster. When a virtual warehouse or multi-cluster warehouse is not running, it does not consume any Snowflake credits. The credit consumption is based on the virtual warehouse size and is billed by the second with a one-minute minimum.\n* **Serverless compute:** the on-demand compute resources used for tasks like data loading, transformation, and querying. The Snowflake cloud services layer acts as the control unit for all requests in a Snowflake account. Snowflake automatically manages the resources for this layer based on workload needs. Customers get 10% of their daily compute credits for free to use the cloud services layer, which is usually enough to cover most customers' cloud services usage.\n* **Snowflake features:** compute resources consumed by advanced Snowflake capabilities, such as Search Optimization and Snowpipe. Snowflake automatically resizes and scales these compute resources up or down as needed for each workload. It determines the ideal compute size based on a dynamic analysis of recent run statistics for the same task.\n\n* **Cloud services compute:** the behind-the-scenes compute used for metadata management, authentication, and other system-level tasks. Snowflake charges for cloud storage based on the average monthly compressed and ingested data. The compression rates vary by file type. All data ingested into Snowflake is compressed, encrypted, and columnarized. Snowflake's cloud storage services enable data redundancy and replication across multiple data centers.\n\nObviously, your compute usage will have the biggest impact on Snowflake cost management, so you should be able to estimate your usage in order to optimize effectively.\n\nIt's important to note that the compute resources used for cloud services are only billed if they exceed 10% of the daily virtual warehouse usage, ensuring that businesses are not charged for the baseline system overhead.\n\n### Snowflake storage costs\n\nSnowflake's data storage costs are determined by a flat rate per terabyte of data stored. The fee is calculated based on the average number of on-disk bytes stored within your Snowflake account each day.\n\nThe Snowflake storage pricing model is designed to be transparent and predictable, allowing businesses to plan and budget their expenses more effectively, without the need to understand potentially complex pricing tiers.\n\n### Snowflake data transfer costs\n\nSnowflake does not charge any fees for ingesting data into your account. However, it does impose a fee for data egress, which is the transfer of data out of your Snowflake account. This per-byte data egress fee is charged when you transfer data from your Snowflake account to a different region, either within the same cloud platform or to a completely different cloud provider.\n\nThe specific data egress fee rate depends on the region where your Snowflake account is hosted. For example, for a Snowflake account hosted in the AWS US-EAST-1 region, the data egress fee rate will be influenced by factors such as the Snowflake edition you are using and the cloud provider (e.g., AWS). These factors can affect the Snowflake cost per credit, as demonstrated in the example provided.\n\nWhile Snowflake's separation of ingestion and egress costs can be advantageous for customers, keeping track of all these usage-based charges can become complex. Fortunately, it provides several tools to help customers monitor and manage these various costs associated with their Snowflake usage.\n\n## Snowflake cost monitoring tools\n\nManaging the costs of your Snowflake cloud warehouse usage boils down to proactively implementing measures that offer visibility into the usage patterns and then optimizing accordingly. Snowflake provides three mechanisms that help you maintain a proactive approach when it comes to cost monitoring:\n\n### Budgets\n\nWith Snowflake, users have to specify a monthly budget, which is essentially their desired spending limit (this applies to an account or a custom group of Snowflake objects). When the spending limit is projected to be exceeded, Snowflake will send a notification daily to a designated email addresses.\n\nBudgets track the serverless feature usage for the supported objects within the budget. For a full list of those supported objects and features, see Snowflake’s [Monitoring credit usage with budgets](https://docs.snowflake.com/en/user-guide/budgets) .\n\nTo help you set an appropriate spending limit, Snowflake provides guidance based on your credit usage over the past three months, as well as an estimate of your current monthly spend (in case you're activating the account-level budget mid-month).\n\n### Resource monitors\n\nTo help customers with Snowflake cost monitoring, the warehouse provides robust resource monitoring capabilities that help to prevent unexpected spikes in credit consumption. Each active virtual warehouse consumes Snowflake credits, and the resource monitoring tools enable users to track the credit usage for their warehouses as well as the associated cloud services.\n\nSnowflake resource monitors allow users to set a credit quota, which is the maximum number of Snowflake credits they want to allocate for their virtual warehouses and cloud services. Users can establish Snowflake cost per credit limits for specific time intervals or date ranges. When the credit usage approaches or exceeds the specified quota, the resource monitor can take various actions, such as sending alert notifications and automatically suspending user-managed warehouses.\n\nA Snowflake resource monitor has to specify the following:\n\n* **Credit quota:** the maximum number of Snowflake credits allowed for the monitored resources, including both user-managed warehouses and cloud services.\n* **Monitor level:** determines whether the resource monitor applies to all warehouses on the account or a subset of those.\n* **Schedule:** sets the frequency for the credit quota reset (daily, weekly, monthly, etc.) and the start/end dates for the Snowflake resource monitor.\n* **Actions:** specifies the actions the resource monitor should take when certain percentages of the credit quota are reached, such as sending notifications and suspending warehouses.\n\nSnowflake allows you to create, modify, and remove resource monitors as needed, treating them as first-class objects. For example, a user could set up a weekly resource monitor for their entire Snowflake account to help manage credit consumption across all their virtual warehouses and cloud services.\n\nA Snowflake resource monitor can take the following actions when a threshold is reached:\n\n1. Notify: the monitor sends a notification without taking any other action.\n2. Notify and suspend: the monitor sends a notification and suspends the warehouses once any running queries have been completed.\n3. Notify and suspend immediately: the monitor sends a notification, cancels any running queries, and then suspends the Snowflake warehouses.\n\n### Alerts\n\nAlerts in Snowflake are a relatively recent feature that allow users to receive notifications when data in a table changes. This can be particularly useful for Snowflake cost management, as alerts can be set up to monitor data, enabling a more granular level of resource monitoring.\n\nFor instance, an alert can be configured to notify administrators when a warehouse's size has been modified. In Snowflake, each doubling of a warehouse's size corresponds to a doubling of the credits consumed. Therefore, being alerted to potentially unauthorized changes in warehouse size can be critical for maintaining cost control.\n\nUsing Alerts for Snowflake cost monitoring involves setting up relevant thresholds based on your budget and usage patterns, and then configuring the relevant alerts accordingly directly in Snowflake. You can choose to receive these alerts via email, PagerDuty, Slack, or any other communication channel that keeps your relevant stakeholders informed.\n\nTo make the most of these cost alerts, customize the notification content to include information such as the current cost, the cost threshold, and the specific resource (account, warehouse, or database) that triggered the alert.\n\n## Stay on top of your Snowflake costs with RST Data\n\nSnowflake cost monitoring and management are crucial for ensuring the long-term success of your data infrastructure. Leveraging the various mechanisms outlined in this post can help you gain the necessary visibility into your usage patterns, identify optimization opportunities, and maintain control over your cloud data warehouse spending.\n\nWhether you're a seasoned Snowflake user or exploring what a modern data platform can do for your business, staying on top of your costs is an essential part of building an efficient and cost-effective data infrastructure. By proactively monitoring and optimizing your Snowflake usage, you can unlock the full potential of this powerful cloud data warehouse and ensure that your data initiatives deliver maximum value to your organization.\n\nIf you'd like to learn more about the advantages of Snowflake and discuss how to optimize your data strategy, we'd be happy to chat. Feel free to use our [contact form](https://www.rst.software/contact) to schedule a data strategy call with our team of data experts, and let's explore how Snowflake can help you drive business growth and innovation.\n\n## People also ask\n\nNo items found.\n\n  \n\nWant more posts from the author?\n\nMagdalena Jackiewicz\n\nEditorial Expert\n\nRead more\n\n## Want to read more?\n\nData\n\n### ELT Process: unlock the future of data integration with Extract, Load, Transform\n\nUnlock the future of data integration with our ELT process guide. Learn how Extract, Load, Transform can streamline your data workflow.\n\nData\n\n### Data integration: different techniques, tools and solutions\n\nMaster data integration with our comprehensive guide on techniques, tools, and solutions. Enhance your data strategies effectively.\n\nData\n\n### Supply chain analytics examples – 18 modern use cases\n\nExplore real-world applications with our guide on supply chain analytics examples. See how data insights transform operations.\n\nNo results found.\n\nThere are no results with this criteria. Try changing your search. [** **](https://library.relume.io/components?elements=video-lightbox&layout=text-align-left%7Ctext-align-center#)\n\n## Have a groundbreaking product idea?\n\nWe’ve delivered over 100 international projects  \nand are always happy to help!\n\nBook a free consultation\n\n## You need tech expertise.  \nWe’re here to provide it.\n\nRST Team\n\nWe are the part of\n\nWROCŁAW - HQ\n\nul. Racławicka 2-4  \n53-146 Wrocław  \nPoland  \n  \nhi@rst.software\n\nŚWIDNICA\n\nul. Esperantystów 17  \n58-100 Świdnica  \nPoland  \n  \nhi@rst.software\n\nJOIN US\n\nWork with us  \nrekrutacja@rst.com.pl\n\nJoin us\n\nCOMPLIANCE\n\n[Information Note of the Personal Data Administrator](https://rst.software/legal-info/information-note-of-the-personal-data-administrator)\n\n[Cookies Policy](https://rst.software/legal-info/cookies-policy)\n\n[Privacy Policy](https://rst.software/legal-info/privacy-policy)\n\nRST sp. z o.o. sp. k.     •    VAT-ID: PL884-271-11-31     •    REGON: 021232406     •    KRS: 0000354129     •    D-U-N-S®: 425462936\n\n© RST Software 2026\n\n[](https://www.instagram.com/rstkariera/) [](https://www.youtube.com/channel/UCr-ho6sg5ZQcd0YenCLWoiQ) [](https://www.facebook.com/RSTkariera/) [](https://twitter.com/rst_software) [](https://www.linkedin.com/company/rst-software-masters/) [](https://www.behance.net/rstsoftware)\n\nen\n\nPL"],"full_content":null},{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/container-cost-governance","title":"Costs associated with apps with containers | Snowflake Documentation","publish_date":null,"excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nDeveloper Snowflake Native App Framework Costs associated with apps with containers\n\n# Costs associated with apps with containers ¶\n\n Feature — Generally Available\n\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\n\nThis topic describes the costs associated with developing, publishing and using a\nSnowflake Native App with Snowpark Container Services. It contains information for both providers and consumers.\n\n## Costs to consumers ¶\n\nA Snowflake Native App may incur costs in the consumer account. The total cost of running\na Snowflake Native App with Snowpark Container Services is determined by the following:\n\n* Costs determined by the provider\n* Infrastructure costs\n\n### Costs determined by the provider ¶\n\nA provider may monetize a Snowflake Native App using any of the paid listing pricing models that are available in the Snowflake Marketplace. These models include subscription based and usage based plans.\n\nThis cost to the consumer is determined by the provider. Consumers pay for provider software via the Snowflake Marketplace in addition to costs associated with running Snowflake\ninfrastructure, including warehouses and compute pools.\n\n### Infrastructure costs ¶\n\nAll infrastructure costs, including those related to compute pools, warehouse compute, storage, and\ndata transfer are the responsibility of the consumer of a Snowflake Native App.\n\nA consumer can use the IN ACCOUNT clause of the SHOW COMPUTE POOLS command to see all compute pools in their account\nand the current state of the compute pool. Costs are not incurred when a compute pool is suspended.\n\nA Snowflake Native App with Snowpark Container Services requires at least one compute pool and might require multiple compute pools to run as\nintended. A consumer has full control over the compute resources that the app requires, and may suspend a\ncompute pool or drop an application at any time.\n\nSeparate charges for compute pool compute related to the Snowflake Native App with Snowpark Container Services appear on the customer billing\nstatement. A consumer can determine the compute pool billing charges for a Snowflake Native App with Snowpark Container Services using the ACCOUNT USAGE views provided by\nSnowpark Container Services.\n\nFor more details, such as the consumption table for compute pools, contact your account representative.\n\n## Costs to providers ¶\n\nProviders can also incur costs when developing and maintaining a Snowflake Native App with Snowpark Container Services, including the\nfollowing:\n\n* Providers incur Snowpark Container Services compute costs associated with both initial development and\n  ongoing testing and support for their Snowflake Native App. The compute cost may be controlled through\n  orchestration of compute pools during provider-side development and testing.\n* The storage of container images can incur costs when a provider creates a new version or patch of\n  a Snowflake Native App with Snowpark Container Services. In this context, the Docker images that the app requires are copied into an image\n  repository that is not directly accessible or observable by the provider or the consumer.\n  \n  Services in the consumer account are created from the versioned images that are stored in this\n  repository. Providers are responsible for the storage costs for the images in this stage, which\n  appear on their Snowflake bill. These costs are aggregated with other storage costs that their account incurs.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Costs to consumers\n2. Costs to providers\n\nRelated content\n\n1. About the Snowflake Native App Framework\n\nLanguage: **English**\n\n* English\n* Français\n* Deutsch\n* 日本語\n* 한국어\n* Português\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details‎\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details‎\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details‎\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details‎\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"],"full_content":null},{"url":"https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history","title":"WAREHOUSE_METERING_HISTORY view | Snowflake Documentation","publish_date":null,"excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nReference General reference SNOWFLAKE database Account Usage WAREHOUSE\\_METERING\\_HISTORY\n\nSchemas:\n    ACCOUNT\\_USAGE , READER\\_ACCOUNT\\_USAGE\n\n# WAREHOUSE\\_ METERING\\_ HISTORY view ¶\n\nThis Account Usage view can be used to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within the last 365 days (1 year).\n\n## Columns ¶\n\n|Column Name |Data Type |Description |\n| --- | --- | --- |\n|READER\\_ACCOUNT\\_NAME |VARCHAR |Name of the reader account where the warehouse usage took place. Column only included in view in READER\\_ACCOUNT\\_USAGE schema. |\n|START\\_TIME |TIMESTAMP\\_LTZ |The date and beginning of the hour (in the local time zone) in which the warehouse usage took place. |\n|END\\_TIME |TIMESTAMP\\_LTZ |The date and end of the hour (in the local time zone) in which the warehouse usage took place. |\n|WAREHOUSE\\_ID |NUMBER |Internal/system-generated identifier for the warehouse. |\n|WAREHOUSE\\_NAME |VARCHAR |Name of the warehouse. |\n|CREDITS\\_USED |NUMBER |Total number of credits used for the warehouse in the hour. This is a sum of CREDITS\\_USED\\_COMPUTE and CREDITS\\_USED\\_CLOUD\\_SERVICES. This value does not take into account the adjustment for cloud services , and may therefore be greater than the credits that are billed. To determine how many credits were actually billed, run queries against the METERING\\_DAILY\\_HISTORY view . |\n|CREDITS\\_USED\\_COMPUTE |NUMBER |Number of credits used for the warehouse in the hour. |\n|CREDITS\\_USED\\_CLOUD\\_SERVICES |NUMBER |Number of credits used for cloud services in the hour. |\n|CREDITS\\_ATTRIBUTED\\_COMPUTE\\_QUERIES |NUMBER |Number of credits attributed to queries in the hour. . . Includes only the credit usage for query execution and doesn’t include warehouse idle time usage. |\n\n## Usage notes ¶\n\n* In the ACCOUNT\\_USAGE schema, latency for the view is up to 180 minutes (3 hours), except for the CREDITS\\_USED\\_CLOUD\\_SERVICES column. Latency for\n  CREDITS\\_USED\\_CLOUD\\_SERVICES is up to 6 hours.\n* In the READER\\_ACCOUNT\\_USAGE schema, latency for the view is up to 24 hours.\n* Warehouse idle time is not included in the CREDITS\\_ATTRIBUTED\\_COMPUTE\\_QUERIES column.\n  \n  See Examples for a query that calculates the cost of idle time.\n\n* If you want to reconcile the data in this view with a corresponding view in the ORGANIZATION USAGE schema , you must first set the timezone of the session to UTC. Before querying the Account Usage view, execute:\n  \n  > ```\n  > ALTER SESSION SET TIMEZONE = UTC ;\n  > ```\n  > \n  > Copy\n  > \n  >\n\n## Examples ¶\n\nFor example, to determine the cost of idle time for each warehouse for the last 10 days, execute the following statement:\n\n```\nSELECT \n  ( SUM ( credits_used_compute ) - \n    SUM ( credits_attributed_compute_queries )) AS idle_cost , \n  warehouse_name \n FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n WHERE start_time >= DATEADD ( 'days' , - 10 , CURRENT_DATE ()) \n  AND end_time < CURRENT_DATE () \n GROUP BY warehouse_name ;\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Columns\n2. Usage notes\n3. Examples\n\nLanguage: **English**\n\n* English\n* Français\n* Deutsch\n* 日本語\n* 한국어\n* Português\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details‎\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details‎\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details‎\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details‎\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"],"full_content":null}],"errors":[],"warnings":[{"type":"warning","message":"Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.","detail":null}],"usage":[{"name":"sku_extract_excerpts","count":4}]}
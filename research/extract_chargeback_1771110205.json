{"extract_id":"extract_f973c54b3a6c4a75b23ba932f8627073","results":[{"url":"https://docs.snowflake.com/en/user-guide/cost-attributing","title":"Attributing cost | Snowflake Documentation","publish_date":null,"excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nGuides Cost & Billing Visibility Attributing cost\nSection Title: Attributing cost ¶\nContent:\nAn organization can apportion the cost of using Snowflake to logical units within the organization (for example, to different\ndepartments, environments, or projects). This chargeback or showback model is useful for accounting purposes and pinpoints\nareas of the organization that could benefit from controls and optimizations that can reduce costs.\nTo attribute costs to different groups like departments or projects, use the following recommended approach:\nUse object tags to associate resources and users with departments or projects.\nUse query tags to associate individual queries with departments or projects when the queries are\nmade by the same application on behalf of users belonging to multiple departments.\nSection Title: Attributing cost ¶ > Types of cost attribution scenarios ¶\nContent:\nThe following cost attribution scenarios are the most commonly encountered. In these scenarios, warehouses are used as an\nexample of a resource that incurs costs.\n**Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate\nwarehouses with a department. You can use these object tags to attribute the costs incurred by those warehouses to that\ndepartment entirely.\n**Resources that are shared by users from multiple departments:** An example of this is a warehouse shared by users from\ndifferent departments. In this case, you use object tags to associate each user with a department. The costs of queries are\nattributed to the users. Using the object tags assigned to users, you can break down the costs by department.\n**Applications or workflows shared by users from different departments:** An example of this is an application that issues\nqueries on behalf of its users. In this case, each query executed by the application is assigned a query tag that identifies\nthe team or cost center of the user on whose behalf the query is being made.\nThe next sections explain how to set up object tags in your accounts and provide the details for each of these cost attribution\nscenarios.\nSection Title: Attributing cost ¶ > Setting up object tags for cost attribution ¶\nContent:\nWhen you set up tags to represent the groupings that you want to use for cost attribution, you should determine if the\ngroupings apply to a single account or multiple accounts. This determines how you set up your tags.\nFor example, suppose that you want to attribute costs based on department.\nIf the resources used by the department are located in a single account, you create the tags in a database in that account.\nIf the resources used by the department span multiple accounts, you create the tags in a key account in your organization (for example, in your organization account ),\nand you make those tags available in other accounts through replication .\nThe next sections explain how to create the tags, replicate the tags, and apply the tags to resources.\nCreating the tags\nReplicating the tag database\nTagging the resources and users\nNote\nThe examples in these sections use the custom role `tag_admin` , which is assumed to have been granted the privileges to\ncreate and manage tags. Within your organization, you can use more granular privileges for object tagging to develop a secure tagging strategy.\nSection Title: Attributing cost ¶ > Setting up object tags for cost attribution ¶ > Creating the tags ¶\nContent:\nAs part of designing the strategy, decide on the database and schema where you plan to create the tags.\nYou can create a dedicated database and schema for the tags.\nIf you want to tag resources in different accounts across your organization, you can create the tags in a key account in your\norganization (for example, in your organization account ).\nThe following example creates a database named `cost_management` and a schema named `tags` for the tags that you plan to use:\n```\nUSE ROLE tag_admin ; \n\n CREATE DATABASE cost_management ; \n CREATE SCHEMA tags ;\n```\nCopy\nWith `cost_management` and `tags` selected as the current database and schema, create a tag named `cost_center` and set\nthe values allowed for the tag to the names of cost centers:\n```\nCREATE TAG cost_center \n  ALLOWED_VALUES 'finance' , 'marketing' , 'engineering' , 'product' ;\n```\nCopy\n ... \nSection Title: Attributing cost ¶ > Viewing cost by tag in SQL ¶\nContent:\nYou can attribute costs within an account or across accounts in an organization:\n**Attributing costs within an account**You can attribute costs within an account by querying the following views in the ACCOUNT_USAGE schema:\nTAG_REFERENCES view : Identifies objects (for example, warehouses and users) that have tags.\nWAREHOUSE_METERING_HISTORY view : Provides credit usage for warehouses.\nQUERY_ATTRIBUTION_HISTORY view : Provides the compute costs for queries. The cost per query is\nthe warehouse credit usage for executing the query.For more information on using this view, see About the QUERY_ATTRIBUTION_HISTORY view .\n**Attributing costs across accounts in an organization**Within an organization, you can also attribute costs for resources that are used **exclusively by a single department** by\nquerying views in the ORGANIZATION_USAGE schema from the organization account .Note\nIn the ORGANIZATION_USAGE schema, the TAG_REFERENCES view is only available in the organization account.\nThe QUERY_ATTRIBUTION_HISTORY view is only available in the ACCOUNT_USAGE schema for an account. There is no\norganization-wide equivalent of the view.\nThe next sections explain how to attribute costs for some of the common cost-attribution scenarios :\nResources not shared by departments\nResources shared by users from different departments\nResources used by applications that need to attribute costs to different departments\nSection Title: Attributing cost ¶ > Viewing cost by tag in SQL ¶ > Resources not shared by departments ¶\nContent:\nSuppose that you want to attribute costs by department and that each department uses a set of dedicated warehouses.\nIf you tag warehouses with a `cost_center` tag to identify the department that owns the warehouse, you can join the\nACCOUNT_USAGE TAG_REFERENCES view with the WAREHOUSE_METERING_HISTORY view on the `object_id` and `warehouse_id` columns to get usage\ninformation by warehouse, and you can use the `tag_value` column to identify the departments that own those warehouses.\nThe following SQL statement performs this join:\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost ¶ > Viewing cost by tag in SQL ¶ > Resources shared by users from different departments ¶\nContent:\nSuppose that users in different departments share the same warehouses and you want to break down the credits used by each\ndepartment. You can tag the users with a `cost_center` tag to identify the department that they belong to, and you can join\nthe TAG_REFERENCES view with the QUERY_ATTRIBUTION_HISTORY view .\nNote\nYou can only get this data for a single account at a time. You cannot execute a query that retrieves this data across\naccounts in an organization.\nThe next sections provide examples of SQL statements for attributing costs for shared resources.\nCalculating the cost of user queries for the last month\nCalculating the cost of user queries by department without idle time\nCalculating the cost of queries by users without idle time\nCalculating the cost of queries by users without tags\n ... \nSection Title: Attributing cost ¶ > ... > Resources shared by users from different departments ¶ > Calculating the cost of queries by users without tags ¶\nContent:\nThe following example calculates the cost of queries by users who are not tagged. You can use this to verify that tags are\nbeing applied consistently to users.\n```\nSELECT qah . user_name , SUM ( qah . credits_attributed_compute ) as total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n    LEFT JOIN snowflake . account_usage . tag_references tr \n    ON qah . user_name = tr . object_name AND tr . DOMAIN = 'USER' \n  WHERE \n    start_time >= dateadd ( month , - 1 , current_date ) \n    AND qah . user_name IS NULL OR tr . object_name IS NULL \n  GROUP BY qah . user_name \n  ORDER BY total_credits DESC ;\n```\nCopy\n```\n+------------+---------------+ \n | USER_NAME  | TOTAL_CREDITS | \n |------------+---------------| \n | RSMITH     |  0.1830555556 | \n +------------+---------------+\n```\nSection Title: Attributing cost ¶ > Viewing cost by tag in SQL ¶ > Resources used by applications that need to attribute costs to different departments ¶\nContent:\nThe examples in this section calculate the costs for one or more applications that are powered by Snowflake.\nThe examples assume that these applications set query tags that identify the application for all queries executed. To set the\nquery tag for queries in a session, execute the ALTER SESSION command. For example:\n```\nALTER SESSION SET QUERY_TAG = 'COST_CENTER=finance' ;\n```\nCopy\nThis associates the `COST_CENTER=finance` tag with all subsequent queries executed during the session.\nYou can then use the query tag to trace back the cost incurred by these queries to the appropriate departments.\nThe next sections provide examples of using this approach.\nCalculating the cost of queries by department\nCalculating the cost of queries (excluding idle time) by query tag\nCalculating the cost of queries (including idle time) by query tag\nSection Title: Attributing cost ¶ > ... > Calculating the cost of queries by department ¶\nContent:\nThe following example calculates the compute credits and the credits used for the query acceleration service for the finance department. This depends on the `COST_CENTER=finance` query tag being applied to the original queries that were executed.\nNote that the costs exclude idle time.\n```\nSELECT \n    query_tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_tag = 'COST_CENTER=finance' \n  GROUP BY query_tag ;\n```\nCopy\n```\n+---------------------+-----------------+------+ \n | QUERY_TAG           | COMPUTE_CREDITS | QAS  | \n |---------------------+-----------------|------| \n | COST_CENTER=finance |      0.00576115 | null | \n +---------------------+-----------------+------+\n```\nSection Title: Attributing cost ¶ > ... > Calculating the cost of queries (excluding idle time) by query tag ¶\nContent:\nThe following example calculates the cost of queries by query tag and includes queries without tags (identified as “untagged”).\n```\nSELECT \n    COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n  GROUP BY tag \n  ORDER BY compute_credits DESC ;\n```\nCopy\n```\n+-------------------------+-----------------+------+ \n | TAG                     | COMPUTE_CREDITS | QAS  | \n |-------------------------+-----------------+------+ \n | untagged                | 3.623173449     | null | \n | COST_CENTER=engineering | 0.531431948     | null | \n |-------------------------+-----------------+------+\n```\nSection Title: Attributing cost ¶ > ... > Calculating the cost of queries (including idle time) by query tag ¶\nContent:\nThe following example distributes the idle time that is not captured in the per-query cost across departments in proportion\nto their usage of the warehouse.\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n      AND start_time < CURRENT_DATE \n  ), \n  tag_credits AS ( \n    SELECT \n        COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n        SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n      GROUP BY tag \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n      FROM tag_credits \n  ) \n SELECT \n    tc . tag , \n    tc . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM tag_credits tc , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\nCopy\nSection Title: Attributing cost ¶ > ... > Calculating the cost of queries (including idle time) by query tag ¶\nContent:\n```\n+-------------------------+--------------------+ \n | TAG                     | ATTRIBUTED_CREDITS | \n +-------------------------+--------------------| \n | untagged                |        9.020031304 | \n | COST_CENTER=finance     |        1.027742521 | \n | COST_CENTER=engineering |        1.018755812 | \n | COST_CENTER=marketing   |       0.4801370376 | \n +-------------------------+--------------------+\n```\nSection Title: Attributing cost ¶ > Viewing cost by tag in Snowsight ¶\nContent:\nYou can attribute costs by reporting on the use of resources that have the `cost_center` tag. You can access this data in Snowsight .\nSwitch to a role that has access to the ACCOUNT_USAGE schema .\nIn the navigation menu, select Admin » Cost management .\nSelect Consumption .\nFrom the Tags drop-down, select the `cost_center` tag.\nTo focus on a specific cost center, select a value from the list of the tag’s values.\nSelect Apply .\nFor more details about filtering in Snowsight, see Filter by tag ."],"full_content":"[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nGuides Cost & Billing Visibility Attributing cost\n\n# Attributing cost ¶\n\nAn organization can apportion the cost of using Snowflake to logical units within the organization (for example, to different\ndepartments, environments, or projects). This chargeback or showback model is useful for accounting purposes and pinpoints\nareas of the organization that could benefit from controls and optimizations that can reduce costs.\n\nTo attribute costs to different groups like departments or projects, use the following recommended approach:\n\n* Use object tags to associate resources and users with departments or projects.\n* Use query tags to associate individual queries with departments or projects when the queries are\n  made by the same application on behalf of users belonging to multiple departments.\n\n## Types of cost attribution scenarios ¶\n\nThe following cost attribution scenarios are the most commonly encountered. In these scenarios, warehouses are used as an\nexample of a resource that incurs costs.\n\n* **Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate\n  warehouses with a department. You can use these object tags to attribute the costs incurred by those warehouses to that\n  department entirely.\n  \n  \n* **Resources that are shared by users from multiple departments:** An example of this is a warehouse shared by users from\n  different departments. In this case, you use object tags to associate each user with a department. The costs of queries are\n  attributed to the users. Using the object tags assigned to users, you can break down the costs by department.\n  \n  \n* **Applications or workflows shared by users from different departments:** An example of this is an application that issues\n  queries on behalf of its users. In this case, each query executed by the application is assigned a query tag that identifies\n  the team or cost center of the user on whose behalf the query is being made.\n  \n  \n\nThe next sections explain how to set up object tags in your accounts and provide the details for each of these cost attribution\nscenarios.\n\n## Setting up object tags for cost attribution ¶\n\nWhen you set up tags to represent the groupings that you want to use for cost attribution, you should determine if the\ngroupings apply to a single account or multiple accounts. This determines how you set up your tags.\n\nFor example, suppose that you want to attribute costs based on department.\n\n* If the resources used by the department are located in a single account, you create the tags in a database in that account.\n* If the resources used by the department span multiple accounts, you create the tags in a key account in your organization (for example, in your organization account ),\n  and you make those tags available in other accounts through replication .\n\nThe next sections explain how to create the tags, replicate the tags, and apply the tags to resources.\n\n* Creating the tags\n* Replicating the tag database\n* Tagging the resources and users\n\nNote\n\nThe examples in these sections use the custom role `tag_admin` , which is assumed to have been granted the privileges to\ncreate and manage tags. Within your organization, you can use more granular privileges for object tagging to develop a secure tagging strategy.\n\n### Creating the tags ¶\n\nAs part of designing the strategy, decide on the database and schema where you plan to create the tags.\n\n* You can create a dedicated database and schema for the tags.\n* If you want to tag resources in different accounts across your organization, you can create the tags in a key account in your\n  organization (for example, in your organization account ).\n\nThe following example creates a database named `cost_management` and a schema named `tags` for the tags that you plan to use:\n\n```\nUSE ROLE tag_admin ; \n\n CREATE DATABASE cost_management ; \n CREATE SCHEMA tags ;\n```\n\nCopy\n\nWith `cost_management` and `tags` selected as the current database and schema, create a tag named `cost_center` and set\nthe values allowed for the tag to the names of cost centers:\n\n```\nCREATE TAG cost_center \n  ALLOWED_VALUES 'finance' , 'marketing' , 'engineering' , 'product' ;\n```\n\nCopy\n\n### Replicating the tag database ¶\n\nIf you have an organization with multiple accounts and you want to make the tags available in these other accounts, set up your accounts for replication , and create a replication group in a main account (for example, in the organization account ). Set up this replication group to replicate the database\ncontaining the tags.\n\nFor example, to replicate the tags to the accounts named `my_org.my_account` and `my_org.my_account_2` , execute this\nstatement in your organization account:\n\n```\nCREATE REPLICATION GROUP cost_management_repl_group \n  OBJECT_TYPES = DATABASES \n  ALLOWED_DATABASES = cost_management \n  ALLOWED_ACCOUNTS = my_org . my_account_1 , my_org . my_account_2 \n  REPLICATION_SCHEDULE = '10 MINUTE' ;\n```\n\nCopy\n\nThen, in each account in which you want to make the tags available, create a secondary replication group, and refresh this\ngroup from the primary group:\n\n```\nCREATE REPLICATION GROUP cost_management_repl_group \n  AS REPLICA OF my_org . my_org_account . cost_management_repl_group ; \n\n ALTER REPLICATION GROUP cost_management_repl_group REFRESH ;\n```\n\nCopy\n\n### Tagging the resources and users ¶\n\nAfter creating and replicating the tags, you can use these tags to identify the warehouses and users belonging to each\ndepartment. For example, because the sales department uses both `warehouse1` and `warehouse2` , you can set the `cost_center` tag to `'SALES'` for both warehouses.\n\nTip\n\nIdeally, you should have workflows that automate the process of applying these tags when you create resources and users.\n\n```\nUSE ROLE tag_admin ; \n\n ALTER WAREHOUSE warehouse1 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse2 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse3 SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n\n ALTER USER finance_user SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n ALTER USER sales_user SET TAG cost_management . tags . cost_center = 'SALES' ;\n```\n\nCopy\n\n## Viewing cost by tag in SQL ¶\n\nYou can attribute costs within an account or across accounts in an organization:\n\n* **Attributing costs within an account**\n  \n  You can attribute costs within an account by querying the following views in the ACCOUNT\\_USAGE schema:\n  \n    + TAG\\_REFERENCES view : Identifies objects (for example, warehouses and users) that have tags.\n    + WAREHOUSE\\_METERING\\_HISTORY view : Provides credit usage for warehouses.\n    + QUERY\\_ATTRIBUTION\\_HISTORY view : Provides the compute costs for queries. The cost per query is\n        the warehouse credit usage for executing the query.\n        \n        For more information on using this view, see About the QUERY\\_ATTRIBUTION\\_HISTORY view .\n* **Attributing costs across accounts in an organization**\n  \n  Within an organization, you can also attribute costs for resources that are used **exclusively by a single department** by\n  querying views in the ORGANIZATION\\_USAGE schema from the organization account .\n  \n  Note\n  \n    + In the ORGANIZATION\\_USAGE schema, the TAG\\_REFERENCES view is only available in the organization account.\n    + The QUERY\\_ATTRIBUTION\\_HISTORY view is only available in the ACCOUNT\\_USAGE schema for an account. There is no\n        organization-wide equivalent of the view.\n\nThe next sections explain how to attribute costs for some of the common cost-attribution scenarios :\n\n* Resources not shared by departments\n* Resources shared by users from different departments\n* Resources used by applications that need to attribute costs to different departments\n\n### Resources not shared by departments ¶\n\nSuppose that you want to attribute costs by department and that each department uses a set of dedicated warehouses.\n\nIf you tag warehouses with a `cost_center` tag to identify the department that owns the warehouse, you can join the\nACCOUNT\\_USAGE TAG\\_REFERENCES view with the WAREHOUSE\\_METERING\\_HISTORY view on the `object_id` and `warehouse_id` columns to get usage\ninformation by warehouse, and you can use the `tag_value` column to identify the departments that own those warehouses.\n\nThe following SQL statement performs this join:\n\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\n\nCopy\n\n```\n+-------------+-------------+-----------------+ \n | TAG_NAME    | TAG_VALUE   |   TOTAL_CREDITS | \n |-------------+-------------+-----------------| \n | NULL        | untagged    |    20.360277159 | \n | COST_CENTER | Sales       |    17.173333333 | \n | COST_CENTER | Finance     |      8.14444444 | \n +-------------+-------------+-----------------+\n```\n\nYou can run a similar query to perform the same attribution for all the accounts in your organization using views in the\nORGANIZATION\\_USAGE schema from the organization account . The rest of the query\ndoes not change.\n\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ORGANIZATION_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ORGANIZATION_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n          AND tag_database = 'COST_MANAGEMENT' AND tag_schema = 'TAGS' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\n\nCopy\n\n### Resources shared by users from different departments ¶\n\nSuppose that users in different departments share the same warehouses and you want to break down the credits used by each\ndepartment. You can tag the users with a `cost_center` tag to identify the department that they belong to, and you can join\nthe TAG\\_REFERENCES view with the QUERY\\_ATTRIBUTION\\_HISTORY view .\n\nNote\n\nYou can only get this data for a single account at a time. You cannot execute a query that retrieves this data across\naccounts in an organization.\n\nThe next sections provide examples of SQL statements for attributing costs for shared resources.\n\n* Calculating the cost of user queries for the last month\n* Calculating the cost of user queries by department without idle time\n* Calculating the cost of queries by users without idle time\n* Calculating the cost of queries by users without tags\n\n#### Calculating the cost of user queries for the last month ¶\n\nThis following SQL statement calculates the costs for the last month.\n\nIn this example, idle time is distributed among the users in proportion to their usage.\n\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n  ), \n  user_credits AS ( \n    SELECT user_name , SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n      GROUP BY user_name \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n    FROM user_credits \n  ) \n SELECT \n    u . user_name , \n    u . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM user_credits u , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\n\nCopy\n\n```\n+-----------+--------------------+ \n | USER_NAME | ATTRIBUTED_CREDITS | \n |-----------+--------------------+ \n | FINUSER   | 6.603575468        | \n | SALESUSER | 4.321378049        | \n | ENGUSER   | 0.6217131392       | \n |-----------+--------------------+\n```\n\n#### Calculating the cost of user queries by department without idle time ¶\n\nThe following example attributes the compute cost to each department through the queries executed by users in that department.\nThis query depends on the user objects having a tag that identifies their department.\n\n```\nWITH joined_data AS ( \n  SELECT \n      tr . tag_name , \n      tr . tag_value , \n      qah . credits_attributed_compute , \n      qah . start_time \n    FROM SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES tr \n      JOIN SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n        ON tr . domain = 'USER' AND tr . object_name = qah . user_name \n ) \n SELECT \n    tag_name , \n    tag_value , \n    SUM ( credits_attributed_compute ) AS total_credits \n  FROM joined_data \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY tag_name , tag_value \n  ORDER BY tag_name , tag_value ;\n```\n\nCopy\n\n```\n+-------------+-------------+-----------------+ \n | TAG_NAME    | TAG_VALUE   |   TOTAL_CREDITS | \n |-------------+-------------+-----------------| \n | COST_CENTER | engineering |   0.02493688426 | \n | COST_CENTER | finance     |    0.2281084988 | \n | COST_CENTER | marketing   |    0.3686840545 | \n |-------------+-------------+-----------------|\n```\n\n#### Calculating the cost of queries by users without idle time ¶\n\nThis following SQL statement calculates the costs per user for the past month (excluding idle time).\n\n```\nSELECT user_name , SUM ( credits_attributed_compute ) AS credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE \n    start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY user_name ;\n```\n\nCopy\n\n```\n+-----------+--------------------+ \n | USER_NAME | ATTRIBUTED_CREDITS | \n |-----------+--------------------| \n | JSMITH    |       17.173333333 | \n | MJONES    |         8.14444444 | \n | SYSTEM    |         5.33985393 | \n +-----------+--------------------+\n```\n\n#### Calculating the cost of queries by users without tags ¶\n\nThe following example calculates the cost of queries by users who are not tagged. You can use this to verify that tags are\nbeing applied consistently to users.\n\n```\nSELECT qah . user_name , SUM ( qah . credits_attributed_compute ) as total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n    LEFT JOIN snowflake . account_usage . tag_references tr \n    ON qah . user_name = tr . object_name AND tr . DOMAIN = 'USER' \n  WHERE \n    start_time >= dateadd ( month , - 1 , current_date ) \n    AND qah . user_name IS NULL OR tr . object_name IS NULL \n  GROUP BY qah . user_name \n  ORDER BY total_credits DESC ;\n```\n\nCopy\n\n```\n+------------+---------------+ \n | USER_NAME  | TOTAL_CREDITS | \n |------------+---------------| \n | RSMITH     |  0.1830555556 | \n +------------+---------------+\n```\n\n### Resources used by applications that need to attribute costs to different departments ¶\n\nThe examples in this section calculate the costs for one or more applications that are powered by Snowflake.\n\nThe examples assume that these applications set query tags that identify the application for all queries executed. To set the\nquery tag for queries in a session, execute the ALTER SESSION command. For example:\n\n```\nALTER SESSION SET QUERY_TAG = 'COST_CENTER=finance' ;\n```\n\nCopy\n\nThis associates the `COST_CENTER=finance` tag with all subsequent queries executed during the session.\n\nYou can then use the query tag to trace back the cost incurred by these queries to the appropriate departments.\n\nThe next sections provide examples of using this approach.\n\n* Calculating the cost of queries by department\n* Calculating the cost of queries (excluding idle time) by query tag\n* Calculating the cost of queries (including idle time) by query tag\n\n#### Calculating the cost of queries by department ¶\n\nThe following example calculates the compute credits and the credits used for the query acceleration service for the finance department. This depends on the `COST_CENTER=finance` query tag being applied to the original queries that were executed.\n\nNote that the costs exclude idle time.\n\n```\nSELECT \n    query_tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_tag = 'COST_CENTER=finance' \n  GROUP BY query_tag ;\n```\n\nCopy\n\n```\n+---------------------+-----------------+------+ \n | QUERY_TAG           | COMPUTE_CREDITS | QAS  | \n |---------------------+-----------------|------| \n | COST_CENTER=finance |      0.00576115 | null | \n +---------------------+-----------------+------+\n```\n\n#### Calculating the cost of queries (excluding idle time) by query tag ¶\n\nThe following example calculates the cost of queries by query tag and includes queries without tags (identified as “untagged”).\n\n```\nSELECT \n    COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n  GROUP BY tag \n  ORDER BY compute_credits DESC ;\n```\n\nCopy\n\n```\n+-------------------------+-----------------+------+ \n | TAG                     | COMPUTE_CREDITS | QAS  | \n |-------------------------+-----------------+------+ \n | untagged                | 3.623173449     | null | \n | COST_CENTER=engineering | 0.531431948     | null | \n |-------------------------+-----------------+------+\n```\n\n#### Calculating the cost of queries (including idle time) by query tag ¶\n\nThe following example distributes the idle time that is not captured in the per-query cost across departments in proportion\nto their usage of the warehouse.\n\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n      AND start_time < CURRENT_DATE \n  ), \n  tag_credits AS ( \n    SELECT \n        COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n        SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n      GROUP BY tag \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n      FROM tag_credits \n  ) \n SELECT \n    tc . tag , \n    tc . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM tag_credits tc , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\n\nCopy\n\n```\n+-------------------------+--------------------+ \n | TAG                     | ATTRIBUTED_CREDITS | \n +-------------------------+--------------------| \n | untagged                |        9.020031304 | \n | COST_CENTER=finance     |        1.027742521 | \n | COST_CENTER=engineering |        1.018755812 | \n | COST_CENTER=marketing   |       0.4801370376 | \n +-------------------------+--------------------+\n```\n\n## Viewing cost by tag in Snowsight ¶\n\nYou can attribute costs by reporting on the use of resources that have the `cost_center` tag. You can access this data in Snowsight .\n\n1. Switch to a role that has access to the ACCOUNT\\_USAGE schema .\n2. In the navigation menu, select Admin » Cost management .\n3. Select Consumption .\n4. From the Tags drop-down, select the `cost_center` tag.\n5. To focus on a specific cost center, select a value from the list of the tag’s values.\n6. Select Apply .\n\nFor more details about filtering in Snowsight, see Filter by tag .\n\n## About the QUERY\\_ ATTRIBUTION\\_ HISTORY view ¶\n\nYou can use the QUERY\\_ATTRIBUTION\\_HISTORY view to attribute cost based on queries. The cost per\nquery is the warehouse credit usage for executing the query. This cost does not include any other credit usage that is incurred\nas a result of query execution. For example, the following are not included in the query cost:\n\n* Data transfer costs\n* Storage costs\n* Cloud services costs\n* Costs for serverless features\n* Costs for tokens processed by AI services\n\nFor queries that are executed concurrently, the cost of the warehouse is attributed to individual queries based on the weighted\naverage of their resource consumption during a given time interval.\n\nThe cost per query does not include warehouse _idle time_ . Idle time is a period of time in which no queries are running in the\nwarehouse and can be measured at the warehouse level.\n\n## Additional examples of queries ¶\n\nThe next sections provide additional queries that you can use for cost attribution:\n\n* Grouping similar queries\n* Attributing costs of hierarchical queries\n\n### Grouping similar queries ¶\n\nFor recurrent or similar queries, use the `query_hash` or `query_parameterized_hash` to group costs\nby query.\n\nTo find the most expensive recurrent queries for the current month, execute the following statement:\n\n```\nSELECT query_parameterized_hash , \n       COUNT (*) AS query_count , \n       SUM ( credits_attributed_compute ) AS total_credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n  AND start_time < CURRENT_DATE \n  GROUP BY query_parameterized_hash \n  ORDER BY total_credits DESC \n  LIMIT 20 ;\n```\n\nCopy\n\nFor an additional query based on query ID, see Examples .\n\n### Attributing costs of hierarchical queries ¶\n\nFor stored procedures that issue multiple hierarchical queries, you can compute the attributed query costs for the\nprocedure by using the root query ID for the procedure.\n\n1. To find the root query ID for a stored procedure, use the ACCESS\\_HISTORY view . For example,\n   to find the root query ID for a stored procedure, set the `query_id` and execute the following statements:\n   \n   ```\n   SET query_id = '<query_id>' ; \n   \n    SELECT query_id , \n          parent_query_id , \n          root_query_id , \n          direct_objects_accessed \n     FROM SNOWFLAKE . ACCOUNT_USAGE . ACCESS_HISTORY \n     WHERE query_id = $ query_id ;\n   ```\n   \n   Copy\n   \n   For more information, see Ancestor queries with stored procedures .\n2. To sum the query cost for the entire procedure, replace `<root_query_id>` and execute the following statements:\n   \n   ```\n   SET query_id = '<root_query_id>' ; \n   \n    SELECT SUM ( credits_attributed_compute ) AS total_attributed_credits \n     FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n     WHERE ( root_query_id = $ query_id OR query_id = $ query_id );\n   ```\n   \n   Copy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nLanguage: **English**\n\n* English\n* Français\n* Deutsch\n* 日本語\n* 한국어\n* Português"},{"url":"https://medium.com/snowflake/finops-at-snowflake-dogfooding-multi-cloud-management-at-huge-scale-52b734e5de71","title":"FinOps at Snowflake: Dogfooding Multi-Cloud Management at Huge Scale | by Alex Landis | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2025-02-15","excerpts":["Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nSearch\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-52b734e5de71---------------------------------------)\n·\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-52b734e5de71---------------------------------------)\nBest practices, tips & tricks from Snowflake experts and community\nAlex Landis\n6 min read\n·\nFeb 15, 2025\n--\n3\nListen\nShare\nTo a certain extent, FinOps has been built into the DNA of Snowflake from the beginning. [Snowflake’s cloud data platform](https://www.snowflake.com/en/data-cloud/platform/) leverages third-party cloud partners for unlimited scale, handling tens of billions of customer jobs daily, meaning successful FinOps practices are a core competency for success. As our FinOps capabilities have matured and despite the ever-evolving tooling landscape, what do we come back to time and again for cloud financial management? Snowflake’s own platform.\nSection Title: Snowflake as a FinOps platform\nContent:\nProcessing and analyzing cost and usage reports from multiple cloud service providers showcases Snowflake’s core features for large-scale data management. For example, the AWS CUR is directly ingested from S3 to establish automated, controlled billing data access. While that is also true for other FinOps tools, integrating raw cloud billing data with other in-house data sources transforms these datasets from AWS-focused cost insights to company performance measures and enables real-time, data-driven enterprise decision making.\nPress enter or click to view image in full size\nThis implementation is further outlined here: (https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration)\nThis is the backbone of Snowflake’s cloud financial management practice, providing all teams a pipeline to connect to cloud expense tables married to business context. Cost data by itself is great, but understanding costs against other internal metrics (sales regions, SKUs, various CSPs, age of deployment, Engineering service area, etc.) unlocks insights that the rest of the business can actually comprehend and take action against. This is the beginning of forming a strategic advantage.\n ... \nSection Title: Snowflake as a FinOps platform > Cloud Performance = Business Performance\nContent:\nThe above tooling, team structure, and focus on business context allows for us to use per-unit drivers to visualize Snowflake’s cloud business. The core unit metrics used are: cost-per and price-per compute credit, stored terabyte, and transferred terabyte. Using these three drivers, we’re able to paint a complete picture of Snowflake’s platform, customer profile, cloud providers, and total business. The diagram below gives a general overview of how we think about these Key Performance Metric levels:\nPress enter or click to view image in full size\nUsing this structure, we’re able to quickly understand questions such as:\nSection Title: Snowflake as a FinOps platform > Cloud Performance = Business Performance\nContent:\nWhat changes to customer contracts have most impacted our overall cloud and company margin?\nWhich of our cloud service provider contracts need to be reassessed and when? What impact will that have on specific portions of Snowflake’s business?\nHow are portions of Snowflake’s platform performing in different geographic locations? How should we understand the impact of moving to new cloud regions or Snowflake deployments? What impact will pricing in those areas have on company metric performance in any of the higher-verticals?\nWhich Engineering teams are spending most efficiently? What platform areas contribute the most to cost per credit? How fast are costs in each platform area growing vs. the rest?\nAre our deployments actually earning the forecasted margin? If not, what are the underlying drivers causing a divergence?\nAre we priced appropriately? Are there certain areas where we need to explore re-pricing?\nThese metrics are used across our internal teams for the business performance questions mentioned above, product and engineering decisions, and core enterprise practices such as accounting and compliance.\nSection Title: Snowflake as a FinOps platform > Aligned Accounting Practices\nContent:\nDrawing a full line through our business with this level of fidelity enables all teams, not just Engineering, to manage Snowflake’s business with greater levels of granularity. A great example is our Accounting team, one of the key risk managers for the company.\nSnowflake’s Product Finance and cloud Accounting teams work in lockstep across to ensure compliant recognition of all expenses from our third-party cloud service providers, laddering up to our public reporting. As is typical, Product Finance relies on rulings from our accounting team to refine forecast inputs, confirm expense buckets for margin analysis, and business cases, while product finance owns business context behind transactions to validate their treatment. In all these situations, both teams level set on the above mentioned per-unit measures.\nDuring month-end close and quarterly reporting cycles, these teams work especially closely to ensure completeness and accuracy of product-specific financial data in the company’s consolidated financial statements. The unit economics referenced above are used by Snowflake’s accounting for these entries, drawing a straight line through our processes. Product Finance also takes an automated data-driven approach to providing Accounting with monthly flux analyses.\nSection Title: Snowflake as a FinOps platform > Aligned Accounting Practices\nContent:\nMaintaining tight and effective controls is critical to running a public company at scale. With all teams aligned on these metrics at such a granular level unlocks these core processes, as well as ways to monitor for anomalous activity.\nSection Title: Snowflake as a FinOps platform > Beyond Standard Anomaly Alerting\nContent:\nIn addition to the above, our platform enables contextual alerting outside of normal, widely-used, cost-focused anomaly alerts. Advanced, actionable anomaly reporting includes notifications around items as varied as Snowflake warehouse sizing and auto-suspension policy violations, long-running server alerts, mismatches among cloud accounts, tax regions, and cost centers, savings plan coverage decreases, etc. We also connect to tools such as Slack to ensure the data is being shared in places where end-users are active. Many of these are currently live, with more being built and refined as needed.\nUtilizing this framework allows for these alerts to be done at varying levels of granularity across the verticals explained above. This wouldn’t be possible with an ‘out of the box’ FinOps solution. A few examples where we have found incidents through business context include:\nResources that should have been deleted but continue to exist\nOversized and misconfigured VMs relative to what was initially sized\nOpportunities for configuration standardization across our 3 clouds\nOpportunities to pass additional savings to our customers through architectural changes\nInconsistent cost to entity mapping for Accounting purposes\nAffirmation of proper contract discount application\nSection Title: Snowflake as a FinOps platform > Beyond Standard Anomaly Alerting\nContent:\nEach of these processes are meant to insulate Snowflake from massive, highly-variable cloud expense risk. Whether that is expense size, accounting compliance, or strategic decision making, all teams could not consistently speak the same language without the Snowflake platform. Dogfooding ultimately enables the business to move fast and be versatile to the ever-changing environment.\nSection Title: Snowflake as a FinOps platform > Strategic Advantage\nContent:\nWe’re always working to improve our FinOps practice, but are pushing at the forefront of what FinOps can be. By using the approach we’ve discussed we have:\nOutcomes tied directly to company margin performance tracked in single basis points\nTraditionally less technical roles incorporating data-focused skills and processes\nBuilt a common language for cloud usage across all teams within the company\nOur FinOps approach is not just about tracking costs — it’s about using Snowflake’s own advanced data platform capabilities to drive consistency. By dogfooding our own platform, we’ve created a monitoring system that provides granular visibility, control, and accountability. With the single pane of glass and continued fine tuning of our FinOps practice we’ll continue to drive additional value for our customers, our north star.\nThis is the first in a series of Snowflake FinOps blogs planned in 2025.\nFinops\nSnowflake\nCloud Cost Management\nForecasting\nSaaS\n--\n--\n3\n[](https://medium.com/snowflake?source=post_page---post_publication_info--52b734e5de71---------------------------------------)\n[](https://medium.com/snowflake?source=post_page---post_publication_info--52b734e5de71---------------------------------------)\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--52b734e5de71---------------------------------------)\nSection Title: Snowflake as a FinOps platform > Strategic Advantage\nContent:\n11K followers\n· Last published 1 day ago\nBest practices, tips & tricks from Snowflake experts and community"],"full_content":"Sitemap\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nWrite\n\nSearch\n\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-52b734e5de71---------------------------------------)\n\n·\n\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-52b734e5de71---------------------------------------)\n\nBest practices, tips & tricks from Snowflake experts and community\n\nAlex Landis\n\n6 min read\n\n·\n\nFeb 15, 2025\n\n\\--\n\n3\n\nListen\n\nShare\n\nTo a certain extent, FinOps has been built into the DNA of Snowflake from the beginning. [Snowflake’s cloud data platform](https://www.snowflake.com/en/data-cloud/platform/) leverages third-party cloud partners for unlimited scale, handling tens of billions of customer jobs daily, meaning successful FinOps practices are a core competency for success. As our FinOps capabilities have matured and despite the ever-evolving tooling landscape, what do we come back to time and again for cloud financial management? Snowflake’s own platform.\n\n## Snowflake as a FinOps platform\n\nProcessing and analyzing cost and usage reports from multiple cloud service providers showcases Snowflake’s core features for large-scale data management. For example, the AWS CUR is directly ingested from S3 to establish automated, controlled billing data access. While that is also true for other FinOps tools, integrating raw cloud billing data with other in-house data sources transforms these datasets from AWS-focused cost insights to company performance measures and enables real-time, data-driven enterprise decision making.\n\nPress enter or click to view image in full size\n\nThis implementation is further outlined here: (https://docs.snowflake.com/en/user-guide/data-load-s3-config-storage-integration)\n\nThis is the backbone of Snowflake’s cloud financial management practice, providing all teams a pipeline to connect to cloud expense tables married to business context. Cost data by itself is great, but understanding costs against other internal metrics (sales regions, SKUs, various CSPs, age of deployment, Engineering service area, etc.) unlocks insights that the rest of the business can actually comprehend and take action against. This is the beginning of forming a strategic advantage.\n\n## Data Empowered Teams For Multi-Cloud FinOps\n\nSnowflake’s superpower is not only in offering in-house tooling for cloud financial management, but also in the analytics-forward culture of the company across all functions. Snowflake Product Finance primarily manages internal FinOps (customer-facing FinOps is a strong competency, but owned separately), requiring strong FP&A backgrounds and the ability to write SQL and understand nuances of internal data pipelines. This gives our FinOps stakeholders the ability to dig deep for answers and to have technical engineering discussions beyond what can be done by most FP&A orgs. We can iterate quickly and have developed tooling that scales to serve a large Product and Engineering org without the potential risk of our data team being a bottleneck. While there are no true ‘runners’ in FinOps, our tooling and team layout allow for deep understanding of our cloud footprints and the ability to quickly triage many types of anomalies.\n\n## Outcomes: Consistent Business Views, Aligned Accounting Practices, and Advanced Anomaly Alerting\n\n## Cloud Performance = Business Performance\n\nThe above tooling, team structure, and focus on business context allows for us to use per-unit drivers to visualize Snowflake’s cloud business. The core unit metrics used are: cost-per and price-per compute credit, stored terabyte, and transferred terabyte. Using these three drivers, we’re able to paint a complete picture of Snowflake’s platform, customer profile, cloud providers, and total business. The diagram below gives a general overview of how we think about these Key Performance Metric levels:\n\nPress enter or click to view image in full size\n\nUsing this structure, we’re able to quickly understand questions such as:\n\n* What changes to customer contracts have most impacted our overall cloud and company margin?\n* Which of our cloud service provider contracts need to be reassessed and when? What impact will that have on specific portions of Snowflake’s business?\n* How are portions of Snowflake’s platform performing in different geographic locations? How should we understand the impact of moving to new cloud regions or Snowflake deployments? What impact will pricing in those areas have on company metric performance in any of the higher-verticals?\n* Which Engineering teams are spending most efficiently? What platform areas contribute the most to cost per credit? How fast are costs in each platform area growing vs. the rest?\n* Are our deployments actually earning the forecasted margin? If not, what are the underlying drivers causing a divergence?\n* Are we priced appropriately? Are there certain areas where we need to explore re-pricing?\n\nThese metrics are used across our internal teams for the business performance questions mentioned above, product and engineering decisions, and core enterprise practices such as accounting and compliance.\n\n## Aligned Accounting Practices\n\nDrawing a full line through our business with this level of fidelity enables all teams, not just Engineering, to manage Snowflake’s business with greater levels of granularity. A great example is our Accounting team, one of the key risk managers for the company.\n\nSnowflake’s Product Finance and cloud Accounting teams work in lockstep across to ensure compliant recognition of all expenses from our third-party cloud service providers, laddering up to our public reporting. As is typical, Product Finance relies on rulings from our accounting team to refine forecast inputs, confirm expense buckets for margin analysis, and business cases, while product finance owns business context behind transactions to validate their treatment. In all these situations, both teams level set on the above mentioned per-unit measures.\n\nDuring month-end close and quarterly reporting cycles, these teams work especially closely to ensure completeness and accuracy of product-specific financial data in the company’s consolidated financial statements. The unit economics referenced above are used by Snowflake’s accounting for these entries, drawing a straight line through our processes. Product Finance also takes an automated data-driven approach to providing Accounting with monthly flux analyses.\n\nMaintaining tight and effective controls is critical to running a public company at scale. With all teams aligned on these metrics at such a granular level unlocks these core processes, as well as ways to monitor for anomalous activity.\n\n## Beyond Standard Anomaly Alerting\n\nIn addition to the above, our platform enables contextual alerting outside of normal, widely-used, cost-focused anomaly alerts. Advanced, actionable anomaly reporting includes notifications around items as varied as Snowflake warehouse sizing and auto-suspension policy violations, long-running server alerts, mismatches among cloud accounts, tax regions, and cost centers, savings plan coverage decreases, etc. We also connect to tools such as Slack to ensure the data is being shared in places where end-users are active. Many of these are currently live, with more being built and refined as needed.\n\nUtilizing this framework allows for these alerts to be done at varying levels of granularity across the verticals explained above. This wouldn’t be possible with an ‘out of the box’ FinOps solution. A few examples where we have found incidents through business context include:\n\n* Resources that should have been deleted but continue to exist\n* Oversized and misconfigured VMs relative to what was initially sized\n* Opportunities for configuration standardization across our 3 clouds\n* Opportunities to pass additional savings to our customers through architectural changes\n* Inconsistent cost to entity mapping for Accounting purposes\n* Affirmation of proper contract discount application\n\nEach of these processes are meant to insulate Snowflake from massive, highly-variable cloud expense risk. Whether that is expense size, accounting compliance, or strategic decision making, all teams could not consistently speak the same language without the Snowflake platform. Dogfooding ultimately enables the business to move fast and be versatile to the ever-changing environment.\n\n## Strategic Advantage\n\nWe’re always working to improve our FinOps practice, but are pushing at the forefront of what FinOps can be. By using the approach we’ve discussed we have:\n\n* Outcomes tied directly to company margin performance tracked in single basis points\n* Traditionally less technical roles incorporating data-focused skills and processes\n* Built a common language for cloud usage across all teams within the company\n\nOur FinOps approach is not just about tracking costs — it’s about using Snowflake’s own advanced data platform capabilities to drive consistency. By dogfooding our own platform, we’ve created a monitoring system that provides granular visibility, control, and accountability. With the single pane of glass and continued fine tuning of our FinOps practice we’ll continue to drive additional value for our customers, our north star.\n\nThis is the first in a series of Snowflake FinOps blogs planned in 2025.\n\nFinops\n\nSnowflake\n\nCloud Cost Management\n\nForecasting\n\nSaaS\n\n\\-- \n\n\\--\n\n3\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--52b734e5de71---------------------------------------)\n\n[](https://medium.com/snowflake?source=post_page---post_publication_info--52b734e5de71---------------------------------------)\n\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--52b734e5de71---------------------------------------)\n\n11K followers\n\n· Last published 1 day ago\n\nBest practices, tips & tricks from Snowflake experts and community\n\n## Written by Alex Landis\n\n12 followers\n\n· 1 following\n\n## Responses (3)\n\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--52b734e5de71---------------------------------------)\n\nSee all responses"},{"url":"https://www.snowflake.com/en/engineering-blog/tag-based-budgets-cost-attribution/","title":"Align Your Budgets with Your Business Using Tag-Based Cost Attribution","publish_date":"2025-08-25","excerpts":["Core Platform\nAug 25, 2025 | 4 min read\nSection Title: Align Your Budgets with Your Business: A New Approach to Cost Attribution\nContent:\nIn a dynamic organization, it’s critical to align cloud costs with specific business units, projects or cost centers. This ensures clear accountability and accurate financial planning. However, manually tracking which resources belong to which budget can be a time-consuming and error-prone process. To simplify this, we’re excited to introduce a more intuitive and powerful way to manage your budgets in Snowflake. ## Announcing tag-based budgets This new capability directly leverages the [object tags](https://docs.snowflake.com/user-guide/object-tagging/introduction) you already use across Snowflake. Object tagging has always been a powerful mechanism for classifying and organizing your resources. Now, you can extend that power directly to cost management. Instead of manually selecting a list of resources for a budget, you can simply tell a budget to monitor a specific tag — now generally available. The budget will then automatically track the combined costs of all objects that share that tag. ## See it in action: Budgeting made simple The power of this approach lies in its simplicity and how it integrates into your existing workflows. Getting started is easy. Let's say you're kicking off a new initiative, \"Project Phoenix.\" 1. **Tag your resources:** To capture a complete picture of your project's credit usage, you apply the `project = 'phoenix'` tag to all its relevant resources.\nSection Title: Align Your Budgets with Your Business: A New Approach to Cost Attribution\nContent:\n```\n+ **Start with your main compute.** Apply the tag to dedicated resources like your **warehouses** and **compute pools** .\n+ **Leverage tag inheritance.** Next, apply the same tag to your **databases** or **schemas** . This is an important step for two reasons: It captures credit usage from the database itself (e.g., from replication), and it automatically cascades the tag to all objects within. This ensures that credit usage from serverless features — like serverless tasks, auto-clustering or materialized views — is captured with minimal effort.\n+ **Get granular when needed.** The system is designed for flexibility. A tag applied directly to a specific object will always take precedence over an inherited tag, giving you precise control over credit attribution. For example, to exclude a set of serverless tasks from your \"Project Phoenix\" budget, you can simply apply a different tag to them, like `project = 'shared'` .\n```\n ... \nSection Title: ... > Key benefits for your organization * **Responsive and complete cost-tracking:** This new approach eliminates manual, end-of-month reconciliat...\nContent:\n**Reliable cost attribution:** This feature enables costs to be assigned to the proper business unit or project, providing a financial record that more accurately reflects your business structure as it evolves through the month.\n**Streamlined management:** By leveraging the familiar object tagging system, you dramatically reduce administrative overhead. Managing a budget's scope is now as simple and intuitive as applying a tag.\n**Flexible and backward-compatible:** The new tag-based method works alongside the existing option to select individual resources. This gives you the flexibility to use tags for dynamic projects while managing simpler budgets resource by resource. Your existing budgets will continue to work without any changes.\nSection Title: Align Your Budgets with Your Business: A New Approach to Cost Attribution > ... > Authors\nContent:\nDinesh Haridas\nSiyoung Oh\nSection Title: Align Your Budgets with Your Business: A New Approach to Cost Attribution > ... > Authors > Share Article\nContent:\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fengineering-blog%2Ftag-based-budgets-cost-attribution&title=Align+Your+Budgets+with+Your+Business%3A+A+New+Approach+to+Cost+Attribution)\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fengineering-blog%2Ftag-based-budgets-cost-attribution&text=Align+Your+Budgets+with+Your+Business%3A+A+New+Approach+to+Cost+Attribution)\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fengineering-blog%2Ftag-based-budgets-cost-attribution)\nSection Title: Align Your Budgets with Your Business: A New Approach to Cost Attribution > ... > Authors > Just For You\nContent:\nData Engineering\nSection Title: Align Your Budgets with Your Business: A New Approach to Cost Attribution > ... > Supercharging DML Performance with Snowflake Gen2 Warehouses\nContent:\nLars Volker | Noble Mushtak | Rudi Leibbrandt\nJul 24, 2025 | 6 min read\nCore Platform\nSection Title: ... > Snowflake Gen2 Warehouses: Blazing Fast Performance, Now Available on All 3 Clouds Across Expanded Regions\nContent:\nPD Dutta\nJul 25, 2025 | 3 min read\nSubscribe to our blog newsletter\nGet the best, coolest and latest delivered to your inbox each week\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\nSection Title: Align Your Budgets with Your Business: A New Approach to Cost Attribution > Where Data Does More\nContent:\n30-day free trial\nNo credit card required\nCancel anytime\n[start for free](https://signup.snowflake.com/)\nwatch a demo\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n[Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n[Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n[Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n[Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n[Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n[Technology](https://www.snowflake.com/en/solutions/industries/technology/)\nLearn * [Resource Library](https://snowflake.com/en/resources/)\nSection Title: Align Your Budgets with Your Business: A New Approach to Cost Attribution > Where Data Does More\nContent:\nLive Demos\n[Fundamentals](https://www.snowflake.com/en/fundamentals/)\n[Training](https://www.snowflake.com/en/resources/learn/training/)\n[Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n[Snowflake University](https://learn.snowflake.com/en/)\n[Developer Guides](https://www.snowflake.com/en/developers/guides)\n[Documentation](https://docs.snowflake.com/)\n[Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n[Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\nCookie Settings\n[Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n[Legal](https://www.snowflake.com/en/legal/)\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"],"full_content":"Core Platform\n\nAug 25, 2025 | 4 min read\n\n# Align Your Budgets with Your Business: A New Approach to Cost Attribution\n\nIn a dynamic organization, it’s critical to align cloud costs with specific business units, projects or cost centers. This ensures clear accountability and accurate financial planning. However, manually tracking which resources belong to which budget can be a time-consuming and error-prone process. To simplify this, we’re excited to introduce a more intuitive and powerful way to manage your budgets in Snowflake. ## Announcing tag-based budgets This new capability directly leverages the [object tags](https://docs.snowflake.com/user-guide/object-tagging/introduction) you already use across Snowflake. Object tagging has always been a powerful mechanism for classifying and organizing your resources. Now, you can extend that power directly to cost management. Instead of manually selecting a list of resources for a budget, you can simply tell a budget to monitor a specific tag — now generally available. The budget will then automatically track the combined costs of all objects that share that tag. ## See it in action: Budgeting made simple The power of this approach lies in its simplicity and how it integrates into your existing workflows. Getting started is easy. Let's say you're kicking off a new initiative, \"Project Phoenix.\" 1. **Tag your resources:** To capture a complete picture of your project's credit usage, you apply the `project = 'phoenix'` tag to all its relevant resources.\n   \n    + **Start with your main compute.** Apply the tag to dedicated resources like your **warehouses** and **compute pools** .\n    + **Leverage tag inheritance.** Next, apply the same tag to your **databases** or **schemas** . This is an important step for two reasons: It captures credit usage from the database itself (e.g., from replication), and it automatically cascades the tag to all objects within. This ensures that credit usage from serverless features — like serverless tasks, auto-clustering or materialized views — is captured with minimal effort.\n    + **Get granular when needed.** The system is designed for flexibility. A tag applied directly to a specific object will always take precedence over an inherited tag, giving you precise control over credit attribution. For example, to exclude a set of serverless tasks from your \"Project Phoenix\" budget, you can simply apply a different tag to them, like `project = 'shared'` .\n2. **Create your budget:** In the **Cost Management** interface, you create a new budget. When asked which resources to monitor, you simply select the tag `project = 'phoenix'` (which corresponds to the [ADD\\_TAG](https://docs.snowflake.com/en/sql-reference/classes/budget/methods/add_tag) API).\n That's it. Your budget is now active and tracking all the credit usage for Project Phoenix — from your dedicated warehouses to the serverless features running within your tagged databases. For broader tracking, a single budget can also monitor multiple tags. For example, you could create a single \"Q3 Campaigns\" budget to track the total cost of resources tagged with `project = 'phoenix'` or those tagged with `project = 'griffin'` .\n\nFigure 1: The budget PROJ\\_PHOENIX is configured to monitor all resources that have been assigned the project = 'phoenix' tag, either directly or through inheritance.\n\n## Key benefits for your organization * **Responsive and complete cost-tracking:** This new approach eliminates manual, end-of-month reconciliation. Budgets are refreshed automatically multiple times a day. When you change an object's tag, you can expect the cost attribution to be reflected in your budget within a matter of hours. Crucially, once updated, the system automatically backfills the resource's costs for the _entire current month_ , ensuring your view is always complete for the period.\n* **Reliable cost attribution:** This feature enables costs to be assigned to the proper business unit or project, providing a financial record that more accurately reflects your business structure as it evolves through the month.\n* **Streamlined management:** By leveraging the familiar object tagging system, you dramatically reduce administrative overhead. Managing a budget's scope is now as simple and intuitive as applying a tag.\n* **Flexible and backward-compatible:** The new tag-based method works alongside the existing option to select individual resources. This gives you the flexibility to use tags for dynamic projects while managing simpler budgets resource by resource. Your existing budgets will continue to work without any changes.\n\n###### Authors\n\nDinesh Haridas\n\nSiyoung Oh\n\n###### Share Article\n\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fengineering-blog%2Ftag-based-budgets-cost-attribution&title=Align+Your+Budgets+with+Your+Business%3A+A+New+Approach+to+Cost+Attribution)\n\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fengineering-blog%2Ftag-based-budgets-cost-attribution&text=Align+Your+Budgets+with+Your+Business%3A+A+New+Approach+to+Cost+Attribution)\n\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fengineering-blog%2Ftag-based-budgets-cost-attribution)\n\n#### Just For You\n\nData Engineering\n\n##### Supercharging DML Performance with Snowflake Gen2 Warehouses\n\nLars Volker | Noble Mushtak | Rudi Leibbrandt\n\nJul 24, 2025 | 6 min read\n\nCore Platform\n\n##### Snowflake Gen2 Warehouses: Blazing Fast Performance, Now Available on All 3 Clouds Across Expanded Regions\n\nPD Dutta\n\nJul 25, 2025 | 3 min read\n\nSubscribe to our blog newsletter\n\nGet the best, coolest and latest delivered to your inbox each week\n\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice.\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\n[start for free](https://signup.snowflake.com/)\n\nwatch a demo\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n* [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n* [Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n* [Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n* [Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n* [Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n* [Technology](https://www.snowflake.com/en/solutions/industries/technology/)\n\nLearn * [Resource Library](https://snowflake.com/en/resources/)\n* Live Demos\n* [Fundamentals](https://www.snowflake.com/en/fundamentals/)\n* [Training](https://www.snowflake.com/en/resources/learn/training/)\n* [Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n* [Snowflake University](https://learn.snowflake.com/en/)\n* [Developer Guides](https://www.snowflake.com/en/developers/guides)\n* [Documentation](https://docs.snowflake.com/)\n\n* [Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n* [Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\n* Cookie Settings\n* [Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Legal](https://www.snowflake.com/en/legal/)\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"},{"url":"https://yukidata.com/snowflake-finops-guide/","title":"Snowflake FinOps: Complete Guide to Automated Cost Optimization | Yuki","publish_date":"2025-09-19","excerpts":["Section Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization\nContent:\nBy Perry Tapiero\nSeptember 19, 2025 | 5 min read\nYour Snowflake bill increased 40% last quarter, but query performance actually got worse.\nSound familiar?\nAll FinOps organizations eventually run into this wall when they outgrow Snowflake’s basic auto-suspend and resource monitors. While Snowflake’s per-second billing offers flexibility, it also means a single efficient query can eat through hundreds of dollars – which is why manual warehouse management can’t keep pace with enterprise-sized workloads.\nThe bright side: modern Snowflake FinOps fixes this with automated systems that allow you to optimize spend and performance in real time.\nHow do you get to that point? Read on. We’ll share all of our FinOps best practices,e automation strategies, and real-world tactics that have helped enterprises cut monthly Snowflake costs by 30% or more.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > What is FinOps for Snowflake?\nContent:\nFinOps is the marriage of engineering, finance, and data teams in the cloud. When it comes to Snowflake, that means balancing performance, cost, and quality across your data platform – all while maintaining business agility\nUnlike your traditional cloud FinOps that focus on infrastructure, Snowflake FinOps requires managing:\n**Credit-based consumption:** At $2-4 per credit (depending on your region and edition) with cost scaling linearly from X-Small (1 credit/hour) to 6X-Large (512 credits/hour).\n**Query-level optimization:** Using QUERY_HISTORY and WAREHOUSE_METERING_HISTORY views to identify expensive queries before they ruin your budget.\n**Dynamic warehouse scaling:** Beyond basic auto-suspend, implementing intelligent scaling based on queue depth and query complexity\n**Storage and** [**data transfer**](https://yukidata.com/blog/snowflake-data-transfer-costs-complete-guide/) **:** Long-term storage and cross-regional replication can add significant costs if not managed with lifestyle policies and governance.\nThe big challenge here? Manual management. That same hands-on approach that worked so well for traditional infrastructure of the past breaks down when you apply the same method to millions of queries and dozens of ever-changing warehouses.\n*With Yuki, warehouse optimization is automated with a single toggle – no more manual resizing or monitoring.*\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > Why Manually Managing Snowflake For FinOps Is So Expensive\nContent:\nBefore we get into the “how to fix it” section of the article, let’s address the real cost of manual Snowflake management. These are the expenses that don’t show up on your bill, but completely ruin your ROI.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > The Engineering Time Tax\nContent:\nYour data engineers aren’t working for you to babysit warehouses. Yet many teams still find themselves spending hours each week manually resizing, suspending, and monitoring clusters. Even if that’s just 5-10 hours per engineer, that’s *thousands* of dollars lost in productivity per month – before you even add in wasted compute.\nManual management also results in issues like:\n**Overprovisioned warehouses** kept large *just in case*\n**Idle compute** burning credits overnight\n**Compliance gaps** from inconsistent chargeback and lack of audit trails\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > The Speed vs. Cost Dilemma\nContent:\nFast-growing companies always run into this growing pain: should they optimize for speed or for cost? Most choose speed, leaving them with:\n**Overprovisioned warehouses** to avoid performance bottlenecks\n**Warehouse sprawl** as teams created dedicated resources for SLAs\n**Idle compute** running 24/7 “just in case”\nThe result? Snowflake costs outgrowing revenue and eating into margin and ROI.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > The Compliance Complexity\nContent:\nEnterprise FinOps needs control, not just cost reduction. You’ll find manual approaches falling short of this:\n**Granular spend attribution** across cost centers and teams\n**Real-time budget enforcement** to prevent runaway costs\n**Audit trails** for compliance and chargeback scenarios\n**Role-based access** maintaining security while enabling autonomy\nThese challenges only continue to compound as you scale, making manual management not just expensive, but impossible to maintain.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > 6 Ways to Improve Efficiency for Your Snowflake FinOps Setup\nContent:\nThe key to an efficient (and not super expensive) Snowflake FinOps setup is to be systematic with your approach. Address immediate optimization opportunities, then move on to more long-term scalability.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > : Implement Granular Cost Attribution\nContent:\n**The problem:** Snowflake’s native cost reporting only shows warehouse-level spending. You need query- and team-level attribution for effective chargeback.\n**The solution:** Build automated tagging and attribution using Snowflake’s metadata like this:\n```\n-- Set up cost attribution table\nCREATE TABLE cost_attribution AS\nSELECT\n  qh.query_id,\n  qh.user_name,\n  qh.warehouse_name,\n  qh.database_name,\n  qh.schema_name,\n  qh.credits_used_cloud_services,\n  -- Extract team from username pattern or use session context\n  CASE\n    WHEN qh.user_name ILIKE '%analytics%' THEN 'Analytics Team'\n    WHEN qh.user_name ILIKE '%eng%' THEN 'Engineering Team'\n    ELSE 'General'\n  END as team_attribution,\n  qh.start_time\nFROM snowflake.account_usage.query_history qh\nWHERE qh.start_time >= dateadd(day, -30, current_timestamp());\n```\nYou can use this to pull key metrics like:\nCost per team per month: SUM(credits_used * $3) GROUP BY team_attribution\nMost expensive users: SUM(credits_used) GROUP BY user_name\nWarehouse efficiency: credits_used / execution_time ratio\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > : Automated Chargeback and Showback Models\nContent:\n**The problem:** No clear cost attribution means that teams often treat Snowflake as if it were “free,” leading to wasteful usage and budget overruns.\n**The solution:** Automated cost attribution that gets you accountability without expensive administration overhead:\n**Tag-based attribution** that automatically assigns costs to projects and business units\n**Usage-based chargeback** for shared warehouses using per-query\n**Predictive showback** forecasting team spend based on current trends\n**ROI tracking** connecting data platform costs to business outcomes\nManual tagging doesn’t scale. You need systems that automatically attribute costs based on your usage patterns, user roles, and business context.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > : Proactive Budget Management and Alerts\nContent:\n**The problem:** Reactive alerts document overspend after it happens. They don’t prevent it.\n**The solution:** Intelligent budget management that actually prevents overruns before they occur:\n**Predictive alerting** based on usage trends and historical patterns\n**Automated spend controls** scaling resources down when budgets risk\n**Multi-tiered notifications** that escalates teams to finance as spending approaches limits\n**Business-context budgeting** adjusting limits based on revenue cycles\nEffective budget management isn’t about saying “no”. It’s saying “yes” to the right workloads while automating the rest.\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > : Strategic Workload Investment\nContent:\n**The problem:** All workloads aren’t equal. Fraud detection needs different allocation outside of monthly reports.\n**The solution:** ROI-device resource allocation aligning your spending with real business values:\n**Workload classification** that automatically identifies business-critical vs development queries\n**Priority-based allocation** so you can be certain critical workloads always get resources first\n**Cost-per-insight analysis** which lets you measure business value per dollar spent\n**Automated lifecycle management** that lets you retire low-value processes while scaling high-impact ones\nThis step lets you move past generic optimization into business-aware optimizations you can create the perfect system for varying workloads.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > : Automated Governance and Compliance\nContent:\n**The problem:** Manual governance doesn’t scale. Maintaining control without slowing growth becomes more and more impossible as teams grow.\n**The solution:** Intelligent guardrails that maintain control *and* enable autonomy:\n**Role-based resource limits** that prevent unauthorized warehouse creation or sizing\n**Automated compliance monitoring** to keep data governance policies in place\n**Policy-driven scaling** that allows you to apply optimization rules based on workload classification\n**Audit-ready reporting** so you can get complete visibility for compliance and chargeback\nThese six approaches build out a foundation for your FinOps platform to operate automatically, letting your teams focus on innovation while maintaining growth.\n*Yuki lets you organize Snowflake costs into business domains, apply budgets, and monitor daily usage trends – making FinOps governance actionable.*\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > Snowflake FinOps Quick Wins: How to Get Started > Fix #2: Identifying Cost Drivers\nContent:\nBefore you dig into automation, you need visibility. This query helps you quickly find your top cost drivers (i.e., those users and queries that are burning through the bulk of your credits) so you can decide if you need to optimize, reclassify, or reallocate workloads.\n```\n-- Find your most expensive queries from the last 30 days\nSELECT\n  query_text,\n  warehouse_name,\n  total_elapsed_time/1000 as seconds,\n  credits_used_cloud_services,\n  (credits_used_cloud_services * 3) as estimated_cost_usd\nFROM snowflake.account_usage.query_history\nWHERE start_time >= dateadd(day, -30, current_timestamp())\nORDER BY credits_used_cloud_services DESC\nLIMIT 10;\n```\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > Snowflake FinOps Challenges to Watch Out For\nContent:\nBut before you get ahead of yourself, make sure to keep an eye out for these unique FinOps challenges.\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > AI & Machine Learning Pipeline Economics\nContent:\nML workloads have completely different requirements than your standard BI queries. You’re often left with static provisioning that either wastes money during model training downtimes or fails during intensive periods.\n**The solution:** Workload-aware optimization that automatically provisioning specialized resources for training, switching to standard compute for interference, managing your entire pipeline lifecycle without manual intervention.\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > The Future of Snowflake FinOps\nContent:\nFinOps strategies have to continue to evolve alongside Snowflake. That means always being on  your toes and ready to adapt to the next new thing.\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > The Future of Snowflake FinOps > Multi-Cloud FinOps\nContent:\nAs cloud adoption only continues to grow, FinOps must be prepared to manage:\n**Cross-cloud cost optimization** considering data gravity and egress charges\n**Vendor-agnostic optimization** that maintains efficiency regardless of the cloud provider\n**Unified governance** across different platforms and pricing models\nMulti-cloud FinOps requires you to look beyond Snowflake and see the bigger total data platform economics picture across all cloud providers.\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > The Future of Snowflake FinOps > Free cost analysis\nContent:\nTake 5 minutes to learn how much money you can save on your Snowflake account.\nBy clicking Submit you’re confirming that you agree with our Terms and Conditions.\n[### Follow us on LinkedIn](https://www.linkedin.com/company/yukidata)\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > The Future of Snowflake FinOps > Free cost analysis > Related posts\nContent:\n[](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > Related posts\nContent:\n[](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n ... \nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > Related posts > Free cost analysis\nContent:\nTake 5 minutes to learn how much money you can save on your Snowflake account.\nBy clicking Submit you’re confirming that you agree with our Terms and Conditions.\n[](https://yukidata.com)\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > Related posts > Free cost analysis > Solutions\nContent:\n[Snowflake Cost Optimization](https://yukidata.com/snowflake-cost-optimization/)\n[Snowflake Dynamic Routing](https://yukidata.com/snowflake-dynamic-routing/)\n[Forecasting](https://yukidata.com/forecasting/)\n[Snowflake dbt](https://yukidata.com/snowflake-dbt/)\n[Enterprise-Grade Platform](https://yukidata.com/enterprise-grade-platform/)\n[Stable Performance](https://yukidata.com/stable-performance/)\n[Snowflake Cost Optimization](https://yukidata.com/snowflake-cost-optimization/)\n[Snowflake Dynamic Routing](https://yukidata.com/snowflake-dynamic-routing/)\n[Forecasting](https://yukidata.com/forecasting/)\n[Snowflake dbt](https://yukidata.com/snowflake-dbt/)\n[Enterprise-Grade Platform](https://yukidata.com/enterprise-grade-platform/)\n[Stable Performance](https://yukidata.com/stable-performance/)\n[Data Application](https://yukidata.com/snowflake-data-applications/)\n[Cyber](https://yukidata.com/cyber/)\n[Fintech](https://yukidata.com/fintech/)\n[Gaming](https://yukidata.com/gaming/)\n[E-commerce](https://yukidata.com/ecommerce/)\n[Data Application](https://yukidata.com/snowflake-data-applications/)\n[Cyber](https://yukidata.com/cyber/)\n[Fintech](https://yukidata.com/fintech/)\n[Gaming](https://yukidata.com/gaming/)\n[E-commerce](https://yukidata.com/ecommerce/)\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > Related posts > Free cost analysis > Solutions > Resources\nContent:\n[Blog](https://yukidata.com/blog/)\n[Customers](https://yukidata.com/customers/)\n[Blog](https://yukidata.com/blog/)\n[Customers](https://yukidata.com/customers/)"],"full_content":"[](https://yukidata.com)\n\n* Solutions\n  \n  Close Solutions Open Solutions\n  \n  ###### By Use Case\n  \n  [Snowflake Cost Optimization](https://yukidata.com/snowflake-cost-optimization/) [Snowflake Dynamic Routing](https://yukidata.com/snowflake-dynamic-routing/) [Forecasting](https://yukidata.com/forecasting/) [Snowflake dbt](https://yukidata.com/snowflake-dbt/)\n  \n  [Enterprise-Grade Platform](https://yukidata.com/enterprise-grade-platform/) [Stable Performance](https://yukidata.com/stable-performance/)\n  \n  Security\n  \n  [Data Application](https://yukidata.com/snowflake-data-applications/) [Free Trial](https://yukidata.com/free-trial)\n  \n  ###### By Industry\n  \n  [Cyber](https://yukidata.com/cyber/) [Fintech](https://yukidata.com/fintech/) [Gaming](https://yukidata.com/gaming/) [E-commerce](https://yukidata.com/ecommerce/)\n* Resources\n  \n  Close Resources Open Resources\n  \n  ###### Resources\n  \n  [Blog](https://yukidata.com/blog/) [Customers](https://yukidata.com/customers/) [News](https://yukidata.com/news/) [Documentation](https://docs.yukidata.com/)\n  \n  [](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n  \n  ### [How to Set Up Snowflake OAuth with dbt](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n  \n  [Learn More >](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n  \n  [](https://yukidata.com/qwilt/)\n  \n  ### [How Qwilt Cut 63% of Snowflake Costs in Days](https://yukidata.com/qwilt/)\n  \n  [Learn More >](https://yukidata.com/qwilt/)\n* [About Us](https://yukidata.com/about/)\n* [Contact Us](https://yukidata.com/contact/)\n\n[Get Started](https://yukidata.com/request-demo/)\n\n[Free Trial](https://yukidata.com/free-trial)\n\n[](https://yukidata.com)\n\n* Solutions\n  \n  Close Solutions Open Solutions\n  \n  [Free Trial](https://yukidata.com/free-trial) [Snowflake Cost Optimization](https://yukidata.com/snowflake-cost-optimization/) [Snowflake Dynamic Routing](https://yukidata.com/snowflake-dynamic-routing/) [Forecasting](https://yukidata.com/forecasting/) [Snowflake dbt](https://yukidata.com/snowflake-dbt/) [Enterprise-Grade Platform](https://yukidata.com/enterprise-grade-platform/) [Stable Performance](https://yukidata.com/stable-performance/)\n  \n  Security\n  \n  [Data Application](https://yukidata.com/snowflake-data-applications/) [Cyber](https://yukidata.com/cyber/) [Fintech](https://yukidata.com/fintech/) [Gaming](https://yukidata.com/gaming/) [E-commerce](https://yukidata.com/ecommerce/)\n* Resources\n  \n  Close Resources Open Resources\n  \n  [Blog](https://yukidata.com/blog/) [Customers](https://yukidata.com/customers/) [News](https://yukidata.com/news/) [Documentation](https://yukidata.com/customers/)\n* [About Us](https://yukidata.com/about/)\n* [Contact Us](https://yukidata.com/contact/)\n\n# Snowflake FinOps: Complete Guide to Automated Cost Optimization\n\nBy Perry Tapiero\n\nSeptember 19, 2025 | 5 min read\n\nYour Snowflake bill increased 40% last quarter, but query performance actually got worse.\n\nSound familiar?\n\nAll FinOps organizations eventually run into this wall when they outgrow Snowflake’s basic auto-suspend and resource monitors. While Snowflake’s per-second billing offers flexibility, it also means a single efficient query can eat through hundreds of dollars – which is why manual warehouse management can’t keep pace with enterprise-sized workloads.\n\nThe bright side: modern Snowflake FinOps fixes this with automated systems that allow you to optimize spend and performance in real time.\n\nHow do you get to that point? Read on. We’ll share all of our FinOps best practices,e automation strategies, and real-world tactics that have helped enterprises cut monthly Snowflake costs by 30% or more.\n\n## What is FinOps for Snowflake?\n\nFinOps is the marriage of engineering, finance, and data teams in the cloud. When it comes to Snowflake, that means balancing performance, cost, and quality across your data platform – all while maintaining business agility\n\nUnlike your traditional cloud FinOps that focus on infrastructure, Snowflake FinOps requires managing:\n\n* **Credit-based consumption:** At $2-4 per credit (depending on your region and edition) with cost scaling linearly from X-Small (1 credit/hour) to 6X-Large (512 credits/hour).\n* **Query-level optimization:** Using QUERY\\_HISTORY and WAREHOUSE\\_METERING\\_HISTORY views to identify expensive queries before they ruin your budget.\n* **Dynamic warehouse scaling:** Beyond basic auto-suspend, implementing intelligent scaling based on queue depth and query complexity\n* **Storage and** [**data transfer**](https://yukidata.com/blog/snowflake-data-transfer-costs-complete-guide/) **:** Long-term storage and cross-regional replication can add significant costs if not managed with lifestyle policies and governance.\n\nThe big challenge here? Manual management. That same hands-on approach that worked so well for traditional infrastructure of the past breaks down when you apply the same method to millions of queries and dozens of ever-changing warehouses.\n\n_With Yuki, warehouse optimization is automated with a single toggle – no more manual resizing or monitoring._\n\n## Why Manually Managing Snowflake For FinOps Is So Expensive\n\nBefore we get into the “how to fix it” section of the article, let’s address the real cost of manual Snowflake management. These are the expenses that don’t show up on your bill, but completely ruin your ROI.\n\n### The Engineering Time Tax\n\nYour data engineers aren’t working for you to babysit warehouses. Yet many teams still find themselves spending hours each week manually resizing, suspending, and monitoring clusters. Even if that’s just 5-10 hours per engineer, that’s _thousands_ of dollars lost in productivity per month – before you even add in wasted compute.\n\nManual management also results in issues like:\n\n* **Overprovisioned warehouses** kept large _just in case_\n* **Idle compute** burning credits overnight\n* **Compliance gaps** from inconsistent chargeback and lack of audit trails\n\n### The Speed vs. Cost Dilemma\n\nFast-growing companies always run into this growing pain: should they optimize for speed or for cost? Most choose speed, leaving them with:\n\n* **Overprovisioned warehouses** to avoid performance bottlenecks\n* **Warehouse sprawl** as teams created dedicated resources for SLAs\n* **Idle compute** running 24/7 “just in case”\n\nThe result? Snowflake costs outgrowing revenue and eating into margin and ROI.\n\n### The Compliance Complexity\n\nEnterprise FinOps needs control, not just cost reduction. You’ll find manual approaches falling short of this:\n\n* **Granular spend attribution** across cost centers and teams\n* **Real-time budget enforcement** to prevent runaway costs\n* **Audit trails** for compliance and chargeback scenarios\n* **Role-based access** maintaining security while enabling autonomy\n\nThese challenges only continue to compound as you scale, making manual management not just expensive, but impossible to maintain.\n\n## 6 Ways to Improve Efficiency for Your Snowflake FinOps Setup\n\nThe key to an efficient (and not super expensive) Snowflake FinOps setup is to be systematic with your approach. Address immediate optimization opportunities, then move on to more long-term scalability.\n\n### \\: Implement Granular Cost Attribution\n\n**The problem:** Snowflake’s native cost reporting only shows warehouse-level spending. You need query- and team-level attribution for effective chargeback.\n\n**The solution:** Build automated tagging and attribution using Snowflake’s metadata like this:\n\n```\n-- Set up cost attribution table\nCREATE TABLE cost_attribution AS\nSELECT\n  qh.query_id,\n  qh.user_name,\n  qh.warehouse_name,\n  qh.database_name,\n  qh.schema_name,\n  qh.credits_used_cloud_services,\n  -- Extract team from username pattern or use session context\n  CASE\n    WHEN qh.user_name ILIKE '%analytics%' THEN 'Analytics Team'\n    WHEN qh.user_name ILIKE '%eng%' THEN 'Engineering Team'\n    ELSE 'General'\n  END as team_attribution,\n  qh.start_time\nFROM snowflake.account_usage.query_history qh\nWHERE qh.start_time >= dateadd(day, -30, current_timestamp());\n```\n\nYou can use this to pull key metrics like:\n\n* Cost per team per month: SUM(credits\\_used \\* $3) GROUP BY team\\_attribution\n* Most expensive users: SUM(credits\\_used) GROUP BY user\\_name\n* Warehouse efficiency: credits\\_used / execution\\_time ratio\n\n### \\: Automated Chargeback and Showback Models\n\n**The problem:** No clear cost attribution means that teams often treat Snowflake as if it were “free,” leading to wasteful usage and budget overruns.\n\n**The solution:** Automated cost attribution that gets you accountability without expensive administration overhead:\n\n* **Tag-based attribution** that automatically assigns costs to projects and business units\n* **Usage-based chargeback** for shared warehouses using per-query\n* **Predictive showback** forecasting team spend based on current trends\n* **ROI tracking** connecting data platform costs to business outcomes\n\nManual tagging doesn’t scale. You need systems that automatically attribute costs based on your usage patterns, user roles, and business context.\n\n### \\: Proactive Budget Management and Alerts\n\n**The problem:** Reactive alerts document overspend after it happens. They don’t prevent it.\n\n**The solution:** Intelligent budget management that actually prevents overruns before they occur:\n\n* **Predictive alerting** based on usage trends and historical patterns\n* **Automated spend controls** scaling resources down when budgets risk\n* **Multi-tiered notifications** that escalates teams to finance as spending approaches limits\n* **Business-context budgeting** adjusting limits based on revenue cycles\n\nEffective budget management isn’t about saying “no”. It’s saying “yes” to the right workloads while automating the rest.\n\n### \\: Automated Warehouse Scaling\n\n**The problem:** Snowflake’s auto-suspend helps with idle time, but it doesn’t actually optimize warehouse size when it comes to workload complexity.\n\n**The solution:** Implement workload-aware scaling. For example, use Snowflake’s WAREHOUSE\\_LOAD\\_HISTORY to track query depth and concurrency, then programmatically adjust sizes via Snowflake Python Connector or Snowpark API:\n\n```\nimport snowflake.connector\n\nconn = snowflake.connector.connect(\n    user='USER',\n    password='PASSWORD',\n    account='ACCOUNT'\n)\n\ncur = conn.cursor()\ncur.execute(\"\"\"\nSELECT AVG(avg_queued_load)\nFROM snowflake.account_usage.warehouse_load_history\nWHERE warehouse_name='COMPUTE_WH'\nAND start_time >= dateadd(minute, -5, current_timestamp());\n\"\"\")\n\nqueue_depth = cur.fetchone()[0]\n\nif queue_depth > 50:\n    cur.execute(\"ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = 'LARGE'\")\nelif queue_depth < 10:\n    cur.execute(\"ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = 'SMALL'\n```\n\nThis approach can help you [reduce warehouse costs](https://yukidata.com/blog/snowflake-warehouse-optimization-guide/) by 20-40% because it lets you match your compute size to actual workload requirements.\n\n### \\: Strategic Workload Investment\n\n**The problem:** All workloads aren’t equal. Fraud detection needs different allocation outside of monthly reports.\n\n**The solution:** ROI-device resource allocation aligning your spending with real business values:\n\n* **Workload classification** that automatically identifies business-critical vs development queries\n* **Priority-based allocation** so you can be certain critical workloads always get resources first\n* **Cost-per-insight analysis** which lets you measure business value per dollar spent\n* **Automated lifecycle management** that lets you retire low-value processes while scaling high-impact ones\n\nThis step lets you move past generic optimization into business-aware optimizations you can create the perfect system for varying workloads.\n\n### \\: Automated Governance and Compliance\n\n**The problem:** Manual governance doesn’t scale. Maintaining control without slowing growth becomes more and more impossible as teams grow.\n\n**The solution:** Intelligent guardrails that maintain control _and_ enable autonomy:\n\n* **Role-based resource limits** that prevent unauthorized warehouse creation or sizing\n* **Automated compliance monitoring** to keep data governance policies in place\n* **Policy-driven scaling** that allows you to apply optimization rules based on workload classification\n* **Audit-ready reporting** so you can get complete visibility for compliance and chargeback\n\nThese six approaches build out a foundation for your FinOps platform to operate automatically, letting your teams focus on innovation while maintaining growth.\n\n_Yuki lets you organize Snowflake costs into business domains, apply budgets, and monitor daily usage trends – making FinOps governance actionable._\n\n## Snowflake FinOps Quick Wins: How to Get Started\n\nIf you’re scratching your head wondering how you’re going to implement any one of those six steps above, don’t be discouraged. Automation can seem overwhelming. Before you dive in, make sure you’ve already implemented these three easy quick fixes:\n\n### Fix #1: Enabling Basic Controls\n\nBefore you get into automation, this puts guardrails in place. Resource monitors mean you can cap resources before budgets spiral.\n\nHere’s an example of what this could look like to prevent runaway spending by suspending warehouses when you reach a certain quota:\n\n```\n-- Create a resource monitor for your main warehouse\nCREATE RESOURCE MONITOR main_warehouse_monitor\nWITH CREDIT_QUOTA = 1000\n  TRIGGERS\n    ON 75 PERCENT DO NOTIFY\n    ON 100 PERCENT DO SUSPEND_IMMEDIATE;\n\n-- Apply to your warehouse\nALTER WAREHOUSE COMPUTE_WH SET RESOURCE_MONITOR = main_warehouse_monitor;\n```\n\n### Fix #2: Identifying Cost Drivers\n\nBefore you dig into automation, you need visibility. This query helps you quickly find your top cost drivers (i.e., those users and queries that are burning through the bulk of your credits) so you can decide if you need to optimize, reclassify, or reallocate workloads.\n\n```\n-- Find your most expensive queries from the last 30 days\nSELECT\n  query_text,\n  warehouse_name,\n  total_elapsed_time/1000 as seconds,\n  credits_used_cloud_services,\n  (credits_used_cloud_services * 3) as estimated_cost_usd\nFROM snowflake.account_usage.query_history\nWHERE start_time >= dateadd(day, -30, current_timestamp())\nORDER BY credits_used_cloud_services DESC\nLIMIT 10;\n```\n\n### Fix #3: Optimizing Warehouse Sizing\n\nThis lets you align your warehouse size with actual usage so you can avoid the trap of running a Large warehouse for a workload that only needs a Small.\n\n```\nSELECT\n  warehouse_name,\n  avg(avg_running) as avg_concurrent_queries,\n  avg(avg_queued_load) as avg_queue_depth\nFROM snowflake.account_usage.warehouse_load_history\nWHERE start_time >= dateadd(day, -7, current_timestamp())\nGROUP BY warehouse_name;\n```\n\nIf your avg\\_queue\\_depth > 100: Scale up your warehouse\n\nIf avg\\_concurrent\\_queries < 1: Consider smaller warehouses or using a longer auto-suspend\n\n## Snowflake FinOps Challenges to Watch Out For\n\nBut before you get ahead of yourself, make sure to keep an eye out for these unique FinOps challenges.\n\n### Maintaining Fraud Detection & Risk Scoring at Scale\n\nFinancial service teams need millisecond response times for fraud detection. Downtime is simply not an option. Status provisioning either wastes compute or fails under spikes.\n\n**The solution:** Invest in a third-party tool that detects anomalies in query volume and dynamically scales compute to meet SLAs, then scales back when that load reduces.\n\n### Multi-Region Compliance & Data Residency\n\nGlobal enterprises have to maintain data residency requirements while continuing to optimize costs across different regions. Doing this manually means having to carefully balance regulatory compliance and [cost optimization](https://yukidata.com/blog/snowflake-optimization-guide/) across multiple geographic regions and regulatory frameworks.\n\n**The solution:** Use automated, region-aware optimization. This lets you maintain compliance while minimizing cross-region data movement costs. Think intelligent query routing that processes all you need within boundaries, optimizing for cost and performance and getting you the best of both worlds.\n\n### AI & Machine Learning Pipeline Economics\n\nML workloads have completely different requirements than your standard BI queries. You’re often left with static provisioning that either wastes money during model training downtimes or fails during intensive periods.\n\n**The solution:** Workload-aware optimization that automatically provisioning specialized resources for training, switching to standard compute for interference, managing your entire pipeline lifecycle without manual intervention.\n\n### Seasonal Business Variations\n\nRetail companies see massive increases in queries during the holidays. Financial services spike at month-end and quarter-end. Traditional static provisioning wastes money during quiet periods, or fails during peaks.\n\n**The solution:** Predictive scaling based on business calendar patterns and historical data. Systems can then automatically pre-scale before high-demand periods and then gradually decrease as demand subsides.\n\n[how do these solutions _actually_ work though?? What’s the code or actual solution? These are ideas, not actual tips]\n\n## The Future of Snowflake FinOps\n\nFinOps strategies have to continue to evolve alongside Snowflake. That means always being on  your toes and ready to adapt to the next new thing.\n\n### AI-Native Cost Optimization\n\nIt shouldn’t surprise you to see AI on this list. Next-generation systems will be using machine learning for everything. Not just analysis, but optimization, too:\n\n* **Predictive configurations** for new workloads based on historical patterns\n* [**Automated query tuning**](https://yukidata.com/blog/snowflake-query-optimization/) using reinforcement learning\n* **Intelligent data placement** that minimizes storage and compute costs at the same time\n\nAI-native approaches take you another step further from reactive optimization to [predictive cost management](https://yukidata.com/blog/snowflake-cost-optimization-guide/) . It means you can prevent inefficiencies before they even occur.\n\n### Multi-Cloud FinOps\n\nAs cloud adoption only continues to grow, FinOps must be prepared to manage:\n\n* **Cross-cloud cost optimization** considering data gravity and egress charges\n* **Vendor-agnostic optimization** that maintains efficiency regardless of the cloud provider\n* **Unified governance** across different platforms and pricing models\n\nMulti-cloud FinOps requires you to look beyond Snowflake and see the bigger total data platform economics picture across all cloud providers.\n\n### Automation On a Whole New Level\n\nManual FinOps worked when you had five warehouses and 100 daily queries. At an enterprise scale, it collapses under the weight of hundreds of queries, teams, and compliance requirements.\n\nThe organizations succeeding with Snowflake FinOps aren’t tracking costs in spreadsheets – they’re automating everything.\n\nThat’s where Yuki comes in. Our platform continuously analyzes your Snowflake usage, optimizes workloads in real time, and delivers up to 30% cost reduction in the first month. And it does all that without sacrificing performance?\n\nReady to see what automated FinOps looks like? [Book your personal demo with Yuki today.](https://yukidata.com/request-demo/)\n\nBy Perry Tapiero\n\nPerry Tapiero leads marketing at Yuki, driving demand generation and brand growth for B2B and B2C SaaS companies in FinTech, AdTech, and Cybersecurity. With 15+ years of experience, he specializes in go-to-market strategies, ICP refinement, and managing multi-million-dollar campaigns using HubSpot and Salesforce. Previously at other companies, he led ABM, PBM, and product marketing initiatives that drove ARR growth and helped achieve Gartner Magic Quadrant recognition. Perry was a regular contributor for marketers and now shares his insights on LinkedIn.\n\n#### Table of Contents\n\n### Free cost analysis\n\nTake 5 minutes to learn how much money you can save on your Snowflake account.\n\nBy clicking Submit you’re confirming that you agree with our Terms and Conditions.\n\n[### Follow us on LinkedIn](https://www.linkedin.com/company/yukidata)\n\n#### Related posts\n\n[](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n\n##### [How to Set Up Snowflake OAuth with dbt](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n\n[Learn More >](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n\nFebruary 7, 2026\n\n[](https://yukidata.com/nowflake-cybersecurity-guide/)\n\n##### [Snowflake Cybersecurity: What Changed After 2024 & How to Protect Your Data](https://yukidata.com/nowflake-cybersecurity-guide/)\n\n[Learn More >](https://yukidata.com/nowflake-cybersecurity-guide/)\n\nFebruary 5, 2026\n\n[](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\n\n##### [How to Fix Snowflake Error 251005: “User is Empty” (And Why It Keeps Happening)](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\n\n[Learn More >](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\n\nFebruary 2, 2026\n\n## Related posts\n\n[](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n\n##### [How to Set Up Snowflake OAuth with dbt](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n\n[Learn More >](https://yukidata.com/dbt-cloud-snowflake-oauth/)\n\nFebruary 7, 2026\n\n[](https://yukidata.com/nowflake-cybersecurity-guide/)\n\n##### [Snowflake Cybersecurity: What Changed After 2024 & How to Protect Your Data](https://yukidata.com/nowflake-cybersecurity-guide/)\n\n[Learn More >](https://yukidata.com/nowflake-cybersecurity-guide/)\n\nFebruary 5, 2026\n\n[](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\n\n##### [How to Fix Snowflake Error 251005: “User is Empty” (And Why It Keeps Happening)](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\n\n[Learn More >](https://yukidata.com/snowflake-connector-errors-251005-user-is-empty/)\n\nFebruary 2, 2026\n\n[Back to Blog](https://yukidata.com/blog/)\n\n### Free cost analysis\n\nTake 5 minutes to learn how much money you can save on your Snowflake account.\n\nBy clicking Submit you’re confirming that you agree with our Terms and Conditions.\n\n[](https://yukidata.com)\n\n###### Solutions\n\n* [Snowflake Cost Optimization](https://yukidata.com/snowflake-cost-optimization/)\n* [Snowflake Dynamic Routing](https://yukidata.com/snowflake-dynamic-routing/)\n* [Forecasting](https://yukidata.com/forecasting/)\n* [Snowflake dbt](https://yukidata.com/snowflake-dbt/)\n* [Enterprise-Grade Platform](https://yukidata.com/enterprise-grade-platform/)\n* [Stable Performance](https://yukidata.com/stable-performance/)\n\n* [Snowflake Cost Optimization](https://yukidata.com/snowflake-cost-optimization/)\n* [Snowflake Dynamic Routing](https://yukidata.com/snowflake-dynamic-routing/)\n* [Forecasting](https://yukidata.com/forecasting/)\n* [Snowflake dbt](https://yukidata.com/snowflake-dbt/)\n* [Enterprise-Grade Platform](https://yukidata.com/enterprise-grade-platform/)\n* [Stable Performance](https://yukidata.com/stable-performance/)\n\n* [Data Application](https://yukidata.com/snowflake-data-applications/)\n* [Cyber](https://yukidata.com/cyber/)\n* [Fintech](https://yukidata.com/fintech/)\n* [Gaming](https://yukidata.com/gaming/)\n* [E-commerce](https://yukidata.com/ecommerce/)\n\n* [Data Application](https://yukidata.com/snowflake-data-applications/)\n* [Cyber](https://yukidata.com/cyber/)\n* [Fintech](https://yukidata.com/fintech/)\n* [Gaming](https://yukidata.com/gaming/)\n* [E-commerce](https://yukidata.com/ecommerce/)\n\n###### Resources\n\n* [Blog](https://yukidata.com/blog/)\n* [Customers](https://yukidata.com/customers/)\n\n* [Blog](https://yukidata.com/blog/)\n* [Customers](https://yukidata.com/customers/)\n\n###### Company\n\n* [About](https://yukidata.com/about/)\n* [Contact Us](https://yukidata.com/contact/)\n\n* [About](https://yukidata.com/about/)\n* [Contact Us](https://yukidata.com/contact/)\n\n[Facebook](https://www.facebook.com/share/15ETfFU352/?mibextid=wwXIfr) [X-twitter](https://x.com/Yuki70376254) [Linkedin](https://www.linkedin.com/company/yukidata)\n\nAvailable in marketplace\n\n[](https://aws.amazon.com/marketplace/pp/prodview-uuqxoox7z5gra)\n\n[](https://app.snowflake.com/marketplace/listings/Yuki)\n\nDesigned with fun by Buzzhunter\n\nSkip to content\n\nOpen toolbar\n\nAccessibility Tools\n\n* Increase Text\n* Decrease Text\n* Grayscale\n* High Contrast\n* Negative Contrast\n* Light Background\n* Links Underline\n* Readable Font\n* Reset"},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization","publish_date":null,"excerpts":["Section Title: Cost Optimization > Recommendations\nContent:\n**Establish a consistent and granular cost attribution strategy:** Implement robust and organizationally consistent tagging and\nlabeling strategies across all resources (storage objects,\nwarehouses, accounts, queries) to accurately allocate costs to\nspecific teams, products, or initiatives. **Embed cost accountability into your organization's DNA:** Implement a feedback system to transparently show or charge back\ncloud costs to relevant business units or teams, increasing\naccountability. **Deliver clear, historical consumption insights:** Utilize\nconsistent in-tool visualizations or custom dashboards to monitor\nconsumption and contextualize spend on the platform with unit\neconomics. **Investigate anomalous consumption activity:** Review anomaly\ndetection to identify unforeseen cost anomalies and investigate\ncause and effect trends. **Control**\n**Proactively monitor all platform usage:** Define and enforce\nbudgets for projects and services, setting soft quotas to limit\nresource consumption and prevent runaway spending. **Forecast consumption based on business needs:** Establish a\nforecast process to project future spend needs based on business and\ntechnical needs. **Enforce cost guardrails for organizational resources:** Set up\nautomated checks (e.g., Tasks, query insights) and resource\nguardrails (e.g., warehouse timeout, storage policies, resource\nmonitors) to identify unusual usage patterns and potential\noverspending as they occur.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nImplementing robust and organizationally consistent tagging and labeling\nstrategies across all resources (e.g. storage objects, warehouses,\naccounts, queries) is crucial to accurately allocate costs to specific\nteams, products, or initiatives and linking actions to outcomes.\n**Tagging in Snowflake**\nTagging can be done at several levels:\nSection Title: Cost Optimization > Visibility > Overview > Establish a consistent and granular cost attribution strategy\nContent:\n**Snowflake object tagging:** Snowflake allows you to apply [object-level tags](https://docs.snowflake.com/en/user-guide/object-tagging/introduction) (key-value pairs) to accounts, warehouses, databases, schemas, users,\ntables, and more. These tags are fundamental for apportioning costs\nacross departments, environments (dev, test, prod), projects, or lines\nof business. Tags can also support [inheritance](https://docs.snowflake.com/en/user-guide/object-tagging/inheritance) and [propagation](https://docs.snowflake.com/en/user-guide/object-tagging/propagation) ,\nsimplifying tagging across dependent objects. For example, instead of\ntagging each individual table underneath a schema, tagging the schema\nwill cause all tables to inherit the tag of the schema. This\nsignificantly reduces the manual effort required for tagging and\nensures that new objects created within a tagged schema or propagated\nworkflow automatically inherit the correct cost attribution. Snowflake\nstrongly recommends tags for warehouses, databases, tables, and users\nto enable granular cost breakdowns. You can use the [TAG_REFERENCES view](https://docs.snowflake.com/sql-reference/account-usage/tag_references) in SNOWFLAKE.ACCOUNT_USAGE to combine with common usage views like\nWAREHOUSE_METERING_HISTORY and TABLE_STORAGE_METRICS to allocate usage\nto relevant business groups.\nSection Title: Cost Optimization > Visibility > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nObject Tags are best utilized when\nSnowflake objects are not shared across cost owners. **Query tags for granular workload attribution:** [Query tags](https://docs.snowflake.com/en/user-guide/cost-attributing) can be set via session parameters (e.g., ALTER SESSION SET QUERY_TAG =\n'your_tag';) or directly within SQL clients or ETL tools. This\nassociates individual queries with specific departments, projects, or\napplications, even when using shared warehouses. This is extremely\nvaluable for shared warehouses where multiple teams or applications\nuse the same compute resource, allowing for granular showback. It is\nalso easy to programmatically make changes to query tags within\nscripts or processes to allocate costs appropriately. Query tags can\nbe found in the QUERY_HISTORY view of the SNOWFLAKE.ACCOUNT_USAGE\nschema.\nSection Title: Cost Optimization > Visibility > Overview > Establish a consistent and granular cost attribution strategy\nContent:\n**Tagging models**\nIn the initial setup of a business unit or use case, it is important to\nconsider the [model for tagging](https://docs.snowflake.com/en/user-guide/cost-attributing) costs within the platform via shared or dedicated resources. These fall\ninto three large buckets:\n**Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate warehouses with a\ndepartment. You can use object tags to attribute the costs incurred by\nthose warehouses to that department entirely.\n**Resources shared by users from multiple departments:** An example of\nthis is a warehouse shared by users from different departments. In\nthis case, you use object tags to associate each user with a\ndepartment. The costs of queries are attributed to the users. Using\nthe object tags assigned to users, you can break down the costs by\ndepartment.\n**Applications or workflows shared by users from different departments:** An example of this is an application that issues\nqueries on behalf of its users. In this case, each query executed by\nthe application is [assigned a query tag](https://docs.snowflake.com/en/sql-reference/sql/alter-session) that identifies the team or cost center of the user for whom the query\nis being made.\nSection Title: Cost Optimization > Visibility > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nEach model has its pros and cons, including how to handle concepts such\nas idle time or whether to show/charge back attributed or billed\ncredits. Review each model before deploying resources. If an\norganization is caught between models, a common approach is to start in\na shared resource environment and graduate to dedicated resources as the\nworkload increases.\n**Tag enforcement**\nClear and consistent naming conventions for accounts, warehouses,\ndatabases, schemas, and tables facilitate immediate cost understanding.\nEnforcing robust tagging policies (e.g., requiring specific tags for new\nresource creation and using automated scripts to identify untagged\nresources) is crucial for accurate data interpretation and effective\ncost management. Without tag enforcement, it is difficult to accurately\nallocate all costs and can require manual effort, like extensive\ntag-mapping tables. Tag values are enforced within an account, but if a\nmulti-account strategy is needed for your organization, a tag [database can be replicated](https://docs.snowflake.com/en/user-guide/cost-attributing) and leveraged across all accounts to ensure consistent values are used.\nFor best-in-class visibility, it is recommended to have a tagging\nstrategy and tag all resources in an organization to allocate costs to\nrelevant owners.\nSection Title: Cost Optimization > Visibility > Overview > Embed cost accountability into your organization's DNA\nContent:\nTo effectively manage Snowflake spend and align business structure to\ntechnical resources, you should implement a system of showback or\nchargeback. This approach is crucial for promoting accountability and\noptimizing resource usage as there is a single owner for each object\nwithin the platform.\n**Showback**\nIf cost accountability models have not been implemented previously,\nconsider a showback model. This involves transparently reporting\nSnowflake costs to different departments or projects to raise awareness\nof their costs. By showing each team their monthly consumption (broken\ndown by warehouse usage, query costs, and storage, etc.), it encourages\na cost-conscious culture. This initial step helps teams understand the\nfinancial impact of their actions without the immediate pressure of\nbudget cuts. Tools like Snowflake's built-in [Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) & [budget](https://docs.snowflake.com/en/user-guide/budgets) views, third-party cost management platforms, or custom dashboards can\nbe used to provide these reports.\n**Chargeback**\nSection Title: Cost Optimization > Visibility > Overview > Embed cost accountability into your organization's DNA\nContent:\nFor more financially mature organizations, a chargeback model can be\nvery effective for managing costs. This system directly bills\ndepartments for their Snowflake usage. This creates a powerful financial\nincentive for teams to optimize their workloads. To make this transition\nsmooth and fair, you need to define clear rules for cost allocation. By\nimplementing chargeback, you turn each department into a financial\nstakeholder, encouraging them to right-size their warehouses, suspend\nthem during idle periods, and write more efficient queries. This shift\nin accountability leads to a more disciplined and cost-effective use of\nyour Snowflake environment.\nIn either case, having a centralized dashboard or visual for all\norganizations to review intra-period is critical for financial\naccountability and next-step actions.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Normalize consumption with unit economic metrics**\nFor organizations to achieve comprehensive financial visibility, it is\nrecommended best practice to move beyond tracking aggregate spend and\nimplement Unit Economics Metrics. Unit economics provides a powerful\nmethodology for normalizing cloud consumption by tying platform costs to\nspecific business or operational drivers. This per-unit approach helps\nyou understand cost efficiency, measure the ROI of your initiatives, and\nmake data-driven decisions about resource allocation and optimization.\nBy translating abstract credit consumption into tangible metrics, you\ncan empower technical and business teams with a shared language for\ndiscussing value and cost. These metrics are commonly tracked across\ntime to show changes in efficiency or business impact.\n**Efficiency metrics (technical KPIs)**\nEfficiency Metrics are technical Key Performance Indicators (KPIs) that\nconnect cloud costs directly to platform operations and workloads. They\nare crucial for engineering teams and platform owners to identify\ninefficiencies, optimize resource usage, and understand the cost drivers\nof the data platform itself. These metrics provide the granular,\noperational view needed to manage the platform's performance day-to-day.\nSome common examples include:\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Cost per customer:** Attributes a portion of the total platform cost\nto each of the company's end customers. This is a powerful metric for\nunderstanding profitability and cost-to-serve at a customer level.\n**Cost per project:** Allocates cloud costs to specific internal\nprojects, products, or initiatives. This enables accurate\nproject-based accounting and helps assess the financial viability of\nnew features or services.\n**\"Bring Your Own Metric\":** Custom define unit economic metrics that\nare unique to your organization's business model. Examples could\ninclude Cost per Transaction, Cost per Shipment, or Cost per Ad\nImpression. Creating these tailored metrics ensures the most accurate\nalignment between cloud spend and core business value.\nIf Snowflake is in the value chain for orders, the cost per order can be a good metric to tie Snowflake consumption to Business Demand Drivers.\n**Visualize metrics with Snowsight tools and external BI tools**\n ... \nSection Title: Cost Optimization > Control > Overview > Proactively monitor all platform usage\nContent:\nTo effectively manage and [control Snowflake spend](https://docs.snowflake.com/en/user-guide/cost-controlling) ,\nit is essential to establish and enforce cost guardrails. Implementing a [budgeting system](https://docs.snowflake.com/en/user-guide/budgets) is a key\nFinOps practice that promotes cost accountability and optimizes resource\nusage by providing teams with visibility into their consumption and the\nability to set alerts and automated actions. Budgeting helps to prevent\nunexpected cost overruns and encourages a cost-conscious culture.\n**Set budgets permissions**\nTo establish effective budgets, it's crucial to define [roles and privileges](https://docs.snowflake.com/en/user-guide/budgets) by configuring the role, team, or user responsible for the resources.\nThis ensures that budget tracking aligns with specific business units or\nprojects, enabling accurate cost attribution and accountability. By\nlinking consumption to the relevant stakeholders, you can create a clear\nshowback or chargeback model, which is vital for fostering a sense of\nownership over spending. This configuration should be part of a broader,\nconsistent tagging strategy to ensure all costs are properly allocated\nto departments, environments, or projects.\n**Create budget categories**\n ... \nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\nForecasting Snowflake consumption should be a strategic business\nfunction, not a mere technical prediction. The goal is to establish a\ntransparent basis for budgeting and optimizing ROI by linking\nconsumption directly to measurable business outcomes. In a dynamic,\nusage-based environment where compute costs are the most volatile\nelement of the bill, a robust framework must integrate quantitative\nanalysis of historical usage with qualitative insights into future\nbusiness drivers. The following framework outlines how to build and\nmaintain a comprehensive consumption forecast.\n**Establish the Baseline**\nThis phase focuses on understanding the source of spend and establishing\ngranular cost accountability.\n**Identify demand drivers and unit economics:** To understand what\ndrives Snowflake spend, correlate historical credit, storage, and data\ntransfer usage with key business metrics like cost per customer or per\ntransaction. Use Snowflake's [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) schema, including the WAREHOUSE_METERING_HISTORY and QUERY_HISTORY\nviews, as the primary data sources for this analysis.\n**Granular cost attribution:** Accurately tie costs back to business\nteams or workloads by implementing a mandatory tagging strategy for\nall warehouses and queries. Align these tags with your organization's\nfinancial structure to provide clear cost segmentation."],"full_content":"Data for Breakfast Around the World\n\nDrive impact across your organization with data and agentic intelligence.\n\n[register now](https://www.snowflake.com/en/data-for-breakfast/)\n\n[Snowflake for Developers](https://www.snowflake.com/content/snowflake-site/global/en/developers) [Guides](https://www.snowflake.com/content/snowflake-site/global/en/developers/guides) Cost Optimization\n\n## Cost Optimization\n\nWell Architected Framework Team\n\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\n\n## Overview\n\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\n\n## Principles\n\n#### Business impact: Align cost with business value\n\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n\n#### Visibility: Understand & contextualize your consumption footprint\n\nYou can neither control nor optimize what you can't see. Gain deep and granular insight into all aspects of your cloud spending, fostering transparency, and attributing costs effectively.\n\n#### Control: Establish guardrails and governance\n\nEstablish policies and mechanisms to govern resource provisioning and consumption, preventing unnecessary costs and enforcing financial boundaries.\n\n#### Optimize: Maximize efficiency and value\n\nContinuously improve the efficiency of your resources and workloads to maximize the value derived from your platform investment.\n\n## Recommendations\n\nThe following key recommendations are covered within each principle of\nCost Optimization:\n\n* **Business Impact**\n  \n    + **Consider cost as a design constraint:** Integrate cost\n        considerations into the architecture and design process from the\n        very beginning, making it a key non-functional requirement alongside\n        performance, security, and reliability.\n    + **Quantify value:** Develop metrics to quantify the business value\n        delivered by cloud resources (e.g., revenue per Snowflake Credit,\n        cost per customer, efficiency gains).\n    + **Trade-off analysis:** Understand the inherent trade-offs between\n        cost, performance, reliability, and security, and make informed\n        decisions that align with business priorities.\n    + **Measure business value KPIs baseline:** Once metrics to quantify\n        business value are identified and trade-offs between cost,\n        performance, and reliability are established, you need to document a\n        “baseline measurement” in order to track progress again.\n        Furthermore, you should establish a regular cadence for refreshing\n        this measurement to ensure value realization is in line with\n        expectations and business goals.\n* **Visibility**\n  \n    + **Understand Snowflake’s resource billing models:** Review\n        Snowflake’s billing models to align technical and non-technical\n        resources on financial drivers and consumption terminology.\n    + **Establish a consistent and granular cost attribution strategy:** Implement robust and organizationally consistent tagging and\n        labeling strategies across all resources (storage objects,\n        warehouses, accounts, queries) to accurately allocate costs to\n        specific teams, products, or initiatives.\n    + **Embed cost accountability into your organization's DNA:** Implement a feedback system to transparently show or charge back\n        cloud costs to relevant business units or teams, increasing\n        accountability.\n    + **Deliver clear, historical consumption insights:** Utilize\n        consistent in-tool visualizations or custom dashboards to monitor\n        consumption and contextualize spend on the platform with unit\n        economics.\n    + **Investigate anomalous consumption activity:** Review anomaly\n        detection to identify unforeseen cost anomalies and investigate\n        cause and effect trends.\n* **Control**\n  \n    + **Proactively monitor all platform usage:** Define and enforce\n        budgets for projects and services, setting soft quotas to limit\n        resource consumption and prevent runaway spending.\n    + **Forecast consumption based on business needs:** Establish a\n        forecast process to project future spend needs based on business and\n        technical needs.\n    + **Enforce cost guardrails for organizational resources:** Set up\n        automated checks (e.g., Tasks, query insights) and resource\n        guardrails (e.g., warehouse timeout, storage policies, resource\n        monitors) to identify unusual usage patterns and potential\n        overspending as they occur.\n    + **Govern resource creation and administration:** Establish clear\n        guidelines and automated processes for provisioning and maintaining\n        resources, ensuring that only necessary and appropriately sized\n        resources are deployed (e.g., warehouse timeout, storage policies,\n        resource monitors).\n* **Optimize**\n  \n    + **Compute workload-aligned provisioning:** Continuously monitor\n        resource health metrics to resize and reconfigure to match actual\n        workload requirements.\n    + **Leverage managed services:** Prioritize fully managed Snowflake\n        services (e.g., Snowpipe, Auto-clustering, [Query Acceleration Service](https://docs.snowflake.com/en/user-guide/query-acceleration-service) )\n        to offload operational overhead and often achieve better cost\n        efficiency.\n    + **Data storage types & lifecycle management:** Utilize appropriate\n        storage types and implement appropriate storage configuration to\n        right-size workloads to your storage footprint. Move data to cheaper\n        tiers or delete it when no longer needed.\n    + **Workload & architectural optimization:** Leverage architectural\n        decisions for cost-optimized resource utilization while meeting\n        business needs.\n    + **Limit data transfer:** Move, secure, and back up the appropriate\n        footprint across cloud regions.\n    + **Improve continually:** As new capabilities or usage patterns\n        emerge, establish a consistent testing framework to identify areas\n        for cost efficiency.\n\n## Business Impact\n\n#### Overview\n\nTo maximize organizational outcomes, Snowflake consumption must be\nexplicitly tied to business value. While cost optimization ensures\nefficiency, it does not guarantee that spend is aligned with outcomes\nthat matter to stakeholders. Alignment of business value to cost ensures\nworkloads, pipelines, dashboards, and advanced analytics are\ncontinuously evaluated not only for cost but also for the value they\ndeliver. This approach ensures Snowflake delivers as a strategic\nbusiness platform rather than a technical expense.\n\n#### Recommendations\n\nBusiness value to cost alignment represents a maturity step in FinOps on\nSnowflake. By embedding benchmarking, impact analysis, SLA definition,\nusage metrics, ROI measures, and business impact evaluation into daily\noperations, organizations can ensure that Snowflake consumption is\ncontinuously justified, optimized, and communicated in business terms.\nThis elevates the conversation with leadership from cost oversight to\nvalue realization and ensures that Snowflake is understood as a platform\nfor growth, innovation, and competitive advantage.\n\n#### Consider cost as a design constraint\n\nCost-Aware Architecting is the practice of embedding financial\naccountability directly into the design and development of Snowflake\nworkloads. By shifting left—introducing cost considerations early in the\narchitecture lifecycle—organizations ensure that ingestion,\ntransformation, analytics, and distribution workloads are not only\nperformant but also aligned with budget expectations. Many cost overruns\nin Snowflake originate from architectural decisions made without cost\nimplications in mind.\n\nFor example, designing ingestion with sub-second latency when daily\nfreshness is sufficient, or selecting inefficient table designs that\nincrease query scanning. These can lead to disproportionate spend.\nShifting cost awareness into architecture helps prevent inefficiencies\nbefore they occur and reinforces Snowflake’s role as a cost-effective\nenabler of business value.\n\nAt the ingestion layer, best practices include balancing latency versus\ncost by selecting appropriate services (e.g., Snowpipe, Snowpipe\nStreaming, or third-party tools) and choosing the right storage format\n(e.g., native tables, Iceberg). For transformations, design with\nfrequency versus SLA in mind to ensure data freshness matches the\nbusiness need. For analytics, apply schema design best practices such as\nthoughtful clustering key choices and pruning strategies to reduce\nconsumed credits. In distribution, optimize data transfer by monitoring\negress patterns and applying cost-saving practices like the [Snowflake Data Transfer Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) .\n\n#### Quantify value\n\nTo maximize organizational outcomes, Snowflake consumption must be\nexplicitly tied to measurable business value and clearly communicated in\nterms that resonate with stakeholders. Establishing baselines using\nSnowflake’s [Account Usage views](https://docs.snowflake.com/en/sql-reference/account-usage) creates a reference point, while tracking the current state highlights\ntrends in performance and consumption. Defining explicit goal\nstates—such as reduced cost per decision, improved time-to-market, or\nbroader data access—ties workloads directly to outcomes that matter to\nstakeholders. Outliers that diverge from these goals should be flagged\nfor review and optimization to prevent wasted resources. Best practices\ninclude applying unit economic measures related to your field (e.g. cost\nper terabyte analyzed or cost per fraud case prevented) and publishing\nROI dashboards that continuously link Snowflake consumption to business\noutcomes. By incorporating measurement into daily operations,\norganizations can move the conversation with leadership from cost\noversight to demonstrable value realization, positioning Snowflake as a\nclear enabler of enterprise growth and innovation.\n\n#### Trade-off analysis\n\nDefining SLAs or explicit business needs ensures that Snowflake\nworkloads are aligned with their intended purpose and that consumption\nlevels are justified by business outcomes. Some Snowflake workloads can\nbecome over-engineered or maintained without clear justification. Tying\neach workload to an SLA or business requirement prevents waste and\nensures that investment aligns with value. Before implementation, it is\ncrucial to document and align on the value of meeting an SLA,\nidentifying all stakeholders who rely on the workload. This includes\ndifferentiating between tangible outcomes, such as increased revenue,\nand intangible outcomes, such as compliance or data trust. Efficient\ncustomers use both [Snowflake Resource Monitors](https://docs.snowflake.com/en/user-guide/resource-monitors) and Budgets features to enforce guardrails that ensure workloads remain\nwithin acceptable cost-performance boundaries. All design decisions have\ntrade-offs, and explicitly calling out the expected outcomes leads to\nstreamlined decision-making in the future when outcomes are reviewed.\n\n#### Measure business value KPIs baseline\n\nBenchmarking establishes performance and cost baselines for Snowflake\nworkloads and compares them against internal standards as well as\nperformance and cost results from previous tech solutions. These\nbenchmarks can measure workload efficiency, the adoption of specific\nSnowflake features, and the alignment of workload costs to business\noutcomes. Without benchmarks, organizations lack the context to\ndetermine if their Snowflake consumption is delivering economies of\nscale or value back to the business. Benchmarking allows teams to\nidentify best practices, track improvements over time, and highlight\noutliers that may be driving unnecessary spend or delivering unexpected\nvalue.\n\nBest practices include measuring technical unit economic metrics (e.g.\ncredits per 1K queries, credits per 1 TB scanned), warehouse efficiency\nand utilization by workload type, and business unit economics (e.g.\ncredits per customer acquired, credits per partner onboarded, or credits\nper data product-specific KPIs). This provides a more comprehensive\npicture of consumption in relation to cost and value. Outliers should be\nhighlighted in executive communications as either success stories or\ncautionary examples. Benchmarking should be embedded in a continuous\nimprovement loop, where insights drive action, action improves\nefficiency, and those improvements are effectively measured.\n\n## Visibility\n\n#### Overview\n\nThe Snowflake Visibility principle is designed to transform opaque cloud\nspending into actionable insights, fostering financial accountability\nand maximizing business value within your Snowflake environment. It is\nfoundational to the FinOps framework, as you cannot control, optimize,\nor attribute business value to what you cannot see. To effectively\nmanage and optimize cloud costs in Snowflake, it's crucial to align\norganizationally to an accountability structure of spend, gain deep and\ngranular insight into all aspects of your cloud spending, and\ntransparently display it to the appropriate stakeholders to take action.\n\n#### Recommendations\n\nImplementing a robust FinOps visibility framework in Snowflake,\nsupported by cross-functional collaboration, enables each business\nfunction to access timely and relevant usage and cost data. This\nempowers them to understand the business impact of their consumption and\ntake prompt action when anomalies arise. To meet this vision, consider\nthe following recommendations based on industry best practices and\nSnowflake's capabilities:\n\n#### Understand Snowflake’s resource billing models\n\nIt is essential to review Snowflake's billing models to align technical\nand non-technical resources on financial drivers and consumption\nterminology. Snowflake's elastic, credit-based consumption model charges\nseparately for compute (Virtual Warehouses, Compute Pools, etc),\nstorage, data transfer, and various serverless features (e.g., Snowpipe,\nAutomatic Clustering, Search Optimization, Replication/Failover, AI\nServices). Understanding the interplay of these billing types ensures\nyou can attribute costs associated with each category’s unique usage\nparameters. High-level categories are below.\n\n* **Compute (Virtual Warehouses, Snowpark Container Services, Openflow):** This is often the most dynamic and largest portion of\n  Snowflake spend. Virtual Warehouses are billed per-second after an\n  initial 60-second minimum when active, with credit consumption\n  directly proportional to warehouse size (e.g., an “X-Small” Gen1\n  warehouse consumes one credit per hour, a 'Small' consumes two credits\n  per hour, doubling with each size increase) while SPCS (via [compute pools](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) )\n  are billed for all uptime with a minimum of five minutes. [Openflow](https://docs.snowflake.com/en/user-guide/data-integration/openflow/cost) is billed per second of runtime with a 60-second minimum.\n* **Storage:** Costs are based on the average monthly compressed data\n  volume stored, including active data, Time Travel (data retention),\n  and Fail-safe (disaster recovery) data. The price per terabyte (TB)\n  varies by cloud provider and region.\n* **Serverless features:** Snowflake Serverless features use resources\n  managed by Snowflake, not the user, which automatically scale to meet\n  the needs of a workload. This allows Snowflake to pass on efficiencies\n  and reduce platform administration while providing increased\n  performance to customers. The cost varies by feature and is outlined\n  in Snowflake’s Credit Consumption Document .\n* **Cloud services layer:** This encompasses essential background\n  services, including query compilation, metadata management,\n  information schema access, access controls, and authentication. Usage\n  for cloud services is only charged if the daily consumption of cloud\n  services exceeds 10% of the daily usage of virtual warehouses.\n* **AI features:** Snowflake additionally offers artificial intelligence\n  features that run on Snowflake-managed compute resources, including\n  Cortex AISQL functions (e.g. COMPLETE, CLASSIFY, etc.), Cortex\n  Analyst, Cortex Search, Fine Tuning, and Document AI. The usage of\n  these features, often with tokens, are converted to credits to unify\n  with the rest of Snowflake’s billing model. Details are listed in the\n  Credit Consumption Document.\n* **Data transfer:** Data transfer is the process of moving data into\n  (ingress) and out of (egress) Snowflake. This generally happens via\n  egress on cross-region [data replication](https://docs.snowflake.com/en/user-guide/account-replication-cost) , [copying into/out of stage, function calls](https://docs.snowflake.com/user-guide/cost-understanding-data-transfer) ,\n  and cross/same region [SPCS data transfer](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) .\n  Depending on the cloud provider and the region used during data\n  transfer, charges vary.\n* **Data sharing & rebates:** Snowflake offers an opt-out Data\n  Collaboration rebate program that allows customers to offset credits\n  by data consumed with shared outside organizations. This rebate is\n  proportional to the consumption of your shared data by consumer\n  Snowflake accounts. See the latest terms and more details here .\n\n#### Establish a consistent and granular cost attribution strategy\n\nImplementing robust and organizationally consistent tagging and labeling\nstrategies across all resources (e.g. storage objects, warehouses,\naccounts, queries) is crucial to accurately allocate costs to specific\nteams, products, or initiatives and linking actions to outcomes.\n\n**Tagging in Snowflake**\n\nTagging can be done at several levels:\n\n* **Snowflake object tagging:** Snowflake allows you to apply [object-level tags](https://docs.snowflake.com/en/user-guide/object-tagging/introduction) (key-value pairs) to accounts, warehouses, databases, schemas, users,\n  tables, and more. These tags are fundamental for apportioning costs\n  across departments, environments (dev, test, prod), projects, or lines\n  of business. Tags can also support [inheritance](https://docs.snowflake.com/en/user-guide/object-tagging/inheritance) and [propagation](https://docs.snowflake.com/en/user-guide/object-tagging/propagation) ,\n  simplifying tagging across dependent objects. For example, instead of\n  tagging each individual table underneath a schema, tagging the schema\n  will cause all tables to inherit the tag of the schema. This\n  significantly reduces the manual effort required for tagging and\n  ensures that new objects created within a tagged schema or propagated\n  workflow automatically inherit the correct cost attribution. Snowflake\n  strongly recommends tags for warehouses, databases, tables, and users\n  to enable granular cost breakdowns. You can use the [TAG\\_REFERENCES view](https://docs.snowflake.com/sql-reference/account-usage/tag_references) in SNOWFLAKE.ACCOUNT\\_USAGE to combine with common usage views like\n  WAREHOUSE\\_METERING\\_HISTORY and TABLE\\_STORAGE\\_METRICS to allocate usage\n  to relevant business groups. Object Tags are best utilized when\n  Snowflake objects are not shared across cost owners.\n* **Query tags for granular workload attribution:** [Query tags](https://docs.snowflake.com/en/user-guide/cost-attributing) can be set via session parameters (e.g., ALTER SESSION SET QUERY\\_TAG =\n  'your\\_tag';) or directly within SQL clients or ETL tools. This\n  associates individual queries with specific departments, projects, or\n  applications, even when using shared warehouses. This is extremely\n  valuable for shared warehouses where multiple teams or applications\n  use the same compute resource, allowing for granular showback. It is\n  also easy to programmatically make changes to query tags within\n  scripts or processes to allocate costs appropriately. Query tags can\n  be found in the QUERY\\_HISTORY view of the SNOWFLAKE.ACCOUNT\\_USAGE\n  schema.\n\n**Tagging models**\n\nIn the initial setup of a business unit or use case, it is important to\nconsider the [model for tagging](https://docs.snowflake.com/en/user-guide/cost-attributing) costs within the platform via shared or dedicated resources. These fall\ninto three large buckets:\n\n* **Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate warehouses with a\n  department. You can use object tags to attribute the costs incurred by\n  those warehouses to that department entirely.\n* **Resources shared by users from multiple departments:** An example of\n  this is a warehouse shared by users from different departments. In\n  this case, you use object tags to associate each user with a\n  department. The costs of queries are attributed to the users. Using\n  the object tags assigned to users, you can break down the costs by\n  department.\n* **Applications or workflows shared by users from different departments:** An example of this is an application that issues\n  queries on behalf of its users. In this case, each query executed by\n  the application is [assigned a query tag](https://docs.snowflake.com/en/sql-reference/sql/alter-session) that identifies the team or cost center of the user for whom the query\n  is being made.\n\nEach model has its pros and cons, including how to handle concepts such\nas idle time or whether to show/charge back attributed or billed\ncredits. Review each model before deploying resources. If an\norganization is caught between models, a common approach is to start in\na shared resource environment and graduate to dedicated resources as the\nworkload increases.\n\n**Tag enforcement**\n\nClear and consistent naming conventions for accounts, warehouses,\ndatabases, schemas, and tables facilitate immediate cost understanding.\nEnforcing robust tagging policies (e.g., requiring specific tags for new\nresource creation and using automated scripts to identify untagged\nresources) is crucial for accurate data interpretation and effective\ncost management. Without tag enforcement, it is difficult to accurately\nallocate all costs and can require manual effort, like extensive\ntag-mapping tables. Tag values are enforced within an account, but if a\nmulti-account strategy is needed for your organization, a tag [database can be replicated](https://docs.snowflake.com/en/user-guide/cost-attributing) and leveraged across all accounts to ensure consistent values are used.\nFor best-in-class visibility, it is recommended to have a tagging\nstrategy and tag all resources in an organization to allocate costs to\nrelevant owners.\n\n#### Embed cost accountability into your organization's DNA\n\nTo effectively manage Snowflake spend and align business structure to\ntechnical resources, you should implement a system of showback or\nchargeback. This approach is crucial for promoting accountability and\noptimizing resource usage as there is a single owner for each object\nwithin the platform.\n\n**Showback**\n\nIf cost accountability models have not been implemented previously,\nconsider a showback model. This involves transparently reporting\nSnowflake costs to different departments or projects to raise awareness\nof their costs. By showing each team their monthly consumption (broken\ndown by warehouse usage, query costs, and storage, etc.), it encourages\na cost-conscious culture. This initial step helps teams understand the\nfinancial impact of their actions without the immediate pressure of\nbudget cuts. Tools like Snowflake's built-in [Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) & [budget](https://docs.snowflake.com/en/user-guide/budgets) views, third-party cost management platforms, or custom dashboards can\nbe used to provide these reports.\n\n**Chargeback**\n\nFor more financially mature organizations, a chargeback model can be\nvery effective for managing costs. This system directly bills\ndepartments for their Snowflake usage. This creates a powerful financial\nincentive for teams to optimize their workloads. To make this transition\nsmooth and fair, you need to define clear rules for cost allocation. By\nimplementing chargeback, you turn each department into a financial\nstakeholder, encouraging them to right-size their warehouses, suspend\nthem during idle periods, and write more efficient queries. This shift\nin accountability leads to a more disciplined and cost-effective use of\nyour Snowflake environment.\n\nIn either case, having a centralized dashboard or visual for all\norganizations to review intra-period is critical for financial\naccountability and next-step actions.\n\n#### Deliver clear historical consumption insights\n\nThe most mature FinOps customers are those who programmatically and\nstrategically drive consumption insights across the business. This\ninvolves three core elements:\n\n* **Platform cost tracking:** Pinpoint specific Snowflake credit\n  consumption (compute, storage, serverless, AI, and data transfer),\n  usage patterns, and efficiency opportunities to deconstruct credit\n  usage, understand drivers, identify anomalies, and (eventually) drive\n  forecasting operations.\n* **Normalization of consumption:** Once consumption has been attributed\n  and aggregated to meaningful levels, normalizing it against relevant\n  business and technical metrics contextualizes it in relation to\n  organizational goals. It allows for the natural growth and seasonality\n  of platform usage to be put into context with business and technical\n  demand drivers.\n* **Clear reporting:** Presenting Snowflake cost data in an\n  understandable format for various stakeholders is vital. This enables\n  budgeting, forecasting, KPIs, and business value metrics directly tied\n  to Snowflake credit consumption.\n\n**Track usage data for all platform resources**\n\nTo deliver clear and actionable consumption insights, it is essential to\nleverage the rich usage data that Snowflake natively provides. The\nfoundation for all cost visibility is the **SNOWFLAKE** database, which\ncontains two key schemas for this purpose: [ACCOUNT\\_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) (for granular, account-level data) and [ORGANIZATION\\_USAGE](https://docs.snowflake.com/en/sql-reference/organization-usage) (for a consolidated view across all accounts).\n\n|Metric Category |Description |Key Metrics |Primary Data Sources |\n| --- | --- | --- | --- |\n|Compute & query metrics |Understand the cost of query execution, warehouse consumption, and overall compute health. These are often the most dynamic and largest portion of your spend. |\\- Credits used: total credits by warehouse  \n\\- Query performance: execution time, bytes scanned, compilation time, parameterized query hash  \n\\- Warehouse health: % idle time, queueing, spilling, concurrency |\\- `ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY` (hourly warehouse credit usage)  \n\\- `ACCOUNT_USAGE.QUERY_HISTORY` (detailed query metrics and associated warehouses) |\n|Storage metrics |Costs for compressed data, including active data, Time Travel, and Fail‑safe. |\\- Storage volume (avg monthly compressed GB/TB)  \n\\- Inactive storage (Time Travel, Fail‑safe)  \n\\- Storage growth rates  \n\\- Table access (stale/unused) |\\- `ACCOUNT_USAGE.TABLE_STORAGE_METRICS`  \n\\- `ACCOUNT_USAGE.DATABASE_STORAGE_USAGE_HISTORY`  \n\\- `ACCOUNT_USAGE.ACCESS_HISTORY` |\n|Serverless & AI metrics |Track credit consumption by Snowflake‑managed services and AI features. |\\- Credits used by service  \n\\- Cost per credit‑consuming events |\\- `ACCOUNT_USAGE.<Serverless Feature>_HISTORY`  \n\\- `ORGANIZATION_USAGE.METERING_DAILY_HISTORY`  \n\\- AI views such as `CORTEX_FUNCTIONS_USAGE_HISTORY` , `CORTEX_ANALYST_USAGE_HISTORY` , `DOCUMENT_AI_USAGE_HISTORY` |\n|Data transfer |Cost of moving data into (ingress) and out of (egress) Snowflake, especially cross‑region/cloud. |\\- Bytes transferred  \n\\- Transfer cost by destination  \n\\- Replication vs. egress |\\- `ACCOUNT_USAGE.DATA_TRANSFER_HISTORY`  \n\\- `ORGANIZATION_USAGE.DATA_TRANSFER_DAILY_HISTORY` |\n|Financial metrics |Translate credits to currency and provide org‑wide spend view. |\\- Overall dollar spend (daily)  \n\\- Spend by service type |\\- `ORGANIZATION_USAGE.USAGE_IN_CURRENCY_DAILY`  \n\\- `ORGANIZATION_USAGE.RATE_SHEET_DAILY` |\n\n**Normalize consumption with unit economic metrics**\n\nFor organizations to achieve comprehensive financial visibility, it is\nrecommended best practice to move beyond tracking aggregate spend and\nimplement Unit Economics Metrics. Unit economics provides a powerful\nmethodology for normalizing cloud consumption by tying platform costs to\nspecific business or operational drivers. This per-unit approach helps\nyou understand cost efficiency, measure the ROI of your initiatives, and\nmake data-driven decisions about resource allocation and optimization.\nBy translating abstract credit consumption into tangible metrics, you\ncan empower technical and business teams with a shared language for\ndiscussing value and cost. These metrics are commonly tracked across\ntime to show changes in efficiency or business impact.\n\n**Efficiency metrics (technical KPIs)**\n\nEfficiency Metrics are technical Key Performance Indicators (KPIs) that\nconnect cloud costs directly to platform operations and workloads. They\nare crucial for engineering teams and platform owners to identify\ninefficiencies, optimize resource usage, and understand the cost drivers\nof the data platform itself. These metrics provide the granular,\noperational view needed to manage the platform's performance day-to-day.\nSome common examples include:\n\n* **Cost per 1000 executable queries:** Determines the average cost for\n  a batch of one thousand queries. This metric is useful for\n  understanding the overall cost profile of analytical activity on the\n  platform and, when trended, how efficiency has changed across time.\n* **Cost per TB scanned:** Represents the average scanning cost of data.\n  This metric can help understand the cost implications of table\n  configuration (data ordering/clustering keys, Search Optimization) and\n  query efficiency.\n* **Cost per user:** Measures the average cost to support each active\n  user on the platform. This helps in understanding the cost\n  implications of user growth and identifying expensive usage patterns.\n\n> Customers can track credits (warehouse) per thousand queries within a\n> use case to see how efficiency has evolved over time and determine if\n> they are achieving economies of scale.\n> \n> \n\n**Business metrics (business KPIs)**\n\nBusiness Metrics link cloud spending to meaningful business outcomes and\nvalue drivers. These KPIs are essential for executives, finance teams,\nand product managers to understand the return on investment (ROI) of\ncloud expenditure and to allocate costs accurately across different\nparts of the organization. They answer the critical question: \"What\nbusiness value are we getting for our cloud spend?\" Examples include:\n\n* **Cost per customer:** Attributes a portion of the total platform cost\n  to each of the company's end customers. This is a powerful metric for\n  understanding profitability and cost-to-serve at a customer level.\n* **Cost per project:** Allocates cloud costs to specific internal\n  projects, products, or initiatives. This enables accurate\n  project-based accounting and helps assess the financial viability of\n  new features or services.\n* **\"Bring Your Own Metric\":** Custom define unit economic metrics that\n  are unique to your organization's business model. Examples could\n  include Cost per Transaction, Cost per Shipment, or Cost per Ad\n  Impression. Creating these tailored metrics ensures the most accurate\n  alignment between cloud spend and core business value.\n\nIf Snowflake is in the value chain for orders, the cost per order can be a good metric to tie Snowflake consumption to Business Demand Drivers.\n\n**Visualize metrics with Snowsight tools and external BI tools**\n\nA critical component of cost governance is the effective visualization\nof spending and usage data. Raw data, while comprehensive, is often\ndifficult to interpret and act upon. By translating cost and usage\nmetrics into interactive dashboards and reports, you can empower\nstakeholders—from engineers to executives—to understand spending\npatterns, troubleshoot, and make informed decisions. A multi-layered\napproach can be used to track meaningful cost metrics.\n\n* **Snowsight's built-in cost management capabilities:** Snowsight\n  provides pre-built visuals for usage and credit monitoring directly\n  within the [Snowflake Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) .\n  It allows filtering by tags (e.g., view cost by department tag),\n  credit consumption by object types, and cost insights to optimize the\n  platform.\n* **Creating custom dashboards or Streamlit apps for different stakeholder groups:** Snowsight facilitates the creation of custom\n  dashboards using ACCOUNT\\_USAGE and ORGANIZATION\\_USAGE views. Custom\n  charts in the Dashboards feature and Streamlit apps can both be easily\n  shared. Combined with cost allocation and tagging, this allows for\n  tailored views for finance managers (aggregated spend), engineering\n  managers (warehouse utilization), or data analysts (query\n  performance).\n* **Integrating with third-party BI tools for advanced analytics:** Connecting to Snowflake from tools like Tableau, Power BI, Looker, or\n  custom applications offers highly customizable and extensive control\n  over cost data visualization. Cloud-specific third-party data programs\n  (FinOps platforms) offer easier setup and more out-of-the-box\n  Snowflake cost optimization insights.\n* **Leverage Cortex Code (In Preview):** This AI Assistant capability\n  allows users to query cost and usage data in ACCOUNT\\_USAGE views using\n  natural language natively in the Snowsight UI.\n\n#### Investigate anomalous consumption activity\n\nCost Anomaly Detection is a critical component of visibility that\nleverages machine learning to continuously monitor credit consumption\nagainst historical spending patterns, automatically flagging significant\ndeviations from the established baseline. This proactive monitoring is\nessential for preventing budget overruns and identifying inefficiencies,\nshifting the organization from a reactive to a proactive cost management\nposture to mitigate financial risk. As a best practice, you should\ninitially review anomaly detection on the entire account to gain a broad\nview, then dive deeper into a more granular review for individual\nhigh-spend warehouses. [This approach](https://docs.snowflake.com/en/user-guide/cost-anomalies) allows for more targeted analysis and assigns clear ownership for\ninvestigating any flagged anomalies. There are several methods for\nanomaly detection supported by Snowflake:\n\n**Cost Anomalies in Snowsight**\n\nSnowsight, Snowflake's primary web interface, offers a dedicated Cost\nManagement UI that allows users to visually identify and analyze the\ndetails of any detected cost anomaly. The importance of this intuitive\nvisual interface lies in its ability to make complex cost data\naccessible to a wide range of stakeholders, enabling rapid root cause\nanalysis by correlating a cost spike with specific query history or user\nactivity. One of the tabs in this UI is the Cost Anomaly Detection tab,\nwhich enables you to view cost anomalies at the organization or account\nlevel and explore the top warehouses or accounts driving this change. To\nfoster a culture of cost awareness and accountability, it is a best\npractice to ensure there is an owner for an anomaly detected in the\naccount and set up a [notification (via email)](https://docs.snowflake.com/en/user-guide/cost-anomalies-ui) in the UI itself to ensure that cost anomalies are quickly and\naccurately investigated.\n\n**Programmatic Cost Anomaly Detection**\n\nFor deeper integration and automation, organizations can review [anomalies programmatically](https://docs.snowflake.com/en/user-guide/cost-anomalies-class) using the SQL functions and views available within the SNOWFLAKE.LOCAL\nschema. This approach is important for enabling automation and\nscalability, allowing cost governance to be embedded directly into\noperational workflows, such as feeding anomaly data into third-party\nobservability tools or triggering automated incident response playbooks.\nA key best practice is to utilize this programmatic access to build\ncustom reports and dashboards that align with specific financial\nreporting needs and to create advanced, automated alerting mechanisms\nthat pipe anomaly data into established operational channels, such as\nSlack or PagerDuty.\n\n**Custom Anomaly Detection & Notification**\n\nAlthough anomalies are detected at the account and organization level,\nif you desire to detect anomalies at lower levels (e.g. warehouse or\ntable), it is recommended to leverage Snowflake’s [Anomaly Detection](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) ML class and pair it with a Snowflake [alert](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) to notify owners of more granular anomalies that occur within the\necosystem. This ensures all levels of Snowflake cost can be monitored in\na proactive and effective way. As a best practice, [notifications](https://docs.snowflake.com/en/user-guide/notifications/about-notifications) should be configured for a targeted distribution list that includes the\nbudget owner, the FinOps team, and the technical lead responsible for\nthe associated Snowflake resources, ensuring all stakeholders are\nimmediately aware of a potential cost overrun and can coordinate a swift\nresponse.\n\n## Control\n\n#### Overview\n\nThe Control principle of the Cost Optimization framework is designed to\nmove organizations beyond cost reporting by establishing the necessary\nautomated guardrails and governance policies to manage and secure\nSnowflake consumption proactively. This framework enforces financial\ngovernance by transforming cost visibility into tangible action,\nutilizing features like budgets and resource monitors to prevent\nuncontrolled growth and ensure consumption aligns strictly with\norganizational financial policies. Control is foundational for\nmaximizing the value of the platform by ensuring disciplined and\ncost-effective resource utilization.\n\n#### Recommendations\n\nImplementing a comprehensive control framework, supported by features\nsuch as Resource Monitors, Budgets, and Tagging Policies, empowers\norganizations to enforce financial accountability and maintain budget\npredictability. By adopting these controls, teams can actively manage\nspend, quickly and automatically mitigate cost inefficiencies, and\nensure the disciplined, cost-effective utilization of the entire\nSnowflake environment. The culmination of all of these controls leads to\ngreater platform ROI and minimized financial risk. To meet this goal,\nconsider the following recommendations based on industry best practices\nand Snowflake's capabilities:\n\n#### Proactively monitor all platform usage\n\nTo effectively manage and [control Snowflake spend](https://docs.snowflake.com/en/user-guide/cost-controlling) ,\nit is essential to establish and enforce cost guardrails. Implementing a [budgeting system](https://docs.snowflake.com/en/user-guide/budgets) is a key\nFinOps practice that promotes cost accountability and optimizes resource\nusage by providing teams with visibility into their consumption and the\nability to set alerts and automated actions. Budgeting helps to prevent\nunexpected cost overruns and encourages a cost-conscious culture.\n\n**Set budgets permissions**\n\nTo establish effective budgets, it's crucial to define [roles and privileges](https://docs.snowflake.com/en/user-guide/budgets) by configuring the role, team, or user responsible for the resources.\nThis ensures that budget tracking aligns with specific business units or\nprojects, enabling accurate cost attribution and accountability. By\nlinking consumption to the relevant stakeholders, you can create a clear\nshowback or chargeback model, which is vital for fostering a sense of\nownership over spending. This configuration should be part of a broader,\nconsistent tagging strategy to ensure all costs are properly allocated\nto departments, environments, or projects.\n\n**Create budget categories**\n\nCategorizing costs is fundamental for granular budget management. You\ncan establish budgets based on the [account](https://docs.snowflake.com/en/user-guide/budgets/account-budget) or create [custom categories](https://docs.snowflake.com/en/user-guide/budgets/custom-budget) using Object Tags. Custom tags, such as those for a data product or cost\ncenter, are critical for accurately apportioning costs across different\ndepartments, lines of business, or specific projects. This granular\napproach provides a detailed breakdown of where spending occurs,\nenabling more precise control and informed decision-making regarding\nresource allocation. Implementing robust tagging policies and naming\nconventions ensures consistency and facilitates the interpretation of\ncost data. Because budgets are soft limit objects, objects can be part\nof more than one budget if different perspectives need to be tracked for\ncost (e.g., cost center & workload level budgeting).\n\n**Implement a notification strategy**\n\nEffective budget management relies on timely communication. Setting up\nalerting through emails or webhooks to collaboration tools like Slack\nand Microsoft Teams provides proactive [notification](https://docs.snowflake.com/en/user-guide/budgets/notifications) to key stakeholders when spending approaches or exceeds a defined\nthreshold. These alerts provide teams with an opportunity to review and\nadjust their usage before it leads to significant cost overruns. This\ncapability positions organizations for security success by mitigating\npotential threats through comprehensive monitoring and detection.\n\nNotifications are not limited to just budgets; [Snowflake alerts](https://docs.snowflake.com/en/user-guide/alerts) can also be\nconfigured to systematically notify administrators of unusual or costly\npatterns, such as those listed in the Control and Optimize sections of\nthe Cost Pillar. This ensures that key drivers of Snowflake consumption\ncan be tracked and remediated proactively, even as the platform’s usage\ngrows.\n\n#### Forecast consumption based on business needs\n\nForecasting Snowflake consumption should be a strategic business\nfunction, not a mere technical prediction. The goal is to establish a\ntransparent basis for budgeting and optimizing ROI by linking\nconsumption directly to measurable business outcomes. In a dynamic,\nusage-based environment where compute costs are the most volatile\nelement of the bill, a robust framework must integrate quantitative\nanalysis of historical usage with qualitative insights into future\nbusiness drivers. The following framework outlines how to build and\nmaintain a comprehensive consumption forecast.\n\n**Establish the Baseline**\n\nThis phase focuses on understanding the source of spend and establishing\ngranular cost accountability.\n\n* **Identify demand drivers and unit economics:** To understand what\n  drives Snowflake spend, correlate historical credit, storage, and data\n  transfer usage with key business metrics like cost per customer or per\n  transaction. Use Snowflake's [ACCOUNT\\_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) schema, including the WAREHOUSE\\_METERING\\_HISTORY and QUERY\\_HISTORY\n  views, as the primary data sources for this analysis.\n* **Granular cost attribution:** Accurately tie costs back to business\n  teams or workloads by implementing a mandatory tagging strategy for\n  all warehouses and queries. Align these tags with your organization's\n  financial structure to provide clear cost segmentation.\n\n**Build the predictive model**\n\nThis phase integrates historical trends with strategic business inputs\nto create forward-looking projections.\n\n* **Historical trend analysis:** Analyze past usage for trends,\n  seasonality, and outliers to inform future projections. Start with\n  simple trend-based forecasting and progressively move to more\n  sophisticated models, leveraging Snowflake’s built-in [SNOWFLAKE.ML.FORECAST function](https://docs.snowflake.com/en/user-guide/ml-functions/forecasting) for time-series forecasting.\n* **Driver-based forecasting:** Integrate planned business initiatives\n  and new projects directly into the model. Collaborate with business\n  leaders to gather strategic inputs such as projected customer growth,\n  new product launches, or increased data ingestion from marketing\n  campaigns.\n* **Scenario modeling:** Develop multiple forecast scenarios (e.g.,\n  \"conservative,\" \"base case,\" \"aggressive\") by applying varied growth\n  factors to key business drivers. This enables flexible planning and\n  helps mitigate financial risk.\n\n**Operationalize and optimize**\n\nThis phase links the forecast to continuous monitoring, governance, and\nproactive cost controls.\n\n* **Continuous monitoring and variance analysis:** Regularly compare\n  actual consumption against the forecast to identify and investigate\n  significant variances. This feedback loop is crucial for refining the\n  underlying model and adapting to evolving business needs.\n* **Collaborative governance:** Ensure a single source of truth for\n  consumption data by establishing a regular FinOps review session with\n  Finance, Engineering, and Business teams. Use customized dashboards to\n  present data in business-friendly terms.\n* **Implement predictive budget controls:** Shift from reactive spending\n  to a proactive model. Utilize Snowflake Resource Monitors and Budgets,\n  which employ monthly-level time-series forecasting, to define credit\n  quotas and trigger automated alerts or suspensions to prevent cost\n  overruns.\n\n#### Enforce cost guardrails for organizational resources\n\nTo effectively manage Snowflake expenditure and prevent unforeseen\ncosts, it is crucial to implement a robust framework of resource\ncontrols. These controls act as automated guardrails, ensuring that\nresource consumption for compute, storage, and other services aligns\nwith your financial governance policies. By proactively setting policies\nand remediating inefficiencies, you can maintain budget predictability\nand maximize the value of your investment in the platform.\n\n**Compute controls**\n\nControlling compute consumption is often the most critical aspect of\nSnowflake cost management, as it typically represents the largest\nportion of spend. Snowflake offers several features to manage warehouse\nusage and prevent excessive costs.\n\n* **Implement resource monitors** : [Resource monitors](https://docs.snowflake.com/en/user-guide/resource-monitors) are a powerful feature for tracking and controlling credit consumption\n  across virtual warehouses or for the entire account. Their primary\n  importance lies in their ability to enforce strict budget limits,\n  preventing cost overruns by automatically [triggering actions](https://docs.snowflake.com/en/user-guide/resource-monitors) ,\n  such as sending notifications and/or suspending warehouses when credit\n  usage reaches a defined quota. For effective governance, it is a best\n  practice to create multiple resource monitors at different\n  granularities (e.g., per-department, per-project) with escalating\n  actions, such as notifying administrators at 80% usage and suspending\n  all assigned warehouses at 100% usage, to cap spending. It is also\n  considered best practice to ensure there is a consistent action tied\n  to the resource monitors based on your organization’s ways of working.\n  For example, it is worth considering that set resource monitors\n  perform actions like notify to specific admin(s) within your\n  organization.\n* **Reduce Runaway Queries:** Runaway or hung queries can lead to\n  significant cost overruns. Managing long-running queries is especially\n  important for environments with ad-hoc users or complex analytical\n  workloads, where a poorly written query can consume credits for hours.\n  \n    + **Statement timeout policies** automatically terminate any query\n        that runs longer than a specified time limit. This serves as an\n        essential guardrail to prevent individual queries from consuming\n        excessive resources and impacting both cost and performance for\n        other users. The best practice is to set the [STATEMENT\\_TIMEOUT\\_IN\\_SECONDS parameter](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) at different levels—for the account, warehouse, specific users, or\n        individual sessions—to tailor controls to different workload\n        patterns, such as allowing longer timeouts for ETL warehouses\n        compared to BI warehouses. [Queued timeout policies](https://docs.snowflake.com/en/user-guide/cost-controlling-controls) can also help remove queries that eclipse a reasonable time\n        threshold and could have been run elsewhere by users trying to\n        receive a response.\n    + **Policy-based automation** can also cancel queries pre-emptively.\n        An example is using stored procedures that leverage the [SYSTEM$CANCEL\\_QUERY](https://docs.snowflake.com/en/sql-reference/functions/system_cancel_query) function to terminate statements that exceed predefined runtime\n        thresholds or contain ill-advised logic, such as exploding joins.\n        This approach allows you to more finely customize the types of\n        queries you want to cancel, as you have full control over defining\n        the stored procedure logic.\n* **Auto-suspend policies** : Auto-suspend policies are a foundational\n  cost control for virtual warehouses, automatically suspending a\n  warehouse after a defined period of inactivity. By default, all\n  warehouses have auto-suspend enabled, however this feature can be\n  disabled, so it’s important to [monitor warehouse auto-suspend configuration](https://docs.snowflake.com/en/user-guide/warehouses-considerations) and ensure proper access controls are set to restrict users from disabling the auto-suspend setting. The best practice for balancing cost versus performance is to reduce the auto-suspend policy to the minimum possible (generally above 60 seconds) without affecting query caching and performance (SLA) expectations.\n\n**Storage Controls**\n\nWhile storage costs are generally lower than compute costs, they can\ngrow significantly over time. Understanding the different components of\nstorage cost and implementing policies to manage the types of storage is\nkey to keeping these costs in check.\n\n* **Staged files:** [Staged](https://docs.snowflake.com/en/user-guide/data-load-considerations-stage) files are files that have been prepped for bulk data loading/unloading\n  (stored in compressed or uncompressed format). They can be stored in\n  an [external stage](https://docs.snowflake.com/en/user-guide/data-load-s3-create-stage) using cloud provider’s blob storage (such as Amazon S3) or an [internal stage](https://docs.snowflake.com/en/user-guide/data-load-local-file-system-create-stage) within the Snowflake platform. You are only charged for data stored in\n  internal stages.\n  \n+ To help control costs on staged files, as well as improve\n        performance of data loads, you can ensure successfully ingested\n        files are removed by using the PURGE = TRUE option of the [COPY INTO <table>](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table) command . Alternatively, use the [REMOVE](https://docs.snowflake.com/en/sql-reference/sql/remove) command to remove the files in the stage.\n* **Active Storage:** [Active Storage](https://docs.snowflake.com/en/sql-reference/info-schema/table_storage_metrics) consists of the data in a table that can be actively queried against\n  at the current point of time (i.e., without using Time-Travel\n  commands) **.** To control active storage costs, you can create a [Storage Lifecyle Policy](https://docs.snowflake.com/LIMITEDACCESS/storage-lifecycle-policy/storage-lifecycle-policies) to automatically archive or delete data based on an expiration policy\n  you create.\n* **Time Travel:** [Time Travel](https://docs.snowflake.com/en/user-guide/data-time-travel) data is data that is maintained for all historical changes of a table\n  through its retention time, allowing for easy recovery of changed or\n  deleted data.\n  \n    + Time Travel data retention is controlled using the [DATA\\_RETENTION\\_TIME\\_IN\\_DAYS](https://docs.snowflake.com/en/user-guide/data-time-travel) parameter , which can be set at a number of different object\n        levels (i.e. Account, Database, Schema, Table), so it is important\n        to monitor this setting to ensure you do not have excessive data\n        retention where it is not needed.\n    + Snowflake has a standard [data retention period](https://docs.snowflake.com/en/user-guide/data-time-travel) of one day. This is automatically enabled for all accounts, but for some tables, particularly [large, high churn tables](https://docs.snowflake.com/en/user-guide/tables-storage-considerations) , this one day can still result in excessive costs. To reduce costs in these cases, you can create these tables as transient with zero Time Travel retention (i.e., DATA\\_RETENTION\\_TIME\\_IN\\_DAYS=0) and periodically insert a copy of the table contents into a permanent table.\n* **Fail-safe:** In addition to Time-Travel, Snowflake retains\n  historical data for seven days after the Time-Travel retention period\n  expires as [Fail-Safe](https://docs.snowflake.com/en/user-guide/data-failsafe) data. This data can be requested for worst-case scenario data\n  recovery. Fail-safe applies to all tables that are created as the\n  permanent [table type](https://docs.snowflake.com/en/user-guide/tables-temp-transient) (which is the default table type).\n  \n+ However, for ETL or data modeling, it is common to have tables that\n        are created on a short-term basis to support transformations. For\n        these use cases, it is recommended that you use [Temporary and Transient tables](https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs) ,\n        which do not utilize fail-safe storage and thus do not incur\n        additional storage costs.\n* **Retained for clones:** This is data that is stored because it is\n  referenced by a clone, despite the data being deleted or outside the\n  retention period of the base table that was cloned. To control costs\n  related to these “stale” clones, it is recommended that you monitor [RETAINED\\_FOR\\_CLONE\\_BYTES](https://community.snowflake.com/s/article/How-to-manage-RETAINED-FOR-CLONE-BYTES) and drop clones that are no longer needed. You can leverage the [Alerts and Notifications features](https://docs.snowflake.com/en/guides-overview-alerts) to alert you when RETAINED\\_FOR\\_CLONE\\_BYTES exceeds a threshold, prompting you to take action.\n\n**Serverless Features**\n\nFor serverless features, which do not use warehouse compute and\ntherefore cannot leverage the Resource Monitor feature, we recommend\nsetting up a budget. [Budgets](https://docs.snowflake.com/en/user-guide/budgets) allow\nyou to define a monthly spending limit on the [compute costs](https://docs.snowflake.com/en/user-guide/cost-understanding-compute) for a Snowflake account or a custom group of Snowflake objects. When the\nspending limit is projected to be hit, a notification is sent. While\nBudgets do not explicitly allow you to suspend serverless features upon\nreaching a limit (the way that Resource Monitors do), Budgets can be\nconfigured to not only send emails, but also send [notifications](https://docs.snowflake.com/en/user-guide/budgets/notifications) to a cloud message queue or other webhooks (such as Microsoft Teams,\nSlack, or PagerDuty). This then gives you the ability to trigger other\nactions for remediation.\n\n#### Govern resource creation and administration\n\nTo prevent uncontrolled spend as organizations scale, it's essential to\nhave a clear management strategy for Snowflake resources, most notably,\nvirtual warehouses. This strategy should encompass a defined\nprovisioning process, ongoing object management, and automated platform\nenforcement to foster agility while maintaining financial discipline.\n\n**Centralized vs. decentralized management**\n\nOrganizations tend to adopt one of two primary approaches to managing\nSnowflake resources:\n\n* **Centralized management:** A dedicated team, such as a platform\n  Center of Excellence (CoE), handles resource creation and\n  administration policies. This ensures consistency, adheres to best\n  practices, and facilitates robust cost control. This model is ideal\n  for large enterprises where strict governance and chargeback are\n  paramount.\n* **Decentralized management:** Individual business units or teams\n  manage their own resources. This provides greater autonomy and speed\n  but can lead to resource sprawl, inconsistent practices, and\n  significant cost inefficiencies if not properly governed.\n\n**Striking the balance: the federated model**\n\nThe most effective strategy often lies in a hybrid, or federated, model.\nThis approach combines centralized governance (policies defined by a\nCoE) with decentralized execution (teams having the freedom to create\nresources within those guardrails). This balance enables agility while\nmitigating financial risk.\n\n**Core Principles for Governance**\n\nRegardless of the chosen model, these principles are essential for\neffective governance:\n\n* **Limit resource creation:** Restrict the ability to create or modify\n  virtual warehouses and other high-cost resources to a small number of\n  trusted roles to prevent uncontrolled growth.\n* **Establish a transparent workflow:** Create a clear, simple workflow\n  for provisioning resources, especially for larger warehouses. For\n  example, any request for a warehouse of medium size or larger should\n  require a business justification and an assigned cost center.\n* **Near real-time visibility:** Triggered visibility is non-negotiable\n  for monitoring resource creation and resizing. Configure alerts that\n  notify FinOps or CoE teams whenever a new warehouse is created or an\n  existing one is modified outside of a provisioning workflow. This\n  allows for immediate review and prevents overprovisioning.\n* **Enforce tagging:** Make a mandatory tagging strategy a prerequisite\n  for all resource creation. This ensures every credit spent can be\n  accurately attributed to the correct department or project, enabling\n  robust chargeback and accountability.\n* **Automate deactivation:** To prevent object sprawl, implement\n  policies that identify and deactivate stale resources after a\n  predetermined period of disuse.\n\n## Optimize\n\n#### Overview\n\nThe Optimization principle of the Cost Optimization framework focuses on continuously improving the efficiency of your Snowflake resources. This includes optimizing compute, storage, data transfer, and managed services by understanding their usage and identifying areas for improvement. The frequency of optimization efforts should be guided by the metrics established in the Visibility principle and monitored through the Control principle. All recommendations within this Optimize principle are to be reviewed on a regular cadence and balanced with business and performance needs.\n\n#### Recommendations\n\nSnowflake offers numerous optimization controls within its platform.\nThese features are designed to enhance efficiency and reduce\nadministrative overhead for your various workloads. Coupled with\noperational best practices that utilize features in a healthy manner,\nyou can balance performance goals with cost governance requirements to\nmeet business objectives.\n\nBy implementing these recommendations, you will be able to:\n\n* Reduce administration time\n* Increase workload efficiency\n* Balance cost controls and guardrails to meet service level agreements\n  and business objectives\n* Achieve healthy growth and economies of scale within your organization\n\nTo foster healthy growth and achieve economies of scale within your\norganization, we recommend the following, drawing upon industry best\npractices and Snowflake's capabilities.\n\n#### Compute workload-aligned provisioning\n\nCompute is the most significant part of any organization’s Snowflake\nspend, typically accounting for 80% or more of spend. A good warehouse\ndesign should incorporate the principles below:\n\n* Separate warehouses by workload (e.g., ELT versus analytics versus\n  data science)\n* Workload size in bytes should match the t-shirt size of the warehouse\n  in the majority of the workloads–larger warehouse size doesn’t always\n  mean faster\n* Align warehouse size for optimal cost-performance settings\n* [Utilize Multi-Cluster Warehouse](https://docs.snowflake.com/en/user-guide/warehouses-considerations) configuration to solve for high concurrency\n* Utilize [Query Acceleration Services](https://docs.snowflake.com/en/user-guide/query-acceleration-service) to help with infrequent, large data scans\n* For memory-intensive workloads, use a warehouse type of Snowpark\n  Optimized or higher memory resource constraint configurations as\n  appropriate\n* Set appropriate auto-suspend settings - longer for high cache use,\n  lower for no cache reuse\n* Set appropriate warehouse query timeout settings for the workload and\n  the use cases it supports.\n\nUsing the principles above ensures that your compute costs are well\nmanaged and balanced with optimal benefits.\n\n**Separate warehouses by workload**\n\nDifferent workloads (e.g., data engineering, analytics, AI and\napplications) have varying characteristics. [Separating these to be serviced by different virtual warehouses](https://docs.snowflake.com/en/user-guide/warehouses-considerations) can help ensure relevant features in Snowflake can be utilized.\n\nSome examples of this include:\n\n* Optimize dashboards and reports by [reusing the warehouse SSD cache](https://docs.snowflake.com/en/user-guide/warehouses-considerations) for repeated select queries. This can be achieved by configuring a\n  longer warehouse autosuspend setting.\n* Loading large volumes of data using [optimal file sizes](https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare) and utilizing all available threads in a virtual warehouse\n* Processing large data sets for AI workloads using memory-optimized\n  warehouses\n\n**Warehouse sizing**\n\nMapping the workload to the [right warehouse size](https://docs.snowflake.com/en/user-guide/performance-query-warehouse-size) and configuration is an important consideration of warehouse design.\nThis should consider several factors like query completion time,\ncomplexity, data size, query volume, SLAs, queuing, and balancing\noverall cost objectives. Warehouse sizing involves a cost-benefit\nanalysis that balances performance, cost, and human expectations. Humans\noften have expectations that their queries will not be queued or take a\nlong time to complete, so it is recommended to have dedicated warehouses\nfor teams.\n\nRecommendations for choosing the right-sized warehouse include:\n\n* Follow the principles outlined above and understand that this is a\n  continuous improvement process.\n* Choose a size based on the estimated or actual workload size and\n  monitor.\n* Utilize Snowflake's extensive telemetry data, such as [QUERY\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) and [WAREHOUSE\\_METERING\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history) ,\n  to validate that the warehouse size is impacting the metrics you care\n  about in the direction you intend.\n\n**Optimal warehouse settings**\n\nWhile Snowflake strives for minimal knobs and self-managed tuning, there\nare situations where selecting the right settings for warehouses can\nhelp with optimal cost and/or performance. Some of the key [warehouse settings](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse) include\n\n* Auto suspend\n* Multi-cluster settings\n* Warehouse resource constraints\n* Warehouse type\n\nTo maintain an optimal balance between cost and performance, regularly\nmonitor your resource usage (e.g., weekly or monthly) and set up\nresource monitors to alert you to high credit consumption. When workload\ndemands change, adjust your settings as needed.\n\n**Warehouse consolidation**\n\nIf you find yourself with an excess of provisioned warehouses or a shift\nin workloads necessitating consolidation, apply the aforementioned\nprinciples. Begin with the least utilized warehouses and migrate their\nworkloads to an existing warehouse that handles similar tasks.\n\nThe [WAREHOUSE\\_LOAD\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_load_history) view can help you assess the average number of queries running on a\nwarehouse over a specific period. A useful benchmark is to aim for a\nwarehouse running queries 80% of the time it's active. Continuously\nmonitor your key metrics to ensure they still meet SLA goals and adjust\nwarehouse settings as needed.\n\n#### Leverage Managed Services\n\nTo achieve significant operational efficiency and predictable costs,\nprioritize the use of serverless and managed services. These services\neliminate the need to manage underlying compute infrastructure, allowing\nyour organization to pay for results rather than resource provisioning\nand scaling. Evaluate the following servterless features to reduce costs\nand enhance performance in your environment.\n\n**Storage optimization**\n\nSnowflake offers several serverless features that automatically [manage and optimize your tables](https://docs.snowflake.com/en/user-guide/performance-query-storage) ,\nreducing the need for manual intervention while improving query\nperformance. The following features ensure your data is efficiently\norganized, allowing for faster and more cost-effective qeuerying without\nthe burden of user management.\n\n**Automatic Clustering** is a background process in Snowflake that\norganizes data within a table by sorting it according to predefined\ncolumns. This process is critical for optimizing query performance and\nreducing costs. Benefits include:\n\n* **Improved query pruning:** By sorting data, automatic clustering\n  enables more effective pruning in SQL WHERE clauses, meaning less data\n  needs to be scanned for a given query.\n* **Faster joins:** Clustering also results in quicker and more\n  efficient join operations.\n* **Cost-efficient queries:** These benefits ultimately result in faster\n  and more cost-effective query execution.\n\n**Considerations and Best Practices:**\n\n* **Careful tuning:** It's essential to tune automatic clustering\n  carefully. Overuse of clustering keys, use of highly selective keys,\n  or frequent data manipulation language (DML) operations can\n  significantly increase clustering costs.\n* **Infrequently queried, frequently updated tables:** Exercise caution\n  with tables that are often updated but rarely queried, as clustering\n  costs may outweigh performance improvements.\n* **Cost estimation:** Before enabling, estimate clustering costs for a\n  table using Snowflake's system function for a preliminary cost/benefit\n  analysis.\n* **Strategic cluster key selection:** Optimizing clustering key\n  selection is vital to strike a balance between cost and performance.\n\n**Search Optimization Service (SOS)** enhances the performance of point\nlookup searches by creating a persistent search access path. Its primary\nvalue lies in achieving better pruning for these specific query types,\nwhich is critical for applications requiring quick response times. They\ncan be used in combination with auto-clustering and [Snowflake Optima service](https://docs.snowflake.com/en/user-guide/snowflake-optima) .\n\n**Considerations for SOS:**\n\n* **Excessive indexed columns:** Avoid having too many indexed columns,\n  especially on high-churn tables, as maintenance costs can become\n  substantial.\n* **Inefficient cost-benefit ratio:** Tables that are frequently updated\n  but infrequently queried for point lookups can lead to an inefficient\n  cost/benefit ratio.\n* **Cost estimation:** Before enabling, estimate SOS costs on a given\n  table using a Snowflake system function to perform a preliminary\n  cost/benefit analysis. Index selection is crucial for balancing\n  performance and cost, a topic also addressed in the Workload\n  Optimization section.\n\n**Materialized Views (MVs)** are pre-computed query results stored as a\nseparate table and automatically maintained by Snowflake.\n\n**Benefits of MVs:**\n\n* **Cost efficiency:** It is often cheaper to update a materialized view\n  than to repeatedly execute full scans of large, complex base tables.\n* **Alternative table order:** MVs can provide an alternative table\n  order for queries that do not align with your existing clustering\n  design.\n\n**Considerations for MVs:**\n\n* **Infrequently queried, frequently updated tables:** Avoid creating\n  materialized views on tables that are frequently updated but rarely\n  queried, as automated maintenance costs will negate any potential\n  query cost savings.\n* **Clustering differences:** Be aware of clustering differences between\n  the base table and the materialized view, as this can lead to high\n  maintenance costs.\n\n**[Query Acceleration Service](https://docs.snowflake.com/en/user-guide/query-acceleration-service) (QAS):** QAS is a serverless feature that provides a burst of additional\ncompute resources to accelerate specific parts of a query, rather than\nreplacing an appropriately sized warehouse. It's particularly beneficial\nfor large I/O operation queries, eliminating the need to manually scale\nwarehouses up or down. QAS also helps speed up query execution when\ntable clustering cannot be altered due to other workload dependencies. A\ncost-benefit analysis should always be performed to ensure that the\ncredit consumption from QAS is justified by the performance improvement\nand the avoided cost of a larger warehouse.\n\n**Serverless tasks:** Serverless tasks enable the execution of SQL\nstatements or stored procedures on a user-defined schedule, eliminating\nthe need for a user-managed virtual warehouse. This is a cost-effective\nsolution for infrequent workloads where a warm cache offers minimal\nvalue, or for unpredictable workloads that don't utilize a minimum\n60-second usage.\n\n#### Data storage types & lifecycle management\n\nNext to compute, storage often represents the second-highest cost\ncomponent in Snowflake. Effective storage governance is a critical\nconcern for many industries due to federal and global regulations.\nSnowflake's default settings prioritize maximum data protection, which\nmay not always align with the requirements of every workload or\nenvironment. This section focuses on how to manage and configure\nstorage-related settings appropriately, ensuring that storage costs\nremain reasonable and deliver business value.\n\n**Optimizing table volume and auditing usage**\n\n* **Review policy-driven data lifecycle management:**\n  \n    + **Time Travel & Fail-safe:** Set the [DATA\\_RETENTION\\_TIME\\_IN\\_DAYS](https://docs.snowflake.com/en/sql-reference/parameters) parameter on a per-table or per-schema basis to the minimum required\n        for your business needs. For transient data, use [TRANSIENT](https://docs.snowflake.com/en/user-guide/tables-temp-transient) tables to eliminate Fail-safe costs.\n    + **Retained for clone:** Be mindful of cloning operations. While\n        zero-copy cloning is cost-effective initially, any subsequent DML\n        (Data Manipulation Language) operations on the clone will create new\n        micro-partitions, increasing storage costs. It is recommended to\n        drop clones when they are no longer needed.\n* **Be aware of high-churn tables:**\n  \n+ If a table is updated consistently, inactive storage (Time Travel &\n        Fail-safe data) can grow at a much faster rate than active storage.\n        A high churn table is generally characterized as one that has 40% or\n        more of its storage inactive. Therefore, aligning both the retention\n        time and the use of an appropriate table type with business and\n        recovery requirements is paramount to keeping costs under control.\n        Review High Churn tables on a consistent basis to ensure their\n        configuration is as desired.\n* **Proactively clean up unused objects:**\n  \n    + **Large tables that are never queried:** Establish a process to\n        identify, rename, and eventually drop tables that have not been\n        queried for an extended period. You can use the [ACCESS\\_HISTORY](https://docs.snowflake.com/en/user-guide/access-history) account usage view to review the last time a table was selected\n        from. Snowflake’s [Cost Insights](https://docs.snowflake.com/en/user-guide/cost-insights) will also check weekly for unused tables on your behalf.\n    + **Short-lived permanent tables:** For staging or intermediate data\n        that is rebuilt frequently, use [TRANSIENT](https://docs.snowflake.com/en/user-guide/tables-temp-transient) or [TEMPORARY](https://docs.snowflake.com/en/user-guide/tables-temp-transient) tables instead of permanent tables to avoid unnecessary Time Travel\n        and Fail-safe storage fees.\n* **Perform catalog & stage management:**\n  \n+ **Data archiving:** For historical data that is rarely accessed,\n        consider moving it to cooler tiers with [storage lifecycle policies](https://docs.snowflake.com/LIMITEDACCESS/storage-lifecycle-policy/storage-lifecycle-policies) or deleting it altogether.\n\n**Optimizing Managed Data Structures and Access**\n\n* You do not always need to define cluster keys for all tables (unlike\n  many other relational database management systems) if Snowflake's\n  natural data loading maintains consistent micro-partition min/max\n  values relative to your query patterns. Additionally, you can disable\n  Auto-Clustering on a table while keeping its cluster key definition\n  without incurring extra costs.\n* **Infrequently used materialized views and search optimization paths:** Materialized Views and search optimization paths can incur\n  unnecessary storage and compute costs if they are no longer actively\n  utilized. Materialized Views are most effective for stable data tables\n  with repeated complex aggregations or joins, while search optimization\n  is designed for high-speed point lookup queries. Snowflake’s [Cost Insights feature](https://docs.snowflake.com/en/user-guide/cost-insights) can help identify instances where these objects are rarely used,\n  prompting a review to determine if their performance benefits still\n  outweigh their associated costs.\n\n#### Limit data transfer\n\nData egress, the transfer of data from one cloud provider or region to\nanother, can incur substantial costs, particularly when handling large\ndata volumes. Implementing appropriate tools and best practices is\nessential to minimize these data transfer expenses and maximize business\nvalue when data egress is necessary.\n\n**Tooling: Enable proactive cost management**\n\nLeverage Snowflake's native features to gain visibility and control over\ndata transfer costs before they become a significant expense.\n\n* **Egress Cost Optimizer (ECO):** For providers of data products on the\n  Snowflake Marketplace or private listings, enabling the [Egress Cost Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) (ECO) at the organizational level is a critical best practice. ECO is\n  an automated feature for listings with Cross-Cloud Auto-Fulfillment.\n  It intelligently routes data through a Snowflake-managed cache,\n  allowing you to pay a one-time egress cost for the initial data\n  transfer. After that, expanding to new regions incurs zero additional\n  egress costs for the same dataset. This is a powerful tool for scaling\n  your data sharing without compounding data transfer fees.\n* **Monitoring and alerts:** To effectively manage data transfer costs,\n  utilize Snowflake's [DATA\\_TRANSFER\\_HISTORY](https://docs.snowflake.com/en/sql-reference/organization-usage/data_transfer_history) telemetry view. This view provides detailed insights into data\n  movement between different regions and clouds. Establish dashboards\n  and alerts to meticulously track this usage, enabling prompt detection\n  of any unexpected cost increases.\n\n**Architectural best practices: Design for minimal data movement**\n\nMinimizing data transfer costs for your workloads heavily depends on the\narchitecture of your data pipelines and applications. Adhere to the\nfollowing best practices to achieve this:\n\n* **Unloading Data:**\n  \n    + **Compress data:** When using the [COPY INTO <location>](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location) command to unload data, always use a compression format (e.g., GZIP,\n        BZIP2, or Brotli). This dramatically reduces the volume of data\n        transferred and directly lowers egress costs.\n    + **Filter before unloading:** Before unloading a large dataset, use\n        SQL to filter and transform the data. Unload only the required\n        columns and rows to minimize the volume of data that must leave the\n        Snowflake environment.\n* **Data replication:** Replicating a database to a Snowflake account in\n  a different geographical region or on a different cloud provider\n  incurs data transfer fees. While useful for disaster recovery, this\n  can become expensive if not managed carefully.\n  \n    + **Targeted replication:** If a full database replica is not\n        required, use [replication groups](https://docs.snowflake.com/en/user-guide/account-replication-intro) to replicate only the necessary databases or schemas. This granular\n        approach ensures you only pay for the data you absolutely need to\n        move.\n    + **Consider refresh cadence:** your frequency of refresh will affect\n        the amount of data that is replicated, since it acts like a snapshot\n        rather than every incremental change. Incremental ETL practices are\n        recommended even more with data that is being replicated vs full\n        table reloads.\n* **External network access and functions:**\n  \n    + **Minimize data egress:** [External functions](https://docs.snowflake.com/en/sql-reference/external-functions) and [external network access](https://docs.snowflake.com/en/developer-guide/external-network-access/external-network-access-overview) ,\n        which call remote services (e.g., APIs), transfer data out of\n        Snowflake. For best practice, filter data _before_ sending it to the\n        external service. Avoid writing functions that pass an entire large\n        table as an input; instead, pass only a small, pre-filtered subset.\n    + **Co-locate services:** If possible, deploy the remote service (like\n        an AWS Lambda or Azure Function) in the same cloud and region as\n        your Snowflake account to eliminate cross-region and cross-cloud\n        egress fees.\n* **Cross-cloud auto-fulfillment:**\n  \n+ **Embrace the cache:** As a data provider, enable [Cross-Cloud Auto-Fulfillment](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment) for your listings. As mentioned under Tooling, this feature\n        automates the replication process and works in conjunction with the\n        Egress Cost Optimizer to ensure you pay a single, upfront cost for\n        data transfer, even as your data product expands to new regions.\n* **Cross-region/cross-cloud Iceberg storage:**\n  \n+ **Centralize your catalog:** While Snowflake supports [Iceberg tables](https://docs.snowflake.com/en/user-guide/tables-iceberg-storage) that are external to its managed storage, be mindful of where the\n        data resides. If your Snowflake account and your Iceberg data are in\n        different regions, querying the Iceberg table will result in egress\n        costs from your cloud provider. For a well-architected solution,\n        keep your Iceberg data and your Snowflake account in the same region\n        to avoid these egress charges.\n\n#### Workload optimization\n\nWorkload optimization focuses on identifying the efficiency of your data\nprocessing activities within Snowflake. This involves a holistic\napproach encompassing the review of query syntax, data pipelines, table\nstructures, and warehouse configurations to minimize resource\nconsumption and improve performance. By addressing inefficiencies across\nthese areas, organizations can significantly reduce costs and accelerate\ndata delivery.\n\n**Query syntax optimization**\n\nInefficient queries often lead to excessive and hidden credit\nconsumption. Organizations can identify performance bottlenecks and\nunderstand the cost impact of specific SQL patterns by using Snowflake\nfeatures and adhering to SQL code best practices. This enables\ndevelopment teams to create more efficient and cost-effective code by\nhighlighting poor performing queries. Refer to the Performance\nOptimization Pillar of the Snowflake Well-Architected Framework for\ndetails on how to do this.\n\n**Utilize query history & insights for highlevel monitoring**\n\nFor broader visibility across all workloads, the Snowsight UI and the\nACCOUNT\\_USAGE schema are indispensable.\n\n* When looking for opportunities to improve, it is best to look for\n  queries that have excessive execution time across similarly run\n  queries. The newly created [Grouped Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) aggregates queries to their [parameterized query hash](https://docs.snowflake.com/en/user-guide/query-hash) and allows users to sort based on key performance indicators like\n  total queries, total latency, bytes scanned, and drill down into the\n  query execution across time. It is recommended to start with queries\n  with outsized latency and query runs.\n* The [Query History](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) page in Snowsight provides a high-level, filterable view of past and\n  currently running queries. It allows for you to drill into the Query\n  Profile for individual query statistics and investigation, even while\n  the query is running.\n* Snowflake's [Query Insights](https://docs.snowflake.com/en/user-guide/query-insights) feature (both a set of account usage tables and query profile\n  attributes) are also in Snowsight and can easily surface queries that\n  would benefit from optimization, such as queries with exploding joins\n  or ineffective filtering criteria that Snowflake will surface on your\n  behalf.\n\n**Leverage the query profile for deep-dive analysis**\n\nAfter identifying problematic queries, the [Query Profile](https://docs.snowflake.com/en/user-guide/ui-snowsight-activity) is an essential tool for understanding the execution plan of a query. It\nprovides a detailed, step-by-step breakdown of every operator involved,\nfrom data scanning to final result delivery. To gain visibility into\ninefficiencies, analysts and developers should regularly use the Query\nProfile to identify common anti-patterns like:\n\n* **Nodes with high execution time:** Pinpoint specific operations\n  (e.g., a complex join or a sort operation) that consume the majority\n  of the query's execution time.\n* **\"Exploding\" joins:** Identify Cartesian products or other\n  inefficient joins that create a much larger number of rows than\n  expected.\n* **Excessive table scanning:** In the profile, compare Partitions\n  scanned to Partitions total. A large number of scanned partitions on a\n  clustered table often indicates an opportunity to improve table\n  pruning by adding or refining cluster keys or modifying WHERE clause\n  predicates.\n* **Data spillage:** Look for operators spilling data to local or remote\n  disk. This indicates that the warehouse memory is insufficient for the\n  operation, resulting in significant performance degradation. This\n  might suggest a need to temporarily increase warehouse size for that\n  specific workload or rewrite the query to consume less memory.\n\n**Programmatically deconstruct queries for automated analysis**\n\nFor advanced use cases and automated monitoring, you can\nprogrammatically access query performance data. The [GET\\_QUERY\\_OPERATOR\\_STATS](https://docs.snowflake.com/en/sql-reference/functions/get_query_operator_stats) function can be used to retrieve the granular, operator-level statistics\nfor a given query ID, showing many of the steps and attributes available\nin the query profile view. This allows you to build automated checks\nthat, for instance, flag any query where a full table scan accounts for\nmore than 90% of the execution time or where data spillage exceeds a\ncertain threshold. This approach helps scale performance visibility\nbeyond manual checks.\n\n**Pipeline optimization**\n\nSnowflake pipeline optimization is about designing and managing data\ningestion and transformation processes that are efficient,\ncost-effective, scalable, and low-maintenance, while balancing business\nvalue and SLAs (service levels for freshness and responsiveness). Key\nlevers include architecture patterns (truncate & load versus incremental\nloads), use of serverless managed services (e.g., Snowpipe, Dynamic\nTables), and auditing loading practices to maximize cost and performance\nbenefits.\n\n**Batch loading**\n\nThe COPY INTO\n( [table](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table) or [location](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location) )\ncommand is a foundational and flexible method for bulk data loading from\nan external stage into a Snowflake table. Its importance lies in its\nrole as a powerful, built-in tool for migrating large volumes of\nhistorical data or loading scheduled batch files. The best practice is\nto use COPY INTO for one-time or large batch data loading jobs, which\ncan then be supplemented with more continuous ingestion methods like\nSnowpipe for incremental data. Additional information regarding COPY\nINTO and general data loading best practices can be found in our\ndocumentation [here](https://docs.snowflake.com/user-guide/data-load-considerations) .\nSome additional best practices are outlined below.\n\n* **File and batch sizing:** Optimal performance is achieved with files\n  sized 100–250 MB compressed. Too few large files or too many very\n  small files reduce load parallelism and reduce efficiency.\n* **Parallelism:** Size and configure your warehouse cluster to match\n  the number and typical size of files to be loaded. (e.g., an XS\n  warehouse has eight threads; to utilize it fully, you need at least\n  eight files).\n* **File organization:** Partition files in external stages by logical\n  paths (date, region, etc.) to allow selective/cost-effective loads and\n  enable easy partition-level reloading.\n* **Pattern filtering:** Use COPY's pattern and files parameter to\n  precisely select the right files for each load, particularly to avoid\n  scanning entire stages.\n* **Resource management:** Use resource monitors and low auto-suspend\n  settings on load warehouses to minimize idle compute costs. For more\n  information here see our section on Resource Monitors.\\*\\* \\*\\*\n\n**Serverless ingestion**\n\nWhile named similarly, [Snowpipe](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro) and [Snowpipe Streaming](https://docs.snowflake.com/en/user-guide/snowpipe-streaming/data-load-snowpipe-streaming-overview) are different serverless methods to ingest data. You would utilize one\nversus the other depending on your SLA requirements for data delivery,\nand based on how data is landing for consumption.\n\n* **Snowpipe (file-based micro-batch ingestion)** : Use Snowpipe for\n  continuous ingestion of files where near real-time availability is\n  acceptable and ingestion frequency is moderate. File size guidance is\n  the same as COPY INTO (100-250MB). This leverages serverless compute\n  to avoid warehouse management overhead.\n* **Snowpipe Streaming (rowset-based, real-time ingestion)** : Use\n  Snowpipe Streaming for very low-latency (sub-five-second) ingestion,\n  high throughput, and streaming sources (e.g., Kafka, Kinesis, and\n  event hubs). You will want to ensure “exactly-once guarantees” with\n  offset tokens for idempotency. Ideally, you would batch insert as much\n  as possible to reduce API call overhead, but avoid building streaming\n  applications just to load large \"batch\" files.\n\n**Data Transformation Optimization**\n\nIn general, there are two major transformation strategies followed in\nSnowflake. One is “truncate & load,” which involves full data\nreplacement and reloading, and one is incrementally loading new data\ninto an object, possibly requiring an upsert operation. Below is some\ngeneral guidance on when to use each.\n\n* **Truncate & load:** This is utilized when full data replacement is\n  acceptable and the dataset volume/size allows for fast reloads. In\n  this case, data change patterns are such that identifying incremental\n  deltas is expensive or unreliable and downstream consumers can\n  tolerate occasional brief periods of incomplete data (during load).\n  \n+ **Best practices:** Schedule loads during off-peak or agreed “quiet”\n        windows. Consider transient or temporary tables to stage incoming\n        data, maximize parallelism during the load, and then do an atomic\n        swap/rename to minimize downtime. Ensure that you carefully manage\n        object dependencies, constraints, and statistical metadata refresh\n        after reloads to ensure proper performance once data is loaded\n* **Incremental loads:** Incremental loads are best when datasets are\n  large and a full reload is too costly, slow, or would create\n  unacceptable latency. Change-data-capture (CDC), event streaming, or\n  other means are available to reliably identify deltas (inserts,\n  updates, deletes). Additionally, in these cases, downstream consumers\n  require near-continuous data availability or very low data latency and\n  freshness.\n  \n    + **Best practices:** Design pipelines to deduplicate, merge, and\n        correctly apply CDC deltas using staging tables, Streams, and merge\n        operations. Ideally, you use Stream objects on source tables to\n        track changes efficiently if utilizing Streams or Tasks, and use\n        Tasks to automate processing. Dynamic tables are also an option;\n        please see below. Additionally, for batch files: organize files by\n        partitioned paths and use file loading patterns that enable max\n        parallelism (see COPY INTO guidance below).\n    + Test workloads on [Snowflake’s Generation 2 warehouses](https://docs.snowflake.com/en/user-guide/warehouses-gen2) that incorporate software improvements for data manipulation.\n\nA great example of truncate & load versus incremental can be seen in\nrefresh strategies for [Dynamic Tables (DTs)](https://docs.snowflake.com/en/user-guide/dynamic-tables-about) .\nThey are also a cost-effective and low-maintenance way to maintain data\npipelines. Dynamic tables provide a powerful, automated way to build\ncontinuous data transformation pipelines with SQL, eliminating the need\nfor manual task orchestration that was historically architected with\nstreams & tasks in Snowflake. [Streams & tasks](https://docs.snowflake.com/en/user-guide/data-pipelines-intro) still have their uses, but general guidance and ease of use see more\nSnowflake users leaning towards DTs for automated data pipelines since\nthe pipeline definitions are defined in one object or in a chain of\nobjects.\n\nThe [key concepts of dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh) are defined in our documentation. However, best practices and\ndetermining when to use DTs versus other methods of pipeline tooling in\nSnowflake still warrant discussion, and are compared in Snowflake’s [documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison) .\n\nIn addition to Snowflake’s published [best practices](https://docs.snowflake.com/en/user-guide/dynamic-table-performance-guide) ,\nconsider the following\n\n* **Default to AUTO refresh** : Override only as needed for predictable\n  SLAs.\n* **Keep incremental refresh-friendly queries simple:** Avoid complex\n  nested joins, CTEs, and limit the number of outer joins per DT. The\n  introduction of complexity for incremental refresh may result in\n  longer times for execution, which in turn could force Snowflake in\n  AUTO to perform a full refresh rather than incremental.\n* **Incremental refresh is optimal** when less than 5% of the rows\n  change between refresh cycles, and source tables are well clustered by\n  relevant keys.\n* **For very complex/large transformations:** Chain multiple DTs for\n  better incrementalization, rather than building one massive DT.\n* **Monitor actual lag and refresh metrics** to adjust lag or warehouse\n  sizing as cost and response time needs evolve.\n* **Prefer a dedicated warehouse for DT refresh** during pipeline\n  development and cost analysis to isolate consumption, then consider\n  sharing for production.\n* **Use transient DTs** for high-throughput, non-critical staging steps\n  to keep storage costs down.\n* **Avoid non-deterministic functions in incremental DTs** (e.g.,\n  random, volatile UDFs, queries depending on CURRENT\\_USER).\n\nMore information on dynamic tables versus streams & tasks versus\nmaterialized views can be found in the Snowflake documentation [here](https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison) .\n\n**Table pruning optimization**\n\nTable scanning operations are one of the most resource-intensive aspects\nof query execution. Minimizing the scan of data partitions in a table\n(called partition pruning) can provide significant improvements to both\nperformance and cost for data operations in Snowflake. The account usage\nviews [TABLE\\_QUERY\\_PRUNING\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/table_query_pruning_history) and [COLUMN\\_QUERY\\_PRUNING\\_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/column_query_pruning_history) provide aggregated data on query execution, showing metrics such as\npartitions scanned and rows matched, which helps identify tables with\npoor pruning efficiency. By analyzing this data, you can determine the\nmost frequently accessed columns that are leading to a high number of\nunnecessarily scanned micro-partitions. Common ways to optimize these\naccess patterns are by using Automatic Clustering and Search\nOptimization.\n\nTo determine tables that can most benefit from re-ordering how data is\nstored, you can review Snowflake’s [best practice](https://medium.com/snowflake/supercharging-snowflake-pruning-using-new-account-usage-views-52530b24bf2e) on how to analyze the TABLE\\_QUERY\\_PRUNING\\_HISTORY and\nCOLUMN\\_QUERY\\_PRUNING\\_HISTORY account usage views. Fundamentally,\nreducing the percentage of partitions in each table pruned to the\npercentage of rows returned in a query will lead to the most optimized\ncost and performance for any given workload.\n\nA table’s Ideal pruning state is scanning the same % of rows matched as\npartitions read, minimizing unused read rows.\n\n**Warehouse optimization**\n\nWarehouse concurrency, type, and sizing can impact the execution\nperformance and cost of queries within Snowflake. Review the compute\noptimization section for more information into the tuning of the\nwarehouse and its effect on cost and performance.\n\n#### Improve continually\n\nOptimization is a continuous process that ensures all workloads not only\ndrive maximum business value but also do so in an optimal manner. By\nregularly reviewing, analyzing, and refining your Snowflake environment,\nyou can identify inefficiencies, implement improvements, and adapt your\nplatform to the ever-evolving business needs. The following set of steps\nwill help you continue to improve your environment as you grow:\n\n**Step 1: Identify & investigate workloads to improve**\n\nBegin by regularly reviewing (usually on a weekly, bi-weekly, or monthly\ncadence) workloads that could benefit from optimization, using\nSnowflake's [Cost Insights](https://docs.snowflake.com/en/user-guide/cost-insights) ,\ndeviations in unit economics or health metrics (from the Visibility\nprinciple), or objects hitting control limits (e.g., queries hitting\nwarehouse timeouts from the Control principle). Once identified,\ninvestigate these findings through the Cost Management UI, Cost Anomaly\ndetection, Query History, or custom dashboards with Account Usage Views\nto pinpoint the root cause. Then, using the recommendations in the\nOptimize Pillar, make improvements to the workload or object.\n\n**Step 2: Estimate & test**\n\nBefore implementing changes, estimate the potential impact on cost and\nperformance. Estimation encompasses both the expected amount of time\nrequired to make a change (for instance, consolidating warehouses will\nnecessitate more coordination effort for teams using the resource than\naltering a configuration setting) as well as the hard cost of\nimplementation. Snowflake provides helpful cost estimation functions for\nserverless features, such as [auto clustering](https://docs.snowflake.com/en/sql-reference/functions/system_estimate_automatic_clustering_costs) and [search optimization service](https://docs.snowflake.com/en/sql-reference/functions/system_estimate_search_optimization_costs) ,\nto help make this a more data-driven process. If an estimation tool is\nnot available, making changes in a development or test environment on a\nsubset of the workload can provide an estimate and expected impact.\n\n**Step 3: Decide & implement**\n\nBased on your estimations and test results, decide whether to move\nforward with the change, ensuring the cost-benefit aligns with\nperformance or business needs. If approved, proceed to productionize the\nchange, integrating it into your live environment.\n\n**Step 4: Monitor & analyze**\n\nFinally, monitor and analyze the implemented changes to track and\nvalidate the change’s success over a period of time. This involves using\nthe same investigation methods, like utilizing the Cost Management UI\nand Account Usage Views, and comparing cost and performance metrics\nbefore and after the change to articulate the business impact. Translate\nthe technical improvements into tangible business benefits. For example,\n\"Optimizing this query reduced monthly warehouse costs by $X and\nimproved report generation time by Y minutes, allowing business users to\nmake faster decisions.\" This helps to both demonstrate the value of your\noptimization efforts to stakeholders and business value to the company.\nFinally, course-correct as needed depending on the results of the\nmonitoring\n\nThis continual improvement framework is the culmination of all subtopics\nwithin the Cost Optimization Pillar and provides a consistent way for\nyou to grow healthily on Snowflake.\n\nUpdated Nov 24, 2025\n\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n\n##### On this page\n\nOverview\n\nPrinciples\n\nRecommendations\n\nBusiness Impact\n\nVisibility\n\nControl\n\nOptimize\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\n\\*\n\n\\*\n\n\\* Country United States Canada United Kingdom Germany France Australia "}],"errors":[],"warnings":null,"usage":[{"name":"sku_extract_excerpts","count":5},{"name":"sku_extract_full","count":5}]}

{"search_id":"search_035c6a0a118c431684b8681e02da22d5","results":[{"url":"https://www.snowflake.com/en/blog/collect-logs-traces-snowflake-apps/","title":"Collect Logs and Traces From Your Snowflake Applications","publish_date":"2024-08-21","excerpts":["blog\nSection Title: Category\nContent:\nAI/ML At Snowflake Partner & Customer Value Industry Solutions Product & Technology Strategy & Insights\nData Engineering\nOct 30, 2023 | 4 min read\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables\nContent:\nWe are excited to announce the general availability of Snowflake\nEvent Tables\nfor logging and tracing, an essential feature to boost application observability and supportability for Snowflake developers.\nIn our conversations with developers over the last year, weâ€™ve heard that monitoring and observability are paramount to effectively develop and monitor applications. But previously, developers didnâ€™t have a centralized, straightforward way to capture application logs and traces.\nEnter the new Event Tables feature, which helps developers and data engineers easily instrument their code to capture and analyze logs and traces for all languages: Java, Scala, JavaScript, Python and Snowflake Scripting.\nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables\nContent:\nWith Event Tables, developers can instrument logs and traces from their UDFs, UDTFs, stored procedures, Snowflake Native Apps and Snowpark Container Services, then seamlessly route them to a secure, customer-owned Event Table. Developers can then query Event Tables to troubleshoot their applications or gain insights into performance and code behavior.\nLogs and traces are collected and propagated via Snowflakeâ€™s telemetry APIs, then automatically ingested into your Snowflake Event Table.\nSection Title: ... > Simplify troubleshooting in Snowflake Native Apps\nContent:\nEvent Tables are also supported for Snowflake Native Apps. When a Snowflake Native App runs, it is running in the consumerâ€™s account, generating telemetry data thatâ€™s ingested into their active Event Table.\nOnce the consumer enables event sharing, new telemetry data will be ingested into both the consumer and provider Event Tables. Now the provider has the ability to debug the application thatâ€™s running in the consumerâ€™s account. The provider only sees the telemetry data that is being shared from this data applicationâ€”nothing else.\nSection Title: ... > Improve reliability across a variety of use cases\nContent:\nYou can use Event Tables to capture and analyze logs for various use cases: * As a data engineer building UDFs and stored procedures within queries and tasks, you can instrument your code to analyze its behavior based on input data.\nAs a Snowpark developer, you can instrument logs and traces for your Snowflake applications to troubleshoot and improve their performance and reliability.\nAs a Snowflake Native App provider, you can analyze logs and traces from various consumers of your applications to troubleshoot and improve performance.\nSection Title: ... > Improve reliability across a variety of use cases\nContent:\nSnowflake customers ranging from Capital One to phData are already using Event Tables to unlock value in their organization. â€œThe Event Tables feature simplifies capturing logs in the observability solution we built to monitor the quality and performance of Snowflake data pipelines in Capital One Slingshot,â€ says Yudhish Batra, Distinguished Engineer, Capital One Software. â€œEvent Tables has abstracted the complexity associated with logging from our data pipelinesâ€”specifically, the central Event Table gives us the ability to monitor and alert from a single location.â€\nAs phData migrates its Spark and Hadoop applications to Snowpark, the Event Tables feature has helped architects save time and hassle.\nâ€œWhen working with Snowpark UDFs, some of the logic can become quite complex. In some instances, we had thousands of lines of Java code that needed to be monitored and debugged,â€ says Nick Pileggi,\nPrincipal Solutions Architect at phData\nSection Title: ... > Improve reliability across a variety of use cases\nContent:\n. â€œBefore Event Tables, we had almost no way to see what was happening inside the UDF and correct issues. Once we rolled out Event Tables, the amount of time we spent testing dropped significantly and allowed us to have debug and info-level access to the logs we were generating in Java.â€\nOne large communications service provider also uses logs in Event Tables to capture and analyze failed records during data ingestion from various external services to Snowflake. And a Snowflake Native App provider offering geolocation data services uses Event Tables to capture logs and traces from their UDFs to improve application reliability and performance.\nWith Event Tables, you now have a built-in place to easily and consistently manage logging and tracing for your Snowflake applications. And in conjunction with other features such as Snowflake Alerts and Email Notifications, you can be notified of new events and errors in your applications.\n ... \nSection Title: Collect Logs and Traces From Your Snowflake Applications With Event Tables > ... > Share Article\nContent:\n[](https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps&title=Collect+Logs+and+Traces+From+Your+Snowflake+Applications+With+Event+Tables)\n[](https://x.com/intent/post?url=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps&text=Collect+Logs+and+Traces+From+Your+Snowflake+Applications+With+Event+Tables)\n[](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.snowflake.com%2Fcontent%2Fsnowflake-site%2Fglobal%2Fen%2Fblog%2Fcollect-logs-traces-snowflake-apps)\n ... \nSection Title: ... > Alerts and Observability for Pipeline Monitoring and Cost Management\nContent:\nShiyi Gu\nNov 23, 2022 | 5 min read\nSubscribe to our blog newsletter\nGet the best, coolest and latest delivered to your inbox each week\nBy submitting this form, I understand Snowflake will process my personal information in accordance with their Privacy Notice."]},{"url":"https://www.metaplane.dev/snowflake-native-app","title":"Snowflake Native App - Metaplane","excerpts":["New Metaplane and Datadog join forces to bring software and data observability together.\nLearn More\nPlatform\nPlatform Overview Learn all about Metaplane\nData CI/CD Prevent data quality issues in PRs\nMonitoring and anomaly detection Be the first to know when something breaks\nSchema change alerts Increase awareness for the entire data team\nLineage and impact analysis Data pipeline visibility from source to usage\nJob monitoring Eliminate pipeline latency issues\nData Insights Explore how your data is being used\nAlerting One rally point for all of your stakeholders\nStart monitoring today Sign up, connect your tool(s) in minutes, and start monitoring right away.\nCustomers Pricing Integrations\nIntegrations\nIntegrations overview Learn all the tools Metaplane integrates with\nSection Title: DATA WAREHOUSES\nContent:\nBigQuery [Clickhouse](https://docs.metaplane.dev/docs/clickhouse) [Databricks](https://docs.metaplane.dev/docs/databricks) Redshift Snowflake [AWS S3](https://docs.metaplane.dev/docs/s3)\nSection Title: DATA WAREHOUSES > Ingestion\nContent:\nAirbyte Fivetran\nSection Title: DATA WAREHOUSES > Ingestion > Transformations\nContent:\ndbt Cloud dbt Core\nSection Title: DATA WAREHOUSES > Ingestion > Transformations > transactional databases\nContent:\nMySQL PostgreSQL SQL Server\nSection Title: DATA WAREHOUSES > Ingestion > Transformations > REVERSE ETL\nContent:\nCensus Hightouch\nSection Title: DATA WAREHOUSES > Ingestion > Transformations > notifications\nContent:\nJira\nSection Title: DATA WAREHOUSES > Ingestion > Transformations > orchestration\nContent:\nAirflow\nSection Title: DATA WAREHOUSES > Ingestion > Transformations > Business intelligence\nContent:\nLooker Metabase Mode PowerBI Sigma Tableau Hex\nResources\n[Docs Get started with Metaplane in under 15 minutes](https://docs.metaplane.dev/) Resource Library Webinars, podcasts, live events, and white paper downloads Changelog See the latest Metaplane product updates Data Observability Guides How to get started with the fundamentals of data observability Blog â€” In Data We Trust Guides and perspectives from data leaders\nBook a demo\nSnowflake Native App\nSection Title: Use your existing credits with our Snowflake Native App\nContent:\nAll of Metaplaneâ€™s robust data observability features, brought directly into your Snowflake environment.\nTalk to sales\nPay with credits\nSkip the procurement process\nHelping the worldâ€™s best data teams TRUST THEIR DATA\nSection Title: ... > Get our Snowflake native app and use your existing credits\nContent:\n[Setup now, for free](https://app.snowflake.com/marketplace/listing/GZTSZ7NSX7E/metaplane-metaplane-data-observability-platform)\nSection Title: ... > End-to-end observability inside Snowflake\nContent:\nAll of Metaplaneâ€™s robust data observability features brought directly into your Snowflake environment.\nSection Title: ... > Your data never leaves your warehouse\nContent:\nWith our native app, you can install monitors directly within your warehouse, helping keep your data all the more secure.\nSection Title: Use your existing credits with our Snowflake Native App > ... > Use your Snowflake credits\nContent:\nPay for monitors with your existing Snowflake credits and skip the extra procurement step.\nSection Title: ... > Without Metaplane, we wouldnâ€™t be as proactive with data quality. There would be a lot of u...\nContent:\nAdam Smith\nAnalytics Engineer @ Imperfect Foods\nSection Title: Book a Demo\nContent:\nWeâ€™ll show you the product (yes, really) and talk through how to use Metaplane to achieve your data observability goals.\nIf you cannot find a time that works for you, please email team@metaplane.dev and we'll try to accommodate another time.\nSection Title: Book a Demo > Marion Pavillet\nContent:\nSenior Analytics Engineer\nSenior Analytics Engineer\nSection Title: Book a Demo > Book a Demo\nContent:\nWeâ€™ll show you the product (yes, really) and talk through how to use Metaplane to achieve your data observability goals.\nIf you cannot find a time that works for you, please email team@metaplane.dev and we'll try to accommodate another time.\nBuild trust in your data\nDetect and fix problems quickly\nMonitor what matters most to you\nSection Title: ... > 95% test coverage. In just a few clicks, Mux got full visibility into the quality and perfo...\nContent:\nMarion Pavillet\nSenior Analytics Engineer\nData observability for high-leverage data teams. Save time and preserve trust by being the first to know of data quality issues.\nSection Title: Book a Demo > Book a Demo > Contact\nContent:\nteam@metaplane.dev\nSection Title: Book a Demo > Book a Demo > Platform\nContent:\nMonitoring and anomaly detection\nUsage analytics\nData CI/CD\nSchema change alerts\nLineage and impact analysis\nJob monitoring\nAlerts\nIntegrations\nSection Title: Book a Demo > Book a Demo > RESOURCES\nContent:\n[Docs](https://docs.metaplane.dev)\nChangelog\nBlog\nResources\nData observability guide\ndbt Alerting tool\nInteractive query builder\nSection Title: Book a Demo > Book a Demo > company\nContent:\nAbout\nCustomers\nPricing\nCareers\nInvestors\nSection Title: Book a Demo > Book a Demo > LEGAL\nContent:\nTerms\nPrivacy\nCookies\nSection Title: Book a Demo > Book a Demo > Subscribe to stay updated\nContent:\nStay up-to-date with the latest product updates and resources.\nEmail address\nSubscribe\nError subscribing.\nSubscribed! We'll email you with updates."]},{"url":"https://docs.snowflake.com/en/developer-guide/logging-tracing/logging-tracing-overview","title":"Logging, tracing, and metrics | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nDeveloper Logging, Tracing, and Metrics\nSection Title: Logging, tracing, and metrics Â¶\nContent:\nYou can record the activity of your Snowflake function and procedure handler code (including code you write using Snowpark APIs ) by capturing log messages and trace events from the code as it executes.\nOnce youâ€™ve collected the data, you can query it with SQL to analyze the results.\nLogging, tracing, and metrics are among the observability features Snowflake provides to make it easier for you to debug and optimize\napplications. Snowflake captures observability data in a structure based on the [OpenTelemetry](https://opentelemetry.io/) standard.\nIn particular, you can record and analyze the following:\nLog messages â€” Independent, detailed messages with information about the state of a\nspecific piece of your code.\nMetrics data â€” CPU and memory metrics that Snowflake generates.\nTrace events â€” Structured data you can use to get information spanning and grouping\nmultiple parts of your code.\nSection Title: Logging, tracing, and metrics Â¶ > Get started Â¶\nContent:\nUse the following high-level steps to begin capturing and using log and trace data.\nSection Title: Logging, tracing, and metrics Â¶ > Get started Â¶\nContent:\nEnsure that you have an active event table. You can do one of the following:Snowflake collects telemetry data from your code in the event table. Use the default event table that is active by default. Create and set as active an event table . Set telemetry levels so that data is collected.With levels, you can specify which data â€“ and how much data â€“ is collected. Make sure the levels are set correctly. Begin emitting log or trace data from handler code.Once youâ€™ve created an event table and associated it with your account, you can use an API in your handlerâ€™s language to emit log\nmessages.\nSection Title: Logging, tracing, and metrics Â¶ > Get started Â¶\nContent:\nAfter youâ€™ve captured log and trace data, you can query the data to analyze the results.For more information on instrumenting your code, see the following:\nLogging messages from functions and procedures\nTrace events for functions and procedures\nQuery the event table to analyze collected log and trace data.For more information, see the following:\nViewing log messages\nViewing metrics data\nViewing trace data\nSection Title: Logging, tracing, and metrics Â¶ > Set telemetry levels Â¶\nContent:\nYou can manage the level of telemetry data stored in the event table â€” such as log, trace, and metrics data â€” by setting the level\nfor each type of data. Use level settings to ensure that youâ€™re capturing the amount and kind of data you want.\nFor more information, see Setting levels for logging, metrics, and tracing .\nSection Title: Logging, tracing, and metrics Â¶ > Compare log messages and trace events Â¶\nContent:\nThe following table compares the characteristics and benefits of log messages and trace events.\nSection Title: Logging, tracing, and metrics Â¶ > Compare log messages and trace events Â¶\nContent:\n| Characteristic | Log entries | Trace events |\n| Intended use | Record detailed but unstructured information about the state of your code. Use this information to understand what happened during |  |\n| a particular invocation of your function or procedure. | Record a brief but structured summary of each invocation of your code. Aggregate this information to understand behavior of your |  |\n| code at a high level. |  |  |\n| Structure as a payload | None. A log entry is just a string. | Structured with attributes you can attach to trace events. Attributes are key-value pairs that can be easily queried with a SQL query. |\n| Supports grouping | No. Each log entry is an independent event. | Yes. Trace events are organized into spans. A span can have its own attributes. |\nSection Title: Logging, tracing, and metrics Â¶ > Compare log messages and trace events Â¶\nContent:\n| Characteristic | Log entries | Trace events |\n| Intended use | Record detailed but unstructured information about the state of your code. Use this information to understand what happened during |  |\n| a particular invocation of your function or procedure. | Record a brief but structured summary of each invocation of your code. Aggregate this information to understand behavior of your |  |\n| code at a high level. |  |  |\n| Structure as a payload | None. A log entry is just a string. | Structured with attributes you can attach to trace events. Attributes are key-value pairs that can be easily queried with a SQL query. |\n| Quantity limits | Unlimited. All log entries emitted by your code are ingested into the event table. | The number of trace events per span is capped at 128. There is also a limit on the number of span attributes. |\n ... \nSection Title: Logging, tracing, and metrics Â¶ > Compare log messages and trace events Â¶\nContent:\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nGet started\nSet telemetry levels\nCompare log messages and trace events\nRelated content\nEnabling telemetry collection\nLogging messages from functions and procedures\nTrace events for functions and procedures\nCollecting metrics data\nLanguage: **English**\nEnglish\nFranÃ§ais\nDeutsch\næ—¥æœ¬èªž\ní•œêµ­ì–´\nPortuguÃªs"]},{"url":"https://www.snowflake.com/en/developers/guides/getting-started-with-snowflake-trail-for-observability/","title":"Getting Started with Snowflake Trail for Observability","excerpts":["Section Title: Getting Started with Snowflake Trail for Observability\nContent:\nMatt Barreiro\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/getting-started-with-snowflake-trail-for-observability)\nSection Title: Getting Started with Snowflake Trail for Observability > Overview\nContent:\nSnowflake Trail is Snowflake's suite of observability capabilities that enable its users to better monitor, troubleshoot, debug and take actions on pipelines, apps, user code and compute utilizations.\nAs you can see above, Snowflake Trail utilizes core observability data â€” logs, metrics, traces, events, alerts, and notifications â€” to provide comprehensive workload monitoring across AI, applications, pipelines, and infrastructure.\nThis quickstart is intended to help tie together all the components of Snowflake Trail, and in turn, help you get started with Observability in Snowflake. While this quickstart will walk you through the basics of enabling and viewing telemetry, you will need to dive deeper into each area in order to fully understand Snowflake Trail. So wherever possible, links will be provided to additional quickstarts, documentation, and resources.\nSection Title: Getting Started with Snowflake Trail for Observability > Overview > What You'll Learn:\nContent:\nHow to enable telemetry collection in your Snowflake account\nThe difference between system views and telemetry data in Snowflake\nHow to use the Snowsight monitoring interfaces for traces, logs, and query history\nHow to access and understand observability data in Snowflake\n ... \nSection Title: ... > (Option 2): Setting Telemetry Levels via SQL\nContent:\nOpen a new SQL worksheet or a [workspace](https://app.snowflake.com/_deeplink/#/workspaces?utm_source=snowflake-devrel&utm_medium=developer-guides&utm_content=getting-started-with-snowflake-trail-for-observability&utm_cta=developer-guides-deeplink) .\nRun the following:\n-- Switch to ACCOUNTADMIN\nUSE ROLE ACCOUNTADMIN;\n-- Set account level values\nALTER ACCOUNT SET LOG_LEVEL = 'INFO';\nALTER ACCOUNT SET METRIC_LEVEL = 'ALL';\nALTER ACCOUNT SET TRACE_LEVEL = 'ALWAYS';\n```\n\nCopy\n```\nNote that valid and default values are as follows:\n ... \nSection Title: Getting Started with Snowflake Trail for Observability > Enabling Telemetry > Additional Resources\nContent:\n[Setting levels for logging, metrics, and tracing | Snowflake Documentation](https://docs.snowflake.com/en/developer-guide/logging-tracing/telemetry-levels)\n[How Snowflake determines the level in effect | Snowflake Documentation](https://docs.snowflake.com/en/developer-guide/logging-tracing/telemetry-levels)\n ... \nSection Title: Getting Started with Snowflake Trail for Observability > Logs > Why Logs Are Useful\nContent:\nLogs help you:\nDebug issues by providing detailed error messages and stack traces\nMonitor application behavior and performance\nAudit operations and track important events\nUnderstand the flow of execution in complex procedures\nIdentify patterns in application usage or errors\nSection Title: Getting Started with Snowflake Trail for Observability > Logs > Accessing Logs in Snowsight\nContent:\nTo view logs in Snowsight:\nNavigate to **Monitoring** Â» **Traces & Logs**\nClick on the **Logs** tab to switch from the default traces view.\n(Optional) Use the filters to find specific logs. For example:\n**Time Range** can be set either by using the drop-down or by clicking on the graph.\n**Severity** can be used to select specific log levels (DEBUG, WARN, etc).\n**Languages** allows filtering by handler code language (Python, Java, etc).\n**Database** allows filtering by specific procedures, functions, or applications.\n**Record** allows selecting Logs, Events, or All.\n ... \nSection Title: Getting Started with Snowflake Trail for Observability > AI Observability\nContent:\n**Evaluations** : Use AI Observability to systematically evaluate the performance of your generative AI applications and agents using the LLM-as-a-judge technique. You can use metrics, such as accuracy, latency, usage, and cost, to quickly iterate on your application configurations and optimize performance.\n**Comparison** : Compare multiple evaluations side by side and assess the quality and accuracy of responses. You can analyze the responses across different LLMs, prompts, and inference configurations to identify the best configuration for production deployments.\n**Tracing** : Trace every step of application executions across input prompts, retrieved context, tool use, and LLM inference. Use it to debug individual records and refine the app for accuracy, latency, and cost.\n ... \nSection Title: Getting Started with Snowflake Trail for Observability > Conclusion And Resources > What You Learned\nContent:\n**Enabled telemetry collection** at the account level to start capturing logs, metrics, and traces\n**Know the difference** between System Views (historical data) and Telemetry data (event-driven observability)\n**Utilize traces** to gain end-to-end insight of execution flows and identify performance bottlenecks\n**Analyze logs** to debug issues and monitor application behavior\n**Leverage Query History** to optimize query performance and troubleshoot database operations\n**Monitor data loading** activities through Copy History for all ingestion methods\n**Track automated operations** using Task History for pipeline monitoring\n**Observe Dynamic Tables** refresh patterns and data freshness\n**Explore AI observability** concepts for monitoring AI/ML workloads and Cortex AI functions\n**Build a foundational understanding** of the tools available for observability across your Snowflake workloads"]},{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/monitoring","title":"Use monitoring for an app - Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nEN\nEnglish\nFranÃ§ais\nDeutsch\næ—¥æœ¬èªž\ní•œêµ­ì–´\nPortuguÃªs\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nBuilders\nSnowflake DevOps\nObservability\nSnowpark Library\nSnowpark API\nSpark workloads on Snowflake\nMachine Learning\nSnowflake ML\nSnowpark Code Execution Environments\nSnowpark Container Services\nFunctions and procedures\nLogging, Tracing, and Metrics\nSnowflake APIs\nSnowflake Python APIs\nSnowflake REST APIs\nSQL API\nApps\nStreamlit in Snowflake\nAbout Streamlit in Snowflake\nGetting started\nDeploy a sample app\nCreate and deploy Streamlit apps using Snowsight\nCreate and deploy Streamlit apps using SQL\nCreate and deploy Streamlit apps using Snowflake CLI\nStreamlit object management\nBilling considerations\nSecurity considerations\nPrivilege requirements\nUnderstanding owner's rights\nPrivateLink\nLogging and tracing\nApp development\nRuntime environments\nDependency management\nFile organization\nSecrets and configuration\nEditing your app\nMigrations and upgrades\nIdentify your app type\nMigrate to a container runtime\nMigrate from ROOT_LOCATION\nFeatures\nExternal access\nGit integration\nRow access\npolicies\nSharing Streamlit in Snowflake apps\nSleep timer\nLimitations and library changes\nTroubleshooting Streamlit in Snowflake\n[Streamlit open-source library documentation](https://docs.streamlit.io/)\nSnowflake Native App Framework\nSnowflake Declarative Sharing\nSnowflake Native SDK for Connectors\nExternal Integration\nExternal Functions\nKafka and Spark Connectors\nSnowflake Scripting\nSnowflake Scripting Developer Guide\nTools\nSnowflake CLI\nGit\nDrivers\nOverview\nConsiderations when drivers reuse sessions\nScala versions\nReference\nAPI Reference\nDeveloper Snowflake Native App Framework Monitoring\nSection Title: Use monitoring for an app Â¶\nContent:\nFeature â€” Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nThis topic describes how providers can monitor consumer app health for a Snowflake Native App.\nSection Title: Use monitoring for an app Â¶ > Monitor consumer application health Â¶\nContent:\nYour application can report its health status to Snowflake, which allows you to\nmonitor the health of consumer instances of your app.\nTo report health status, your app uses the system-defined `SYSTEM$REPORT_HEALTH_STATUS(VARCHAR)` function, passing in the health status\nas an enum value:\n`OK` : The consumer instance is healthy.\n`FAILED` : The consumer instance is in an error state.\n`PAUSED` : The consumer manually paused the app.\nYou can use the `LAST_HEALTH_STATUS` and `LAST_HEALTH_STATUS_UPDATED_ON` fields\nof the APPLICATION_STATE view to monitor the health of consumer instances of your app. The `LAST_HEALTH_STATUS` field has the most recent value passed in by the app running in the consumer account.\nThe following code sample demonstrates using the APPLICATION_STATE view\nto retrieve the health status of all consumer instances of your app:\nSection Title: Use monitoring for an app Â¶ > Monitor consumer application health Â¶\nContent:\n```\nSELECT \n    CONSUMER_ORGANIZATION_NAME , \n    CONSUMER_ACCOUNT_NAME , \n    LAST_HEALTH_STATUS , \n    LAST_HEALTH_STATUS_UPDATE_TIME \n FROM \n    SNOWFLAKE . ACCOUNT_USAGE . APPLICATION_STATE \n WHERE \n    PROVIDER_ORG_NAME = '<your_provider_org_name>' \n    AND APPLICATION_NAME = '<your_app_name>' \n ORDER BY \n    LAST_HEALTH_STATUS_UPDATE_TIME DESC ;\n```\nCopy\nThe preceding query may return results similar to the following:\n```\nCONSUMER_ORG_NAME    CONSUMER_ACCOUNT_NAME    LAST_HEALTH_STATUS    LAST_HEALTH_STATUS_UPDATE_TIME \n ------------------   ---------------------    ------------------    ------------------------------- \n consumer_org_1       consumer_account_1       OK                    2024-01-15 10:30:00.000 \n consumer_org_2       consumer_account_2       FAILED                2024-01-15 09:45:00.000 \n consumer_org_3       consumer_account_3       PAUSED                2024-01-14 16:20:00.000\n```\nSection Title: Use monitoring for an app Â¶ > Monitor consumer application health Â¶\nContent:\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nMonitor consumer application health\nRelated content\nLogging, tracing, and metrics\nSection Title: Use monitoring for an app Â¶ > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\n ... \nSection Title: Use monitoring for an app Â¶ > Privacy Preference Center > Your Privacy > Strictly Necessary Cookies\nContent:\nAlways Active\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\nCookies Detailsâ€Ž\n ... \nSection Title: Use monitoring for an app Â¶ > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"]},{"url":"https://medium.com/snowflake/finops-cost-management-in-snowflake-insights-from-a-streamlit-app-92d74477c137","title":"FinOps Cost Management in Snowflake â€” Insights from a Streamlit App | by Eladio RincÃ³n Herrera | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2025-10-17","excerpts":["Section Title: FinOps Cost Management in Snowflake â€” Insights from a Streamlit App > Why FinOps Matters\nContent:\nIn the case of Snowflake, it provides native transparency through `ACCOUNT_USAGE` and `ORGANIZATION_USAGE` views, giving teams near real-time insights into where credits are being consumedâ€”whether in warehouses, queries, or storage. Its credit-based billing model is also directly recognized in the FOCUS specification, ensuring alignment with the industryâ€™s standard for cloud cost and usage data. You can see the details of the FOCUS v1.2 spec [here](https://focus.finops.org/focus-specification/v1-2/) .\n*Note: At the time of writing, FOCUS v2.1 is the latest version. Snowflake mappings are still primarily aligned with v1.2, which means some newer dimensions (e.g., richer invoice identifiers, commitments) require external enrichment pipelines.*\n ... \nSection Title: ... > What is Possible in the Snowflake Portal\nContent:\nThe Snowflake web portal provides native visibility into cost and usage, enabling FinOps practices directly within the platform. From the Billing & Usage section, customers can download detailed usage statements that include credits consumed, billing currency, and invoiced amounts. These statements serve as the source of truth for financial reconciliation and can be mapped into the FOCUS specification for standardized reporting.\nIn addition, the portal exposes Snowsight dashboards for monitoring warehouse utilization, query activity, and service-level costs. Finance and engineering teams can leverage these dashboards to track trends, attribute spend, and spot anomalies without external tools. While deeper FinOps workflows may require exporting data to FOCUS-compliant pipelines, the portal ensures that organizations always have a clear, auditable view of Snowflake consumption and spend.\n ... \nSection Title: ... > Common Metrics Often Analyzed in Snowflake Portal\nContent:\nSee the official [Snowflake docs](https://docs.snowflake.com/en/user-guide/warehouses-overview?utm_source=chatgpt.com) and [Datadogâ€™s Snowflake metrics guide](https://www.datadoghq.com/blog/snowflake-metrics) . **Storage & Data Usage Metrics\n** [Storage metrics](https://select.dev/posts/snowflake-storage?utm_source=chatgpt.com) include costs tied to staging, Time Travel retention, table sizes, and stage usage. These help identify underutilized storage and fine-tune retention policies.\nSection Title: ... > Why Build a Custom Cost Analytics App?\nContent:\nSnowflakeâ€™s native portal and Snowsight dashboards provide useful insights into usage, spend, and warehouse performance â€” great for quick checks and financial reconciliation. But when teams need interactive, FinOps-oriented analysis, a custom Streamlit app inside Snowflake delivers more. By combining live telemetry ( `ACCOUNT_USAGE` ) with pre-aggregated snapshots, applying cost-aware caching, and drilling into query-level cost drivers, the app goes beyond static reporting. It enforces permission checks, adds filters by service or warehouse, and surfaces the underlying SQL so Finance and Engineering share the same transparent view. In short, it transforms Snowflake telemetry into a collaborative cockpit for FinOps, aligning data with FOCUS dimensions and enabling actionable insights, accountability, and optimization at scale.\nDemo Environment:\nPress enter or click to view image in full size\nDaily Trend\nSection Title: FinOps Cost Management in Snowflake â€” Insights from a Streamlit App > A Few Useful Query Patterns\nContent:\n```\n-- Daily credits used by warehouse  \nSELECT  \n  WAREHOUSE_NAME,  \n  TO_DATE(START_TIME) AS USAGE_DATE,  \n  ROUND(SUM(CREDITS_USED), 3) AS WH_CREDITS  \nFROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY  \nWHERE START_TIME >= '2025-08-29' AND START_TIME < '2025-08-30'  \n  --AND WAREHOUSE_NAME = 'MY_WH'  \nGROUP BY 1,2  \nORDER BY 2,1;\n```\n```\n-- Daily credits used across all warehouses  \nSELECT TO_DATE(START_TIME) AS USAGE_DATE, SUM(CREDITS_USED) AS DAILY_CREDITS  \nFROM SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY  \nWHERE START_TIME >= '2025-08-29' AND START_TIME < '2025-08-30'  \nGROUP BY USAGE_DATE  \nORDER BY USAGE_DATE;\n```\n ... \nSection Title: FinOps Cost Management in Snowflake â€” Insights from a Streamlit App > Summary\nContent:\nSnowflake already gives organizations powerful cost visibility through its portal, usage views, and Snowsight dashboards. But for teams adopting FinOps at scale, native reporting is just the starting point. By aligning Snowflakeâ€™s credit-based model with the FOCUS specification, and extending telemetry with a custom Streamlit app, companies can bridge the gap between financial governance and engineering workflows.\nThe result: a collaborative, cost-aware cockpit that empowers Finance, Engineering, and Product teams to share the same transparent view of usage, detect cost drivers early, and keep cloud spending predictable without slowing innovation.\nSection Title: FinOps Cost Management in Snowflake â€” Insights from a Streamlit App > ðŸ“š Further Reading\nContent:\n[Deep Dive: Snowflakeâ€™s Query Cost Attribution â€” Greybeam Blog](https://blog.greybeam.ai/snowflake-cost-per-query)\nImplement Cost Monitoring Dashboards using Snowsight â€” Medium\nSnowflake Database â€” Account Usage & Information Schema â€” Medium\n[Optimize Snowflake Spend with the New Cost Management Interface â€” Snowflake Blog](https://www.snowflake.com/en/blog/optimize-spend-new-cost-management-interface/)\nMastering Snowflake Warehouse Utilization â€” Medium\n[A Deep Dive into Snowflake Storage Costs â€” Select.dev](https://select.dev/posts/snowflake-storage)\n[Key Snowflake Metrics â€” Datadog Blog](https://www.datadoghq.com/blog/snowflake-metrics/)\nFinops\nStreamlit App\nSnowflake\nCloud Computing\nData Analysis\n--\n--\n[](https://medium.com/snowflake?source=post_page---post_publication_info--92d74477c137---------------------------------------)"]},{"url":"https://docs.snowflake.com/en/developer-guide/builders/observability","title":"Observability in Snowflake apps | Snowflake Documentation","excerpts":["Section Title: Observability in Snowflake apps Â¶\nContent:\nThrough observability built into Snowflake, you can ensure that your applications are running as efficiently as possible.\nUsing the practices and features described in this topic, you can make the most of observability features that show you where you\ncan improve your code.\nSection Title: Observability in Snowflake apps Â¶ > What is observability? Â¶\nContent:\nIn an observable system, you can understand whatâ€™s happening internally through external evidence generated by the systemâ€”evidence\nthat includes telemetry data, alerts, and notifications.\nThrough the evidence of internal functioning it provides, observability makes it easier for you to troubleshoot hard-to-understand behaviors\non a production system. This is especially true in a distributed system, where evidence collected from observation provides a view of\nbehavior across multiple components. Rather than disrupting a production environment to diagnose issues, you can analyze the collected\ndata from it.\nWith an observable system, you can start to answer questions such as the following:\nHow well is the system performing?\nWhere is there latency and whatâ€™s causing it?\nWhy is a particular component or process not working as it should?\nWhat improvements can be made?\nSection Title: Observability in Snowflake apps Â¶ > Observability in Snowflake Â¶\nContent:\nSnowflake supports a model that provides built-in observable data while also giving you ways to add more instrumentation where you need it.\nWhile Snowflake provides support for telemetry data such as logs, metrics, and traces (which are typical of observability), it also\nincludes other features you can use to keep track of system usage and performance.\nThe following lists features you can use to receive and analyze system performance and usage.\n|Collected telemetry data |As your application generates logs, metrics, and traces, Snowflake collects that telemetry data in an event table. Using\nSnowsight, you can explore the data, looking for patterns.\nYou can emit custom telemetry into the event table to provide contextual, domain-specific information to expedite debugging. |\n| --- | --- |\n|History Tables |Use the following views and their associated tables to monitor all usage in your account.\nSection Title: Observability in Snowflake apps Â¶ > Observability in Snowflake Â¶\nContent:\nQuery History\nCopy History\nTasks |\n|Alerts and notifications |Alerts allow for customizable triggering conditions, actions, and a schedule, in combination with notification integrations for proactive monitoring. |\n|Extensibility with third-party tools |The Snowflake event table adopts [OpenTelemetry](https://opentelemetry.io/docs/) standards, so your\nSnowflake telemetry can easily be consumed by other ecosystem tools. |\n ... \nSection Title: Observability in Snowflake apps Â¶ > ... > Types of telemetry data Â¶\nContent:\nTo ensure that the telemetry data you collect is broadly useful, Snowflake telemetry is built on the standard [OpenTelemetry](https://opentelemetry.io/docs/) (sometimes called OTel) framework, an incubating project of the Cloud Native Compute Foundation. Through this framework (and APIs and\ntools designed for it), you can reuse collected data with tools besides Snowflake .\nThrough OpenTelemetry, you can instrument application code to add observability where you want it.\nSnowflake event tables collect log, span, and metrics data in the OpenTelemetry data model. The following describes each type of telemetry\ndata collected in an event table.\n|Logs |Logs record individual operations performed by code. Each log message is generated at\na discrete point during the execution of the code.\nSection Title: Observability in Snowflake apps Â¶ > ... > Types of telemetry data Â¶\nContent:\n**Instrumenting code** You can log from your code using libraries standard for the language youâ€™re using, as listed in Logging from handler code .\n**Viewing data** You can view log messages for analysis\neither by querying the event table or looking at the visualizations provided in Snowsight.\nThe following image from Snowsight shows a list of collected log messages for a two-hour period in a single database.\n|\n| --- | --- |\n|Metrics |Metrics are measurements calculated over a time period. These values include CPU and memory measurements.\n**Instrumenting code** Snowflake emits metrics data automatically as your code executes, so you donâ€™t need to instrument your code.\n**Viewing data** You can view metrics data for analysis either by\nquerying the event table or looking at the visualizations provided in Snowsight.\n ... \nSection Title: Observability in Snowflake apps Â¶ > Tools for analysis and visualization Â¶\nContent:\nThe following lists tools you might integrate with Snowflake event tables:\n[Snowflake integration for Datadog](https://docs.datadoghq.com/integrations/snowflake_web/)\nSnowflake integration for Grafana dashboardFor an introduction to using Grafana with Snowflake, see [How to monitor Snowflake with Grafana Cloud](https://grafana.com/blog/2023/05/24/how-to-monitor-snowflake-with-grafana-cloud/) .\n[Snowflake data source for Grafana](https://grafana.com/docs/plugins/grafana-snowflake-datasource/latest/)\n[Snowflake integration for Grafana Cloud](https://grafana.com/docs/grafana-cloud/monitor-infrastructure/integrations/integration-reference/integration-snowflake/)\n[Observe for Snowflake](https://app.snowflake.com/marketplace/listing/GZTYZY3AR0U/observe-inc-observe-for-snowflake) , Observeâ€™s native app for observability\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)"]},{"url":"https://www.unraveldata.com/resources/snowflake-data-observability-buyers-guide/","title":"Snowflake Data Observability Buyerâ€™s Guide | Unravel Data","publish_date":"2025-12-16","excerpts":["Unravel launches free Snowflake native app Read press release\nWHY OUR AI\nPLATFORMDATA OBSERVABILITY FORPLATFORM & PRICINGUSE CASESFeaturedGet a Free Health Check ReportFor Databricks For SnowflakeTour Unravelâ€™s Key Product Features for YourselfExplore Now\nDatabricks\nSnowflake\nGoogle Cloud BigQuery\nAmazon EMR\nCloudera\nPlatform Overview\nAll Integrations & APIs\nPlans & Pricing\nCloud Cost Management & FinOps\nOperations & Troubleshooting\nPipeline & App Optimization\nAutofix & Prevention\nData Quality & Reliability\nCloud Migration\nRESOURCESTOP TOPICSTOP TECHNOLOGIESTOP CONTENT TYPESFeaturedAI Agents: Empower Data Teams With Actionability TMRead MoreData Actionability TM : Empower Your TeamRead More\nAI & Automation\nCost Optimization & FinOps\nTroubleshooting & DataOps\nPerformance & Data Eng\nCloud Migration\nCI/CD\nExplore All\nDatabricks\nSnowflake\nBigQuery\nExplore All\nCustomer Stories\nProduct Docs\nResearch & Reports\nWebinars & Podcasts\nExplore All\nCUSTOMERS\nCOMPANY\nAbout Unravel\nCustomers\nCareers\nPartners\nEvents\nNews & Press\nSUPPORT\nFAQ\n[Customer\nPortal](https://customers.unraveldata.com/login?ec=302&startURL=%2Fs%2F)\n[Documentation](https://docs.unraveldata.com/?lang=en)\nContact Us\nSELF-GUIDED TOUR\nFREE HEALTH CHECK\nSELF-GUIDED TOUR\nFREE HEALTH CHECK\nSnowflake\nSection Title: Snowflake Data Observability Buyerâ€™s Guide\nContent:\nA Platform Leader's Guide to Choosing the Right Solution Snowflake observability is critical for enterprise data teams managing performance, costs, and reliability at scale. As your environment grows, native monitoring tools often fall short. Platform owners [â€¦]\nDownload Now\nSection Title: ... > **A Platform Leader's Guide to Choosing the Right Solution**\nContent:\nSnowflake observability is critical for enterprise data teams managing performance, costs, and reliability at scale. As your environment grows, native monitoring tools often fall short. Platform owners are left blind to cost overruns, performance bottlenecks, and data quality issues that impact business-critical workflows.\nThe challenge? The Snowflake observability market is fragmented.\nFinOps tools focus on cost. DevOps platforms emphasize performance. DIY solutions require constant maintenance. AI-native systems promise automation but vary wildly in execution. Knowing where to start (and which approach actually delivers the visibility and control you need) can feel overwhelming.\nThis buyer's guide cuts through the noise. It provides a practical, actionable framework for evaluating and deploying the right Snowflake observability solution for your organization.\n ... \nSection Title: Snowflake Data Observability Buyerâ€™s Guide > **What is Snowflake Observability?**\nContent:\nSnowflake provides native capabilities like Resource Monitors, Query History views, and Account Usage schemas. These offer foundational monitoring. But enterprise teams need unified Snowflake observability that connects infrastructure performance with business impact, showing not just *what* failed, but *why* it matters and *how* to fix it.\n ... \nSection Title: Snowflake Data Observability Buyerâ€™s Guide > **What This Guide Covers**\nContent:\nDownload this comprehensive Snowflake observability buyer's guide to discover:\nThe five core data observability domains every enterprise needs to cover (based on Gartner's 2024 framework)\nHow different Snowflake observability solution types (DIY, FinOps, DevOps, native tools, and AI-native platforms) compare across cost, performance, and data quality use cases\nHow the emerging discipline of DataFinOps extends beyond cost governance to connect spending with performance and reliability\nWhich Snowflake observability approach best aligns with your specific goals: cost control, data quality assurance, performance tuning, or scalability\nA phased deployment roadmap for rolling out your selected solution with confidence and minimizing disruption\nReal-world decision criteria used by enterprise data platform leaders to evaluate Snowflake observability vendors\n**Ready to optimize your Snowflake environment?**\n ... \nSection Title: Snowflake Data Observability Buyerâ€™s Guide > **Snowflake Observability FAQs**\nContent:\nBased on Gartner's framework: FinOps (cost management and optimization), Performance (compute efficiency and query tuning), Data Quality (reliability and correctness), Governance (compliance and security), and Operational Intelligence (automated insights connecting all domains).\n**How does AI-native observability differ from traditional monitoring?**\nTraditional monitoring requires humans to interpret dashboards and implement fixes manually. AI-native Snowflake observability platforms analyze patterns automatically, recommend optimizations, and can implement changes based on governance policies you define. This moves teams from insight to action without constant manual intervention.\n**What should I look for when evaluating Snowflake observability solutions?**\n ... \nSection Title: Snowflake Data Observability Buyerâ€™s Guide > **Snowflake Observability FAQs** > Other Useful Links\nContent:\nWhy Our AI\nDatabricks\nSnowflake\nGoogle Cloud BigQuery\nCloudera\nCloud Cost Management\nOperations & Troubleshooting\nPipeline & App Optimization\nAutofix & Prevention\nData Quality & Reliability\nCloud Migration\nPlatform Overview\nAll Integrations & APIs\nPlans & Pricing\nRESOURCES\nAI & Automation\nCost Optimization & FinOps\nTroubleshooting & DataOps\nPerformance & Data Eng\nCloud Migration\nCI/CD\nDatabricks\nSnowflake\nBigQuery\nCustomer Stories\nProduct Docs\nResearch & Reports\nWebinars & Podcasts\nInsights\nExplore All\nCOMPANY\nAbout Unravel\nCustomers\nCareers\nPartners\nEvents\nNews & Press\nFAQ\n[Customer Portal](https://customers.unraveldata.com/login?ec=302&startURL=%2Fs%2F)\n[Documentation](https://docs.unraveldata.com/?lang=en)\nContact Us\nDetect, fix, and prevent data issues with the most advanced AI-native data observability and FinOps platform on the market. Powerful automated action you fully control."]},{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/event-about","title":"Use logging and event tracing for an app - Snowflake Documentation","excerpts":["Developer Snowflake Native App Framework Configure logging and event tracing for an app\nSection Title: Use logging and event tracing for an app Â¶\nContent:\nFeature â€” Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nThis topic describes how providers can configure a Snowflake Native App to record log messages and trace events.\nSection Title: Use logging and event tracing for an app Â¶ > About log messages and trace events in an app Â¶\nContent:\nThe Snowflake Native App Framework supports using the Snowflake logging and tracing functionality to gather information about an app. Providers can configure an app to record and analyze the following:\nLog messages â€” Independent, detailed messages with information about\nthe state of a specific piece app code.\nTrace events â€” Structured data that providers can\nuse to get information spanning and grouping multiple parts of your code. Trace events allows an app to emit information related\nto its performance and behavior.\nMetrics - Information about stored procedure and UDF resource consumption\nbased on the CPU and memory metrics that Snowflake generates.\nTo configure an app to emit log messages and trace events, providers set the log and trace levels in the manifest file.\nSee Set the log and trace levels for an app .\n ... \nSection Title: ... > Considerations when migrating from the previous event sharing functionality Â¶\nContent:\nWhen migrating from the existing event sharing functionality to use event definitions, providers\nshould consider the following.\nThe previous event sharing functionality is equivalent to the OPTIONAL ALL event definition.\nPublished versions and patches of an app that used the previous functionality will have the\nOPTIONAL ALL event definition by default. Providers do not need to add this event definition\nto the manifest file.\nTo begin using event definitions, providers can add supported event definitions to the manifest\nfile. This is applicable to new apps as well as new versions and patches of existing apps.\nNote\nTo being begin requesting more granular log and event sharing, providers only have to add\nevent definitions to the manifest file. No other actions are required for providers.\n ... \nSection Title: Use logging and event tracing for an app Â¶ > Monitor consumer application health Â¶\nContent:\nYou can use the `LAST_HEALTH_STATUS` and `LAST_HEALTH_STATUS_UPDATED_ON` columns\nof the APPLICATION_STATE view to monitor the health of consumer instances of your\napp. The `LAST_HEALTH_STATUS` column has the following possible values:\n`OK` : The consumer instance is healthy.\n`FAILED` : The consumer instance is in an error state.\n`PAUSED` : The consumer manually paused the app.\nThe following code sample demonstrates using the `APPLICATION_STATE` view\nto retrieve the health status of all consumer instances of your app:\n```\nSELECT \n    CONSUMER_ORGANIZATION_NAME , \n    CONSUMER_ACCOUNT_NAME , \n    LAST_HEALTH_STATUS , \n    LAST_HEALTH_STATUS_UPDATE_TIME \n FROM \n    SNOWFLAKE . ACCOUNT_USAGE . APPLICATION_STATE \n WHERE \n    PROVIDER_ORG_NAME = '<your_provider_org_name>' \n    AND APPLICATION_NAME = '<your_app_name>' \n ORDER BY \n    LAST_HEALTH_STATUS_UPDATE_TIME DESC ;\n```\nCopy\nSection Title: Use logging and event tracing for an app Â¶ > Monitor consumer application health Â¶\nContent:\nThe preceding query may return results similar to the following:\n```\nCONSUMER_ORG_NAME    CONSUMER_ACCOUNT_NAME    LAST_HEALTH_STATUS    LAST_HEALTH_STATUS_UPDATE_TIME \n ------------------   ---------------------    ------------------    ------------------------------- \n consumer_org_1      consumer_account_1       OK                    2024-01-15 10:30:00.000 \n consumer_org_2      consumer_account_2       FAILED                2024-01-15 09:45:00.000 \n consumer_org_3      consumer_account_3       PAUSED                2024-01-14 16:20:00.000\n```\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nSection Title: Use logging and event tracing for an app Â¶ > Monitor consumer application health Â¶\nContent:\nAbout log messages and trace events in an app\nAbout event sharing\nConsiderations when using event sharing\nConsiderations when migrating from the previous event sharing functionality\nWorkflow - Set up event sharing for an app\nMonitor consumer application health\nRelated content\nLogging, tracing, and metrics\n ... \nSection Title: Use logging and event tracing for an app Â¶ > ... > Your Privacy > Performance Cookies\nContent:\nPerformance Cookies\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\nCookies Detailsâ€Ž"]},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization","excerpts":["Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Business Impact > Overview > Consider cost as a design constraint\nContent:\nAt the ingestion layer, best practices include balancing latency versus\ncost by selecting appropriate services (e.g., Snowpipe, Snowpipe\nStreaming, or third-party tools) and choosing the right storage format\n(e.g., native tables, Iceberg). For transformations, design with\nfrequency versus SLA in mind to ensure data freshness matches the\nbusiness need. For analytics, apply schema design best practices such as\nthoughtful clustering key choices and pruning strategies to reduce\nconsumed credits. In distribution, optimize data transfer by monitoring\negress patterns and applying cost-saving practices like the [Snowflake Data Transfer Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) .\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Track usage data for all platform resources**\nTo deliver clear and actionable consumption insights, it is essential to\nleverage the rich usage data that Snowflake natively provides. The\nfoundation for all cost visibility is the **SNOWFLAKE** database, which\ncontains two key schemas for this purpose: [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) (for granular, account-level data) and [ORGANIZATION_USAGE](https://docs.snowflake.com/en/sql-reference/organization-usage) (for a consolidated view across all accounts).\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Snowsight's built-in cost management capabilities:** Snowsight\nprovides pre-built visuals for usage and credit monitoring directly\nwithin the [Snowflake Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) . It allows filtering by tags (e.g., view cost by department tag),\ncredit consumption by object types, and cost insights to optimize the\nplatform. **Creating custom dashboards or Streamlit apps for different stakeholder groups:** Snowsight facilitates the creation of custom\ndashboards using ACCOUNT_USAGE and ORGANIZATION_USAGE views. Custom\ncharts in the Dashboards feature and Streamlit apps can both be easily\nshared. Combined with cost allocation and tagging, this allows for\ntailored views for finance managers (aggregated spend), engineering\nmanagers (warehouse utilization), or data analysts (query\nperformance).\nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Integrating with third-party BI tools for advanced analytics:** Connecting to Snowflake from tools like Tableau, Power BI, Looker, or\ncustom applications offers highly customizable and extensive control\nover cost data visualization. Cloud-specific third-party data programs\n(FinOps platforms) offer easier setup and more out-of-the-box\nSnowflake cost optimization insights. **Leverage Cortex Code (In Preview):** This AI Assistant capability\nallows users to query cost and usage data in ACCOUNT_USAGE views using\nnatural language natively in the Snowsight UI.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Compute workload-aligned provisioning\nContent:\nFollow the principles outlined above and understand that this is a\ncontinuous improvement process.\nChoose a size based on the estimated or actual workload size and\nmonitor.\nUtilize Snowflake's extensive telemetry data, such as [QUERY_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/query_history) and [WAREHOUSE_METERING_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history) ,\nto validate that the warehouse size is impacting the metrics you care\nabout in the direction you intend.\n**Optimal warehouse settings**\nWhile Snowflake strives for minimal knobs and self-managed tuning, there\nare situations where selecting the right settings for warehouses can\nhelp with optimal cost and/or performance. Some of the key [warehouse settings](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse) include\n ... \nSection Title: Cost Optimization > Optimize > Overview > Limit data transfer\nContent:\nData egress, the transfer of data from one cloud provider or region to\nanother, can incur substantial costs, particularly when handling large\ndata volumes. Implementing appropriate tools and best practices is\nessential to minimize these data transfer expenses and maximize business\nvalue when data egress is necessary.\n**Tooling: Enable proactive cost management**\nLeverage Snowflake's native features to gain visibility and control over\ndata transfer costs before they become a significant expense."]}],"usage":[{"name":"sku_search","count":1}]}

{
  "search_id": "search_3ec59848470b486c97f9029303bab58f",
  "results": [
    {
      "url": "https://www.flexera.com/blog/finops/snowpark-container-services/",
      "title": "Snowpark Container Services 101: A comprehensive overview (2026)",
      "publish_date": "2026-01-27",
      "excerpts": [
        "[Book your personalized demo](https://www.flexera.com/products/flexera-one/saas-management)\nIT Visibility\nITAM\nSnow Atlas\nCloud License Management\nSaaS Management\nFinOps\nCloud Cost Optimization\nCloud Commitment Management\nContainer Optimization\nVirtual Machine Optimization\nData Cloud Optimization\nApplication Readiness\nSecurity\nIntegrations\nTechnology Intelligence Platform\nAll Products\nSuccessCustomer SuccessServices & TrainingSupport[2025 Technology Intelligence Awards](https://info.flexera.com/ITAM-REPORT-State-of-IT-Asset-Management)The results are in\u2014see how our 2025 winners and honorable mentions are shaping the future of Technology Intelligence.\n[See the winners](https://www.flexera.com/customer-success/awards)\nServices\nTraining\n[Flexera support portal](https://community.flexera.com/s/support-hub)\n[Flexera product documentation](https://docs.flexera.com)\n[Snow product documentation](https://docs.snowsoftware.io/)\nTechnology Intelligence Awards\n[Flexera community](https://community.flexera.com/s/)\nResourcesResources[Flexera 2026 IT Priorities Report](https://www.flexera.com/resources/reports/ITV-REPORT-IT-Priorities)AI ROI, sustainability, cost and risk: Discover the latest IT trends shaping tomorrow\u2019s IT landscape in Flexera\u2019s 2026 IT Priorities Report.\n ... \nSection Title: ... > What is Snowpark Container Services?\nContent:\n*Snowpark Container Services (SPCS)* is a fully managed container service integrated into Snowflake. You push [Open Container Initiative-compliant images](https://opencontainers.org/) to your account\u2019s private Open Container Initiative (OCI) image registry, then run those images as long-running services, finite Snowflake job services, or callable Snowflake service functions on Snowflake-managed compute pools.\nSnowflake SPCS guarantees that data remains secure and generally does not leave Snowflake\u2019s governed environment unless explicitly configured by the user for external access; your containers have fast local access to tables, stages and even secrets via Snowflake\u2019s security integrations. You also benefit from Snowflake\u2019s native features, such as role-based access control (RBAC) , governance , monitoring and auto-scaling.\nSection Title: ... > What is Snowpark Container Services?\nContent:\nSnowpark Container Services Overview (Source: [Snowflake](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) )\nAs of 2025, Snowpark Container Services is generally available across AWS, Microsoft Azure and Google Cloud Platform commercial regions, with some exceptions (e.g., not available in most government regions or the Google Cloud me-central2 region). It is unavailable for trial accounts except for running notebooks. Check out [Snowflake\u2019s documentation for the latest region availability](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) .\nAccording to [Snowflake\u2019s official documentation](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) :\nSection Title: ... > What is Snowpark Container Services?\nContent:\nSnowpark Container Services is a fully managed container offering designed to facilitate the deployment, management and scaling of containerized applications within the Snowflake ecosystem. This service enables users to run containerized workloads directly within Snowflake, ensuring that data doesn\u2019t need to be moved out of the Snowflake environment for processing. Unlike traditional container orchestration platforms like Docker or Kubernetes, Snowpark Container Services offers an OCI runtime execution environment specifically optimized for Snowflake. This integration allows for the seamless execution of OCI images, leveraging Snowflake\u2019s robust data platform.\nSection Title: ... > Snowpark Container Services Core Concepts Explained\nContent:\nSnowflake manages the container runtime, scheduling, scaling and patching. You provide the container image and the service or job specification. Containers can execute SQL or call Snowflake APIs to read from and write to tables and stages. Access is controlled via Snowflake roles and policies. Each Snowflake account includes a private OCI-compliant image registry. You push images there and Snowflake pulls them when launching services or jobs. Snowflake Compute pools are sets of VM nodes that host your services and jobs. Pools autoscale between a minimum and maximum node count, with choices for instance families (including GPU-enabled options where supported). Supports any programming language or framework inside the container, including GPU acceleration for AI/ML workloads.\n ... \nSection Title: ... > Common Use Cases for Snowpark Container Services\nContent:\n**Third-Party and Partner Apps** \u2014 Snowflake\u2019s own ecosystem (Native Apps marketplace) will host containerized apps from partners on Snowpark Container Services. As an engineer, you could also deploy your company\u2019s custom app via SPCS.\n ... \nSection Title: ... > Snowpark Container Services Architecture Components and Technical Design\nContent:\n**Snowpark Container Services Architecture TL;DR:** Snowpark Container Services (SPCS) runs OCI images inside Snowflake on managed compute pools. You push images to Snowflake\u2019s private OCI registry, create a compute pool with an instance family and node limits, then define services or job services via a YAML spec. Long-running services are restarted automatically. Job services run to completion. Service functions expose container endpoints as SQL-callable functions. Outbound networking is blocked by default and must be allowed via External Access Integrations or private connectivity. Secrets, caller\u2019s rights, volumes, logging and monitoring are built into the SPCS spec and control plane.\nSnowpark Container Services Architecture (Source: [Snowflake](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/working-with-services) )\n ... \nSection Title: ... > **Step 9** \u2014Build and Test a Docker Image\nContent:\nPrepare your application code and [Dockerfile](https://docs.docker.com/build/concepts/dockerfile/) .\nLet\u2019s create a simple Dockerfile for a test application. For this example, we will create a very basic Python app that echoes a simple message:\n```\n# Dockerfile\nFROM python:3.9-slim\nCOPY hello.py /app/hello.py\nENTRYPOINT [\"python\", \"/app/spcs_demo_app.py\"]\n```\nAnd spcs_demo_app.py:\n```\nprint(\"Hello from Snowpark Container Services (SPCS) !!!!\")\n```\nDOCKERFILE and Python script content \u2013 Snowflake Snowpark Container Services\nBuild and run it locally:\n```\ndocker build -t spcs_demo_app-app:latest .\ndocker run --rm spcs_demo_app-app:latest\n```\nBuilding and Running Docker image locally for testing \u2013 Snowpark Container Services\nIf this runs correctly, you\u2019ve got a working container image.\n ... \nSection Title: ... > **Step 11** \u2014Define the Service Specification (YAML)\nContent:\n```\nspec:\n\t containers:\n\t- name: spcs\n\t\timage: <myorg-myacct>.registry.snowflakecomputing.com/spcs_demo_db/public/spcs_demo_repo/spcs_demo_app-app:latest\n\t\t ports:\n\t\t\t - containerPort: 80\n\t\tenv:\n\t\t\t- name: SNOWFLAKE_WAREHOUSE\n\t\t\t\t value: spcs_demo_wh\n\tendpoints:\n\t- name: spcs-demo-endpoint\n\t\tport: 80\n\t\tpublic: true\n```\nSave this to a local file, `service_spec.yaml` . Here\u2019s what it says:\nUnder containers, we define a container named hello using our image. We list its ports so Snowflake knows it listens on port 80. We set an environment variable `SNOWFLAKE_WAREHOUSE` , which in this case tells the container which Snowflake warehouse to use (Snowflake injects all the credentials automatically).\nUnder endpoints, we create spcs-demo-endpoint on port 80 and mark it public: true so external clients can reach it. (If you don\u2019t need external access, omit or set public: false).\n ... \nSection Title: ... > Snowpark Container Services Data Transfer Costs\nContent:\nOn AWS, Snowflake does **not** charge for data transfer within the same region (aside from a special SPCS fee, see below).\nTransfer to a different AWS region is about **$20/TB** .\nOn Azure, inter-region (same continent) is **~$20/TB** , cross-continent up to **~$50/TB** .\n**\u27a5 Snowpark Container Services internal data transfer**\nWhen containers move data between compute (within Snowflake), Snowflake applies a nominal fee *even if staying in the same region* . On AWS, Snowpark Container Services data transfers in the same region cost ~ **$3.07 per TB** . It\u2019s a small fee to account for internal network traffic. Also note that, Snowflake *caps* these SPCS transfer fees: any given day, your SPCS data transfer charge will be reduced by up to 10% of that day\u2019s compute cost. In effect, you never pay SPCS transfer that exceeds 10% of compute spend, which keeps it modest.\nSection Title: ... > Snowpark Container Services Data Transfer Costs\nContent:\nSnowflake Snowpark Container Service Data Transfer Cost, AWS Snowflake Snowpark Container Service Data Transfer Cost, Azure Snowflake Snowpark Container Service Data Transfer Cost, GCP\nThe true cost of Snowpark Container Services is a sum of all these. Snowflake also provides ACCOUNT_USAGE views (like [SNOWPARK_CONTAINER_SERVICES_HISTORY](https://docs.snowflake.com/en/sql-reference/account-usage/snowpark_container_services_history) ) so you can query exactly how many credits each pool used. It also shows you any Snowpark Container Services block storage usage and data transfer costs.\n ... \nSection Title: Snowpark Container Services 101: A comprehensive overview (2026) > Frequently Asked Questions (FAQs)\nContent:\nSnowpark Container Services is like a \u201cserverless Kubernetes\u201d for Snowflake. You don\u2019t manage the control plane, pods, or networking. Snowflake does all of that. This makes it simpler and more secure for Snowflake-centric workloads. On Kubernetes or EC2, you would set up VPCs, clusters and manage scaling yourself. With SPCS, you only specify the desired compute (pool size) and Snowflake handles the orchestration. The main limitation is less flexibility: you can\u2019t run arbitrary OS tasks outside of containers and you\u2019re limited to Snowflake\u2019s supported instance types and regions. But the big advantage is data locality and built-in Snowflake security.\n**Which Snowflake editions support Snowpark Container Services?**\n ... \nSection Title: Snowpark Container Services 101: A comprehensive overview (2026) > Frequently Asked Questions (FAQs)\nContent:\n[Kubernetes pods vs containers: 4 key differences and how they work together](https://www.flexera.com/blog/finops/kubernetes-architecture-kubernetes-pods-vs-containers-4-key-differences-and-how-they-work-together/ \"Kubernetes pods vs containers: 4 key differences and how they work together\")\n[AWS cost optimization tools and tips: Ultimate guide [2025]](https://www.flexera.com/blog/finops/aws-cost-optimization-8-tools-and-tips-to-reduce-your-cloud-costs/ \"AWS cost optimization tools and tips: Ultimate guide [2025]\")\n[Optimize cloud costs: Using automation to avoid waste](https://www.flexera.com/blog/finops/optimize-cloud-costs-using-automation-to-avoid-waste/ \"Optimize cloud costs: Using automation to avoid waste\")\n[FinOps for AI: Governing the unique economics of intelligent\n ... \nSection Title: ... > [Cloud Cost Optimization demo](https://info.flexera.com/CM-DEMO-Cloud-Cost-Optimization-Req...\nContent:\nFebruary 22, 2023\nFinOps\n ... \nSection Title: ... > [Agentic FinOps for AI: autonomous optimization for Snowflake, Databricks and AI cloud cost...\nContent:\nFebruary 12, 2026\nFinOps"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views",
      "title": "Snowpark Container Services costs | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nDeveloper Snowpark Container Services Snowpark Container Services Costs\n\n# Snowpark Container Services costs \u00b6\n\n Feature \u2014 Generally Available\n\nSnowpark Container Services is available to accounts in AWS, Microsoft Azure, and Google Cloud Platform commercial regions , with some exceptions. For more information, see Available regions and considerations .\n\nThe costs associated with using Snowpark Container Services can be categorized into storage cost, compute pool cost, and data\ntransfer cost.\n\n## Storage cost \u00b6\n\nWhen you use Snowpark Container Services, storage costs associated with Snowflake, including the cost of Snowflake stage usage\nor database table storage, apply. For more information, see Exploring storage cost . In addition, the\nfollowing cost considerations apply:\n\n* **Image repository storage cost:** The implementation of the image repository uses\n  a Snowflake stage. Therefore, the associated cost for using the Snowflake stage applies.\n* **Log storage cost:** When you store local container logs in event tables , event table storage\n  costs apply.\n* **Mounting volumes cost:**\n  \n    + When you mount a Snowflake stage as a volume, the cost of using the Snowflake stage applies.\n    + When you mount storage from the compute pool node as a volume, it appears as local storage in the container. But there is no\n        additional cost because the local storage cost is covered by the cost of the compute pool node.\n* **Block storage cost:** When you create a service that uses block storage , you are billed for block storage and snapshot storage. For more information about storage pricing, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) . The SPCS Block Storage Pricing table in this document provides the information.\n\n## Compute pool cost \u00b6\n\nA compute pool is a collection of one or more virtual machine (VM) nodes on which Snowflake\nruns your Snowpark Container Services jobs and services. The number and type (instance family) of the nodes in the compute pool\n(see CREATE COMPUTE POOL ) determine the credits it consumes and thus the cost you pay. For more information, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\n\nYou incur charges for a compute pool in the IDLE, ACTIVE, STOPPING, or RESIZING state, but not when it is in a STARTING or\nSUSPENDED state. To optimize compute pool expenses, you should leverage the AUTO\\_SUSPEND feature (see CREATE COMPUTE POOL).\n\nThe following views provide usage information:\n\n* **ACCOUNT\\_USAGE views**\n  \n  The following ACCOUNT\\_USAGE views contain Snowpark Container Services credit usage information:\n  \n    + The SNOWPARK\\_CONTAINER\\_SERVICES\\_HISTORY view offers\n        credit usage information (hourly consumption) exclusively for Snowpark Container Services.\n    + In the METERING\\_DAILY\\_HISTORY view , query for rows in which the `service_type` column contains the value `SNOWPARK_CONTAINER_SERVICES` .\n    + In the METERING\\_HISTORY view , query for rows in which the `service_type` column contains the value `SNOWPARK_CONTAINER_SERVICES` .\n* **ORGANIZATION\\_USAGE views**\n  \n    + In the METERING\\_DAILY\\_HISTORY view , use the `SERVICE_TYPE = SNOWPARK_CONTAINER_SERVICES` query filter.\n\n## Data transfer cost \u00b6\n\nData transfer is the process of moving data into (ingress) and out of (egress) Snowflake. For more information, see Understanding data transfer cost . When you use Snowpark Container Services, the following additional cost\nconsiderations apply:\n\n* **Outbound data transfer:** Snowflake applies the same data transfer rate for outbound data transfers from services and jobs\n  to other cloud regions and to the internet, consistent with the rate for all Snowflake outbound data transfers. For more\n  information, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) (table 4a).\n  \n  You can query the DATA\\_TRANSFER\\_HISTORY ACCOUNT\\_USAGE view for\n  usage information. The `transfer_type` column identifies this cost as the `SNOWPARK_CONTAINER_SERVICES` type.\n* **Internal data transfer:** This class of data transfer refers to data movements across compute entities within Snowflake, such as\n  between two compute pools or a compute pool and a warehouse, that resulted from executing a service function .\n  For more information, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) (tables 4(a) for AWS, 4(b) for Azure, and the column titled \u201cSPCS Data Transfer to Same Cloud Provider, Same Region\u201d).\n  \n  To view the costs associated with internal data transfer, you can do the following:\n  \n    + Query the INTERNAL\\_DATA\\_TRANSFER\\_HISTORY view in the ACCOUNT\\_USAGE schema.\n    + Query the DATA\\_TRANSFER\\_HISTORY view in the ACCOUNT\\_USAGE schema. The `transfer_type` column identifies this cost as the `INTERNAL` type.\n    + Query the DATA\\_TRANSFER\\_HISTORY view in the ORGANIZATION\\_USAGE schema.\n        The `transfer_type` column identifies this cost as the `INTERNAL` type.\n    + Query the DATA\\_TRANSFER\\_DAILY\\_HISTORY view in the ORGANIZATION\\_USAGE schema. The `service_type` column identifies this cost as the `INTERNAL_DATA_TRANSFER` type.\n    + Query the RATE\\_SHEET\\_DAILY view in the ORGANIZATION USAGE\n        schema. The `service_type` column identifies this cost as the `INTERNAL_DATA_TRANSFER` type.\n    + Query the USAGE\\_IN\\_CURRENCY\\_DAILY view in the ORGANIZATION USAGE\n        schema. The `service_type` column identifies this cost as the `INTERNAL_DATA_TRANSFER` type.\n\nNote\n\nData transfer costs are currently not billed for Snowflake accounts on Google Cloud.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Storage cost\n2. Compute pool cost\n3. Data transfer cost\n\nRelated content\n\n1. Snowpark Container Services\n2. Snowpark Container Services: Working with compute pools\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/native-apps/security-na-spcs",
      "title": "Secure a Snowflake Native App with Snowpark Container Services | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nDeveloper Snowflake Native App Framework Security requirements and guidelines Secure an app with containers\n\n# Secure a Snowflake Native App with Snowpark Container Services \u00b6\n\n Feature \u2014 Generally Available\n\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\n\nThis topic describes the security considerations for a Snowflake Native App with Snowpark Container Services. In addition to the general\nsecurity requirements for all apps, apps with containers have specific security implications\nand considerations. The security review process for apps with containers includes a thorough\nexamination of the container images they contain.\n\nSnowflake uses container image scanning tools to detect known vulnerabilities and\nsecurity best practice violations.\n\n## Network isolation and egress control \u00b6\n\nApps with containers use strict network isolation and egress control measures to help prevent unauthorized\ndata exfiltration and to protect consumer data. Each app with containers runs in its own isolated network\nenvironment, with controlled access to external systems and services.\n\nSnowflake uses network monitoring and filtering mechanisms to detect and block suspicious egress traffic\npatterns. App providers are required to explicitly declare all external end points in the application\nmanifest, which undergoes a security review.\n\nConsumer data is protected using the following:\n\n* Secure data access patterns.\n* Encryption in transit and at rest.\n* Fine-grained access controls.\n\nThe Snowflake Native App Framework ensures that app with containers can only access the specific data and resources to which\nan app has been granted access. This minimizes the risk of data exfiltration.\n\n## Additional approval requirements for apps with containers \u00b6\n\nSnowflake implements an additional approval process for an app with containers. The approval is\nmandatory before an app with containers can be published to the Snowflake Marketplace. Before a provider\ncan create a public or private listing for an app with containers, they must be approved by the\nSnowflake Product Security team.\n\nProviders who successfully pass this approval process are authorized to publish a public listing\nfor an app with containers. This allows the app to be discoverable and accessible to Snowflake customers.\n\nIf a provider does not pass the approval process, they may not publish a listing for an\napp with containers.\n\n### Initiate the provider approval process \u00b6\n\nWhen a provider sets the DISTRIBUTION=EXTERNAL property for an application package of an app with\ncontainers, Snowflake returns the following error if the provider has not been approved to publish an app with\ncontainers:\n\n```\nError Code: 093197 Account is not allowed to create application package versions or patches with \n Snowpark Container Services for EXTERNAL distribution\n```\n\nIf you receive this error, you must submit a [security questionnaire](https://docs.google.com/forms/d/1XLjbcSrp689kXEvVELa6KbEUOPfsJIirSTG5pGQDMZE/edit?ts=65fb4866) to begin the approval process.\n\nThe security questionnaire assesses the following:\n\n* The provider\u2019s security practices.\n* The provider\u2019s compliance readiness.\n\nSubmitting the security questionnaire begins the provider approval process.\n\n### Evaluation of the security questionnaire \u00b6\n\nAfter a provider submits the security questionnaire, Snowflake\u2019s Security and Compliance\nteam evaluates each response and the documentation included by the provider. Responses are\nevaluated to ensure alignment with industry best practices and standards.\n\nIn some cases, providers may be asked to provide additional information or undergo a more in-depth\nreview to clarify any potential concerns or risks.\n\nAfter reviewing the questionnaire, Snowflake makes a decision to allow the provider to publish an\napp with containers. If a provider is not approved, they must wait until Snowflake Native App with Snowpark Container Services is generally\navailable.\n\nThe provider receives an email from Snowflake indicating if they are approved or wait listed\nuntil general availability.\n\n### Scanning an app with containers \u00b6\n\nAfter a provider is approved, the app with containers undergoes the automated security scan. This\nscan includes a normal app security scan and a scan of the container images included in the app.\n\nThe guidelines for how long the security scan takes to complete are:\n\n|App size |Approximate time to complete scan |\n| --- | --- |\n|Five images or fewer / smaller than 40 GB |Less than 8 hours |\n|Ten images or fewer / smaller than 70 GB |Less than 24 hours |\n|Ten images or more / larger than 70 GB |2 business days or more |\n\nCaution\n\nThe time frames provided are for information only. They do not constitute formal\nservice-level agreements (SLAs).\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Network isolation and egress control\n2. Additional approval requirements for apps with containers\n\nRelated content\n\n1. Security requirements and guidelines for a Snowflake Native App\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/snowpark-container-services/monitoring-services",
      "title": "Snowpark Container Services: Monitoring Services | Snowflake Documentation",
      "excerpts": [
        "Overview\nBuilders\nSnowflake DevOps\nObservability\nSnowpark Library\nSnowpark API\nSpark workloads on Snowflake\nMachine Learning\nSnowflake ML\nSnowpark Code Execution Environments\nSnowpark Container Services\nFunctions and procedures\nLogging, Tracing, and Metrics\nSnowflake APIs\nSnowflake Python APIs\nSnowflake REST APIs\nSQL API\nApps\nStreamlit in Snowflake\nAbout Streamlit in Snowflake\nGetting started\nDeploy a sample app\nCreate and deploy Streamlit apps using Snowsight\nCreate and deploy Streamlit apps using SQL\nCreate and deploy Streamlit apps using Snowflake CLI\nStreamlit object management\nBilling considerations\nSecurity considerations\nPrivilege requirements\nUnderstanding owner's rights\nPrivateLink\nLogging and tracing\nApp development\nRuntime environments\nDependency management\nFile organization\nSecrets and configuration\nEditing your app\nMigrations and upgrades\nIdentify your app type\nMigrate to a container runtime\nMigrate from ROOT_LOCATION\nFeatures\nGit integration\nExternal access\nRow access\npolicies\nSleep timer\nLimitations and library changes\nTroubleshooting Streamlit in Snowflake\nRelease notes\n[Streamlit open-source library documentation](https://docs.streamlit.io/)\nSnowflake Native App Framework\nSnowflake Declarative Sharing\nSnowflake Native SDK for Connectors\nExternal Integration\nExternal Functions\nKafka and Spark Connectors\nSnowflake Scripting\nSnowflake Scripting Developer Guide\nTools\nSnowflake CLI\nGit\nDrivers\nOverview\nConsiderations when drivers reuse sessions\nScala versions\nReference\nAPI Reference\n ... \nSection Title: Snowpark Container Services: Monitoring Services \u00b6\nContent:\nFeature \u2014 Generally Available\nSnowpark Container Services is available to accounts in AWS, Microsoft Azure, and Google Cloud Platform commercial regions , with some exceptions. For more information, see Available regions and considerations .\nSnowflake provides a variety of mechanism for monitoring services, jobs and compute pool. The following sections describe the details.\nA user needs the appropriate privileges on services, jobs, and compute pools to access the monitoring data. For more information, see Privileges needed to perform operations on the service and Compute pool privileges .\nSection Title: Snowpark Container Services: Monitoring Services \u00b6 > Publishing and accessing container logs \u00b6\nContent:\nSnowflake automatically collects and stores container logs \u2014 whatever your application container emits to standard\noutput and standard error \u2014 to an event table for later analysis, unless you choose to opt out.\nEnsure that your code outputs useful information that can help with debugging your service or conducting retrospective analysis\nof your services and jobs.\nUse a combination of the following settings to control which container logs are sent to the event table:\nIn the service specification, use the logExporter field\nto indicate which stream (stdout/stderr) should be sent to the event table.\nIn CREATE SERVICE or ALTER SERVICE command, specify the LOG_LEVEL parameter to indicate the severity at which logs are collected.\nSection Title: Snowpark Container Services: Monitoring Services \u00b6 > Publishing and accessing container logs \u00b6\nContent:\nWhen a service container is running, you can also retrieve the container log, without saving the logs to the event table by using the SYSTEM$GET_SERVICE_LOGS system function. This process is most useful during development and testing\nof your service code.\n ... \nSection Title: Snowpark Container Services: Monitoring Services \u00b6 > ... > Access compute pool metrics \u00b6\nContent:\nCompute pool metrics offer insights into the nodes in the compute pool and the services running on them. Each node reports node-specific metrics, such as the amount of available memory for containers, as well as service metrics, like the memory usage by individual containers. The compute pool metrics provide information from a node\u2019s perspective.\nEach node has a metrics publisher that listens on TCP port 9001. Other services can make an HTTP GET request with the path `/metrics` to port 9001 on the node. To discover the node\u2019s IP address, retrieve SRV records (or A records) from DNS for the `discover.monitor. _compute_pool_name_ .cp.spcs.internal` hostname. Then, create another service in your account that actively polls each node to retrieve the metrics.\nSection Title: Snowpark Container Services: Monitoring Services \u00b6 > ... > Access compute pool metrics \u00b6\nContent:\nThe body in the response provides the metrics using the [Prometheus format](https://prometheus.io/docs/instrumenting/exposition_formats/) as shown in the following example metrics:\n```\n# HELP node_memory_capacity Defines SPCS compute pool resource capacity on the node \n # TYPE node_memory_capacity gauge \n node_memory_capacity{snow_compute_pool_name=\"MY_POOL\",snow_compute_pool_node_instance_family=\"CPU_X64_S\",snow_compute_pool_node_id=\"10.244.3.8\"} 1 \n node_cpu_capacity{snow_compute_pool_name=\"MY_POOL\",snow_compute_pool_node_instance_family=\"CPU_X64_S\",snow_compute_pool_node_id=\"10.244.3.8\"} 7.21397383168e+09\n```\nNote the following:\n ... \nSection Title: Snowpark Container Services: Monitoring Services \u00b6 > ... > Access compute pool metrics \u00b6\nContent:\nSnowflake does not provide any aggregation of metrics. For example, to get metrics for a given service, you must query all nodes that are running instances of that service.\nThe compute pool must have a DNS-compatible name for you to access the metrics.\nThe endpoint exposed by a compute pool can be accessed by a service using a role that has the OWNERSHIP or MONITOR privilege on the compute pool.\nFor a list of available compute pool metrics, see Available platform metrics .\n**Example**\nFor an example of configuring Prometheus to poll your compute pool for metrics, see the [compute pool metrics tutorials](https://github.com/Snowflake-Labs/spcs-templates/tree/main/user-metrics) .\n ... \nSection Title: Snowpark Container Services: Monitoring Services \u00b6 > ... > Available platform metrics \u00b6\nContent:\n`gpu` : Index of the gpu from which this metric originated, starting with 0. `reason` : Explains the container state. This attribute appears only for metrics that end with reason suffix. `spcs.container.state.pending.reason`\n`FailedToPullImage` : Container cannot pull image. `FailingToStartContainer` : Container cannot be started. It is getting scheduled to the node, but then fails. `ServiceRunError` : Runtime error occurred resulting in the container eviction. `ServiceSpecError` : Container cannot be scheduled because error in service specification. `ServiceCreateError` : Error during container initialization. `Initializing` : Container is currently initializing. `Creating` : Container in process of creating, for example, pulling an image. `container.state.last.finished.reason`\n`Done` : Container finished without error. `Failed` : Container terminated with an error.\n ... \nSection Title: ... > Publishing Prometheus application metrics \u00b6\nContent:\n```\n\nCopy\n\nNote\n\nThe Prometheus sidecar container is only supported for services (not jobs). If you want to collect application metrics for a job, it must push the metrics to the OTel collector.\n\n### Accessing application metrics and traces in the event table \u00b6\n\nYou can query the event table to retrieve application metrics. The following query retrieves the application metrics collected in the past hour.\n\n```\nSELECT timestamp , record :metric . name , value\nFROM < current_event_table_for_your_account >\nWHERE timestamp > dateadd ( hour , - 1 , CURRENT_TIMESTAMP ())\nAND resource_attributes : \"snow.service.name\" = < service_name >\nAND scope : \"name\" != 'snow.spcs.platform'\nAND record_type = 'METRIC'\nORDER BY timestamp DESC\nLIMIT 10 ;\n ... \nSection Title: ... > Publishing Prometheus application metrics \u00b6\nContent:\n```\n\nCopy\n\nTraces can be visualized in the [Snowflake trail](https://www.snowflake.com/en/data-cloud/snowflake-trail/) viewer. Metrics and traces contain both user-defined and Snowflake-defined attributes as resource and record attributes. Note that the `snow.` prefix is reserved for Snowflake-generated attributes, Snowflake ignores custom attributes that use this prefix. To see a list of Snowflake defined attributes see Available platform metrics . [Example code](https://github.com/Snowflake-Labs/spcs-templates/tree/main/application-observability) is provided in both Python and Java that demonstrates instrumenting an application with custom metrics and traces using the OTLP SDK. The examples show how to configure Snowflake Trace ID generation for compatibility with the Snowflake trail viewer for traces. ## Accessing platform events \u00b6\n\nSnowflake records events that provide visibility into the status and history of services.\n ... \nSection Title: ... > Publishing Prometheus application metrics \u00b6\nContent:\nYou can access these events in the following ways:\n\n* **Using the service helper method:** The <service\\_name>!SPCS\\_GET\\_EVENTS table function returns events collected by Snowflake from the containers of the specified service. The following list explains the advantages of using this table function:\n  \n    + You can retrieve events for a specific service. + You can retrieve events within a specified time range. + The caller doesn\u2019t need access to the entire events table, which can be beneficial for customers with strict information security requirements. The following SELECT statement uses the table function to retrieve platform events for the specified service recorded in the past hour:\n  \n```\n ... \nSection Title: ... > Publishing Prometheus application metrics \u00b6\nContent:\nSELECT TIMESTAMP , RESOURCE_ATTRIBUTES , RECORD , VALUE\nFROM < your_event_table >\nWHERE TIMESTAMP > DATEADD ( hour , - 1 , CURRENT_TIMESTAMP ())\nAND RESOURCE_ATTRIBUTES : \"snow.service.name\" = '<your_service_name>'\nAND RECORD_TYPE = 'EVENT'\nAND SCOPE : \"name\" = 'snow.spcs.platform'\nORDER BY TIMESTAMP DESC\nLIMIT 10 ;\n ... \nSection Title: ... > Publishing Prometheus application metrics \u00b6\nContent:\nThe following example of a value in the `resource_attribute` column identifies a specific service for which the event is recorded\n      \n      ```\n      { \n       \"snow.compute_pool.name\" : \"TUTORIAL_COMPUTE_POOL\" , \n       \"snow.compute_pool.id\" : 123 , \n       \"snow.database.name\" : \"TUTORIAL_DB\" , \n       \"snow.database.id\" : 456 , \n       \"snow.schema.name\" : \"DATA_SCHEMA\" , \n       \"snow.schema.id\" : 789 , \n       \"snow.service.container.name\" : \"echo\" , \n       \"snow.service.name\" : \"ECHO_SERVICE2\" , \n       \"snow.service.id\" : 212 , \n       \"snow.service.type\" : \"Service\" \n       }\n      ```\n      \n      Copy\n  + **SCOPE:** Indicates the origin of the event. For platform events, the name of the scope is `snow.spcs.platform` , as shown in the following example:\n      \n      ```\n      { \"name\" : \"snow.spcs.platform\" }\n      ```\n      \n      Copy\n  + **RECORD\\_TYPE:** For platform events, EVENT is the RECORD\\_TYPE.\n ... \nSection Title: ... > Publishing Prometheus application metrics \u00b6\nContent:\nPublishing container logs\n3. Accessing container logs\n4. Using the <service-name>!SPCS\\_GET\\_LOGS function\n5. Using event table\n6. Using SYSTEM$GET\\_SERVICE\\_LOGS\n7. Access platform metrics\n8. Accessing event-table service metrics\n9. Access compute pool metrics\n10. Available platform metrics\n11. Publishing and accessing application metrics\n12. Publishing OTLP application metrics and traces\n13. Publishing Prometheus application metrics\n14. Accessing application metrics and traces in the event table\n15. Accessing platform events\n16. Query platform events\n17. Supported events\n18. Guidelines and limitations\n\nRelated content\n\n1. Working with Compute Pools\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas\n```"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/developer-guide/native-apps/container-cost-governance",
      "title": "Costs associated with apps with containers | Snowflake Documentation",
      "excerpts": [
        "Developer Snowflake Native App Framework Costs associated with apps with containers\n\n# Costs associated with apps with containers \u00b6\n\n Feature \u2014 Generally Available\n\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\n\nThis topic describes the costs associated with developing, publishing and using a\nSnowflake Native App with Snowpark Container Services. It contains information for both providers and consumers.\n\n## Costs to consumers \u00b6\n\nA Snowflake Native App may incur costs in the consumer account. The total cost of running\na Snowflake Native App with Snowpark Container Services is determined by the following:\n\n* Costs determined by the provider\n* Infrastructure costs\n\n### Costs determined by the provider \u00b6\n\nA provider may monetize a Snowflake Native App using any of the paid listing pricing models that are available in the Snowflake Marketplace. These models include subscription based and usage based plans.\n\nThis cost to the consumer is determined by the provider. Consumers pay for provider software via the Snowflake Marketplace in addition to costs associated with running Snowflake\ninfrastructure, including warehouses and compute pools.\n\n### Infrastructure costs \u00b6\n\nAll infrastructure costs, including those related to compute pools, warehouse compute, storage, and\ndata transfer are the responsibility of the consumer of a Snowflake Native App.\n\nA consumer can use the IN ACCOUNT clause of the SHOW COMPUTE POOLS command to see all compute pools in their account\nand the current state of the compute pool. Costs are not incurred when a compute pool is suspended.\n\nA Snowflake Native App with Snowpark Container Services requires at least one compute pool and might require multiple compute pools to run as\nintended. A consumer has full control over the compute resources that the app requires, and may suspend a\ncompute pool or drop an application at any time.\n\nSeparate charges for compute pool compute related to the Snowflake Native App with Snowpark Container Services appear on the customer billing\nstatement. A consumer can determine the compute pool billing charges for a Snowflake Native App with Snowpark Container Services using the ACCOUNT USAGE views provided by\nSnowpark Container Services.\n\nFor more details, such as the consumption table for compute pools, contact your account representative.\n\n## Costs to providers \u00b6\n\nProviders can also incur costs when developing and maintaining a Snowflake Native App with Snowpark Container Services, including the\nfollowing:\n\n* Providers incur Snowpark Container Services compute costs associated with both initial development and\n  ongoing testing and support for their Snowflake Native App. The compute cost may be controlled through\n  orchestration of compute pools during provider-side development and testing.\n* The storage of container images can incur costs when a provider creates a new version or patch of\n  a Snowflake Native App with Snowpark Container Services. In this context, the Docker images that the app requires are copied into an image\n  repository that is not directly accessible or observable by the provider or the consumer.\n  \n  Services in the consumer account are created from the versioned images that are stored in this\n  repository. Providers are responsible for the storage costs for the images in this stage, which\n  appear on their Snowflake bill. These costs are aggregated with other storage costs that their account incurs.\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Costs to consumers\n2. Costs to providers\n\nRelated content\n\n1. About the Snowflake Native App Framework\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/",
      "title": "Cost Optimization - Snowflake",
      "excerpts": [
        "Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake\u2019s resource billing models\nContent:\n**Compute (Virtual Warehouses, Snowpark Container Services, Openflow):** This is often the most dynamic and largest portion of\nSnowflake spend. Virtual Warehouses are billed per-second after an\ninitial 60-second minimum when active, with credit consumption\ndirectly proportional to warehouse size (e.g., an \u201cX-Small\u201d Gen1\nwarehouse consumes one credit per hour, a 'Small' consumes two credits\nper hour, doubling with each size increase) while SPCS (via [compute pools](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) )\nare billed for all uptime with a minimum of five minutes. [Openflow](https://docs.snowflake.com/en/user-guide/data-integration/openflow/cost) is billed per second of runtime with a 60-second minimum.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake\u2019s resource billing models\nContent:\nThis generally happens via\negress on cross-region [data replication](https://docs.snowflake.com/en/user-guide/account-replication-cost) , [copying into/out of stage, function calls](https://docs.snowflake.com/user-guide/cost-understanding-data-transfer) ,\nand cross/same region [SPCS data transfer](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) . Depending on the cloud provider and the region used during data\ntransfer, charges vary. **Data sharing & rebates:** Snowflake offers an opt-out Data\nCollaboration rebate program that allows customers to offset credits\nby data consumed with shared outside organizations. This rebate is\nproportional to the consumption of your shared data by consumer\nSnowflake accounts. See the latest terms and more details here .\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Track usage data for all platform resources**\nTo deliver clear and actionable consumption insights, it is essential to\nleverage the rich usage data that Snowflake natively provides. The\nfoundation for all cost visibility is the **SNOWFLAKE** database, which\ncontains two key schemas for this purpose: [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) (for granular, account-level data) and [ORGANIZATION_USAGE](https://docs.snowflake.com/en/sql-reference/organization-usage) (for a consolidated view across all accounts)."
      ]
    },
    {
      "url": "https://select.dev/posts/snowpark-container-services",
      "title": "A Beginner's Guide to Snowpark Container Services: Understanding the Building Blocks and Pricing",
      "publish_date": "2025-11-15",
      "excerpts": [
        "Section Title: ... > What is Snowpark Container Services?\nContent:\nSnowpark Container Services lets you run containerized applications directly inside Snowflake. It is Snowflake's version of AWS ECS, Google Cloud Run, or Azure Container Instances. But instead of running in your cloud account, your containers run in Snowflake's compute environment and have native access to your Snowflake data.\nThe best part is your containers can directly query Snowflake tables, call Snowflake functions, and access your data without the need to create a service account, managing credentials for data access, or moving data outside the platform. Also the users of your app will be granted access using a simple grant statement, so you don\u2019t need to build authentication software or manage user / passwords, etc. I recently deployed a container app for an AWS / Snowflake customer, and they chose SPCS instead of ECS because the user management and security is much simpler.\n ... \nSection Title: ... > Compute Pools\nContent:\nThe main billing component of SPCS is the compute pools. On the surface, SPCS seems to be very affordable. An Extra Small costs only 0.06 Credits per hour. At $3 per credit, an XS would cost $4.32 per day or $1576 per year. Similar hardware on Cloud Run would cost just under $3 / day. So SPCS is more expensive, but has the added benefits we discussed.\nWhen a compute pool resumes, you are billed for a minimum of 5 minutes.\n ... \nSection Title: ... > Snowpark Container Services Cost Management Strategies\nContent:\nGiven these limitations, here are practical approaches if you don\u2019t want your app to run 24x7:\n**1. Manual Suspend/Resume (Best for Dev/Test)**\n```\n1 -- When you're done working: 2 ALTER  SERVICE my_app SUSPEND ; 3 4 -- The service will auto-resume when someone accesses it 5 -- This is manual but reliable\n```\n**2. Scheduled Tasks (Good for Predictable Usage)**\n```\n1 -- Suspend every night at 6 PM 2 CREATE  TASK suspend_service_nightly 3  SCHEDULE  = 'USING CRON 0 18 * * * America/New_York' 4 AS 5 ALTER  SERVICE my_app SUSPEND ; 6 7 -- it will auto resume when someone uses next.\n```\n**3. Monitor and Alert (Essential for Production)**\n ... \nSection Title: ... > Running Batch Jobs using Compute Pools\nContent:\nSince SPCS Compute Pools have a much cheaper cost per credit, we can leverage this as a much cheaper way to run Python (or any language) batch jobs in Snowflake. Let\u2019s say you have a Python job that is containerized and pushed to a Snowflake registry, and a YAML file that describes the service pushed to a Stage. Then you can run an `execute job service ...` command to run the code in that container. These services will turn off when they are finished, allowing the Compute Pool to auto-suspend normally. This is a nice trick to harness cheaper compute in Snowflake!\n```\n1 EXECUTE  JOB SERVICE 2 IN COMPUTE  POOL my_compute_pool 3  NAME  =  my_batch_job 4 FROM @my_stage 5  SPEC  = 'job_spec.yaml' ;\n```\nSection Title: ... > Monitoring Snowpark Container Services cost in Snowsight\nContent:\nThe Snowsight UI provides a convenient way to monitor SPCS spend. Using accountadmin role or a role with access to monitor usage, navigate to Admin \u2192 Cost Management \u2192 Consumption then change the Service Type filter to SPCS.\nSection Title: ... > Key Takeaways\nContent:\n**Compute pools don't idle if services are running** - The `AUTO_SUSPEND_SECS` setting only applies when the pool has no active services\n**You must explicitly manage service lifecycle** - Use `ALTER SERVICE ... SUSPEND` when not in use, and enable `AUTO_RESUME = TRUE` for automatic wake-up (for http apps)\n**Auto-suspend doesn't work for public endpoints** - Services with public endpoints can auto-resume but won't auto-suspend based on inactivity\n**Plan your cost management strategy upfront** - Decide whether you'll use manual suspension, scheduled tasks, or just keep services running and budget accordingly\n**Every object needs permissions** - Image repositories, stages, compute pools, external access integrations, and services all require specific grants"
      ]
    },
    {
      "url": "https://medium.com/@athavale.mandar/automating-snowflake-resource-monitor-management-0d4b1b0d2f34",
      "title": "Automating Snowflake Resource Monitor Management | by Athavale Mandar | Aug, 2025 | Medium",
      "publish_date": "2025-08-14",
      "excerpts": [
        "Sitemap\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F0d4b1b0d2f34&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nWriting is for everyone.\n[Register for Medium Day](https://events.zoom.us/ev/Av7REBItl8l_9abuYg_Iyhrgx4cwt8FEGYhzPou4dCMBDIhOV8ZQ~AmiyQniI6sZwr3sSvUHXWMpdX5wpciIv0a3EWsjOm0kEgiush-6TTsavY_EhDomBRAK8a2foXpncjXcEQADVKgkbMA?source=---medium_day_banner-----------------------------------------)\nAthavale Mandar\nFollow\n3 min read\n\u00b7\nAug 14, 2025\n1\n1\nListen\nShare\n**Automating Snowflake Resource Monitor Management**\nPress enter or click to view image in full size\nManaging Snowflake resource quotas efficiently is crucial for cost control and performance optimization. In this article, I\u2019ll walk you through a fully automated solution using Snowflake\u2019s native features, including tagging, warehouse metering history, Cortex ML forecasting, and tasks. This will help to dynamically update resource monitor quotas for virtual warehouses based on historical usage patterns every month.\nSection Title: \ud83e\uddf1 Step 1: Metadata Preparation\nContent:\nTo begin, we categorize Snowflake warehouses by environment using **tags** . This helps us segment usage and apply environment-specific quotas.\n```\n-- Example: Tagging warehouses  \nCREATE  DATABASE SAMPLE_DB;  \nCREATE  SCHEMA SAMPLE_DB.SAMPLE_SCHEMA;  \nCREATE  TAG SAMPLE_DB.SAMPLE_SCHEMA.ENVIRONMENT ALLOWED_VALUES  =  ( 'DEV' , 'UAT' , 'PRD' );  \nALTER  WAREHOUSE WH_DEV  SET  TAG SAMPLE_DB.SAMPLE_SCHEMA.ENVIRONMENT  = 'DEV' ;  \nALTER  WAREHOUSE WH_UAT  SET  TAG SAMPLE_DB.SAMPLE_SCHEMA.ENVIRONMENT  = 'UAT' ;  \nALTER  WAREHOUSE WH_PRD  SET  TAG SAMPLE_DB.SAMPLE_SCHEMA.ENVIRONMENT  = 'PRD' ;\n```\nThis tagging enables us to filter and analyze usage data by environment (DEV, UAT, PRD) in subsequent steps.\nSection Title: \ud83e\uddf1 Step 1: Metadata Preparation > \ud83d\udcca Step 2: Training Data Preparation\nContent:\nWe use the `WAREHOUSE_METERING_HISTORY` view to extract credit consumption data. The goal is to build a month-level view of usage per environment for the past 12 months.\n ... \nSection Title: \ud83e\uddf1 Step 1: Metadata Preparation > \ud83d\udd27 Step 4: Resource Monitor Update\nContent:\nWe sum the forecasted credits by environment and use these values to update or create resource monitors.\n```\n-- Creating resource monitors  \n-- Example for DEV  \nCREATE OR REPLACE RESOURCE MONITOR RM_DEV  \nWITH CREDIT_QUOTA = ( _SELECT SUM($3) FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))_ ,  \nTRIGGERS ON 80 PERCENT DO NOTIFY  \nTRIGGERS ON 90 PERCENT DO NOTIFY  \nTRIGGERS ON 100 PERCENT DO SUSPEND; --You need to use query output as credit quota value instead of actual SQL query\n```\nRepeat for UAT and PRD environments.\n ... \nSection Title: \ud83e\uddf1 Step 1: Metadata Preparation > \u2705 Final Thoughts\nContent:\nThis solution leverages Snowflake\u2019s powerful ecosystem to automate resource quota management with native ML capabilities. It ensures that environment specific warehouses are controlled based on actual usage trends, reducing manual effort and optimizing costs.\nIf you\u2019re facing similar challenges or want to explore more automation in Snowflake, feel free to connect or drop your thoughts in the comments!\nI am Mandar, working as a Snowflake Platform manager. You can find me here on [LinkedIn](https://www.linkedin.com/in/mandarathavale/) . Thanks for reading this article. Please feel free to provide your valuable feedback.\nSnowflake\nInfosys\nSnowflake Data Cloud\nMachine Learning\nCost Optimization\n1\n1\n1\nFollow\nSection Title: \ud83e\uddf1 Step 1: Metadata Preparation > Written by Athavale Mandar\nContent:\n43 followers\n\u00b7 57 following\nFollow\nSection Title: \ud83e\uddf1 Step 1: Metadata Preparation > Responses (1)\nContent:\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--0d4b1b0d2f34---------------------------------------)\nWrite a response\nWhat are your thoughts?\nCancel\nRespond\nAbhijit K\nAug 14\n```\nNice one Mandar! Thanks for sharing your insights.\n```\n34\nReply"
      ]
    },
    {
      "url": "https://medium.com/@vuppala.venkat/snowflake-cost-optimization-strategies-221e14b1bec0",
      "title": "Snowflake cost optimization strategies | by Vuppala Venkat - Medium",
      "publish_date": "2024-01-27",
      "excerpts": [
        "Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nSearch\nSection Title: Snowflake cost optimization strategies\nContent:\nVuppala Venkat\n3 min read\n\u00b7\nJan 27, 2024\n--\nListen\nShare\nThe overall cost associated with employing Snowflake includes expenses related to data transfer, storage, and compute resources.\n**Compute Resources:**\nWithin Snowflake, the use of compute resources incurs costs in Snowflake credits. The billed amount for compute resources is calculated by multiplying the consumed credits by the credit price, dependent on the Snowflake edition, region, and cloud platform.\nThere are three types of compute resources in Snowflake that consume credits:\nSection Title: Snowflake cost optimization strategies\nContent:\n**Virtual Warehouse Compute:** These user-managed compute resources use credits during data loading, query execution, and other DML operations. Virtual warehouses are billed for the credits they actively consume.\n**Serverless Compute:** Features like Search Optimization and Snowpipe use Snowflake-managed compute resources. These resources are automatically resized by Snowflake to match workload requirements, minimizing costs.\n**Cloud Services Compute:** This layer of the Snowflake architecture consumes credits for behind-the-scenes tasks. Charges apply only if daily consumption exceeds 10% of the daily warehouse usage.\n**Storage Resources:**\nThe monthly cost of storing data in Snowflake is based on a flat rate per terabyte (TB). Storage expenses are calculated monthly, considering the average number of on-disk bytes stored each day.\n**Data Transfer Resources:**\n ... \nSection Title: Snowflake cost optimization strategies\nContent:\nAdjust the timeout setting and closely monitor its effects on cost, cache, and overall performance. **Query time-out:** The default query timeout is set to 48 hours. In case of a poorly structured, long-running query, it persists for the entire 48-hour duration before termination. Adjust the STATEMENT_TIMEOUT_IN_SECONDS for the warehouse according to its workload. If the lengthiest successful query in the pipeline takes 3 hours, consider setting STATEMENT_TIMEOUT_IN_SECONDS to 4 or 5 hours to provide additional buffer for concurrent queries. **Monitor and Tune Queries** : Regularly monitor the longest successful queries and tune them using best practices. **Monitor time-out and failed queries** : Closely monitor queries that have failed due to timeouts. The provided query will generate a list of failed queries that ran for over 30 minutes within the past 30 days.\n ... \nSection Title: Snowflake cost optimization strategies\nContent:\nselect a.*,(datediff(minute, start_time, end_time)) diff_minutes from query_history a where execution_status<>\u2019SUCCESS\u2019 and start_time > dateadd(MONTH,-30,CURRENT_DATE())and datediff(minute, start_time, end_time)>30and error_code=\u2019000630'order by (datediff(minute, start_time, end_time)) desc\n**Use Resource Monitors** : Implement resource monitors on warehouses and accounts for oversight.\n**Optimizing Storage Resources:**\nStorage costs make up a smaller portion of the Snowflake bill. To save on storage expenses:\n**Adjust Time Travel Settings** : The DATA_RETENTION_TIME_IN_DAYS parameter impacts storage costs. Adjust this parameter based on business requirements, considering the trade-off with time travel rollback ability.\n**Utilize Cloning** : Cloning can be employed to copy tables, schemas, databases, and other objects efficiently.\n**Optimizing Pipelines Costs:**\nTo minimize pipeline costs, consider the following strategies:"
      ]
    },
    {
      "url": "https://www.dataops.live/blog/monitoring-and-debugging-your-snowpark-container-services-workloads",
      "title": "Monitoring and debugging your Snowpark Container Services workloads",
      "publish_date": "2025-09-01",
      "excerpts": [
        "[Learn DataOps.live](https://www.dataops.live/learn-dataopslive)\n[Contact Us](https://www.dataops.live/contact-us)\n[Platform](https://www.dataops.live/platform)\nPlatform overview\n[#### DataOps.live platform overview Automate, orchestrate, and govern every part of your data pipeline with one platform, purpose-built for Snowflake. Learn more ->](https://www.dataops.live/platform)\nAlliances\n[Alliances -> Meet our Tech, SI, and Ecosystem partners.](https://www.dataops.live/partners)\n[Snowflake](https://www.dataops.live/snowflake) [AWS](https://www.dataops.live/aws)\n[Products](https://www.dataops.live/products)\nNative Apps Overview\n[# DataOps.live Native Apps Bring software engineering rigor to your data pipelines. View products ->](https://www.dataops.live/products)\nNative Apps for Snowflake\n[Dynamic Transformation Accelerate your dbt\u2122 projects.](https://www.dataops.live/dynamic-transformation) [Dynamic Delivery Accelerate your CI/CD pipeline.](https://www.dataops.live/dynamic-delivery)\n ... \nSection Title: Monitoring and debugging your Snowpark Container Services workloads > Related resources\nContent:\n[#### View all articles 172](https://www.dataops.live/blog)\nSection Title: Monitoring and debugging your Snowpark Container Services workloads > Related resources\nContent:\n[Snowflake Mar 06, 2024 ### Connecting to Snowflake from a Snowpark Container Services (SPCS) container Discover how to securely connect to Snowflake from a Snowpark Container Services (SPCS) container using a provided... Posted by Colin Bradford 2 min read](https://www.dataops.live/blog/connecting-to-snowflake-from-a-snowpark-container-services-spcs-container) [Announcement Jun 08, 2023 ### DataOps.live Joins the AWS Software Partner Path, Completes AWS Foundational Technical Review, and Earns AWS Qualified Software Certification DataOps.live announced that it has joined the Amazon Web Services (AWS) Partner Network (APN) on the Software Path and... Posted by Patrick Connolly 2 min read](https://www.dataops.live/blog/dataops.live-joins-the-aws-software-partner-path-completes-aws-foundational-technical-review-and-earns-aws-qualified-software-certification) [Snowflake Dec 20, 2023 ### OneWeb:\nSection Title: Monitoring and debugging your Snowpark Container Services workloads > Related resources\nContent:\nSatellite providers goes stellar in six weeks with DataOps.live platform, Snowflake and AWS DataOps.live & Snowflake & AWS are enabling OneWeb to govern & automate every data pipeline while creating a powerful... Posted by DataOps.live 2 min read](https://www.dataops.live/blog/case-study-oneweb-aws) [\\ Mar 21, 2019 ### Snowflake\u2014the best time series database in the world? Guy Adams discusses the limitations of TSDB & the benefits of using a cloud-based SQL data warehouse like Snowflake for... Posted by DataOps.live 3 min read](https://www.dataops.live/blog/snowflake-the-best-time-series-database-in-the-world)\nSection Title: Monitoring and debugging your Snowpark Container Services workloads > Related resources\nContent:\nDataops.live is a fully-managed SaaS platform for building and deploying Data Products.\n[](https://www.linkedin.com/company/dataopslive/) [](https://www.youtube.com/@dataops)"
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}
{
  "extract_id": "extract_85cd7a2591ac4244a8f8d2c39878ff0f",
  "results": [
    {
      "url": "https://blog.greybeam.ai/snowflake-cost-per-query/",
      "title": "Deep Dive: Snowflake's Query Cost and Idle Time Attribution",
      "publish_date": "2024-10-22",
      "excerpts": [
        "[](https://www.greybeam.ai/)\n\n* [Blog](https://blog.greybeam.ai/)\n* [Waitlist](https://greybeam.ai)\n* [Customer Stories](https://blog.greybeam.ai/tag/customer-story/)\n\n Subscribe\n\nSep 9, 2024 13 min read How-To\n\n# A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query\n\nSnowflake's new QUERY\\_ATTRIBUTION\\_HISTORY view\n\nSnowflake recently released a new feature for granular cost attribution down to individual queries through the `QUERY_ATTRIBUTION_HISTORY` view in `ACCOUNT_USAGE` . As a company focused on SQL optimization, we at Greybeam were eager to dive in and see how this new capability compares to our own custom cost attribution logic. What we found was surprising - and it led us down a rabbit hole of query cost analysis.\n\n### The Promise and Limitations of QUERY\\_ATTRIBUTION\\_HISTORY\n\nThe new view aims to provide visibility into the compute costs associated with each query. Some key things to note:\n\n* Data is only available from July 1, 2024 onwards\n* Short queries (<100ms) are excluded\n* Idle time is not included in the attributed costs\n* There can be up to a 6 hour delay in data appearing\n\nThere's also a `WAREHOUSE_UTILIZATION` view that displays cost of idle time. At the time of writing, this must be enabled by your Snowflake support team.\n\n### Our Initial Findings\n\nWe set up a test with an X-Small warehouse and 600 second auto-suspend to dramatically illustrate idle time. Running a series of short queries (mostly <500ms) over an hour, we expected to see a very small fraction of the total credits in that hour attributed to our queries, but we were very wrong.\n\nOn September 4th at the 14th hour, ~40 seconds of queries were executed and some how in the `QUERY_ATTRIBUTION_HISTORY` view it showed that nearly half of the total credits (0.43 of 0.88) attributed to query execution. This seemed impossibly high given the short query runtimes, yet the pattern continues.\n\nQUERY\\_ATTRIBUTION\\_HISTORY aggregated by the hour.\n\nThis may just be an anomaly in our Snowflake account, so try it yourself.\n\n```\nWITH query_execution AS (\n    SELECT\n        qa.query_id\n        , TIMEADD(\n                'millisecond',\n                qh.queued_overload_time + qh.compilation_time +\n                qh.queued_provisioning_time + qh.queued_repair_time +\n                qh.list_external_files_time,\n                qh.start_time\n            ) AS execution_start_time\n        , qh.end_time::timestamp AS end_time\n        , DATEDIFF('MILLISECOND', execution_start_time, qh.end_time)*0.001 as execution_time_secs\n        , qa.credits_attributed_compute\n        , DATE_TRUNC('HOUR', execution_start_time) as execution_start_hour\n        , w.credits_used_compute\n    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY AS qa\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS qh\n        ON qa.query_id = qh.query_id\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS w\n        ON execution_start_hour = w.start_time\n        AND qh.warehouse_id = w.warehouse_id\n    WHERE\n        qh.warehouse_id = 4\n)\n\nSELECT\n    DATE_TRUNC('HOUR', execution_start_time) AS hour\n    , SUM(execution_time_secs) AS total_execution_time_secs\n    , COUNT(*) AS num_queries\n    , SUM(credits_attributed_compute) AS query_credits\n    , ANY_VALUE(credits_used_compute) AS metered_credits\nFROM query_execution\nGROUP BY ALL\nORDER BY 1 ASC\n;\n```\n\n### Digging Deeper\n\nTo investigate further, we compared the results to our own custom cost attribution logic that accounts for idle time. Here\u2019s a snippet of what we found for the same hour:\n\nGreybeam\u2019s internal query cost attribution results\n\nAs you can see, our calculations show much smaller fractions of credits attributed to the actual query runtimes for the first hour, with the bulk going to idle periods. This aligns much more closely with our expectations given the warehouse configuration, and it works historically!\n\n### Potential Issues\n\nAt the time of writing, we\u2019ve identified a few potential problems with the new view:\n\n1. Warehouse ID mismatch\u200a\u2014\u200aThe `warehouse_id` in `QUERY_ATTRIBUTION_HISTORY` doesn't match the actual `warehouse_id` from `QUERY_HISTORY` .\n2. Inflated query costs\u200a\u2014\u200aThe credits attributed to short queries seem disproportionately high in some cases.\n3. Idle time accounting\u200a\u2014\u200aIt\u2019s unclear how idle time factors into the attribution, if at all.\n\nWe\u2019ve raised these concerns with Snowflake, and they\u2019ve recommended filing a support ticket for further investigation. In the meantime, we\u2019ll continue to rely on our custom attribution logic for accuracy.\n\n## Our Approach to Query Cost Attribution\n\nGiven the discrepancies we\u2019ve found, we wanted to share our methodology for calculating per-query costs, including idle time. Here\u2019s an overview of our process:\n\n1. Gather warehouse suspend events\n2. Enrich query data with execution times and idle periods\n3. Create a timeline of all events (queries and idle periods)\n4. Join with `WAREHOUSE_METERING_HISTORY` to attribute costs\n\nBefore we dive in, let\u2019s cover a few basics:\n\nSnippet of WAREHOUSE\\_METERING\\_HISTORY\n\n* We use `WAREHOUSE_METERING_HISTORY` as our source of truth for warehouse compute credits. The credits billed here will reconcile with Snowflake\u2019s cost management dashboards.\n* Credits here are represented on an hourly grain. We like to refer to this as _credits metered_ , analogous to how most homes in North America are metered for their electricity. In our solution, we\u2019ll need to allocate queries and idle times into their metered hours.\n* We use a weighted time-based approach to attribute costs within the metered hour. In reality, Snowflake\u2019s credit attribution is likely much more complex, especially in situations with more clusters or warehouse scaling.\n\nHow we need to break down our queries and idle times.\n\nThe full SQL query will be available at the end of this blog.\n\n### Step 1: Gather Warehouse Suspend Events\n\n\u2757\n\nWe've updated this article with an optimization using `ASOF JOIN` . Check out how to use ASOF JOINs [here](https://blog.greybeam.ai/snowflake-asof-join/) .\n\nFirst, we need to know when warehouses are suspended, this is pulled from [`WAREHOUSE_EVENTS_HISTORY`](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_events_history?ref=blog.greybeam.ai) .\n\n```\nWITH warehouse_events AS (\n    SELECT\n        warehouse_id\n        , timestamp\n        , LAG(timestamp) OVER (PARTITION BY warehouse_id ORDER BY timestamp) as lag_timestamp\n    FROM snowflake.account_usage.warehouse_events_history\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'\n        AND DATEADD('DAY', 15, timestamp) >= current_date\n)\n```\n\nIt\u2019s worth mentioning that the `WAREHOUSE_EVENTS_HISTORY` view has had a reputation for being somewhat unreliable. In fact, Ian from Select considered using this table in his [cost-per-query analysis](https://select.dev/posts/cost-per-query?ref=blog.greybeam.ai) but ultimately decided against it due to these reliability concerns.\n\nHowever, we\u2019ve been in touch with a Snowflake engineer who informed us that recent updates have significantly improved the reliability of this table. While it may not be perfect, we're only using it for the suspended timestamp and not the cluster events, so it\u2019s \u201cclose enough\u201d for our purposes\n\nIn addition, warehouse suspension doesn\u2019t actually occur during the `SUSPEND_WAREHOUSE` event. Technically, it happens when the `WAREHOUSE_CONSISTENT` event is logged. The `WAREHOUSE_CONSISTENT` event indicates that all compute resources associated with the warehouse have been fully released. You can find more information about this event in the [Snowflake documentation](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_events_history?ref=blog.greybeam.ai) .\n\nFor the sake of simplicity (and because the time difference is usually negligible), we\u2019re sticking with the `SUSPEND_WAREHOUSE` event in our analysis. This approach gives us a good balance between accuracy and complexity in our cost attribution model.\n\nBefore moving onto enriching query data, we want to apply filters to reduce the load from table scans. Feel free to adjust the dates as you see fit.\n\n```\nWITH warehouse_list AS (\n    SELECT \n        DISTINCT warehouse_name,\n        warehouse_id\n    FROM warehouse_metering_history\n    WHERE \n        warehouse_name IS NOT NULL\n        AND start_time >= $startDate\n),\n\nwarehouse_events AS (\n    SELECT\n        weh.warehouse_id\n        , weh.timestamp\n    FROM warehouse_events_history as weh\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'        \n),\n\nqueries_filtered AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.warehouse_name\n        , q.warehouse_size\n        , q.role_name\n        , q.user_name\n        , q.query_text\n        , q.query_hash\n        , q.queued_overload_time\n        , q.compilation_time\n        , q.queued_provisioning_time\n        , q.queued_repair_time\n        , q.list_external_files_time\n        , q.start_time\n        , TIMEADD(\n                'millisecond',\n                q.queued_overload_time + q.compilation_time +\n                q.queued_provisioning_time + q.queued_repair_time +\n                q.list_external_files_time,\n                q.start_time\n            ) AS execution_start_time\n        , q.end_time::timestamp AS end_time\n        , w.timestamp AS suspended_at\n        , MAX(q.end_time) OVER (PARTITION BY q.warehouse_id, w.timestamp ORDER BY execution_start_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as end_time_max\n        , LEAD(execution_start_time) OVER (PARTITION BY q.warehouse_id ORDER BY execution_start_time ASC) as next_query_at\n    FROM query_history AS q\n    ASOF JOIN warehouse_events AS w\n        MATCH_CONDITION (q.end_time::timestamp <= w.timestamp)\n        ON q.warehouse_id = w.warehouse_id\n    WHERE\n        q.warehouse_size IS NOT NULL\n        AND q.execution_status = 'SUCCESS'\n        AND start_time >= $startDate\n        AND EXISTS (\n            SELECT 1\n            FROM warehouse_list AS wl\n            WHERE\n                q.warehouse_id = wl.warehouse_id\n            )\n)\n```\n\n### Step 2: Enrich Query Data\n\nIn this step, we take the raw query data and enrich it with additional information that will allow us to breakdown query and idle times into their hourly components. We choose hourly slots because the source of truth for credits comes from `WAREHOUSE_METERING_HISTORY` , which is on an hourly grain.\n\n```\nqueries_enriched AS (\n      SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.execution_start_time\n        , q.end_time::timestamp AS end_time\n        , q.end_time_max AS end_time_running\n        , q.next_query_at\n        , q.suspended_at\n        , (CASE\n            WHEN q.next_query_at > q.suspended_at THEN q.end_time_max\n            WHEN q.next_query_at > q.end_time_max THEN q.end_time_max\n            WHEN q.next_query_at < q.end_time_max THEN NULL\n            WHEN q.next_query_at IS NULL THEN q.end_time\n            END)::timestamp AS idle_start_at\n        , IFF(idle_start_at IS NOT NULL, LEAST(COALESCE(next_query_at, '3000-01-01'), q.suspended_at), NULL)::timestamp AS idle_end_at\n        , HOUR(execution_start_time::timestamp) = HOUR(q.end_time::timestamp) AS is_same_hour_query\n        , HOUR(idle_start_at) = HOUR(idle_end_at) AS is_same_hour_idle\n        , DATE_TRUNC('HOUR', execution_start_time) AS query_start_hour\n        , DATE_TRUNC('HOUR', idle_start_at) as idle_start_hour\n        , DATEDIFF('HOUR', execution_start_time, q.end_time) AS hours_span_query\n        , DATEDIFF('HOUR', idle_start_at, idle_end_at) AS hours_span_idle\n    FROM queries_filtered AS q\n)\n```\n\nKey points to highlight:\n\n1. **Execution Start Time** : We calculate the actual execution start time that the query begins running on the warehouse (thanks [Ian](https://select.dev/posts/cost-per-query?ref=blog.greybeam.ai) !).\n2. **Idle Time Calculation** : We determine idle periods by looking at the gap between our running query end time and the next query\u2019s start time (or warehouse suspension time). This is because it's possible a prior query is still running, so we need to keep track of the running end time and compare it against the start time of the next query. If the next query starts after the end of our current query, then there\u2019s idle time.\n3. **Hour Boundaries** : We identify queries and idle periods that span hour boundaries. This is important because Snowflake bills by the hour, so we need to properly attribute costs that cross these boundaries.\n4. **Warehouse Suspension** : We join with the warehouse\\_events table to identify when warehouses were suspended, which helps us accurately determine the end of idle periods. If the next query starts after the warehouse suspends, then the end of the idle period is the suspension time.\n\n### Step 3: Create Timeline of All Events\n\nWe now need to create an hourly timeline of all events so that we can reconcile our credits with `WAREHOUSE_METERING_HISTORY` . The timeline of all events can be broken down into 4 components:\n\n1. A query executed and ended in the same hour\n2. Idle time started and ended in the same hour\n3. A query executed and ended in a different hour\n4. Idle time started and ended in a different hour\n\n1 and 2 are straight forward since they don\u2019t cross any hourly boundaries we can simply select from the dataset and join directly to `WAREHOUSE_METERING_HISTORY` .\n\n```\nSELECT\n        q.query_id\n        , q.warehouse_id\n        , 'query' AS type\n        , q.execution_start_time AS event_start_at\n        , q.end_time AS event_end_at\n        , q.query_start_hour AS meter_start_hour\n        , NULL AS meter_end_hour\n        , q.execution_start_time AS meter_start_at\n        , q.end_time AS meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    WHERE\n        q.is_same_hour_query = TRUE\n    \n    UNION ALL\n    \n    SELECT\n        'idle_' || q.query_id\n        , q.warehouse_id\n        , 'idle' AS type\n        , q.idle_start_at AS event_start_at\n        , q.idle_end_at AS event_end_at\n        , q.idle_start_hour AS meter_start_hour\n        , NULL AS meter_end_hour\n        , q.idle_start_at AS meter_start_at\n        , q.idle_end_at AS meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    WHERE\n        q.is_same_hour_idle = TRUE\n```\n\nFor 3 and 4, we need a record for each hour that the queries and idle times ran within. For example, if a query ran from 7:55PM to 10:40PM, we\u2019d need a record for 7, 8, 9, and 10PM.\n\nA query that executed across 4 hourly slots (including 0).\n\nOriginally we used a slightly more complicated join:\n\n```\nFROM queries_enriched AS q\nJOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS m\n    ON q.warehouse_id = m.warehouse_id\n    AND m.start_time >= q.meter_start_time\n    AND m.start_time < q.end_time\n```\n\nThis took forever to run on a large account. Instead, we first create records for each hour so that the join to `WAREHOUSE_METERING_HISTORY` is a direct join in the next step.\n\n```\nnumgen AS (\n    SELECT\n        0 AS num\n    UNION ALL \n    \n    SELECT\n        ROW_NUMBER() OVER (ORDER BY NULL)\n    FROM table(generator(ROWCOUNT=>24)) -- assuming no one has idle or queries running more than 24 hours\n),\n\nmega_timeline AS (\n    -- parts 1 and 2 here\n    \n    SELECT\n        'idle_' || q.query_id\n        , q.warehouse_id\n        , 'idle'\n        , q.idle_start_at AS event_start_at\n        , q.idle_end_at AS event_end_at\n        , DATEADD('HOUR', n.num, DATE_TRUNC('HOUR', q.idle_start_at)) AS meter_start_hour\n        , DATEADD('HOUR', n.num + 1, DATE_TRUNC('HOUR', q.idle_start_at)) AS meter_end_hour\n        , GREATEST(meter_start_hour, q.idle_start_at) as meter_start_at\n        , LEAST(meter_end_hour, q.idle_end_at) as meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    LEFT JOIN numgen AS n\n        ON q.hours_span_idle >= n.num\n    WHERE\n        q.is_same_hour_idle = FALSE\n    \n    UNION ALL\n    \n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , 'query'\n        , q.execution_start_time AS event_start_at\n        , q.end_time AS event_end_at\n        , DATEADD('HOUR', n.num, DATE_TRUNC('HOUR', q.execution_start_time)) AS meter_start_hour\n        , DATEADD('HOUR', n.num + 1, DATE_TRUNC('HOUR', q.execution_start_time)) AS meter_end_hour\n        , GREATEST(meter_start_hour, q.execution_start_time) as meter_start_at\n        , LEAST(meter_end_hour, q.end_time) as meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    LEFT JOIN numgen AS n\n        ON q.hours_span_query >= n.num\n    WHERE\n        q.is_same_hour_query = FALSE\n)\n```\n\n### Step 4: Attribute Costs\n\nFinally, with each query and idle period properly allocated to their hourly slots, we can directly join to `WAREHOUSE_METERING_HISTORY` and calculate our credits used.\n\n```\nmetered AS (\n    SELECT\n        m.query_id\n        , m.warehouse_id\n        , m.type\n        , m.event_start_at\n        , m.event_end_at\n        , m.meter_start_hour\n        , m.meter_start_at\n        , m.meter_end_at\n        , m.meter_time_secs\n        , SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour) AS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN snowflake.account_usage.warehouse_metering_history AS w -- inner join because both tables have different delays\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time -- we can directly join now since we used our numgen method\n)\n```\n\nIn this approach we allocate credits based on the proportion of the total execution time in that hour:\n\n1. **Time-based Weighting** : We use the duration of each event (query or idle period) as the basis for our weighting. This is represented by `m.meter_time_secs` .\n2. **Hourly Totals** : We calculate the total time for all events within each hour for each warehouse `SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour)` .\n3. **Credit Allocation** : We then allocate credits to each event based on its proportion of the total time in that hour `(m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute` .\n\nOne important note: This approach assumes that all time within an hour is equally valuable in terms of credit consumption. In reality, Snowflake may have more complex internal algorithms for credit attribution, especially for multi-cluster warehouses or warehouses that change size within an hour. However, this weighted time-based approach provides a reasonable and transparent method for cost attribution that aligns well with Snowflake\u2019s consumption-based billing model.\n\n## Conclusion\n\nWhile Snowflake\u2019s new `QUERY_ATTRIBUTION_HISTORY` view is a promising step towards easier cost attribution, our initial testing reveals some potential issues that need to be addressed. For now, we recommend carefully validating the results against your own calculations and metering history.\n\nWe\u2019re excited to see how this feature evolves and will continue to monitor its accuracy. In the meantime, implementing your own cost attribution logic can provide valuable insights into query performance and resource utilization.\n\nBy accounting for idle time and carefully tracking query execution across hour boundaries, we\u2019re able to get a more complete and accurate picture of costs. This level of detail is crucial for optimizing Snowflake usage and controlling costs effectively.\n\n* * *\n\n## Struggling with Snowflake costs?\n\nAll usage-based cloud platforms can get expensive when not used carefully. There are a ton of controls teams can fiddle with to get a handle on their Snowflake costs. At Greybeam, we\u2019ve built a query performance and observability platform that automagically optimizes SQL queries sent to Snowflake, saving you thousands in compute costs. Reach out to [[email protected]](/cdn-cgi/l/email-protection) to learn more about how we can optimize your Snowflake environment.\n\n* * *\n\n## Full SQL Cost Attribution\n\n```\nSET startDate = DATEADD('DAY', -15, current_date);\nWITH warehouse_list AS (\n    SELECT \n        DISTINCT warehouse_name,\n        warehouse_id\n    FROM warehouse_metering_history\n    WHERE \n        warehouse_name IS NOT NULL\n        AND start_time >= $startDate\n),\n\nwarehouse_events AS (\n    SELECT\n        weh.warehouse_id\n        , weh.timestamp\n    FROM warehouse_events_history as weh\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'        \n),\n\nqueries_filtered AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.warehouse_name\n        , q.warehouse_size\n        , q.role_name\n        , q.user_name\n        , q.query_text\n        , q.query_hash\n        , q.queued_overload_time\n        , q.compilation_time\n        , q.queued_provisioning_time\n        , q.queued_repair_time\n        , q.list_external_files_time\n        , q.start_time\n        , TIMEADD(\n                'millisecond',\n                q.queued_overload_time + q.compilation_time +\n                q.queued_provisioning_time + q.queued_repair_time +\n                q.list_external_files_time,\n                q.start_time\n            ) AS execution_start_time\n        , q.end_time::timestamp AS end_time\n        , w.timestamp AS suspended_at\n        , MAX(q.end_time) OVER (PARTITION BY q.warehouse_id, w.timestamp ORDER BY execution_start_time ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as end_time_max\n        , LEAD(execution_start_time) OVER (PARTITION BY q.warehouse_id ORDER BY execution_start_time ASC) as next_query_at\n    FROM query_history AS q\n    ASOF JOIN warehouse_events AS w\n        MATCH_CONDITION (q.end_time::timestamp <= w.timestamp)\n        ON q.warehouse_id = w.warehouse_id\n    WHERE\n        q.warehouse_size IS NOT NULL\n        AND q.execution_status = 'SUCCESS'\n        AND start_time >= $startDate\n        AND EXISTS (\n            SELECT 1\n            FROM warehouse_list AS wl\n            WHERE\n                q.warehouse_id = wl.warehouse_id\n            )\n),\n\nqueries_enriched AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.execution_start_time\n        , q.end_time::timestamp AS end_time\n        , q.end_time_max AS end_time_running\n        , q.next_query_at\n        , q.suspended_at\n        , (CASE\n            WHEN q.next_query_at > q.suspended_at THEN q.end_time_max\n            WHEN q.next_query_at > q.end_time_max THEN q.end_time_max\n            WHEN q.next_query_at < q.end_time_max THEN NULL\n            WHEN q.next_query_at IS NULL THEN q.end_time\n            END)::timestamp AS idle_start_at\n        , IFF(idle_start_at IS NOT NULL, LEAST(COALESCE(next_query_at, '3000-01-01'), q.suspended_at), NULL)::timestamp AS idle_end_at\n        , HOUR(execution_start_time::timestamp) = HOUR(q.end_time::timestamp) AS is_same_hour_query\n        , HOUR(idle_start_at) = HOUR(idle_end_at) AS is_same_hour_idle\n        , DATE_TRUNC('HOUR', execution_start_time) AS query_start_hour\n        , DATE_TRUNC('HOUR', idle_start_at) as idle_start_hour\n        , DATEDIFF('HOUR', execution_start_time, q.end_time) AS hours_span_query\n        , DATEDIFF('HOUR', idle_start_at, idle_end_at) AS hours_span_idle\n    FROM queries_filtered AS q\n),\n\nnumgen AS (\n    SELECT\n        0 AS num\n    UNION ALL \n    \n    SELECT\n        ROW_NUMBER() OVER (ORDER BY NULL)\n    FROM table(generator(ROWCOUNT=>24)) -- assuming no one has idle or queries running more than 24 hours\n),\n\nmega_timeline AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , 'query' AS type\n        , q.execution_start_time AS event_start_at\n        , q.end_time AS event_end_at\n        , DATEDIFF('MILLISECOND', event_start_at, event_end_at)*0.001 AS event_time_secs\n        , q.query_start_hour AS meter_start_hour\n        , NULL AS meter_end_hour\n        , q.execution_start_time AS meter_start_at\n        , q.end_time AS meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    WHERE\n        q.is_same_hour_query = TRUE\n    \n    UNION ALL\n    \n    SELECT\n        'idle_' || q.query_id\n        , q.warehouse_id\n        , 'idle' AS type\n        , q.idle_start_at AS event_start_at\n        , q.idle_end_at AS event_end_at\n        , DATEDIFF('MILLISECOND', event_start_at, event_end_at)*0.001 AS event_time_secs\n        , q.idle_start_hour AS meter_start_hour\n        , NULL AS meter_end_hour\n        , q.idle_start_at AS meter_start_at\n        , q.idle_end_at AS meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    WHERE\n        q.is_same_hour_idle = TRUE\n\n    UNION ALL\n\n    SELECT\n        'idle_' || q.query_id\n        , q.warehouse_id\n        , 'idle'\n        , q.idle_start_at AS event_start_at\n        , q.idle_end_at AS event_end_at\n        , DATEDIFF('MILLISECOND', event_start_at, event_end_at)*0.001 AS event_time_secs\n        , DATEADD('HOUR', n.num, DATE_TRUNC('HOUR', q.idle_start_at)) AS meter_start_hour\n        , DATEADD('HOUR', n.num + 1, DATE_TRUNC('HOUR', q.idle_start_at)) AS meter_end_hour\n        , GREATEST(meter_start_hour, q.idle_start_at) as meter_start_at\n        , LEAST(meter_end_hour, q.idle_end_at) as meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    LEFT JOIN numgen AS n\n        ON q.hours_span_idle >= n.num\n    WHERE\n        q.is_same_hour_idle = FALSE\n    \n    UNION ALL\n    \n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , 'query'\n        , q.execution_start_time AS event_start_at\n        , q.end_time AS event_end_at\n        , DATEDIFF('MILLISECOND', event_start_at, event_end_at)*0.001 AS event_time_secs\n        , DATEADD('HOUR', n.num, DATE_TRUNC('HOUR', q.execution_start_time)) AS meter_start_hour\n        , DATEADD('HOUR', n.num + 1, DATE_TRUNC('HOUR', q.execution_start_time)) AS meter_end_hour\n        , GREATEST(meter_start_hour, q.execution_start_time) as meter_start_at\n        , LEAST(meter_end_hour, q.end_time) as meter_end_at\n        , DATEDIFF('MILLISECOND', meter_start_at, meter_end_at)*0.001 AS meter_time_secs\n    FROM queries_enriched AS q\n    LEFT JOIN numgen AS n\n        ON q.hours_span_query >= n.num\n    WHERE\n        q.is_same_hour_query = FALSE\n    ),\n\nmetered AS (\n    SELECT\n        m.query_id\n        , REPLACE(m.query_id, 'idle_', '') as original_query_id\n        , m.warehouse_id\n        , m.type\n        , m.event_start_at\n        , m.event_end_at\n        , m.event_time_secs\n        , m.meter_start_hour\n        , m.meter_start_at\n        , m.meter_end_at\n        , m.meter_time_secs\n        , SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour) AS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN warehouse_metering_history AS w\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time\n),\n\nfinal AS (\n    SELECT\n        m.* EXCLUDE total_meter_time_secs, meter_end_at, original_query_id\n        , q.query_text\n        , q.query_hash\n        , q.warehouse_size\n        , q.warehouse_name\n        , q.role_name\n        , q.user_name\n    FROM metered AS m\n    JOIN queries_filtered AS q\n        ON m.original_query_id = q.query_id\n)\nSELECT\n    *\nFROM final\n;\n```\n\n#### Kyle Cheung\n\noptimization for you, you, you, you, and you\n\n### Comments ()\n\n### You might also like...\n\n## Cut Costs by Querying Snowflake Tables in DuckDB with Apache Arrow\n\nJan 30, 2025\n\n## Querying Snowflake Managed Iceberg Tables with DuckDB\n\nDec 12, 2024\n\n## Getting Started with pyIceberg and AWS Glue\n\nDec 6, 2024\n\n[Powered by Ghost](https://ghost.org/)"
      ],
      "full_content": null
    }
  ],
  "errors": [],
  "warnings": [
    {
      "type": "warning",
      "message": "Neither objective nor search_queries were provided, provide at least one to increase the relevance of excerpts.",
      "detail": null
    }
  ],
  "usage": [
    {
      "name": "sku_extract_excerpts",
      "count": 1
    }
  ]
}

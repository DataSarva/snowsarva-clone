{
  "search_id": "search_cfe001f71ef9438da59aef1bc04d338a",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-understanding-compute",
      "title": "Understanding compute cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nEN\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic tables\nStreams and tasks\nRow timestamps\ndbt Projects on Snowflake\nData Unloading\nStorage lifecycle policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Understanding cost Understanding compute cost\nSection Title: Understanding compute cost \u00b6\nContent:\nCompute costs represent credits used for:\nVirtual Warehouse compute \u2014 Virtual warehouses consume credits as they execute queries, load\ndata and perform other DML operations. Virtual Warehouses are user-managed , which means you can directly\ncontrol credit consumption of these resources.\nServerless compute \u2014\nServerless features use compute resources that are managed by Snowflake instead of using virtual warehouses.\nCompute pools \u2014 Compute pools provide the compute resources for Snowpark Container Services.\nCloud Services compute \u2014 Cloud Services is the layer of the Snowflake architecture that\nperforms services that tie together all the different components of Snowflake to process user requests, login, query display, and more.\nCloud Services compute resources are managed by Snowflake.\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nA virtual warehouse is one or more clusters of compute resources that enable executing queries, loading data, and performing other DML\noperations. The web interface and other features use warehouses, such as Cross-Cloud Auto-Fulfillment or display information in dashboards.\nSnowflake credits are used to pay for the processing time used by each virtual warehouse.\nSnowflake credits are charged based on the number of virtual warehouses you use, how long they run, and their size.\nWarehouses come in many sizes. In this table, the size specifies the compute resources per cluster available to the warehouse.\nEach increase in size to the next larger warehouse approximately doubles the computing power and the number of credits billed per\nfull hour that the warehouse runs.\nFor information on credit consumption, see the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nImportant\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nWarehouses are only billed for credit usage while running. When a warehouse is suspended, it does not use any credits.\nThe credit numbers shown above are for a full hour of usage; however, credits are billed per-second, with a 60-second (i.e. 1-minute)\nminimum:\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nEach time a warehouse is started or resumed , the warehouse is billed for 1 minute\u2019s worth of usage based on the hourly\nrate shown above. Each time a warehouse is resized to a larger size, the warehouse is billed for 1 minute\u2019s worth of usage; however, the number\nof credits billed are only for the additional compute resources that are provisioned. For example, resizing from Small\n(2 credits/hour) to Medium (4 credits/hour) results in billing charges for 1 minute\u2019s worth of 2 additional credits. After 1 minute, all subsequent billing is per-second as long as the warehouse runs continuously. Suspending and then resuming a warehouse within the first minute results in multiple charges because the 1-minute minimum starts\nover each time a warehouse is resumed.\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nResizing a warehouse from 5X-Large or 6X-Large to 4X-Large (or smaller) results in a brief period during which the warehouse is\nbilled for both the new compute resources and the old resources while the old resources are quiesced.\nSection Title: Understanding compute cost \u00b6 > Virtual warehouse credit usage \u00b6\nContent:\nFor more information on warehouses in general, see Overview of warehouses and Warehouse considerations .\nTo learn how to view the historical cost of consuming compute resources with virtual warehouses, see Exploring compute cost .\nSection Title: Understanding compute cost \u00b6 > Serverless credit usage \u00b6\nContent:\nServerless credit usage is the result of features relying on compute resources provided by Snowflake rather than user-managed\nvirtual warehouses. These compute resources are automatically resized and scaled up or down by Snowflake as required for each workload.\nFor these serverless features, which usually require continuous and/or maintenance operations, this model is more efficient, allowing\nSnowflake to charge based on the time spent using the resources. In contrast, user-managed virtual warehouses consume credits while running,\nregardless of whether they are performing any work, which may cause them to be overutilized or sit idle.\nSection Title: Understanding compute cost \u00b6 > Serverless credit usage \u00b6\nContent:\nCharges for serverless features are calculated based on total usage of snowflake-managed compute resources measured in *compute-hours* .\nCompute-Hours are calculated on a per second basis, rounded up to the nearest whole second. The number of credits consumed per compute\nhour varies depending on the serverless feature.\nTo learn how many credits are consumed by a serverless feature, refer to the \u201cServerless Feature Credit Table\u201d in the [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nCharges for the use of a serverless feature appear on your bill as an individual line item. Charges for both Snowflake-managed compute\nresources and Cloud Services appear as a single line item for that serverless feature.\nTo learn how to view the historical cost of using serverless compute resources, see Exploring compute cost .\nSection Title: Understanding compute cost \u00b6 > Compute pool credit usage \u00b6\nContent:\nSnowpark Container Services uses compute pools to run its jobs and services.\nA compute pool is a collection of one or more virtual machine (VM) nodes. The number and type of these nodes determine how many credits the\njob or service consumes as it uses the compute pool.\nFor more information about the cost of compute pools, including how to monitor these costs, see Compute pool cost .\nSection Title: Understanding compute cost \u00b6 > Cloud service credit usage \u00b6\nContent:\nThe cloud services layer of the Snowflake architecture is a collection of services that coordinate activities across Snowflake.\nThis layer authenticates users, enforces security, performs query compilation and optimization, handles request query caching, and more.\nCloud services tie together all of the different components of Snowflake, including supporting the use of virtual warehouses.\nThe cloud services layer is constructed of stateless compute resources, running across multiple availability zones and using a highly\navailable, distributed metadata store for global state management. The cloud services layer runs on compute instances provisioned by\nSnowflake from the cloud provider.\nSimilar to virtual warehouse usage, Snowflake credits are used to pay for the usage of the cloud\nservices.\nSection Title: Understanding compute cost \u00b6 > Cloud service credit usage \u00b6\nContent:\nSnowflake Marketplace calculates compute costs for listing auto-fulfillment to VPS regions by using VPS rates. For details on VPS rates, see [Snowflake Service Consumption Table](https://www.snowflake.com/legal-files/CreditConsumptionTable.pdf) .\nSection Title: Understanding compute cost \u00b6 > ... > Understanding billing for cloud services usage \u00b6\nContent:\nUsage for cloud services is charged only if the daily consumption of cloud services exceeds 10% of the daily usage of virtual warehouses.\nThe charge is calculated daily (in the UTC time zone). This ensures that the 10% adjustment is accurately applied each day, at the credit\nprice for that day.\nKeep the following in mind:\nSection Title: Understanding compute cost \u00b6 > ... > Understanding billing for cloud services usage \u00b6\nContent:\nServerless compute does not factor into the 10% adjustment for cloud services.\nThe 10% adjustment for cloud services is calculated daily (in the UTC time zone) by multiplying daily warehouse usage by 10%.\nThe adjustment on the monthly usage statement is equal to the sum of these daily calculations.\nIf cloud services consumption is less than 10% of warehouse compute credits on a given day, then the adjustment for that day is equal to\nthe cloud services used by your account. The daily adjustment never exceeds actual cloud services usage for that day. Thus, the total\nmonthly adjustment may be significantly less than 10%.\nFor example:\nSection Title: Understanding compute cost \u00b6 > ... > Understanding billing for cloud services usage \u00b6\nContent:\n| Date | Compute Credits Used (Warehouses only) | Cloud Services Credits Used | Credit Adjustment for Cloud Services (Lesser of 10% of Compute or Cloud Services) | Credits Billed (Sum of Compute, Cloud Services, and Adjustment) |\n| Nov 1 | 100 | 20 | -10 | 110 |\n| Nov 2 | 120 | 10 | -10 | 120 |\n| Nov 3 | 80 | 5 | -5 | 80 |\n| Nov 4 | 100 | 13 | -10 | 103 |\n| **Total** | **400** | **48** | **-35** | **413** |\nSection Title: Understanding compute cost \u00b6 > Cloud service credit usage \u00b6 > More about cloud services \u00b6\nContent:\nTo learn how to view the historical cost of consuming cloud services resources, see Exploring compute cost , which\nincludes sample queries you can run to see how much of cloud services consumption was\nactually billed and which queries and warehouses have the highest cloud services usage.\nTo learn about patterns that drive cloud services consumption and ways that you might be able to reduce that consumption, see Optimizing cloud services for cost .\nSection Title: Understanding compute cost \u00b6 > What are credits? \u00b6\nContent:\nSnowflake credits are used to pay for the consumption of resources on Snowflake. A Snowflake credit is a unit of measure, and it is\nconsumed only when a customer is using resources, such as when a virtual warehouse is running, the cloud services layer is performing work,\nor serverless features are used.\n**Next Topic**\n* Exploring compute cost\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nVirtual warehouse credit usage\nServerless credit usage\nCompute pool credit usage\nCloud service credit usage\nWhat are credits?\nRelated content\nUnderstanding overall cost\nExploring compute cost"
      ]
    },
    {
      "url": "https://ternary.app/blog/snowflake-cost-optimization/",
      "title": "Snowflake cost optimization: 8 proven strategies for reducing costs",
      "publish_date": "2025-09-22",
      "excerpts": [
        "Section Title: Snowflake cost optimization: 8 proven strategies for reducing costs\nContent:\n**Last updated:** September 22, 2025\nTernary Team\nSnowflake has quickly become a favorite as a data warehousing platform, and for good reasons. This makes many teams jump in, only to realize later that their [Snowflake spend](https://ternary.app/blog/manage-your-snowflake-costs/) is climbing faster than expected.\nThis is because managing Snowflake costs often becomes a sticking point despite the platform being impressive from a tech POV. That\u2019s where Snowflake cost optimization comes in.\nIn this guide, we\u2019ll explore how Snowflake pricing works and how you can stay in control without giving up performance or flexibility.\nSection Title: ... > Snowflake cost components\nContent:\nSnowflake\u2019s architecture has 3 layers:\nStorage\nCompute (which Snowflake calls virtual warehouses)\nCloud services\nThese layers are billed separately, and Snowflake pricing is usage-based, meaning you get charged for what you actually use.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Storage\nContent:\nEvery file, every table, every backup, all adds up in Snowflake. Snowflake charges a monthly fee based on the average amount of storage used over the month. The data is stored in compressed format. Depending on what kind of data you\u2019re working with, like if you\u2019re pulling in a bunch of raw CSVs versus more compact file types, the compression can significantly lower Snowflake storage costs.\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Cloud services layer\nContent:\nThis layer handles all the coordination across the platform, such as authentication, metadata management, and query optimization. It also runs on Snowflake credits, but the cost here is usually a smaller percentage compared to compute. Still, it adds up if you\u2019ve got a lot going on, especially with serverless features in play.\nSpeaking of credits, let\u2019s clarify this: a Snowflake credit is a unit that measures usage.\nOne credit = one unit of usage. Simple. You\u2019re charged credits whenever you\u2019re running a virtual warehouse, leveraging cloud services, or tapping into Snowflake\u2019s serverless features.\nOne more component that Snowflake charges is data transfer between cloud regions or providers. This applies if you\u2019re using features like external tables or exporting data from Snowflake to a data lake.\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Cloud services layer\nContent:\nThese Snowflake cost components vary depending on whether you\u2019re on Amazon Web Services (AWS), Microsoft Azure, or Google Cloud (GCP), and the pricing structure for that is a bit more granular.\nLook at the tables below for Snowflake data transfer charges for AWS, Azure, and GCP:\n[AWS pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ] [Azure pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ] [GCP pricing guide: [Snowflake data transfer charges](https://www.snowflake.com/pricing/pricing-guide/) ]\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Data loading costs\nContent:\n| **Warehouse used** | Small Standard Virtual Warehouse |\n| **Rate** | 2 credits per hour |\n| **Usage** | 2.5 hours daily for 31 days/month |\n| **Monthly Credits** | 2 credits/hour \u00d7 2.5 hours/day \u00d7 31 days = 155 credits/month |\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Storage costs\nContent:\n| **Data stored** | 4TB compressed |\n| **Rate** | $23 per TB/month |\n| **Annual storage cost** | 4 TB \u00d7 $23 \u00d7 12 months = $1,104/year |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Virtual warehouse cost\nContent:\n| **Credits used per year** | 955 credits/month \u00d7 12 = 11,460 credits/year |\n| **Rate per credit** | $2 (with 5% discount: \u00d7 0.95) |\n| **Annual compute cost** | 11,460 \u00d7 $2 \u00d7 0.95 = $21,774/year |\nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > ... > Total annual cost\nContent:\n| **Storage** | $1,104 |\n| **Virtual warehouse** | $21,774 |\n| **Grand Total** | $22,878 per year |\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\nWhile automatic clustering can improve query performance, it runs on serverless compute. This means it racks up Snowflake credits whether anyone\u2019s actually using the table or not.\nIf the table is only getting hit a few times a week, that background compute activity is just silently chipping away at your budget. This is where smart Snowflake cost optimization begins.\nLook for tables with automatic clustering enabled that barely get queried, say, fewer than 100 times per week. Ask yourself if these tables are part of a disaster recovery setup or being shared with another account. If not, it\u2019s probably safe to hit pause.\nTo suspend automatic clustering, run:\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\n| **ALTER** **TABLE** your_table_name **SUSPEND** RECLUSTER; |\nSection Title: ... > 1. Disable automatic clustering on tables that are barely touched\nContent:\nThis one step alone can help reduce Snowflake costs tied to unnecessary background compute.\nSection Title: ... > 2. Drop materialized views that don\u2019t pull their weight\nContent:\nMaterialized views store precomputed results so queries can run faster.\nBut at the same time, they come with both storage and serverless compute costs to keep everything up to date. So if a materialized view is only being queried, say, ten times a week? You\u2019re paying for upkeep that\u2019s barely getting used.\nIt\u2019s a solid move to suspend or remove any materialized views that aren\u2019t actively helping performance. This falls right into the category of low-effort, high-impact snowflake cost optimization. But again, double-check if the view exists for data sharing or backup purposes before you go on a deletion spree.\nTo drop a materialized view, run:\n ... \nSection Title: ... > 7. Reduce transaction lock wait times with batch updates\nContent:\nA sneaky Snowflake cost drain is when queries get blocked by transaction locks.\nThis happens when multiple users run updates or merges on the same table at the same time. Each command locks the table, and while other queries are waiting, they\u2019re still racking up cloud services credits. So even though nothing\u2019s happening, you\u2019re paying for the wait.\nTo avoid this, change how your updates work. Use batch inserts into temporary tables instead of single-row updates. Then run periodic merges from the temp table to the main one. This cuts down on locks and lets Snowflake handle things more efficiently.\nFor workflows that receive a steady stream of new data, consider using a scheduled task to handle updates at intervals, say, every 15 minutes, instead of processing every change as it comes in.\nIt\u2019s a small shift, but it adds up fast. And it\u2019s one of those Snowflake optimization techniques that improves both performance and billing.\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > What is a KPI in Snowflake?\nContent:\nFor Snowflake cost optimization, KPIs are your best friend.\nSnowflake offers a bunch of performance metrics that, when tracked together, paint a full picture. These include basically anything that has a noticeable impact on credit usage, query speed, or system efficiency.\n ... \nSection Title: ... > How to pick a Snowflake FinOps and cost optimization tool\nContent:\nThere are a lot of moving parts in Snowflake, and the right tool should help you control the chaos, not add to it.\nHere\u2019s how to make the right call:\n ... \nSection Title: Snowflake cost optimization: 8 proven strategies for reducing costs > Final thoughts\nContent:\nAt the end of the day, Snowflake cost optimization comes down to how much visibility you have.\nThe more visibility you have into your Snowflake usage, the better decisions you can make to keep costs in check.\nThat\u2019s exactly where [Ternary](https://ternary.app/blog/manage-your-snowflake-costs/) can help.\nTernary gives your team the insights they need to manage Snowflake spend with confidence.\n**Get a clearer view of your Snowflake costs.**\n[Request a demo](https://ternary.app/demo/)\nSection Title: ... > How much do Snowflake credits cost?\nContent:\nThe cost of Snowflake credits depends on your chosen cloud provider, region, and pricing tier. Credits are consumed when using compute, cloud services, or serverless features.\nSection Title: ... > What is the biggest contributor to Snowflake costs?\nContent:\nCompute is usually the biggest cost driver in Snowflake. Virtual warehouses charge based on per-second usage, and costs vary with warehouse size and workload.\nSection Title: ... > Does Snowflake charge for storing old data?\nContent:\nYes, Snowflake charges for storage based on the average compressed data stored per month. Keeping outdated or unused data increases your storage costs."
      ]
    },
    {
      "url": "https://medium.com/@isushilb/cut-snowflake-costs-by-letting-snowflake-run-the-compute-go-serverless-for-tasks-e74aa80ac97b",
      "title": "Cut Snowflake Costs by Letting Snowflake Run the Compute - Medium",
      "publish_date": "2025-08-23",
      "excerpts": [
        "And because serverless tasks are priced at roughly 90% of equivalent warehouse cost, the savings stack up quickly for high-frequency schedules."
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-understanding-overall",
      "title": "Understanding overall cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nEN\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic tables\nStreams and tasks\nRow timestamps\ndbt Projects on Snowflake\nData Unloading\nStorage lifecycle policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Understanding cost\nSection Title: Understanding overall cost \u00b6\nContent:\nNote\nThis topic describes foundational costs associated with using Snowflake (compute costs, storage costs, and data transfer costs).\nSpecific Snowflake features (for example, Snowflake Cortex and Snowpark Container Services) incur costs in unique ways, and are not\ndiscussed in this topic.\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\nThe total cost of using Snowflake is the aggregate of the cost of using data transfer, storage, and compute resources. Snowflake\u2019s\ninnovative cloud architecture separates the cost of accomplishing any task into one of these\nusage types.\nCompute Resources\nUsing compute resources within Snowflake consumes Snowflake credits. The billed cost of using compute resources is\ncalculated by multiplying the number of consumed credits by the price of a credit. For the current price of a credit, see the [Snowflake Pricing Guide](https://www.snowflake.com/pricing/pricing-guide/) .\nThere are three types of compute resources that consume credits within Snowflake:\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\n**Virtual Warehouse Compute** : Virtual warehouses are user-managed compute resources that consume\ncredits when loading data, executing queries, and performing other DML operations. Because Snowflake utilizes per-second billing (with a\n60-second minimum each time the warehouse starts), warehouses are billed only for the credits they actually consume when they are\nactively working. **Serverless Compute** : There are Snowflake features such as Search Optimization and Snowpipe that use Snowflake-managed compute\nresources rather than virtual warehouses. To minimize cost, these serverless compute resources are automatically resized and scaled\nup or down by Snowflake as required for each workload. **Cloud Services Compute** : The cloud services layer of the Snowflake architecture consumes credits as it performs behind-the-scenes\ntasks such as authentication, metadata management, and access control.\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\nUsage of the cloud services layer is charged only if the daily\nconsumption of cloud services resources exceeds 10% of the daily warehouse usage.\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\nFor more details about compute costs, see Understanding compute cost .\nStorage Resources\nThe monthly cost for storing data in Snowflake is based on a flat rate per terabyte (TB). For the current rate, which\nvaries depending on your type of account (Capacity or On Demand) and region (US or EU), see the [Snowflake Pricing Guide](https://www.snowflake.com/pricing/pricing-guide/) .\nStorage is calculated monthly based on the average number of on-disk bytes stored each day in your Snowflake account.\nFor more details about storage costs, see Understanding storage cost .\nData Transfer Resources\nSnowflake does not charge data ingress fees to bring data into your account, but does charge for data egress.\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\nSnowflake charges a per-terabyte fee when you transfer data from a Snowflake account into a different region on the same cloud platform or into a completely different cloud platform. This fee for data egress depends on the region where your Snowflake account is hosted. For details,\nsee the [Snowflake Pricing Guide](https://www.snowflake.com/pricing/pricing-guide/) .\nFor more details about data transfer costs, see Understanding data transfer cost .\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\nThe following example provides insight into the total cost in Snowflake to load and query data.\nSuppose an organization loads data constantly, 24x7. It has two different groups of users (Finance and Sales) using the database in\noverlapping, but different times of the day. It also runs a weekly batch report. This organization:\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\nUses the Standard Edition of Snowflake.\nStores an average of 65 TBs of compressed data (compare with 325 TB without compression).\nLoads data 24x7x365. They use a Small Standard virtual warehouse for this purpose.\nEnables seven finance users to work 5 days a week from 8am until 5pm using a Large Standard virtual warehouse.\nEnables twelve sales users in different geographies to work a total of 16 hours a day (across Europe and the Americas), 5 days a\nweek using a Medium Standard virtual warehouse.\nRuns a complex weekly report every Friday. This report takes approximately 2 hours to run on a 2X-Large standard warehouse.\n**Data Loading Requirements**\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n| Parameter | Customer Requirement | Configuration | Cost |\n| Loading Window | 24 x 7 x 365 | Small Standard Virtual Warehouse (2 credits/hr) | 1,488 credits |\n| (2 credits/hr x 24 hours per day x 31 days per month) |  |  |  |\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n**Storage Requirements**\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n| Data set size (per month) | 65 TB (after compression) |\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n**Compute Requirements**\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n| Parameter | Customer Requirement | Configuration | Cost |\n| Finance Users | 5 Users, 8am-5pm (9 hours) | Large Standard Virtual Warehouse (8 credits/hr) | 1,440 credits (8 credits/hr x 9 hours per day x 20 days per month) |\n| Sales Users | 12 Users, 16 hour time slot | Medium Standard Virtual Warehouse (4 credits/hr) | 1,280 (4 credits/hr x 16 hours per day x 20 days per month) |\n| Complex Query Users | 1 User, 2 hours/day | 2X Standard Virtual Warehouse (32 credits/hr) | 256 (32 credits/hr x 2 hours per day x 4 days per month) |\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n**Total Cost**\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n| Usage Type | Monthly Cost | Total Billed Cost |\n| Compute Cost | 4,464 credits (@ $2/credit) | $8928 |\n| Storage Cost | 65 TB (@ $23/TB) | $1495 |\n| $10,423 |  |  |\nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n**Next Topics**\nUnderstanding compute cost\nUnderstanding storage cost\nUnderstanding data transfer cost\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nHow are costs incurred?\nTotal cost example\nRelated content\nExploring overall cost\nManaging cost in Snowflake"
      ]
    },
    {
      "url": "https://www.montecarlodata.com/blog-snowflake-cost-optimization/",
      "title": "5 Snowflake Cost Optimization Techniques You Should Know",
      "publish_date": "2025-07-06",
      "excerpts": [
        "Section Title: 5 Simple Steps For Snowflake Cost Optimization Without Getting Too Crazy\nContent:\nAUTHOR | Matt Sulkis\nMost data pros know Snowflake\u2019s pricing model is consumption based\u2013you pay for what you use. What many don\u2019t know is Snowflake actually **WANTS** you to optimize your costs and has provided helpful features to rightsize your consumption.\nWaste isn\u2019t good for anyone. Instead of spinning cycles on deteriorated SQL queries, the data cloud provider would rather have you focus those Snowflake credits toward projects like [building data apps](https://www.montecarlodata.com/blog-treat-your-data-like-an-engineering-problem-an-interview-with-snowflake-director-of-product-management-chris-child/) .\nThese types of projects provide higher value to your business\u2026 and not so coincidentally make their solution stickier and lead to more sustainable increased consumption in the long-term.\n**The key with Snowflake cost optimization initiatives is they too need to be right sized.**\nSection Title: 5 Simple Steps For Snowflake Cost Optimization Without Getting Too Crazy\nContent:\nToo light a touch, and your monthly bill will eventually catch the wandering gaze of your CTO or CFO. Cut corners too hard and you could introduce [data reliability](https://www.montecarlodata.com/blog-what-is-data-reliability/) or [data quality issues](https://www.montecarlodata.com/blog-8-data-quality-issues) that cost more than that $1.25 you were trying to save.\nWithout Snowflake cost optimization efforts, the dreaded CFO gaze may come upon your bill.\nIn that spirit, we have put together a simple three step plan for Snowflake cost optimization without getting too crazy.\nSection Title: ... > Snowflake pricing: how the model works\nContent:\nTo understand Snowflake cost optimization strategies and best practices, you first need to know how the consumption based pricing model works.\n[Actual pricing](https://www.snowflake.com/pricing/) depends on a few different variables such as the cloud provider, region, plan type, services, and more. Since we\u2019re not getting too crazy, we will oversimplify a bit.\nEssentially, your Snowflake cost is based on the actual monthly usage of three separate items: storage, compute, and cloud services. You will be charged for any [Snowflake serverless features](https://docs.snowflake.com/en/user-guide/admin-serverless-billing.html) you use as well.\nExample Snowflake pricing in the AWS \u2013 US East region. Image from Snowflake.com.\nSection Title: ... > Snowflake pricing: how the model works\nContent:\nMost customers pay on-demand with Snowflake credits, but you can pre-purchase capacity as well. For example, the on-demand pricing in the AWS-US East region as of April 2022 is $40 per terabyte per month with Snowflake credits priced at $2.00, $3.00, or $4.00 respectively depending on which plan (Standard, Enterprise, Business Critical) was selected.\nSnowflake also offers dynamic pricing. Snowflake dynamic pricing will **adjust prices in real time to help retailers maximize the profitability of each unit sold** , based on inventory levels, competitors\u2019 pricing, and current trends in consumer demand.\nThis can seem a bit abstract and intimidating at first glance. Converting your average workload into expected Snowflake credit spend takes a bit of math (and what is the conversion of Snowflake credits to Shrute bucks?).\n ... \nSection Title: ... > Is there a Snowflake object that does not consume any storage costs?\nContent:\nYou can get started with Snowflake for free with a [trial account](https://docs.snowflake.com/en/user-guide/admin-trial-account) , and the balance of your free usage decreases as you consume credits to use compute and accrue storage costs.\nIn addition, temporary and transient tables do not incur the same fees as standard permanent tables. This helps to manage the storage costs associated with time travel and fail-safe.\nCourtesy of [Snowflake.](https://docs.snowflake.com/en/user-guide/data-cdp-storage-costs)\nAll of this is to say that Snowflake pricing is mainly driven by credit consumption, which is mainly driven by storage and compute; **which is mainly driven by the amount of tables in your environment, the SQL queries being run on those tables, and the sizes of your data warehouses** .\nSection Title: ... > Is there a Snowflake object that does not consume any storage costs?\nContent:\nThat is why our Snowflake cost optimization strategy will focus on optimizing those three areas by leveraging native features, best practices, and other solutions.\nSection Title: ... > Step 1: Snowflake warehouse size optimization\nContent:\nSnowflake virtual warehouses come in 10 sizes. The larger the warehouse the more credits it consumes per hour of actively running queries.\nImage from Snowflake [pricing guide](https://www.snowflake.com/pricing-page-registration-page/) .\nYou might assume the best Snowflake cost optimization strategy would be to keep your warehouse as small as possible, but that\u2019s not necessarily true. That\u2019s because the larger data warehouses also run queries faster.\nThere isn\u2019t a magical formula for how to optimize warehouse size for your typical workloads\u2013 **it\u2019s a process of trial and error** . That being said, there are some Snowflake cost optimization best practices related to rightsizing your warehouse.\n ... \nSection Title: ... > Group similar workloads in the same virtual warehouse\nContent:\nThis is a great example of how you can \u201cget too crazy\u201d with Snowflake cost optimization because if you set the auto-suspend more tightly than any gaps in your query workload, the **warehouse could end up in a continual state of auto-suspend and auto-resume** .\nIn fact, [Snowflake recommends](https://docs.snowflake.com/en/user-guide/warehouses-considerations.html) considering disabling auto-suspend for a warehouse if there are heavy, steady workloads on the warehouse or you want availability with no lag time.\nEssentially, this strategy comes down to making sure you have the right configuration for the right workload.\nHow Snowflake groups their workloads by virtual warehouse, which can help with Snowflake cost optimization efforts. [Source](https://www.snowflake.com/blog/managing-snowflakes-compute-resources/) .\n ... \nSection Title: ... > Leverage data SLAs to define workloads and value to business\nContent:\nIf you\u2019re not sure where to start with setting [data SLAs](https://www.montecarlodata.com/blog-how-to-make-your-data-pipelines-more-reliable-with-slas/) , check out [this article](https://www.montecarlodata.com/blog-one-sla-at-a-time-our-data-quality-journey-at-red-digital/) from Red Ventures senior data scientist Brandon Beidel on how he approached the task.\n ... \nSection Title: ... > Step 2: Snowflake query optimization\nContent:\nSnowflake does not charge per query. However, for the time that data is being queried, Snowflake credits are being consumed. **Query optimization cost** is the process of optimizing and streamling queries to achieve optimal performance, minimize the use of resources, and reduce Snowflake storage costs.\nThe Snowflake cost optimization trick here is to optimize query code and other settings to enable smoother operations **without compromising the integrity of any jobs** . You also don\u2019t want to reach the point where [the time your data engineers are optimizing code is more expensive than the jobs themselves](https://dataexpert.medium.com/3-things-all-data-engineers-should-learn-from-google-7a8fe917597a) .\nSection Title: ... > Step 2: Snowflake query optimization\nContent:\nBest practices for optimizing Snowflake queries could be an [entirely separate blog](https://www.snowflake.com/blog/how-cisco-optimized-performance-on-snowflake-to-reduce-costs-15-part-2/) and were not getting too crazy, so we will focus our attention here on how to leverage helpful features to identify costly outliers.\nWe will also assume you have optimized your session timeout, queue, and other warehouse settings discussed in the previous section.\n ... \nSection Title: ... > Identifying most expensive, deteriorating, and heavy queries\nContent:\nSnowflake has [documented a helpful query](https://quickstarts.snowflake.com/guide/resource_optimization_billing_metrics/index.html?index=..%2F..index) to determine the most expensive queries ran in the last 30 days.\n[Monte Carlo Data Observability Insights](https://www.snowflake.com/datasets/monte-carlo-data-observability-insights/) is available in the Snowflake Data Marketplace and makes reports on an organization\u2019s data health easily available as tables within their Snowflake environment to build custom workflows and dashboards.\nUh oh. Monte Carlo\u2019s heavy queries report is showing that this query has run 156 times in the last 7 days for a total of just over 15 hours of run time. That\u2019s not good for Snowflake cost optimization.\n ... \nSection Title: ... > Step 3: Snowflake table optimization\nContent:\nQuerying large tables can get expensive. Deprecating unused tables is not only a helpful Snowflake cost optimization strategy, it can make it easier for your data consumers to find what they need.\n ... \nSection Title: ... > Search optimization service vs table clustering, and materialized views\nContent:\nNow let\u2019s say you have a large table with billions of rows where the entire table gets queried often for specific information (or at least parts that aren\u2019t clustered). Think of this use case as needing to get all the associated information around one particular object\u2013say all the financial transactions for a given user or all the coupon usage on a specific product.\nSearch optimization service use cases for Snowflake cost optimization. [Source](https://www.snowflake.com/blog/now-generally-available-snowflakes-search-optimization-service-accelerates-queries-dramatically/) .\nThis is where the newer [**search optimization service**](https://docs.snowflake.com/en/user-guide/search-optimization-service.html) can come in handy, especially for queries running more than tens of seconds and returning at least 100k-200k values.\nSection Title: ... > Search optimization service vs table clustering, and materialized views\nContent:\nBefore you ask, yes this is technically Snowflake query optimization, but we\u2019ve placed it in this section because the action is taken on the table. When you add this serverless feature to the table, the maintenance service creates the optimal search path across the data needed to perform the lookups.\n**A materialized view,** available in the Enterprise Edition, is a pre-computed data set stored for later use. This feature is also an effective Snowflake cost optimization strategy when running frequent or complex queries on a subsection of large data sets.\n[Snowflake recommends leveraging a materialized view when:](https://docs.snowflake.com/en/user-guide/views-materialized.html)\n ... \nSection Title: ... > Step 4: Snowflake data lifecycle and retention optimization\nContent:\nHere\u2019s where many teams miss significant cost savings opportunities. By default, Snowflake retains 1-day time-travel and 7-day fail-safe for all tables, which can dramatically increase storage costs, sometimes up to 90\u00d7 more than the base table size for historical copies.\nUnderstanding the cost implications of Snowflake\u2019s data retention features is crucial for cost optimization without getting too crazy about data recovery capabilities. Standard tables in Snowflake include both [Time Travel](https://docs.snowflake.com/en/user-guide/data-time-travel) , which can range from 1-90 days, and [Fail-safe](https://docs.snowflake.com/en/user-guide/data-failsafe) at 7 days, which means you\u2019re paying for multiple copies of your data. But not every table needs this level of protection.\n ... \nSection Title: ... > Zero-copy cloning for development\nContent:\nOne of Snowflake\u2019s most cost-effective features is [zero-copy cloning](https://docs.snowflake.com/en/user-guide/object-clone.html) . Clones leverage Time Travel mechanics without consuming additional storage until the data diverges. This creates an instant copy of your production data for development or testing without doubling storage costs. As the development environment makes changes, only the delta consumes additional storage.\n ... \nSection Title: ... > Step 5: Snowflake monitoring and governance\nContent:\nEven the best optimization strategies need ongoing monitoring to prevent cost creep and catch issues before they hit your monthly bill. This is where proactive governance becomes your safety net.\n ... \nSection Title: ... > Advanced usage analysis and trending\nContent:\nBeyond basic resource monitors, you need to dig into usage patterns to spot trends before they become problems. This means analyzing warehouse utilization over time, tracking storage growth patterns, and identifying query cost trends.\nYou can use Snowflake\u2019s [ACCOUNT_USAGE views](https://docs.snowflake.com/en/sql-reference/account-usage.html) to create custom reporting that shows which warehouses consume the most credits, how storage costs are growing over time, and which queries are becoming more expensive. These analytical queries help you identify cost trends, unusual spikes, and optimization opportunities before they impact your budget.\n ... \nSection Title: ... > Take your Snowflake cost optimization journey one step at a time\nContent:\nThe business adage goes, \u201cDo you want it done fast, good, or cheap? Pick two.\u201d\nAs a data professional moving slowly is rarely an option, and there is no compromising on data quality\u2013bad data is too expensive no matter the price.\nAs a result Snowflake cost optimization sometimes gets the short shrift\u2013and that\u2019s OK. Snowflake is an efficient technology and getting more cost effective every year.\nHowever, following these Snowflake cost optimization best practices can ensure you minimize waste and put your resources to good use\u2013just don\u2019t get too crazy about it.\n***Interested in how data observability can help you spot inefficiencies in your data stack? Book a time to speak with us in the form below.***\nOur promise: we will show you the product.\nSearch\nSearch"
      ]
    },
    {
      "url": "https://www.e6data.com/query-and-cost-optimization-hub/snowflake-cost-optimization-15-proven-tactics-to-cut-your-snowflake-cost",
      "title": "Snowflake Cost Optimization 2025: Code Hacks and Examples",
      "excerpts": [
        "Section Title: How to Optimize Snowflake Costs? {Updated 2025 Guide}\nContent:\n-> For each category, we outline the purpose of the optimization, how to implement it (with SQL configuration or code examples), alternatives to consider (including Snowflake edition-specific features), and a key takeaway.\n ... \nSection Title: ... > But what actually qualifies as high cost on Snowflake?\nContent:\n| **Scope** | **Rule-of-thumb threshold** | **Why it matters** |\n| **Credit price** | Standard \u2248 $2, Enterprise \u2248 $3, Business-Critical \u2248 $4 per credit (See [Pricing](https://www.snowflake.com/en/pricing-options/) ) | Your edition sets the exchange rate for every optimisation effort, so a 500-credit day can mean $1,000 or $3,000, depending on SKUs. |\n| **Cloud services share** | Paying when cloud services credits are >10% of daily compute | Snowflake\u2019s first [10% of cloud services billing is free](https://community.snowflake.com/s/article/Cloud-Services-Billing-Update-Understanding-and-Adjusting-Usage?utm_source=chatgpt.com) ; if you\u2019re billed more, serverless features are usually running hot. |\n| **Query cost** | Interactive query >0.1 credit (~$0.20-$0.60) is pricey; >1 credit is very expensive | Dashboards often finish under 0.01 credit; anything 10-100x of that usually means full-table scans or huge result sets. |\n ... \nSection Title: ... > But what actually qualifies as high cost on Snowflake?\nContent:\n| **Scope** | **Rule-of-thumb threshold** | **Why it matters** |\n| **Credit price** | Standard \u2248 $2, Enterprise \u2248 $3, Business-Critical \u2248 $4 per credit (See [Pricing](https://www.snowflake.com/en/pricing-options/) ) | Your edition sets the exchange rate for every optimisation effort, so a 500-credit day can mean $1,000 or $3,000, depending on SKUs. |\n| **Warehouse size** | Anything above LARGE (> 8 credits /hour /cluster) is \u201cexpensive\u201d unless you truly need it | X-Small bills 1 credit/hour; each step doubles the cost, so 4XL hits [128 credits/hour](https://docs.snowflake.com/en/user-guide/warehouses-considerations?utm_source=chatgpt.com) . Leaving one running for eight hours burns $2,000-$6,000. |\n| **Serverless features** | Serverless spend (Snowpipe, materialised view, automatic clustering) > 15 % of total compute | These services are convenient but can creep; most accounts keep them in the single-digit percentages. |\nSection Title: ... > But what actually qualifies as high cost on Snowflake?\nContent:\n\u200d\nWith these benchmarks in mind, let\u2019s dive into cost optimization techniques specific to different workloads.\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 1. Throttle the dashboard refresh cadence\nContent:\nMany dashboards refresh on a fixed schedule (every minute, every 5 minutes, etc.). If the refresh is more frequent than the underlying data update, Snowflake ends up executing identical queries repeatedly for no new information.\n_For example, refreshing a dashboard every minute while its source table only updates hourly means 59 out of 60 queries do nothing but consume credits. Over a 12-hour workday, that\u2019s ~708 redundant queries \u2013 essentially money burned with no benefit.\n_\n**How to implement**\nIn Tableau / Power BI, set the extract or live-connection schedule to the lowest frequency your SLA allows.\nIf you use Looker, move from persist_for: \"1 minute\" to persist_for: \"5 minutes\" (or longer, depending on data latency requirements) in the view file.\n**Alternative:**\nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 1. Throttle the dashboard refresh cadence\nContent:\n**Event-driven refreshes-** Rather than using a fixed timer, let the BI tool refresh only when new data actually lands. Many teams do this by emitting a notification from Snowflake\u2014say, via a TASK_SUCCESS event in a pipeline, which triggers an API call (or webhook) that tells the BI platform to update its extract. The dashboard wakes up precisely when it needs to, and never otherwise.\n**Takeaway:** Optimize the dashboard\u2019s query frequency first; warehouse-level tuning comes second. Fewer queries not only cut costs directly, but also mean caches last longer and warehouses can stay suspended more often.\nSection Title: ... > 2. Leverage Snowflake\u2019s Free Result Cache with Stable Snapshots\nContent:\nSnowflake keeps the results of any query for [24 hours](https://docs.snowflake.com/en/sql-reference/functions/result_scan#:~:text=Snowflake%20stores%20all%20query%20results%20for%2024%20hours.%20This%20function%20only%20returns%20results%20for%20queries%20executed%20within%20this%20time%20period.) . If a dashboard runs the exact same SQL during that window, Snowflake serves the answer straight from its result cache\u2014no warehouse spin-up, no credits burned.\nThe catch is that both the query text and the underlying data snapshot must match. Many BI queries have nondeterministic aspects (current timestamps, rolling time windows, etc.) or simply pull \u201clatest data\u201d and thus miss out on result caching.\n**How to implement:**\n**1. Query a frozen snapshot, not the live table**\n ... \nSection Title: ... > 2. Leverage Snowflake\u2019s Free Result Cache with Stable Snapshots\nContent:\n[**Materialized Views**](https://docs.snowflake.com/en/user-guide/views-materialized) for pre-aggregated summaries.\n[**Dynamic Tables**](https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison) (formerly \u201cSnowflake tasks + streams\u201d) for incremental, policy-driven freshness.\nBoth cost credits to maintain, but can be cheaper than rerunning complex and large queries all day. (Note: Materialized Views and Dynamic Tables are Enterprise Edition features)\n2. You can also use a SQL converter, which automatically converts one SQL dialect to another without human effort.\n**Takeaway:** Cached results are instantaneous and free \u2013 every cache hit avoids spinning up a warehouse and burning credits. Achieving this might require minor changes (querying stable snapshots or standardizing SQL), but it can drastically cut the cost of high-frequency dashboards.\n\u200d\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data > ... > **5. Let Snowflake store the heavy aggregates**\nContent:\nDashboards often ask the **same** question\u2014\u201ctotal sales by day,\u201d \u201cinventory on hand,\u201d \u201corder by ID\u201d\u2014again and again. When those queries hit raw fact tables, Snowflake must scan gigabytes or even terabytes every single time, racking up credits. Two built-in features prevent that treadmill:\n[**Materialized Views (MVs)**](https://docs.snowflake.com/en/user-guide/views-materialized) store the *result* of a wide aggregation so the dashboard reads a pre-summarised table instead of crunching numbers anew.\n[**Search Optimization Service (SOS)**](https://docs.snowflake.com/en/user-guide/search-optimization-service) builds a serverless index that pinpoints the exact micro-partitions matching a highly selective filter, skipping the rest.\n**How to set them up:**\nSection Title: Reduce Snowflake costs by 60% with e6data > ... > **5. Let Snowflake store the heavy aggregates**\nContent:\n```\n-- Repeated heavy aggregate? Create an MV. CREATE  MATERIALIZED  VIEW  mv_sales_by_day  AS SELECT  order_date,\n SUM (amount)  AS  rev\n FROM    raw_sales\n GROUP BY  order_date;\n -- Dashboard filters on one customer or order at a time? -- Add SOS so Snowflake jumps straight to the matching partitions. ALTER TABLE  big_orders  ADD SEARCH  OPTIMIZATION;\n```\n\u200d\nMVs refresh automatically\u2014only the changed partitions are recomputed, so maintenance credits stay modest. SOS is serverless: you pay based on index size and updates, not for warehouse time.\nDocs: [Materialised Views](https://docs.snowflake.com/en/user-guide/views-materialized) , [Search Optimization Service](https://docs.snowflake.com/en/user-guide/search-optimization-service)\n**Alternatives:**\n ... \nSection Title: ... > 1. Process only new/changed data (streams and tasks for CDC)\nContent:\nTraditional batch ETL might rebuild a whole table every run (\u201cfull refresh\u201d), even if only 1% of the data is new or updated. This wastes credits on scanning unchanged data and writing out duplicate results.\nSnowflake\u2019s Streams & Tasks framework enables a change-data-capture (CDC) approach: you capture only the deltas (new inserts/updates/deletes) and apply those to your target table. By processing only what\u2019s changed since the last run, each load is dramatically lighter, faster, and cheaper than reprocessing full datasets.\n**How to implement:**\nSection Title: ... > 1. Process only new/changed data (streams and tasks for CDC)\nContent:\n```\n-- 1\ufe0f\u20e3 Create a change-data stream on the raw table CREATE OR  REPLACE STREAM stg_orders\n ON TABLE  raw.orders;\n -- 2\ufe0f\u20e3 Hourly task merges the changes into the warehouse table CREATE OR  REPLACE TASK merge_orders\n   WAREHOUSE  =  etl_wh\n   SCHEDULE   = '1 HOUR' AS MERGE INTO  dw.orders t\n USING  stg_orders s\n ON     t.id  =  s.id\n WHEN  MATCHED  THEN  UPDATE\n WHEN NOT  MATCHED  THEN INSERT ;\n```\n\u200d\nDocs: [Introduction to Streams](https://docs.snowflake.com/en/user-guide/streams-intro)\nSection Title: ... > 1. Process only new/changed data (streams and tasks for CDC)\nContent:\n**Alternative**\n1. If you need sub-second latency, switch to [Snowpipe Streaming](https://medium.com/snowflake/snowpipe-streaming-demystified-e1ee385c6d9c) . It pushes row changes directly into Snowflake and charges per row, bypassing the 60-second minimum a warehouse task would incur. The trade-off is complexity and possibly higher cost per row for very large volumes, so Snowpipe Streaming makes sense for low-latency requirements on moderate data volumes.\n2.  Use another real-time ingest engine that can directly land event streams to Iceberg tables in a queryable format in a zero-ETL fashion in less than a minute.\ne6data\u2019s zero-ETL real-time ingest flow\n\u200d\n**Takeaway** : Capture changes once, merge them once, and spend credits only on data that actually arrived since the last run.\nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 2. Use Serverless Tasks for very short jobs\nContent:\nSnowflake Tasks that run on a schedule using a warehouse have a fundamental billing granularity: if a task uses a warehouse, each run is billed a minimum of 60 seconds of that warehouse, even if the SQL completes in 5 seconds. Many ETL jobs, however, especially staging or small incremental loads, finish in just a few seconds.\nSnowflake\u2019s new Serverless Tasks feature addresses this by letting tasks run without a dedicated warehouse, consuming only the compute they actually need per execution (billed per second with no base charge).\n**How to implement:**\n```\n-- Former warehouse-based task  -- CREATE TASK stg_load WAREHOUSE = etl_wh SCHEDULE = '1 MINUTE' AS \u2026  -- Serverless rewrite  CREATE OR  REPLACE TASK stg_load\n   SCHEDULE  = '1 MINUTE' -- same cadence AS CALL  ingest_stage();              -- your stored procedure\n```\n\u200d\nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 2. Use Serverless Tasks for very short jobs\nContent:\nDocs: [Introduction to Tasks](https://docs.snowflake.com/en/user-guide/tasks-intro)\n**Alternatives:**\n1. If serverless tasks are not yet available in your region or account, you can mitigate the 60-second billing overhead by **batching tiny jobs together** . Instead of running a task every minute, combine the logic to run, say, every 10 or 15 minutes on a small warehouse. This way each run does more work and the 60-second minimum is amortized. You\u2019ll introduce a bit of latency (data might be 10 minutes old instead of 1 minute), but you will drastically cut the number of times you pay that minimum.\n2.  If serverless is not available in your region, consider extending your Snowflake\u2019s reach through e6data\u2019s compatible multi-region and hybrid deployment architecture.\n[](https://docs.snowflake.com/en/user-guide/intro-regions) Cloud regions where Snowflake is available\n\u200d\nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 2. Use Serverless Tasks for very short jobs\nContent:\n**Takeaway:** For bursty workloads that run faster than the billing minimum, using serverless tasks ensures you only pay for the seconds you actually use. Even the smallest warehouse would overcharge you in those cases due to the 60-second floor. Serverless tasks are essentially \u201cpay-per-query\u201d and eliminate the waste for sub-minute jobs.\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 4. Ensure files are optimally sized before loading\nContent:\nIf you cannot control the file sizes (e.g., data arrives as it arrives), consider using Snowflake Tasks to batch-load small files. Instead of Snowpipe ingesting every tiny file immediately, you could accumulate files in a stage and have a task that runs every 10 minutes to load all new files at once. This way, the overhead of initiating load is amortized over many files, and you can leverage a running warehouse to load them in bulk.\nAnother alternative is to leverage Snowpipe Streaming, which charges per row rather than per file, avoiding the file overhead cost.\n**Takeaway:** Aiming for file sizes ~100\u2013250 MB compressed is a widely accepted best practice. It allows Snowflake to fully utilize parallel threads without incurring excessive per-file overhead fees. In practical terms, ten 100 MB files will load faster and cheaper than 1,000 1 MB files or one 1,000 MB file.\n ... \nSection Title: ... > 5. Maximize warehouse utilization (Isolate heavy queries)\nContent:\n**Route heavy queries** \u2013 change the warehouse used by the ETL step, TASK, or connection when it runs those specific queries, leaving the original smaller warehouse for everything else.\nDocs *:* [Snowflake \u201cQueries too large to fit in memory \u2013 finding queries that spill\u201d](http://docs.snowflake.com)\n**Alternatives:**\n**Horizontal scaling:** let a medium warehouse burst to extra clusters by increasing MAX_CLUSTER_COUNT; good when concurrency is the bottleneck, not memory.\n**Reduce spill altogether:** recluster tables or rewrite SQL so filters prune more micro-partitions and less data has to be sorted/aggregated.\n**Tolerate light queueing:** a small amount of queueing often means the warehouse is perfectly saturated and credits are already optimised.\n ... \nSection Title: Reduce Snowflake costs by 60% with e6data > ... > 6.Continuously right-size your warehouses\nContent:\n**Alternative:\n** If you prefer guardrails over dashboards, create a post-job script or governance policy that **resets any warehouse larger than** back to its baseline size once a task finishes. That way oversized settings never live past their moment of need.\n**Takeaway:** Optimisation isn\u2019t a one-off project\u2014it\u2019s a habit. A 10-minute, once-a-month utilisation query keeps warehouse sprawl in check and ensures you\u2019re paying for the horsepower you actually use, not yesterday\u2019s experiments.\n[](https://www.reddit.com/r/snowflake/comments/1fpc66d/how_often_do_you_manage_snowflake_warehouse_sizes/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) Reddit thread: How often do you right-size your warehouse?\n\u200d"
      ]
    },
    {
      "url": "https://keebo.ai/2024/09/17/snowflake-cost-optimization-reduction/",
      "title": "Snowflake Cost Savings: How Do Enterprise Teams Optimize Snowflake Storage and Compute Spend - Keebo",
      "publish_date": "2025-12-03",
      "excerpts": [
        "Section Title: Snowflake Cost Savings: How Do Enterprise Teams Optimize Snowflake Storage and Compute Spend\nContent:\n[](https://keebo.ai/author/barzan/) [Barzan Mozafari](https://keebo.ai/author/barzan/ \"Posts by Barzan Mozafari\")\nSeptember 17, 2024\n[Automation](https://keebo.ai/category/automation/) , [Data Engineering](https://keebo.ai/category/data-engineering/) , [FinOps](https://keebo.ai/category/finops/) , [Reducing Snowflake Costs](https://keebo.ai/category/reducing-snowflake-costs/) , [Warehouse Optimization](https://keebo.ai/category/warehouse-optimization/)\nSection Title: Snowflake Cost Savings: How Do Enterprise Teams Optimize Snowflake Storage and Compute Spend\nContent:\nSnowflake is popular among data teams for a number of reasons: scalability, performance, and ease-of-use. But on the flip side, it\u2019s easy to spend more on Snowflake than you originally planned. So, how do enterprise teams optimize Snowflake storage and compute spend while maintaining performance and efficiency? If you\u2019re looking for ways to reduce Snowflake spend, rightsize your contracts, or get the most out of your credits, you\u2019re in the right place. Read on for our tried-and-true Snowflake cost optimization best practices.\nSection Title: ... > Contents\nContent:\nWhat is Snowflake cost optimization?\nIs Snowflake a cost effective solution?\nHow does Snowflake pricing work?\nWhy is Snowflake cost optimization important?\nTop 6 Snowflake cost reduction strategies\nSnowflake cost optimization examples\nSection Title: ... > What is Snowflake cost optimization?\nContent:\nSnowflake cost optimization is the process of monitoring Snowflake usage, identifying unnecessarily high spend, and adjusting your configuration or usage to reduce costs.\nA common misconception is that optimizing Snowflake costs comes at the expense of performance. In our experience, this doesn\u2019t have to be the case. Some cost optimization techniques (e.g. query routing) actually maintain and increase performance while reducing costs. Others (like warehouse optimization) can be configured to only reduce costs with no or very limited negative performance impact.\nSection Title: ... > Is Snowflake a cost effective solution?\nContent:\nSnowflake can be a cost effective data cloud solution. Or, it can be too costly. It depends on how you use it. Understanding how do enterprise teams optimize Snowflake storage and compute spend becomes the defining factor in determining overall cost efficiency.\nOn paper, yes, Snowflake seems to offer a number of options to help achieve significant **Snowflake cost savings** . Limited overhead, flexible pricing, and scalability. If you control your usage and only buy what you need, you can achieve a significant **Snowflake cost reduction** , especially compared to [on-prem or other cloud](https://www.cleo.com/blog/knowledge-base-on-premise-vs-cloud) data platforms.\n ... \nSection Title: ... > Serverless compute\nContent:\nOther Snowflake features like Search Optimization and Snowpipe use serverless compute resources, which Snowflake automatically scales up or down for each workload.  This can be cost-effective for irregular workloads or when you need to handle spikes in activity without maintaining a continuously running virtual warehouse.\nSection Title: ... > Cloud services compute\nContent:\nSnowflake also consumes credits for tasks like authentication, metadata management, API, SQL, query parsing and access control if those tasks exceed 10% of daily warehouse usage.\nSection Title: ... > How much do Snowflake credits cost?\nContent:\nSnowflake credits are priced based on which Snowflake pricing tier (called \u201cEditions\u201d) you use and organizations often leverage the **best snowflake cost management tools** to track and control these costs effectively. There are four [Snowflake Editions](https://docs.snowflake.com/en/user-guide/intro-editions) available to users.\n ... \nSection Title: ... > What does Snowflake use to measure your costs?\nContent:\nWhen exploring what are the leading tools for Snowflake cost visibility, it\u2019s essential to first understand how Snowflake measures your usage and spending. The vast majority of Snowflake costs are measured via your Snowflake Edition, which determines the cost per credit, and the number of credits consumed. By controlling your credit consumption, you can reduce your Snowflake costs.\nSection Title: ... > Why is Snowflake cost optimization important?\nContent:\n[Snowflake optimization](https://keebo.ai/2024/01/19/snowflakes-optimization-advice-then-and-now/) offers a number of benefits that go beyond just reducing your current spend levels. It can also help justify a lower Snowflake contract upon renewal. Additionally, it can help you get the most out of your current provisioned credits.\nSection Title: ... > Reduce Snowflake spend\nContent:\nIf you\u2019re on an On-Demand plan, the immediate benefit of Snowflake cost optimization is a reduction in Snowflake spend leading to measurable **Snowflake cost savings** .\nHowever, it\u2019s important that the optimization approach you use doesn\u2019t consume more resources than you save. For example, if a data engineer spends hours manually adjusting your warehouse, it could end up being a net negative.\n ... \nSection Title: ... > 6 Snowflake cost optimization strategies \u2013 How to save costs on Snowflake\nContent:\nRegardless of your Snowflake cost reduction objectives, understanding what are the leading tools for Snowflake cost visibility is essential to achieving meaningful savings. Today, a wide range of Snowflake cost management tools are available to help organizations monitor, analyze, and reduce their data warehouse expenses. These include both simple, easy-to-manage solutions and advanced platforms that offer deep insights and automation capabilities.\n ... \nSection Title: ... > Rightsizing your virtual warehouses\nContent:\nAnother Snowflake cost optimization tactic is to rightsize your virtual warehouse to match incoming workloads. This approach ensures that you only provision resources you need to complete the task and no more.\nIt\u2019s easy to [adjust your Snowflake warehouse](https://docs.snowflake.com/en/user-guide/performance-query-warehouse-size) sizes in Snowsight or the classic console. Here\u2019s an example of how to make these changes in SQL:\n```\nALTER WAREHOUSE my_wh SET WAREHOUSE_SIZE = small;\n```\nAdjusting warehouse sizes isn\u2019t a set-it-and-forget it process. Far from it. Snowflake workloads are highly dynamic, often changing minute-to-minute (or second-to-second). If your warehouses are too small to handle a suddenly resource-heavy workload, you can end up consuming more credits.\nHere\u2019s an example of what we mean:\n ... \nSection Title: ... > Rightsizing your virtual warehouses\nContent:\nBecause Snowflake has a 60-second minimum for query execution, scaling up to an XL, while speeding up query performance, consumes more credits.\nYou can see how even with this simple example there are complex factors going into Snowflake spend. Given how quickly these changes occur, you can see how this could quickly become someone\u2019s full-time job. If you have to pay an engineer for their time spent on cost optimization, it\u2019s not truly a net positive.\nThat\u2019s one reason we recommend using an AI-powered tool from trusted **snowflake cost management tool brands** to handle this. That way, your engineers are freed up to perform more value-add tasks and projects.\n ... \nSection Title: ... > 4. Data loading patterns\nContent:\nAlthough Snowflake doesn\u2019t charge for incoming data, your data loading patterns can impact your storage and compute costs making it helpful to use the **best snowflake cost management tools** to identify inefficiencies. As such, optimizing how you load your data can reduce your Snowflake spend down the line.\n ... \nSection Title: ... > 5. Performance tuning\nContent:\n[Snowflake performance tuning](https://keebo.ai/2024/07/16/snowflake-performance-tuning-what-data-teams-typically-get-right-and-wrong/) involves a host of tactics that optimize three broad categories of Snowflake activity: data loading, data transformation, and query & task execution times. Some of the tactics we\u2019ve discussed thus far fall into the broad category of performance tuning, but here are three more to add to the list.\n ... \nSection Title: ... > Snowflake budgets\nContent:\nAnother option for keeping your costs under control are [Snowflake budgets](https://keebo.ai/2023/11/17/snowflake-budgets-can-they-help-you-control-costs/) . These budgets track credit usage across all your tables, materialized views, schemas, databases, warehouses, pipes, and tasks. Once you exceed your budget, you\u2019ll automatically get a notification. But if you want to take action to keep your costs under control, you then have to implement one of the strategies listed above.\nSection Title: ... > What strategies should you use to manage compute costs in Snowflake?\nContent:\nThe most common strategies we recommend to manage compute costs in Snowflake include using tools from reliable **snowflake cost management tool brands** , along with warehouse optimization & query routing. Warehouse optimization involves dynamically configuring your warehouses to respond to highs and lows in resource needs. Query routing involves sending queries to unused or underutilized warehouses to ensure you\u2019re getting the most out of your existing resources.\n ... \nSection Title: ... > Snowflake cost optimization examples\nContent:\nLet\u2019s now take a look at three examples of Snowflake cost optimization in practice. Each of these companies used Keebo to automate warehouse optimization and significantly reduced their Snowflake costs as a result.\n ... \nSection Title: ... > Final thoughts on Snowflake cost optimization\nContent:\nLike any other cloud platform, it\u2019s easy for Snowflake costs to get out of control. Snowflake cost optimization, on the other hand, requires intentional, consistent action. Even large data teams struggle to get a handle on all the various practices necessary to reduce your costs, which is why adopting solutions from top **snowflake cost management tool brands** can make the process much easier.\nThat\u2019s why we recommend using an AI-powered tool like Keebo, one of the **best snowflake cost management tools** , to make real-time adjustments that enable you to both save money and also get the most out of your current Snowflake spend.\nThe results speak for themselves. So every second you spend waiting to get started is money and credit usage you\u2019re leaving on the table.\n[*Get started with Keebo now*](https://keebo.ai/contact/) *, and you could start reducing your Snowflake costs by the end of the day.*\n ... \nSection Title: ... > [How Keebo Cuts Snowflake Costs with Smart Optimization Tools](https://keebo.ai/2026/01/26/...\nContent:\nFebruary 5, 2026\n[](https://keebo.ai/2026/01/26/databricks-cost-optmization-strategies/)"
      ]
    },
    {
      "url": "https://www.reddit.com/r/snowflake/comments/1f9onyr/been_diving_deep_into_snowflake_costs_the_last/",
      "title": "Been diving deep into Snowflake costs the last few days, so here are the basics of Snowflake costs and credit consumption : r/snowflake",
      "publish_date": "2024-09-05",
      "excerpts": [
        "Skip to main content Been diving deep into Snowflake costs the last few days, so here are the basics of Snowflake costs and credit consumption : r/snowflake Open menu Open navigation  Go to Reddit Home\nr/snowflake\nExpand user menu Open settings menu\nGo to snowflake r/snowflake \u2022 1y ago\nhornyforsavings [\u0420\u0443\u0441\u0441\u043a\u0438\u0439](https://www.reddit.com/r/snowflake/comments/1f9onyr/been_diving_deep_into_snowflake_costs_the_last/?tl=ru) [T\u00fcrk\u00e7e](https://www.reddit.com/r/snowflake/comments/1f9onyr/been_diving_deep_into_snowflake_costs_the_last/?tl=tr) [Deutsch](https://www.reddit.com/r/snowflake/comments/1f9onyr/been_diving_deep_into_snowflake_costs_the_last/?tl=de) [Magyar](https://www.reddit.com/r/snowflake/comments/1f9onyr/been_diving_deep_into_snowflake_costs_the_last/?tl=hu)\nSection Title: Been diving deep into Snowflake costs the last few days, so here are the basics of Snowflake cost...\nContent:\n[](https://greybeam.medium.com/understanding-snowflake-compute-costs-aa649e7b2e93)\n[](https://greybeam.medium.com/understanding-snowflake-compute-costs-aa649e7b2e93) [](https://greybeam.medium.com/understanding-snowflake-compute-costs-aa649e7b2e93)\n[greybeam.medium.com Open](https://greybeam.medium.com/understanding-snowflake-compute-costs-aa649e7b2e93)\nShare\nastro_manager \u2022 Promoted\ndbt \ud83e\udd1d cosmos \ud83e\udd1d airflow\nDownload\nastronomer.io\ngilbertoatsnowflake\n\u2022 1y ago\nThis is timely, as Snowflake just announced per-query-cost attribution today in GA. Docs here: [https://docs.snowflake.com/sql-reference/account-usage/query_attribution_history?trk=public_post_comment-text](https://docs.snowflake.com/sql-reference/account-usage/query_attribution_history?trk=public_post_comment-text)\nSection Title: Been diving deep into Snowflake costs the last few days, so here are the basics of Snowflake cost...\nContent:\nBlog post here: https://medium.com/snowflake/granular-cost-attribution-and-chargeback-for-warehouse-costs-on-snowflake-cf96fb690967\n7\nbrokenglasshero\n\u2022 1y ago\nThis is amazing\n2\nhornyforsavings\n\u2022 1y ago\nI was doing some testing and this view is completely wrong on my account. I chatted with Kaushal ,the PM who released this feature, and he told me to submit a support ticket haha. I'm going to be releasing an article on Monday on how we calculate cost per query incorporating costs into idle time too!\n1 Continue this thread\nSp00ky_6\n\u2022 1y ago\nYep, following these guidelines can keep costs well under control. Tagging imo is the most underused feature in snowflake by so many customers to track and attribute costs.\n2\njbrune\n\u2022 1y ago\nSection Title: Been diving deep into Snowflake costs the last few days, so here are the basics of Snowflake cost...\nContent:\n```\n\u2022 Snowflake\u2019s innovative cloud architecture separates the cost of accomplishing any task into one of these usage types. \u2022 total cost is the aggregate of \n\t\u25cb using data transfer\n\t\u25cb storage\n\t\t\u00a7 Flat rate per TB (based on monthly avg)\n\t\u25cb compute resources\n\t\t\u00a7 Virtual warehouses\n\t\t\t\u25a1 You can directly control credit consumption\n\t\t\u00a7 Serverless\n\t\t\t\u25a1 Snowpipe, automatic clustering, external tables, materialized views, tasks, etc\n\t\t\u00a7 Cloud services layer\n\t\t\t\u25a1 performs services that tie together all the different components like logins, etc. (usually no charge)\nFrom <https://docs.snowflake.com/en/user-guide/cost-understanding-compute> \n\n\u2022 Warehouses: Credit listed as per hour usage, but are billed at 1 sec increments, with a minimum of 1 min\n\u2022 Serverless: Charges based on total usage of compute resources measured in compute-hours.\nSection Title: Been diving deep into Snowflake costs the last few days, so here are the basics of Snowflake cost...\nContent:\nCompute-Hours are calculated on a per second basis, rounded up to the nearest whole second. The number of credits consumed per compute hour varies depending on the serverless feature. To learn how many credits are consumed by a serverless feature, refer to the \u201cServerless Feature Credit Table\u201d in the Snowflake service consumption table. Charges for the use of a serverless feature appear on your bill as an individual line item. Charges for both Snowflake-managed compute resources and Cloud Services appear as a single line item for that serverless feature. \u2022 Cloud Services: Only charged if > 10% of your warehouse usage, and never exceeds the cloud usage\n\n\u2022 Snowflake does not charge data ingress fees to bring data into your account, but does charge for data egress.\nSection Title: Been diving deep into Snowflake costs the last few days, so here are the basics of Snowflake cost...\nContent:\n\u25cb Different region, same platform\n\t\u25cb Or different cloud platform\n\t\u25cb Per-byte fee depends on the region your account is hosted\n\t\u25cb $20-$90/TB, same cloud/different cloud\n\t\u25cb Different cloud might incur AWS charges\n\n\u2022 Use queries against the views in the account_usage or organization_usage schemas. \u2022 Org is for all Accounts\n\u2022 Can also use 'tags' to group charges together. e.g., warehouse1 & 2 have tag Cost_Ctr:Sales, warehouse3 has tag Cost_Ctr:Support\n```\nSection Title: Been diving deep into Snowflake costs the last few days, so here are the basics of Snowflake cost...\nContent:\n1\nSection Title: Related Answers Section\nContent:\nRelated Answers\nTop features of Snowflake for data analysts\nSnowflake is a popular cloud data warehouse known for its ease of use, performance, and extensive features that benefit data analysts. Here are some of the top features and benefits of Snowflake as highlighted by Redditors:\nSection Title: Related Answers Section > Ease of Use & User Experience\nContent:\n**Convenience and Top-Notch UX** : Many users praise Snowflake for its convenience and excellent user experience. [\"Snowflake user experience is top notch. My most recent job is fully invested into snowflake and it\u2019s so smooth to work with I don\u2019t think I\u2019d take a job maintaining any other kind of warehouse after this. \"](https://www.reddit.com/r/dataengineering/comments/1o03g0b/comment/ni6pewv/)\n**Simplicity and Reliability** : Snowflake is appreciated for its simplicity and reliability, which allows data analysts to focus on insights rather than infrastructure maintenance. [\"As someone who went from a company running on-prem data warehouses to one that uses snowflake, I really could care less about the features, the biggest positive for me is that it just straight up works.\nSection Title: Related Answers Section > Ease of Use & User Experience\nContent:\n\"](https://www.reddit.com/r/dataengineering/comments/1o03g0b/comment/ni6wmgu/)\n**Well-Documented and Thought Out** : The platform is noted for its comprehensive documentation and thoughtful design. [\"Snowflake is by far my favourite. My initial reaction to it was how well thought out it was and how well documented. \"](https://www.reddit.com/r/dataengineering/comments/1o03g0b/comment/ni7k5vv/)\nSection Title: Related Answers Section > Ease of Use & User Experience > Performance & Scalability\nContent:\n**Fast Query Performance** : Snowflake is known for its fast query performance, even on large datasets. [\"It\u2019s extremely fast to boot.\"](https://www.reddit.com/r/dataengineering/comments/1o03g0b/comment/ni6pewv/)\n**Dynamic Tables** : For managing aggregated data, Snowflake's Dynamic Tables offer an efficient solution. [\"Have you considering using Dynamic Tables for this?\"](https://www.reddit.com/r/snowflake/comments/1pnl32m/comment/nu8tbap/)\nSection Title: Related Answers Section > Ease of Use & User Experience > AI/ML Capabilities\nContent:\n**Native AI/ML Functions** : Snowflake offers numerous native AI functions, which are in General Availability (GA) and provide advanced capabilities. [\"Snowflake has a lot more AI funtions than DBX. They are all in GA vs preview in DBX. \"](https://www.reddit.com/r/snowflake/comments/1pjgfew/comment/ntdc9st/)\n**Snowflake Intelligence** : This tool allows for complex analytical queries using semantic views. [\"SNOWFLAKE intelligence with Cortex Analyst added as an agent that is using a sematic view that is configured with your tables. Best thing since sliced bread\"](https://www.reddit.com/r/snowflake/comments/1o3vpx6/comment/niznitw/)\n**Cortex Analyst** : For ad-hoc queries and data analysis, Cortex Analyst is highly recommended. [\"Connect the data to a. Cortex Analyst, and then connect the Analyst to a Cortex agent, and then run the adhoc questions in Intelligence using the agent.\nSection Title: Related Answers Section > Ease of Use & User Experience > AI/ML Capabilities\nContent:\n\"](https://www.reddit.com/r/snowflake/comments/1o3vpx6/comment/niy08as/)\nSection Title: Related Answers Section > Ease of Use & User Experience > Data Governance & Security\nContent:\n**Robust Data Governance** : Snowflake is praised for its strong data governance capabilities. [\"Limited data governance? Tell me you haven\u2019t read the documentation without telling me sheesh\"](https://www.reddit.com/r/dataengineering/comments/1o03g0b/comment/ni7fsrz/)\n**Secure Data Access** : It allows for secure data access and inherits security permissions. [\"Runs natively in Snowflake, inherits security permissions.\"](https://www.reddit.com/r/snowflake/comments/1o3vpx6/comment/niy9ipr/)\nSection Title: Related Answers Section > Ease of Use & User Experience > Flexibility & Stack Compatibility\nContent:\n**Solid Starter Stack** : Snowflake is often part of a recommended stack for data professionals. [\"Yes, that combo is a solid starter stack. Plenty of teams run Python for extract, dbt on Snowflake and Prefect for orchestration.\"](https://www.reddit.com/r/dataengineering/comments/1mrds3t/comment/ncblmuu/)\n**Flexible Table Types** : Snowflake supports both fully managed standard tables and customer-owned Iceberg Lakehouse tables. [\"Snowflake supports both fully managed and secured standard tables as well as customer owned Iceberg Lakehouse tables.\"](https://www.reddit.com/r/snowflake/comments/1pjgfew/comment/ntdc9st/)\nSection Title: Related Answers Section > Ease of Use & User Experience > Cost & Efficiency\nContent:\n**Cost-Effective Aggregation** : Dynamic Tables can be more cost-effective for managing aggregated data. [\"It should be less fussy overall, and can be more cost effective.\"](https://www.reddit.com/r/snowflake/comments/1pnl32m/comment/nu8tbap/)\n**Serverless Tasks** : Snowflake's serverless tasks allow for efficient compute resource management. [\"Serverless tasks being able to share same set of compute that you can size to fit your needs.\"](https://www.reddit.com/r/snowflake/comments/1pjgfew/comment/ntdc9st/)\nSection Title: Related Answers Section > Ease of Use & User Experience > Subreddits for Further Questions\nContent:\n[r/snowflake](https://www.reddit.com/r/snowflake/)\n[r/dataengineering](https://www.reddit.com/r/dataengineering/)\n[r/dataanalytics](https://www.reddit.com/r/dataanalytics/)\nSee Answer\nHow to integrate Snowflake with Python\nCommon pitfalls when using Snowflake\nTips for managing costs in Snowflake\nHow to secure data in Snowflake effectively\nSection Title: More posts you may like\nContent:\nChristmas Snowflake is now live + over 450 free coupons r/GalaxyWatchFace \u2022 5d agoChristmas Snowflake is now live + over 450 free coupons29 upvotes \u00b7 25 comments\nWhat is your monthly Snowflake cost? r/snowflake \u2022 2mo agoWhat is your monthly Snowflake cost?14 upvotes \u00b7 21 comments\nChristmas snowflakes r/resin \u2022 6d agoChristmas snowflakes1058 upvotes \u00b7 9 comments\nDo you know what the 5 most important Snowflake features are for 2026? r/snowflake \u2022 1mo agoDo you know what the 5 most important Snowflake features are for 2026?[](https://medium.com/@tom.bailey.courses/the-5-snowflake-features-that-will-define-2026-a1b720111a0b \"Link from r/snowflake - Do you know what the 5 most important Snowflake features are for 2026?\")medium31 upvotes \u00b7 14 comments\nThe Christmas bonus needs to go somewhere... r/airsoft \u2022 5d agoThe Christmas bonus needs to go somewhere...35 upvotes \u00b7 10 comments\nChristmas bonuses r/Construction \u2022 4d agoChristmas bonuses97 upvotes \u00b7 106 comments\n ... \nSection Title: More posts you may like > View Post in\nContent:\n[\u65e5\u672c\u8a9e](https://www.reddit.com/r/snowflake/comments/1f9onyr/been_diving_deep_into_snowflake_costs_the_last/?tl=ja)\nPublic\nAnyone can view, post, and comment to this community\n0 0"
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}

{"search_id":"search_3c75fdec8f7f442a9bada50d2110a0ff","results":[{"url":"https://docs.snowflake.com/en/user-guide/tasks-graphs","title":"Create a sequence of tasks with a task graph | Snowflake Documentation","excerpts":["Guides Data engineering Streams and tasks Tasks Task graphs\nSection Title: Create a sequence of tasks with a task graph ¶\nContent:\nIn Snowflake, you can manage multiple tasks with a *task graph* , also known as a directed acyclic graph (DAG). A task graph is composed of a root task and dependent child tasks. The dependencies must run in a start-to-finish direction, with no loops. An optional final task, called a *finalizer* , can perform cleanup operations after all other tasks are complete.\nBuild task graphs that have dynamic behavior by specifying logic-based operations in the task body using runtime values, graph level configuration, and return values of parent tasks.\nYou can create tasks and task graphs using supported languages and tools like SQL, JavaScript, Python,\nJava, Scala, or Snowflake Scripting. This topic provides SQL examples. For Python examples, see Managing Snowflake tasks and task graphs with Python .\nYou can also use Snowsight to manage and view your task graphs. For more information, see View tasks and task graphs in Snowsight .\nSection Title: Create a sequence of tasks with a task graph ¶ > Create a task graph ¶\nContent:\nCreate a root task using CREATE TASK , then create child tasks using CREATE TASK .. AFTER to select the parent tasks.\nThe root task defines when the task graph runs . Child tasks are executed in the order defined by the task graph.\nWhen multiple child tasks have the same parent, the child tasks run\nin parallel.\nWhen a task has multiple parents, the task waits for all\npreceding tasks to successfully complete before starting.\n(The task may also run when some parent tasks are skipped. For\nmore information, see Skip or suspend a child task ).\n ... \nSection Title: Create a sequence of tasks with a task graph ¶ > View dependent tasks in a task graph ¶\nContent:\nTo view the child tasks for a root task, call the TASK_DEPENDENTS table function. To retrieve all tasks in a task graph, input the root task when calling the function.\nYou can also use Snowsight to manage and view your task graphs. For more information, see View tasks and task graphs in Snowsight .\n ... \nSection Title: Create a sequence of tasks with a task graph ¶ > Unlink parent and child tasks ¶\nContent:\nDependencies between tasks in a task graph can be severed as a result of the following actions:\nALTER TASK … REMOVE AFTER and ALTER TASK … UNSET FINALIZE remove the link between the target task and the specified\nparent tasks or finalized root task.\nDROP TASK and GRANT OWNERSHIP sever all the target task’s links. For example, root task A has child task B, and task B has child task C. If you drop task B, the link between task A and B is severed and so is the link between task B and C.\nIf any combination of the above actions severs the relationship between the child task and all parent tasks, the\nchild task becomes either a standalone task or a root task.\nNote\nIf you grant the ownership of a task to its current owner, dependency links might not be severed."]},{"url":"https://www.snowflake.com/en/developers/guides/getting-started-with-task-graphs/","title":"Getting Started with Snowflake Task Graphs","excerpts":["With task graphs in Snowflake , you can automatically run sequences of tasks . A task graph, or directed acyclic graph, is a series of tasks composed of a ... Read more A task graph, or directed acyclic graph, is a series of tasks composed of a root task and child tasks, organized by their dependencies. Task graphs flow in a ..."]},{"url":"https://www.snowflake.com/en/developers/guides/getting-started-with-streams-and-tasks/","title":"Getting Started with Streams & Tasks - Snowflake","excerpts":["Section Title: Getting Started with Streams & Tasks\nContent:\nSteven Maser [Snowflake]\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/getting-started-with-streams-and-tasks)\nSection Title: Getting Started with Streams & Tasks > Overview\nContent:\nThis guide will take you through a scenario of using Snowflake's Tasks and Streams capabilities to ingest a stream of data and prepare for analytics.\nStreams provides a change tracking mechanism for your tables and views, enabling and ensuring \"exactly once\" semantics for new or changed data.\nTasks are Snowflake objects to execute a single command, which could be simple SQL command or calling an extensive stored procedure. Tasks can be scheduled or run on-demand, either within a Snowflake Virtual warehouse or serverless.\nThis Lab will also construct a Directed Acyclic Graph (DAG), which is a series of tasks composed of a single root task and additional tasks, organized by their dependencies. Tasks will be combined with table streams to create data pipelines, continuous ELT workflows to process recently received or changed table rows.\nSection Title: Getting Started with Streams & Tasks > Overview\nContent:\nA simulated streaming datafeed will be used for this exercise, using a Snowpark-based Stored Procedure, to simplify your setup and focus on these two capabilities. The simulation will be high-volume, at 1 million transactions a minute (exceeding 15k/second), of credit card purchases and returns.\nThis prerequisite streaming ingestion was modeled to mirror one created from Snowflake's Kafka Connector, without the distraction of having to setup a running Kafka instance.\nWhile not covered in this exercise, one can use these building blocks to further enrich your data with Snowflake Marketplace data, train and deploy machine learning models, perform fraud detection, and other use cases by combining these skills with other Snowflake virtual hands-on labs.\n ... \nSection Title: Getting Started with Streams & Tasks > Create Data Pipeline #2 > d) View DAG Orchestration\nContent:\nIn a new tab (use right-click on \"home icon\" at upper left), Navigate in Snowsight to:\n**Data>Databases>VHOL_ST>PUBLIC>Tasks>LOAD_TASK**\nReview \"Task Details\" tab:\nReview \"Graph\" to see graphical representation of our simple flow:\nRemember your warehouse is 'VHOL_WH'\nReview \"Run History\" to view Task Executions:\nNote: Graph and Task History are recently added previews, so may need to ask on how to enable in your account if these menu options are missing.\n ... \nSection Title: Getting Started with Streams & Tasks > Create Data Pipeline #2 > g) Run Load Task\nContent:\nAfter this addition, resume/enable both tasks. Note we utilized the \"AFTER\" option during creation rather than using \"ALTER\" afterward to create this dependency to the DAG process.\nalter task WAIT_TASK RESUME;\nalter task LOAD_TASK RESUME;\n```\n\nCopy\n```"]},{"url":"https://sonra.io/data-orchestration-deep-dive-snowflake-tasks-an-airflow-replacement/","title":"Data Orchestration Deep Dive Snowflake Tasks. An Airflow replacement? ","publish_date":"2024-12-18","excerpts":["Section Title: ... > Basic Dependencies in Snowflake\nContent:\nJust like in Airflow, Snowflake allows for the definition of dependencies between tasks to construct both linear and complex workflows, including fan-out/fan-in patterns.\n ... \nSection Title: Data Orchestration Deep Dive Snowflake Tasks. An Airflow replacement? > ... > Linear Dependencies\nContent:\nOverall, this setup establishes a sequential flow of tasks, where each task depends on the successful completion of its preceding task. Raw order data is first extracted, then transformed, and finally loaded into Snowflake tables for further analysis or reporting purposes. The use of CRON expressions allows for automated scheduling of these tasks according to specified time intervals.\nSection Title: ... > Fan-in/-out dependencies\nContent:\nIn Snowflake, task dependencies can be structured to handle more complex patterns, including fan-in and fan-out structures. These structures enable orchestration of tasks where multiple upstream tasks contribute to a single downstream task (fan-in), or a single upstream task triggers multiple downstream tasks (fan-out). Below, we’ll explore a scenario illustrating both fan-in and fan-out dependencies using a different example.\n**Example Scenario: E-commerce Data Processing**\nConsider an e-commerce platform that processes data from various sources, including user activity logs and product inventory databases. The platform requires daily updates to ensure timely analysis and decision-making. Let’s design a Snowflake task dependency structure for this scenario.\nSection Title: Data Orchestration Deep Dive Snowflake Tasks. An Airflow replacement? > ... > Fan-Out Structure\nContent:\n**Start Task:**\nRepresents the initiation of the workflow.\nFans out to the fetch tasks for each data source.\n|1\n2\n3\n4\n5 |CREATE TASK start_data_processing_task\nWAREHOUSE = 'COMPUTE_WH'\nSCHEDULE = 'USING CRON 0 0 * * * UTC' -- Run daily at midnight\nAS\nCALL start_data_processing_proc ( ) ; |\n| --- | --- |\n**Fetch User Activity Logs Task:**\nRetrieves user activity logs from the platform’s servers.\nIt is scheduled to run after the start_data_processing_task\n|1\n2\n3\n4\n5\n6 |CREATE TASK fetch_user_activity_logs_task\nWAREHOUSE = 'COMPUTE_WH'\nAFTER start_data_processing_task\nAS\nCALL fetch_user_activity_logs_proc ( ) ; |\n| --- | --- |\n**Fetch Product Inventory Task:**\nRetrieves product inventory data from the database.\nIt is scheduled to run after the start_data_processing_task\n|1\n2\n3\n4\n5\n6 |CREATE TASK fetch_product_inventory_task\nWAREHOUSE = 'COMPUTE_WH'\nAFTER start_data_processing_task\nAS"]},{"url":"https://medium.com/snowflake/productionize-your-ml-workflow-using-snowflake-task-dag-apis-8470aa33172c","title":"Productionize your ML workflow using Snowflake Task/DAG APIs | by Anurag Gupta | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2024-02-13","excerpts":["Section Title: Productionize your ML workflow using Snowflake Task/DAG APIs\nContent:\n**Enhanced Task UI in Snowsight in private preview (PrPr)\n** - This enables users to easily manage and monitor all tasks from a scheduled graph in one place with their runtime, failure errors, and so on. **Snowpark ML\n** - [Modeling API GA](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling) : Python API for preprocessing data, feature engineering, and training models inside Snowflake following familiar Python frameworks such as scikit-learn and XGBoost. - [Model Registry API PuPr](https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-mlops-model-registry) : Snowpark Model Registry allows customers to securely manage models and their metadata in Snowflake, regardless of origin.\nSection Title: Productionize your ML workflow using Snowflake Task/DAG APIs\nContent:\nIn this blog, we’ll go over a real-world example of using and managing a production pipeline end-to-end within Snowflake.\nSection Title: ... > An example: A daily training on transaction data for fraud detection\nContent:\nIt is very common for companies to have their customer’s data (transactions, enrollment, etc.) constantly coming in and being stored in an ongoing transactions table. For this demo, we are going to use SnowparkML to predict whether financial transactions are fraudulent or not. This will not only show how you can use Snowpark ML for the entirety of a typical ML workflow, including development and deployment but also how it can be seamlessly deployed to Snowflake for automation using DAG API.\n ... \nSection Title: Productionize your ML workflow using Snowflake Task/DAG APIs > Where did I define my graph?\nContent:\n[Task graph configuration](https://docs.snowflake.com/en/sql-reference/functions/system_get_task_graph_config) specifies the runtime environment for the entire DAG. With a simple change in DB_NAME one can deploy the same DAG in a different environment. This comes in very handy in real-world scenarios when a production pipeline runs in multiple environments like production, development and regtest environments. With branching, we can traverse different parts of the sub-tree of tasks. And the branch condition can be based on a value determined at runtime. For one-off testing/running the DAG, users can just skip the *schedule* and manually trigger a run with *dag_op.run(dag)* . You may have noticed that you can even pass the raw function to the dependency definition without explicitly creating the *DAGTask* . The Python library automatically creates a task for you with the same name."]}],"usage":[{"name":"sku_search","count":1}]}

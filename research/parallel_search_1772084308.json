{"search_id":"search_0231c7e9b2bc4f23969982fee6627fc5","results":[{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/adding-custom-event-billing","title":"Add billable events to an application package | Snowflake Documentation","excerpts":["Developer Snowflake Native App Framework Add billable events to an app\nSection Title: Add billable events to an application package ¶\nContent:\nFeature — Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nWhen you use Custom Event Billing for a Snowflake Native App, you can charge for specific types of application usage in addition to the existing\nusage-based pricing plans. To set it up, you must perform two high-level steps:\nSet up your application package to emit billable events by following the steps in this topic.\nSelect a usage-based pricing plan with billable events for the listing you use to publish your Snowflake Native App to consumers.\nThis topic describes how to set up your application package to emit billable events using the SYSTEM$CREATE_BILLING_EVENT and SYSTEM$CREATE_BILLING_EVENTS system functions.\n ... \nSection Title: ... > Overview of billable events in an application package ¶\nContent:\nSnowflake supports billable events that are emitted by calling the system function within a stored procedure in the application,\nas outlined by the examples in this topic.\nSnowflake does not support other methods of calculating the base charge for billable events, such as methods that use the output of a\ntable or user-defined function that outputs consumer activity or methods that use telemetry logged in an event table.\nIf you’re uncertain whether a proposed implementation will be supported, contact your Snowflake account representative.\nSection Title: Add billable events to an application package ¶ > Billable event examples ¶\nContent:\nThe examples in this section show how to create stored procedures to emit billable events for common billing\nscenarios. Each of these examples calls the `createBillingEvent` function.\n ... \nSection Title: ... > Call the SYSTEM$CREATE_ BILLING_ EVENT system function ¶\nContent:\nThe examples in this topic call this helper function.\nSection Title: ... > Batch multiple billing events with the SYSTEM$CREATE_ BILLING_ EVENTS system function ¶\nContent:\nThe following example stored procedure shows how to batch multiple Snowflake Native App billing events with the SYSTEM$CREATE_BILLING_EVENTS system function. By using batches, you save time, reduce the likelihood of exceeding call limits, and ensure your billing events are set up correctly.\nFor more details about the parameters and the required types, see SYSTEM$CREATE_BILLING_EVENTS .\n ... \nSection Title: ... > Batch multiple billing events with the SYSTEM$CREATE_ BILLING_ EVENTS system function ¶\nContent:\nnow () } , \n                                       \"base_charge\": 6.1, \n                                       \"objects\": \"obj1\", \n                                       \"additional_info\": \"info1\" \n                                     }, \n                                     { \n                                       \"class\": \"class_2\", \n                                       \"subclass\": \"subclass_2\", \n                                       \"start_timestamp\": ${ Date . now () } , \n                                       \"timestamp\": ${ Date . now () } , \n                                       \"base_charge\": 9.1, \n                                       \"objects\": \"obj2\", \n                                       \"additional_info\": \"info2\" \n                                     } \n                                   ] \n                                 ` ); \n $$;\n```\nSection Title: ... > Batch multiple billing events with the SYSTEM$CREATE_ BILLING_ EVENTS system function ¶\nContent:\nCopy\nSection Title: ... > Example: Billing based on calls to a stored procedure ¶\nContent:\nThe following example shows how to create a stored procedure to emit a billable event when a consumer calls\nthat stored procedure in a Snowflake Native App.\nAdd this example code to your setup script in the same stored procedure that defines the helper function:\n```\n... \n // \n // Send a billable event when a stored procedure is called. \n // \n var event_ts = Date . now (); \n var billing_quantity = 1.0 ; \n var base_charge = billing_quantity ; \n var objects = \"[ \\\"db_1.public.procedure_1\\\" ]\" ; \n var retVal = createBillingEvent ( \"PROCEDURE_CALL\" , \"\" , event_ts , event_ts , base_charge , objects , \"\" ); \n // Run the rest of the procedure ... \n $$;\n```\nCopy\nThis example code creates a stored procedure that calls the `createBillingEvent` function to emit a billable event\nwith the class name `PROCEDURE_CALL` and a base charge of `1.0` .\nNote\n ... \nSection Title: ... > Example: Billing based on rows consumed by a Snowflake Native App ¶\nContent:\nThe following example shows how to create a stored procedure to emit a billable event based on the number of\nrows consumed within a table in the consumer account.\nAdd this example code to your setup script in the same stored procedure that defines the helper function:\n```\n... \n // Run a query and get the number of rows in the result \n var select_query = \"select i from db_1.public.t1\" ; \n res = snowflake . execute ({ sqlText : select_query }); \n res . next (); \n // \n // Send a billable event for rows returned from the select query \n // \n var event_ts = Date . now (); \n var billing_quantity = 2.5 ; \n var base_charge = res . getRowcount () * billing_quantity ; \n var objects = \"[ \\\"db_1.public.t1\\\" ]\" ; \n createBillingEvent ( \"ROWS_CONSUMED\" , \"\" , event_ts , event_ts , base_charge , objects , \"\" ); \n // Run the rest of the procedure ... \n $$;\n```\nCopy\nSection Title: ... > Example: Billing based on rows consumed by a Snowflake Native App ¶\nContent:\nThis example code creates a stored procedure that calls the `createBillingEvent` function to emit a billable event\nwith the class name `ROWS_CONSUMED` and a calculated base charge of `2.5` multiplied by the number of rows in the `db_1.public.t1` table in the consumer account.\nNote\nThe types of the arguments passed to the `createBillingEvent` function must correspond to the typed parameters\nexpected by the SYSTEM$CREATE_BILLING_EVENT system function.\nSection Title: ... > Example: Billing based on the number of rows ingested ¶\nContent:\nThe following example shows how to create a stored procedure to emit a billable event based on the number of rows\ningested into a table.\nAdd this example code to your setup script in the same stored procedure that defines the helper function:\n ... \nSection Title: ... > Example: Billing based on the number of rows ingested ¶\nContent:\nThis example code creates a stored procedure that calls the `createBillingEvent` function to emit a billable event\nwith the class name `ROWS_CHANGED` and a calculated base charge of `2.5` multiplied by the number of rows\ningested in the `db_1.target_table` table.\nNote\nThe types of the arguments passed to the `createBillingEvent` function must correspond to the typed parameters\nexpected by the SYSTEM$CREATE_BILLING_EVENT system function.\nSection Title: ... > Example: Billing based on monthly active rows ¶\nContent:\nMonthly active rows are the number of rows inserted or updated for the first time within a calendar month. Some\nproviders use this metric to only charge consumers for unique rows updated in a month. You can modify this example to instead\ncount unique users, or identify a unique data load location to determine a base charge.\nThe following example shows how to create a stored procedure to emit a billable event based on the number of\nmonthly active rows. Add this example code to your setup script in the same stored procedure that defines the helper function:\n ... \nSection Title: ... > Snowpark Python example: Billing based on rows consumed ¶\nContent:\nTo write your stored procedure in Snowpark Python to bill based on rows consumed by your Snowflake Native App, use the following example:\n ... \nSection Title: ... > Snowpark Python example: Billing based on rows consumed ¶\nContent:\ncollect () \n   return \"Success\" \n\n # Handler function for the stored procedure \n def run ( session ): \n   # insert code to identify monthly active rows and calculate a charge \n   try : \n\n      # Run a query to select rows from a table \n      query =  \"select i from db_1.public.t1\" \n      res = session . sql ( query ) . collect () \n\n      # Define the price to charge per row \n      billing_quantity = 2.5 \n\n      # Calculate the base charge based on number of rows in the result \n      charge = len ( res ) * billing_quantity \n\n      # Current time in Unix timestamp (epoch) time in milliseconds \n      current_time_epoch = int ( time . time () * 1000 ) \n\n      return createBillingEvent ( session , 'ROWS_CONSUMED' , '' , current_time_epoch , current_time_epoch , charge , '[\"billing_event_rows\"]' , '' ) \n   except Exception as ex : \n      return \"Error \" + ex \n $$;\n```\nSection Title: ... > Snowpark Python example: Billing based on rows consumed ¶\nContent:\nCopy\nThis example code creates a stored procedure that defines a helper method that calls the SYSTEM$CREATE_BILLING_EVENT system function,\nas well as a method that calls that helper method, `createBillingEvent` , to emit a billable event\nwith the class name `ROWS_CONSUMED` and a base charge calculated by multiplying a price of `2.5` US dollars by the number of rows in\nthe `db_1.public.t1` table in the consumer account.\nNote\nThe types of the arguments passed to the `createBillingEvent` function must correspond to the typed parameters\nexpected by the SYSTEM$CREATE_BILLING_EVENT system function.\n ... \nSection Title: ... > Validate whether the stored procedures emit billable events ¶\nContent:\nWhile signed in to the consumer account with which you shared your listing, call the stored procedures that you added to your Snowflake Native App.\nFor example, to test the stored procedure created for billing based on monthly active rows , do the following:\nOpen a worksheet and set the context to `db_1.public` .\nRun the following SQL statement:CopyIf the stored procedure returns `Success` , your code is working.\nNote\nIf you run these SQL commands in the provider account that you used to create the application package, you see an error.\nSection Title: ... > Validate the custom event billing pricing plan ¶\nContent:\nTo validate the consumer experience of a Snowflake Native App and confirm that the listing and application package are set up properly, you can query\nthe MARKETPLACE_PAID_USAGE_DAILY View in the DATA_SHARING_USAGE schema of the shared SNOWFLAKE database.\nNote\nDue to latency in the view, run these queries at least two days after first using the Snowflake Native App.\nTo confirm that billable events are successfully generated by a Snowflake Native App and listing,\nrun the following SQL statement in the consumer account that you shared the listing with:\nNote\nReplace the PROVIDER_ACCOUNT_NAME and PROVIDER_ORGANIZATION_NAME values with those of the provider account.\n ... \nSection Title: ... > Validate the custom event billing pricing plan ¶\nContent:\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nOverview of billable events in an application package\nBillable event examples\nTest custom event billing\nRelated content\nAbout the Snowflake Native App Framework\nSYSTEM$CREATE_BILLING_EVENT\nPaid listings pricing models"]},{"url":"https://www.flexera.com/blog/finops/snowflake-native-apps/","title":"Snowflake Native Apps 101: Build and monetize data apps (2026)","publish_date":"2026-01-27","excerpts":["Section Title: ... > 3) **Monetization via Snowflake Marketplace**\nContent:\nProviders can list and sell Snowflake apps in the Snowflake Marketplace . Consumers install these apps directly into their Snowflake accounts, simplifying deployment and making app monetization straightforward.\n ... \nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > ... > 8) **Streamlit Integration**\nContent:\nYou can integrate your apps with Streamlit , which allows you to create interactive dashboards within Snowflake. While the integration is still evolving, it supports embedding visualizations in your apps for end-user analytics.\n ... \nSection Title: ... > What Are the Benefits of Snowflake Native Apps — For Providers?\nContent:\nSnowflake Native Apps offer significant advantages for providers looking to build, distribute and monetize their applications within the Snowflake ecosystem. Here are some of the benefits:\n ... \nSection Title: ... > ➥ **On-Platform Monetization Opportunities**\nContent:\nProviders can easily sell and monetize their Snowflake apps directly within the Snowflake ecosystem via Snowflake Marketplace , bypassing the need for third-party systems.\n ... \nSection Title: ... > ➥ **Simplified Application Management**\nContent:\nConsumers can manage access privileges, event logging and app-related tasks directly within their Snowflake account.\nSection Title: ... > How Do Snowflake Native Applications Work?\nContent:\nSnowflake Native Applications leverage the *Snowflake Native App Framework* to build and deploy data-driven applications directly within the Snowflake ecosystem. These Snowflake apps harness Snowflake’s core features—secure data sharing, analytics, compute and governance—enabling seamless integration and monetization, without requiring data to move outside the platform. The framework supports applications ranging from analytical tools to fully containerized services.\nSnowflake Native App Framework allows:\nSection Title: ... > How Do Snowflake Native Applications Work?\nContent:\nProviders to share data, business logic and application interfaces (e.g., Streamlit apps, stored procedures) using [Snowpark API](https://docs.snowflake.com/en/developer-guide/snowpark/index) , [Python](https://www.python.org/) , [SQL](https://www.w3schools.com/sql/) and [JavaScript](https://www.w3schools.com/js/) .\nApplications to be listed as free or paid offerings on the Snowflake Marketplace or shared privately with select accounts.\nDevelopers to benefit from streamlined testing environments, version control via external repositories and detailed logging for troubleshooting.\nBuilt-in support for structured and unstructured event logging to streamline troubleshooting and performance tracking.\nIntegration with Streamlit to build interactive, user-friendly visual interfaces.\nOn top of that, the Snowflake Native Framework also provides an enhanced developer experience, including:\n ... \nSection Title: ... > Architecture of the Snowflake Native App Framework\nContent:\nThe architecture of the Snowflake Native App Framework operates on a provider-consumer model:\n**Provider** — Creates and shares data and application logic using the framework.\n**Consumer** — Installs and interacts with applications shared by providers.\nSnowflake Native Applications are packaged as **Application Packages** , which contains the necessary logic, metadata and configuration to deploy a Snowflake Native App. This includes:\n**Manifest file** : Configuration details, including setup script locations and versioning.\n**Setup script** : Contains SQL commands for installation and updates.\nThe provider publishes the Snowflake Native app via:\n**Marketplace Listings** — Accessible to all Snowflake users for broad distribution.\n**Private Listings** — Targeted sharing with specific accounts across regions.\n ... \nSection Title: ... > How do Snowflake Native Applications work with Snowpark Container Services?\nContent:\nFor advanced use cases, Snowflake Native Applications can utilize [**Snowpark Container Services**](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/overview) , which enable Snowflake apps to manage containerized workloads within Snowflake. This approach supports high-performance applications, such as machine learning and AI-driven analytics, without externalizing data.\nComponents unique to containerized Snowflake apps:\n**Services specification file** — Applications reference container images stored in the provider’s repository.\n**Compute pool** — A collection of virtual machine nodes where containerized workloads execute.\nSnowflake Native App with Snowpark Container Architecture\nFeatures of Snowpark Container Services include:\n ... \nSection Title: ... > **Step 6** —Adding Data Content to Your Snowflake Native App\nContent:\nIn this step, you’ll enhance your Snowflake Native App by incorporating shared data content. This involves creating and sharing a table within your app package and granting access to app users. Additionally, you’ll create a view for secure data access by consumers.\nFirst, let’s create a table to share with the app. To do so, you can add a table to your app package by writing a SQL script and specifying its execution in the project definition file.\nSo let’s create and populate the table. To do that, you need to create a folder **scripts** and inside that folder, create a file called **shared_content.sql** . Then, add the following code:\nSection Title: ... > **Step 6** —Adding Data Content to Your Snowflake Native App\nContent:\n```\nUSE APPLICATION PACKAGE <% ctx.entities.snowflake_native_app_demo_package.identifier %>;\n\nCREATE SCHEMA IF NOT EXISTS shared_data;\nUSE SCHEMA shared_data;\nCREATE TABLE IF NOT EXISTS wealthy_individuals (\n\t\tid INT,\n\t\t name VARCHAR,\n\t\tstatus VARCHAR\n);\n\nTRUNCATE TABLE wealthy_individuals;\n\nINSERT INTO wealthy_individuals VALUES\n\t\t (1, 'Elon', 'Billionaire'),\n\t\t(2, 'Bernard', 'Billionaire'),\n\t\t(3, 'Jeff', 'Billionaire'),\n\t\t (4, 'Warren', 'Billionaire'),\n\t\t(5, 'Larry', 'Billionaire'),\n\t\t(6, 'Sergey', 'Billionaire'),\n\t\t (7, 'Gautam', 'Billionaire'),\n\t\t(8, 'Carlos', 'Billionaire'),\n\t\t (9, 'Mukesh', 'Billionaire'),\n\t\t(10, 'Bill', 'Millionaire'),\n\t\t(11, 'Mark', 'Millionaire'),\n\t\t(12, 'Larry', 'Millionaire'),\n\t\t (13, 'Michael', 'Millionaire'),\n\t\t(14, 'Aman', 'Millionaire'),\n\t\t(15, 'Warren', 'Billionaire');\n```\nNow, grant access to the table and schema using:\n ... \nSection Title: ... > Monetization and Distribution of Snowflake Native App\nContent:\nRaw datasets\nRefined and enriched data\nHistorical datasets for forecasting and machine learning\nReal-time data streams (like weather or traffic updates)\nSpecialized identity or audience data for analytics\nSnowflake Native Applications\nPre-built data pipelines and transformations\nSnowflake Marketplace leverages Snowflake’s architecture to facilitate the secure sharing of data and applications. Transactions are managed natively, eliminating the need for third-party billing systems. Vendors can offer their products through various pricing models, such as pay-as-you-go, one-time payment, usage-based payment, or subscription-based plans, while benefiting from Snowflake’s built-in analytics to track customer engagement.\nLet’s jump right into the juicy part of the article: a step-by-step guide to monetizing Snowflake Native Applications via Snowflake Marketplace.\n ... \nSection Title: ... > **Step 3** —Create and Configure a Private Listing on the Snowflake Marketplace\nContent:\nTo share your app via the Snowflake Marketplace, start by signing in to Snowsight and navigating to **Data Products > Provider Studio** .\nNavigating to Provider Studio in Snowflake\nClick **+ Listing** to create a new listing and proceed with configuration. Enter a name for the listing and specify the discovery permissions, choosing whether the listing will be public or restricted to specific consumers (e.g., select “ **Only specified consumers** ” for private sharing and select “ **Anyone on the Marketplace** ” for public listing).\nCreating a private listing for only specified consumers – Snowflake Native App\n ... \nSection Title: ... > **Step 4** —Create and Configure a Public Paid Listing on the Snowflake Marketplace\nContent:\nNow, to create a public listing, you need to first contact your Snowflake business development partner to approve your paid listing. If you don’t have a business development partner, you’ll need to [submit a case with Marketplace Operations](https://snowflakecommunity.force.com/s/provider-onboarding-case) . Before proceeding, verify that your [role has the required privileges to create a listing](https://other-docs.snowflake.com/en/collaboration/provider-becoming) .\nOnce everything is in place, you need to log in to Snowsight and go to Data Products > Provider Studio from the menu. Select **+ Listing** to open the Create Listing window. Here, name your listing and set its visibility. To make the listing publicly discoverable, choose “ **Anyone on the Marketplace** ” under the discovery settings.\nCreating a paid private listing for only specified consumers – Snowflake Marketplace\n ... \nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > Conclusion\nContent:\nAnd that’s a wrap! Snowflake Native Apps are built using the Snowflake Native App Framework. This allows developers to create, test and launch apps right in Snowflake. The framework simplifies the process of building, launching and integrating advanced tools. It ensures security and governance by tapping into the Snowflake ecosystem. For providers, these apps provide an easy way to sell their solutions on the Snowflake Marketplace, reaching thousands of customers. Meanwhile, consumers get instant access to the apps without needing a complex setup.\nIn this article, we have covered:\nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > Conclusion\nContent:\nWhat are Native Apps in Snowflake?\nKey features and characteristics of Snowflake Native Apps\nWhat are the benefits of Snowflake Native Apps for providers?\nWhat are the benefits of Snowflake Native Apps for consumers?\nHow do Snowflake Native Apps work?\nStep-by-step guide to create a Snowflake Native App\nMonetization and distribution of Snowflake Native Apps\nStep-by-step monetization process via Snowflake Marketplace\n… and so much more!\nSection Title: Snowflake Native Apps 101: Build and monetize data apps (2026) > FAQs\nContent:\n**What are Native Apps in Snowflake?**\nSnowflake Native Apps are designed specifically to operate within the Snowflake ecosystem without requiring external access or movement of sensitive data outside its environment.\n**How can I develop and test a Snowflake Native App locally?**\nDevelopers can set up their environments using tools like VSCode along with necessary extensions provided by Snowflakes such as CLI support.\n**Can I share my Snowflake Native App with other users?**\nYes! Once published on the marketplace after meeting compliance requirements.\n**Does the Snowflake Native App framework support logging and monitoring?**\nYes! Snowflake Native App framework includes telemetry tools that allow developers to monitor application performance post-deployment.\n**What is Streamlit’s role in Snowflake Native apps?**"]},{"url":"https://docs.snowflake.com/en/developer-guide/logging-tracing/event-table-setting-up","title":"Event table overview - Snowflake Documentation","excerpts":["Developer Logging, Tracing, and Metrics Event table overview\nSection Title: Event table overview ¶\nContent:\nAs your Snowflake objects—including procedures and UDFs—emit telemetry data, Snowflake collects the data in an event table whose\ndata is available for queries. Snowflake includes an event table by default, but you can also create a new one.\nTo collect telemetry data, you must have an active event table and have set telemetry levels to allow data collection. If you don’t already have an active event table,\nSnowflake makes the default event table the active event table.\nWhen collecting telemetry data, you incur costs. To understand these costs—or to reduce or avoid these costs—see Costs of telemetry data collection .\nSection Title: Event table overview ¶ > What is an event table? ¶\nContent:\nAn event table is a special kind of database table with a predefined set of columns. The table’s structure supports the data model for [OpenTelemetry](https://opentelemetry.io/) , a framework for handling telemetry data. When an\nevent table is active, Snowflake collects telemetry data in the table—including data that\nSnowflake itself generates and data that you emit by instrumenting your handler code using certain APIs. You can view the collected data\nby executing SQL queries.\nAfter installation, Snowflake includes a default event table called\nSNOWFLAKE.TELEMETRY.EVENTS. This event table is active and collects data until you deactivate it. You can also create your own .\nSection Title: Event table overview ¶ > Use the default event table ¶\nContent:\nIf you do not set an active event table, Snowflake uses as the active event table a default event table named SNOWFLAKE.TELEMETRY.EVENTS.\nYou can also create your own event tables for specific uses.\nBy default, Snowflake also includes a predefined view called SNOWFLAKE.TELEMETRY.EVENTS_VIEW view ,\nwith which you more securely make event table data available to a range of users. You can manage access to the view with a row access policy .\nNote\nThe default event table supports only a subset of DDL commands supported for event tables you create or for regular tables. For more\ninformation, see Working with event tables .\nSection Title: Event table overview ¶ > ... > Roles for access to the default event table and EVENTS_ VIEW ¶\nContent:\nSnowflake includes the following predefined application roles you can use to manage access to the default event table and EVENTS_VIEW view:\nA person with the ACCOUNTADMIN role can access the default event table and EVENTS_VIEW view and can grant the roles described here\nto other roles for access to them.\nYou must grant these roles to other roles, rather than to a user. For example, you might grant the EVENTS_ADMIN role to another admin\nrole you’ve created for broader administrative use.\n```\nGRANT APPLICATION ROLE SNOWFLAKE . EVENTS_ADMIN TO ROLE my_admin_role ; \n\n GRANT APPLICATION ROLE SNOWFLAKE . EVENTS_VIEWER TO ROLE my_analysis_role ;\n```\nCopy\nEVENTS_VIEWER :\nRole with privileges to execute a SELECT statement on the EVENTS_VIEW view .\nEVENTS_ADMIN :\nRole with the following privileges:\nSection Title: Event table overview ¶ > ... > Roles for access to the default event table and EVENTS_ VIEW ¶\nContent:\nSELECT, TRUNCATE, DELETE on the default event table .\nSELECT on the EVENTS_VIEW view of the default event table.\nUSAGE on the following stored procedures:\nADD_ROW_ACCESS_POLICY_ON_EVENTS_VIEW\nDROP_ROW_ACCESS_POLICY_ON_EVENTS_VIEW\nThis role also has privileges to execute a stored procedure to apply a row access policy (RAP) on the EVENTS_VIEW view\nwhose data is based on the default event table.\nSection Title: Event table overview ¶ > Use the default event table ¶ > Manage access to the EVENTS_ VIEW view ¶\nContent:\nYou can manage access to data in the EVENTS_VIEW view with row access policies . Snowflake provides stored procedures you can use to add and remove a row\naccess policy to the EVENT_VIEW view.\nADD_ROW_ACCESS_POLICY_ON_EVENTS_VIEW(VARCHAR, ARRAY) —Binds\na row access policy to the specified columns in the EVENTS_VIEW.\nDROP_ROW_ACCESS_POLICY_ON_EVENTS_VIEW(VARCHAR) —Deletes\nthe specified row access policy bound to the EVENTS_VIEW.\nNote\nYou must have the EVENTS_ADMIN role to execute these procedures.\nUsing row access policies on the EVENT_VIEW view is an Enterprise Edition feature.\nSection Title: Event table overview ¶ > Use a custom event table ¶\nContent:\nTo create a new event table, execute the CREATE EVENT TABLE command and specify a name for the event table.\nNote\nIf you don’t create an event table, Snowflake uses the default event table to collect telemetry data.\nCreate an event table by executing the CREATE EVENT TABLE command,\nspecifying a name for the event table.\nAssociate the event table with an object by executing the ALTER  command on the object, setting the EVENT_TABLE parameter to the name of your event table.This sets the scope of data captured by the event table to the object with which you’re associating the table.\nSection Title: Event table overview ¶ > Use a custom event table ¶ > Create an event table ¶\nContent:\nTo create an event table, execute the CREATE EVENT TABLE command.\nWhen you create an event table, you do not specify the columns in the table. An event table already has a set of predefined\ncolumns, as described in Event table columns .\nEnsure that you’re using a role that has the CREATE EVENT TABLE privilege .\nExecute the CREATE EVENT TABLE command to create the event table, specifying a name for the event table.You use the event table name to associate the table with an object , such as a database.For example, to create an event table with the name `my_events` , execute the following statement:Copy\nNote\nReplication of event tables is not currently supported. Any event tables that are contained in primary databases\nare skipped during replication.\nSection Title: Event table overview ¶ > Use a custom event table ¶ > Associate an event table with an object ¶\nContent:\nTo specify the object for which an event table is active, execute the ALTER  command on the object.\nAssociating an event table with a database is an Enterprise Edition feature.\nEnsure that you’re using a role that has the required privileges .\nExecute the ALTER  command on the object, setting the EVENT_TABLE parameter to the name of\nyour event table.Setting this parameter sets the object as the scope within which events will be collected in the specified event table.For example, to associate the event table with a database, use ALTER DATABASE, as in the following example:CopyIn this example, Snowflake—depending on how you’ve specified telemetry levels —captures\ntelemetry data for procedures and UDFs in `my_database` in the `telemetry_database.telemetry_schema.my_events` event table.\nSection Title: Event table overview ¶ > ... > Associate an event table with an object ¶ > Supported objects ¶\nContent:\nThe following table lists the objects with which you can associate an event, along with the privileges required to make the association.\nSection Title: Event table overview ¶ > ... > Associate an event table with an object ¶ > Supported objects ¶\nContent:\n| Object | Privileges required | Scope of objects whose data is collected |\n| Account | * ACCOUNTADMIN role. |  |\nSection Title: Event table overview ¶ > ... > Associate an event table with an object ¶ > Supported objects ¶\nContent:\nOWNERSHIP privilege for the account.\nOWNERSHIP or INSERT privileges for the event table . |Procedures and UDFs in the account. Use this for the broadest scope. |\n|Database |* ACCOUNTADMIN role.\nOWNERSHIP or INSERT privileges for the event table . |Procedures and UDFs in the specified database. |\nAn order of precedence determines which event table is used to collect telemetry data for an object. In that precedence order, an event\ntable associated with a database takes precedence over an event table associated with an account.\nAccount » Database\nSection Title: Event table overview ¶ > ... > Associate an event table with an object ¶ > Supported objects ¶\nContent:\nIn other words, if you have event tables associated with both your account and a database `my_database` , telemetry data generated by\nobjects in `my_database` will be collected in the database’s event table. For other databases in the account that don’t have an\nassociated event table, telemetry data will be collected in the event table associated with the account.\n ... \nSection Title: Event table overview ¶ > Use a custom event table ¶ > Set the event table for the account ¶\nContent:\nYou can confirm the EVENT_TABLE value with the SHOW PARAMETERS command:\n```\nSHOW PARAMETERS LIKE 'event_table' IN ACCOUNT ;\n```\nCopy\nSection Title: Event table overview ¶ > Use a custom event table ¶ > Set the event table for a database ¶\nContent:\nTo set up the event table named `my_events` in the schema `my_schema` in the database `my_database` as\nthe active event table for the database `my_database` , execute the following statement:\n```\nALTER DATABASE my_database SET EVENT_TABLE = my_database . my_schema . my_events ;\n```\nCopy\nTo disassociate an event table from a database, execute the ALTER DATABASE command and unset the EVENT_TABLE parameter. For example:\n```\nALTER DATABASE my_database UNSET EVENT_TABLE ;\n```\nCopy\nYou can confirm the EVENT_TABLE value with the SHOW PARAMETERS command:\n```\nSHOW PARAMETERS LIKE 'event_table' IN DATABASE my_database ;\n```\nCopy\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\nSection Title: Event table overview ¶ > Use a custom event table ¶ > Set the event table for a database ¶\nContent:\n[Get your own certification](https://learn.snowflake.com)\nSection Title: Event table overview ¶ > Privacy Preference Center\nContent:\nYour Opt Out Preference Signal is Honored\nYour Privacy\nStrictly Necessary Cookies\nPerformance Cookies\nFunctional Cookies\nTargeting Cookies\n ... \nSection Title: Event table overview ¶ > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details‎\n ... \nSection Title: Event table overview ¶ > Privacy Preference Center > Cookie List\nContent:\nConsent Leg.Interest\ncheckbox label label\ncheckbox label label\ncheckbox label label\nClear\ncheckbox label label\nApply Cancel\nConfirm My Choices\nAllow All\n[](https://www.onetrust.com/products/cookie-consent/)"]},{"url":"https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/","title":"FinOps on Snowflake: Built-In Cost and Performance Control","excerpts":["Section Title: FinOps on Snowflake\nContent:\nTime is money – save both with Snowflake.\n[explore the economic impact of snowflake](https://www.snowflake.com/resource/forrester-the-total-economic-impact-of-the-snowflake-ai-data-cloud/?utm_cta=website-cost-and-performance-forrester-tei)\nSection Title: FinOps on Snowflake > Go from painstaking configurations to a proven, fully-managed service\nContent:\nSince its founding in 2012, Snowflake has provided automated cluster management, maintenance and upgrades — all without downtime — so you can spend time on valuable data projects\nGet **out-of-the-box governance and security through Snowflake Horizon Catalog** without extra configurations or protocols\nSection Title: FinOps on Snowflake > Go from piecemeal dashboards to built-in cost & performance management\nContent:\nGet granular visibility, control and optimization of Snowflake spend through a unified Cost Management Interface .\nCheck query performance easily to proactively save on costs.\nAutomatically benefit from regular rollouts of performance improvements across all workloads.\nSection Title: FinOps on Snowflake > Maximize your Snowflake spend\nContent:\nAdd flexibility in how you use funds committed in your Snowflake Capacity contract.\nDeploy partner solutions faster by simplifying finance and procurement processes.\nBundle your spend to increase your buying power with Snowflake and partners.\nlearn more\nSection Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\nTravelpass CTC Natwest\nTravel and Hospitality “Now, we aren’t so focused on how to build things. We are focused more on what to build.” Dan Shah\nManager of Data Science Read the story * **1 week** for 130 Dynamic Tables to be in production after migration\n**65%** cost savings switching from Databricks to Snowflake\nRead the case study Financial Services “Now with fewer ephemeral failures and higher visibility in Snowflake, we have a platform that’s much easier and cost-effective to operate than managed Spark.” David Trumbell\nHead of Data Engineering, CTC Read the story * **1st** data availability deadline was hit everyday for the 1st time\n**54%** cost savings switching from managed Spark to Snowflake\nSection Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\nRead the case study Financial Services “The speed at which we’ve delivered wouldn’t have been possible with other providers.” Kaushik Ghosh Dastidar\nHead of ESG Cloud Solutions, NatWest Read the story * **6x** reduction in onboarding time from 3 months to 2 weeks\n**$750K** saved in salaries & staff training costs\nRead the case study\n[Resource #### Snowflake Joins the FinOps Foundation Snowflake joins The FinOps Foundation as a Premier Enterprise Member to provide thought leadership and set industry financial best practices. Read more](https://www.finops.org/members/snowflake/)\nResource #### Snowflake Pricing Calculator Curious about Snowflake pricing? Our Snowflake pricing calculator shows credit usage, warehouse costs, and total expenses. Access calculator\nSection Title: FinOps on Snowflake > Saving time on platform admin. Getting to market faster.\nContent:\n[Guide #### Definitive Guide to Managing Spend in Snowflake Learn about considerations for consumption models such as Snowflake's, frameworks for better managing spend, and more. Get the guide](https://www.snowflake.com/en/resources/white-paper/definitive-guide-to-managing-spend-in-snowflake/)\nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation\nContent:\nAccess documentation on Managing Costs and Optimizing Performance in Snowflake.\n[Read about Managing Costs](https://docs.snowflake.com/en/user-guide/cost-management-overview)\n[Read about Optimizing Performance](https://docs.snowflake.com/en/guides-overview-performance)\nSection Title: FinOps on Snowflake > ... > Snowflake Documentation > On-Demand Cost Governance Training\nContent:\nLearn how to successfully examine, control, and optimize Snowflake costs.\n[Register Now](https://learn.snowflake.com/en/courses/OD-FINOPS/)\nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation > Professional Services\nContent:\nEngage Snowflake’s Professional Services for expert advice on optimizing your use of Snowflake.\n[Discover Professional Services](https://www.snowflake.com/snowflake-professional-services/)\nSection Title: FinOps on Snowflake > Even More To Explore > Snowflake Documentation > Priority Support\nContent:\nLearn more about how our Priority Support team can help you reduce consumption spend through performance monitoring, observability, and management.\n[Learn about Priority Support](https://www.snowflake.com/support/priority-support/)"]},{"url":"https://www.snowflake.com/en/developers/guides/resource-optimization-billing-metrics/","title":"Resource Optimization: Billing Metrics","excerpts":["Data for Breakfast Around the World\nDrive impact across your organization with data and agentic intelligence.\n[register now](https://www.snowflake.com/en/data-for-breakfast/)\n[Snowflake for Developers](https://www.snowflake.com/content/snowflake-site/global/en/developers) [Guides](https://www.snowflake.com/content/snowflake-site/global/en/developers/guides) Resource Optimization: Billing Metrics\nSection Title: Resource Optimization: Billing Metrics\nContent:\nMatt Meredith\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/resource-optimization-billing-metrics)\nSection Title: Resource Optimization: Billing Metrics > Overview\nContent:\nThis resource optimization guide represents one module of the four contained in the series. These guides are meant to help customers better monitor and manage their credit consumption. Helping our customers build confidence that their credits are being used efficiently is key to an ongoing successful partnership. In addition to this set of Snowflake Quickstarts for Resource Optimization, Snowflake also offers community support as well as Training and Professional Services offerings. To learn more about the paid offerings, take a look at upcoming education and training .\nThis blog post can provide you with a better understanding of Snowflake's Resource Optimization capabilities.\nSection Title: Resource Optimization: Billing Metrics > Overview > Billing Metrics\nContent:\nBilling queries are responsible for identifying total costs associated with the high level functions of the Snowflake Cloud Data Platform, which includes warehouse compute, snowpipe compute, and storage costs. If costs are noticeably higher in one category versus the others, you may want to evaluate what might be causing that.\nThese metrics also seek to identify those queries that are consuming the most amount of credits. From there, each of these queries can be analyze for their importance (do they need to be run as frequently, if at all) and explore if additional controls need to be in place to prevent excessive consumption (i.e. resource monitors, statement timeouts, etc.).\nSection Title: Resource Optimization: Billing Metrics > Overview > What You’ll Learn\nContent:\nhow to identify and analyze Snowflake consumption across all services\nhow to analyze most resource-intensive queries\nhow to analyze serverless consumption\nSection Title: Resource Optimization: Billing Metrics > Overview > What You’ll Need\nContent:\nA Snowflake Account\nAccess to view [Account Usage Data Share](https://docs.snowflake.com/en/sql-reference/account-usage.html)\nSection Title: Resource Optimization: Billing Metrics > Overview > Related Materials\nContent:\nResource Optimization: Setup & Configuration\nResource Optimization: Usage Monitoring\nResource Optimization: Performance\nSection Title: Resource Optimization: Billing Metrics > Query Tiers\nContent:\nEach query within the Resource Optimization Snowflake Quickstarts will have a tier designation just to the right of its name as \"(T*)\". The following tier descriptions should help to better understand those designations.\nSection Title: Resource Optimization: Billing Metrics > Query Tiers > Tier 1 Queries\nContent:\nAt its core, Tier 1 queries are essential to Resource Optimization at Snowflake and should be used by each customer to help with their consumption monitoring - regardless of size, industry, location, etc.\nSection Title: Resource Optimization: Billing Metrics > Query Tiers > Tier 2 Queries\nContent:\nTier 2 queries, while still playing a vital role in the process, offer an extra level of depth around Resource Optimization and while they may not be essential to all customers and their workloads, it can offer further explanation as to any additional areas in which over-consumption may be identified.\nSection Title: Resource Optimization: Billing Metrics > Query Tiers > Tier 3 Queries\nContent:\nFinally, Tier 3 queries are designed to be used by customers that are looking to leave no stone unturned when it comes to optimizing their consumption of Snowflake. While these queries are still very helpful in this process, they are not as critical as the queries in Tier 1 & 2.\nSection Title: Resource Optimization: Billing Metrics > Billing Metrics (T1) > Tier 1 > Description:\nContent:\nIdentify key metrics as it pertains to total compute costs from warehouses,\nserverless features, and total storage costs.\n ... \nSection Title: Resource Optimization: Billing Metrics > Billing Metrics (T1) > Tier 1 > Primary Schema:\nContent:\nAccount_Usage\n ... \nSection Title: Resource Optimization: Billing Metrics > Billing Metrics (T1) > Tier 1 > SQL\nContent:\n--COMPUTE FROM WAREHOUSES\nSELECT\n'WH Compute' as WAREHOUSE_GROUP_NAME\n,WMH.WAREHOUSE_NAME\n,NULL AS GROUP_CONTACT\n,NULL AS GROUP_COST_CENTER\n,NULL AS GROUP_COMMENT\n,WMH.START_TIME\n,WMH.END_TIME\n,WMH.CREDITS_USED\n,$CREDIT_PRICE\n,($CREDIT_PRICE*WMH.CREDITS_USED) AS DOLLARS_USED\n,'ACTUAL COMPUTE' AS MEASURE_TYPE\nfrom    SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY WMH\nUNION ALL\n--COMPUTE FROM SNOWPIPE\nSELECT\n'Snowpipe' AS WAREHOUSE_GROUP_NAME\n,PUH.PIPE_NAME AS WAREHOUSE_NAME\n,NULL AS GROUP_CONTACT\n,NULL AS GROUP_COST_CENTER\n,NULL AS GROUP_COMMENT\n,PUH.START_TIME\n,PUH.END_TIME\n,PUH.CREDITS_USED\n,$CREDIT_PRICE\n,($CREDIT_PRICE*PUH.CREDITS_USED) AS DOLLARS_USED\n,'ACTUAL COMPUTE' AS MEASURE_TYPE\nfrom    SNOWFLAKE.ACCOUNT_USAGE.PIPE_USAGE_HISTORY PUH\nUNION ALL\n ... \nSection Title: Resource Optimization: Billing Metrics > Most Expensive Queries (T2) > Tier 2 > Primary Schema:\nContent:\nAccount_Usage\n ... \nSection Title: Resource Optimization: Billing Metrics > ... > Tier 2 > Description:\nContent:\nThis summarize the query activity and credit consumption per warehouse over the last month. The query also includes the ratio of queries executed to credits consumed on the warehouse\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 2 > How to Interpret Results:\nContent:\nHighlights any scenarios where warehouse consumption is significantly out of line with the number of queries executed. Maybe auto-suspend needs to be adjusted or warehouses need to be consolidated.\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 2 > Primary Schema:\nContent:\nAccount_Usage\n ... \nSection Title: Resource Optimization: Billing Metrics > Average Cost per Query by Warehouse (T2) > Tier 2 > SQL\nContent:\nORDER BY COST_PER_QUERY DESC\n;\n```\n\nCopy\n```\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > Description:\nContent:\nFull list of tables with auto-clustering and the volume of credits consumed via the service over the last 30 days, broken out by day.\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > How to Interpret Results:\nContent:\nLook for irregularities in the credit consumption or consistently high consumption\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > Primary Schema:\nContent:\nAccount_Usage\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > SQL\nContent:\nSELECT TO_DATE(START_TIME) as DATE\n,DATABASE_NAME\n,SCHEMA_NAME\n,TABLE_NAME\n,SUM(CREDITS_USED) as CREDITS_USED\nFROM \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"AUTOMATIC_CLUSTERING_HISTORY\"\nWHERE START_TIME >= dateadd(month,-1,current_timestamp())\nGROUP BY 1,2,3,4\nORDER BY 5 DESC\n;\n```\n\nCopy\n```\n ... \nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > How to Interpret Results:\nContent:\nLook for irregularities in the credit consumption or consistently high consumption\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > Primary Schema:\nContent:\nAccount_Usage\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > SQL\nContent:\nSELECT\nTO_DATE(START_TIME) as DATE\n,DATABASE_NAME\n,SCHEMA_NAME\n,TABLE_NAME\n,SUM(CREDITS_USED) as CREDITS_USED\nFROM \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"MATERIALIZED_VIEW_REFRESH_HISTORY\"\nWHERE START_TIME >= dateadd(month,-1,current_timestamp())\nGROUP BY 1,2,3,4\nORDER BY 5 DESC\n;\n```\n\nCopy\n```\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > Description:\nContent:\nFull list of tables with search optimization and the volume of credits consumed via the service over the last 30 days, broken out by day.\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > How to Interpret Results:\nContent:\nLook for irregularities in the credit consumption or consistently high consumption\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > Primary Schema:\nContent:\nAccount_Usage\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > SQL\nContent:\nSELECT\nTO_DATE(START_TIME) as DATE\n,DATABASE_NAME\n,SCHEMA_NAME\n,TABLE_NAME\n,SUM(CREDITS_USED) as CREDITS_USED\nFROM \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"SEARCH_OPTIMIZATION_HISTORY\"\nWHERE START_TIME >= dateadd(month,-1,current_timestamp())\nGROUP BY 1,2,3,4\nORDER BY 5 DESC\n;\n```\n\nCopy\n```\n ... \nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > How to Interpret Results:\nContent:\nLook for irregularities in the credit consumption or consistently high consumption\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > Primary Schema:\nContent:\nAccount_Usage\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > SQL\nContent:\nSELECT\nTO_DATE(START_TIME) as DATE\n,PIPE_NAME\n,SUM(CREDITS_USED) as CREDITS_USED\nFROM \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"PIPE_USAGE_HISTORY\"\nWHERE START_TIME >= dateadd(month,-1,current_timestamp())\nGROUP BY 1,2\nORDER BY 3 DESC\n;\n```\n\nCopy\n```\n ... \nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > How to Interpret Results:\nContent:\nLook for irregularities in the credit consumption or consistently high consumption\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > Primary Schema:\nContent:\nAccount_Usage\nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > SQL\nContent:\nSELECT\nTO_DATE(START_TIME) as DATE\n,DATABASE_NAME\n,SUM(CREDITS_USED) as CREDITS_USED\nFROM \"SNOWFLAKE\".\"ACCOUNT_USAGE\".\"REPLICATION_USAGE_HISTORY\"\nWHERE START_TIME >= dateadd(month,-1,current_timestamp())\nGROUP BY 1,2\nORDER BY 3 DESC\n;\n```\n\nCopy\n```\nUpdated Sep 15, 2023\nThis content is provided as is, and is not maintained on an ongoing basis. It may be out of date with current Snowflake instances\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n*\n*\n ... \nSection Title: Resource Optimization: Billing Metrics > ... > Tier 3 > SQL\nContent:\n*\nAdd me to the list to receive dedicated product updates and general availability emails.\nBy submitting this form, I understand Snowflake will process my personal information in accordance with its [Privacy Notice](http://www.snowflake.com/privacy-policy/) . I may unsubscribe through [unsubscribe links](https://info.snowflake.com/2020-Snowflake-Preference-Center.html) at any time.\nSubscribe Now\nLearn * Resource Library\nLive Demos\nFundamentals\nTraining\nCertifications\nSnowflake University\nDeveloper Guides\nDocumentation\nPrivacy Policy\nSite Terms\nCommunication Preferences\nCookie Settings\nDo Not Share My Personal Information\nLegal\n[](https://x.com/Snowflake \"X (Twitter)\")\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n[](https://www.facebook.com/snowflakedb/ \"Facebook\")\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")"]},{"url":"https://www.bluent.com/blog/building-snowflake-native-apps","title":"Building Snowflake Native App Framework: Guide & Best ...","publish_date":"2026-01-22","excerpts":["Blogs, insights, and use cases to help you navigate trends and make smarter decisions.\n- * [Industry Trends & Innovation](https://www.bluent.com/blog)\n+ Get the true facts & shifts that shape your sector.\n+ Discover how AI & Data are redefining your industry.\n+\n- * [Gift Card Analytics](https://www.bluent.com/business-services/gift-card-analytics)\nMaximize ROI with Insightful Gift Card Metrics\n[](https://www.bluent.com/business-services/gift-card-analytics)\n```\n                            Custom business analytics solutions to drive business\n                            \n                            [](https://www.bluent.com/business-services/data-analytics)\n                            \n                            [FinTech Solutions](https://www.bluent.com/business-services/fintech-solutions)\n                            \n                            Empowering Finance, Transforming Futures\n                            \n                            [](https://www.bluent.com/business-services/fintech-solutions)\n```\nWho We Are\nWho We Are\n[Why BluEnt? ](https://www.bluent.com/who-we-are/)\n[Portfolio](https://www.bluent.net/clients/portfolio)\nIndustry Insights\nIndustry Insights\n[Financial Services & Banking](https://www.bluent.com/industry-insights/)\n[Gift Card Analytics](https://www.bluent.com/business-services/gift-card-analytics)\nGet a free quote\nUS[Engineering Services](https://www.bluentcad.com/) [Technology Services](https://www.bluent.net/)\n[Advanced Analytics & AI](https://www.bluent.com/blog/category/advanced-analytics-ai)\n[Data Strategy & Consulting](https://www.bluent.com/blog/category/data-strategy-consulting)\n[Data Integration](https://www.bluent.com/blog/category/data-integration)\n[Gift Card Analytics](https://www.bluent.com/blog/category/gift-card-analytics)\n[FinTech Solutions](https://www.bluent.com/blog/category/fintech-solutions)\n[AI Services & Solutions](https://www.bluent.com/blog/category/ai-services-solutions)\n[Data Governance &\n ... \nSection Title: Snowflake Native App Framework: A Smart Guide for Building Native Apps\nContent:\n[Snowflake’s native app framework](https://www.bluent.com/blog/ai-app-development-with-snowflake) promotes development, distribution, and monetization of data applications directly into Snowflake data cloud. This allows providers to create applications that implement core snowflake functionalities and then share or sell them through the Snowflake marketplace.\n***Let’s understand the key aspects of the Snowflake Native App Framework.***\nTable of Contents\nIntroduction to Snowflake Native App Framework\nCross-Account Deployment Explained\nKey Components & Best Practices\nMonetization Strategies for Developers\nReal-World Implementation: Compliance Reporting Tool for Finance\nConclusion\nFAQs\nSection Title: Snowflake Native App Framework: A Smart Guide for Building Native Apps\nContent:\nAll the native applications run directly within the Snowflake account of the users, thereby removing any requirement of moving/copying data to [enhance data security and governance](https://www.bluent.com/data-governance-compliance) . The framework is designed to protect the intellectual property of app providers, even though the application runs within the consumer’s environment.\nProviders can list their applications publicly/privately on the Snowflake Marketplace which enables monetization via different licensing models. Also, the native app framework allows development of apps that can utilize Snowflake’s built-in features, such as data processing, analytics, and data sharing capabilities.\nCut costs. Scale apps. Stay compliant with Snowflake Native Apps.\nSection Title: ... > Cross-Account Deployment Explained\nContent:\n***What do you mean by cross-account deployment in the Snowflake Native App Framework?***\nCross-account deployment within Snowflake Native App Framework indicates the process through which a provider can make their applications available to the users in various Snowflake accounts. This major aspect of Snowflake’s distribution model is enabled by Snowflake’s [enhanced & secure data sharing capabilities](https://www.bluent.com/secure-data-sharing-services) .\n**Application package creation** – The application provider creates the application packages that contain all the mandatory data-centric content and application logics such as SQL and Python.\n**Listing creation** – Once the applications are developed, the provider can list them in private or public. In private, user can directly get the application in the account. If listed publicly, any Snowflake user can explore the Snowflake marketplace and install the application.\n ... \nSection Title: ... > Monetization Strategies for Developers\nContent:\nSnowflake’s Native App Framework lets developers avail different [monetization strategies](https://www.bluent.com/blog/generative-ai-monetization) for their applications and data products within the [Snowflake ecosystem](https://www.bluent.com/blog/snowflake-openflow) .\nHere are some of the prominent key monetization strategies that are fully employed by various US-based companies such as Deloitte, Optum, JP Morgan Chase & Co, and State Street.\nSection Title: ... > Monetization Strategies for Developers\nContent:\n**Usage-based pricing:** This pricing model bills customers as per their actual consumption of the application’s features or resources. **Custom event billing:** Developers can define various custom billing events within their application code, offering higher flexibility and metering as per specific actions or resource consumption. **Compute pool surcharge monetization:** This pricing model variation allows consumers to be billed according to the compute resources their applications utilize. **Subscription-based pricing:** In this pricing model, a predictable price is offered to the users for a specific time, offering “all you can use access” to the application. **Recurring subscriptions:** Customers need to pay a fee, either monthly, quarterly, or annually, for continuous access.\nSection Title: ... > Monetization Strategies for Developers\nContent:\n**Flexible terms:** This pricing model gives users the freedom to avail the snowflake applications for various term-based durations with options for upfront or later payment.\nSection Title: ... > Monetization Strategies for Developers\nContent:\nThese strategies leverage Snowflake Marketplace and on-platform billing capabilities to streamline revenue generation. Some of the US-based companies have reported to increase their cost savings by 35% while achieving 27% faster rate of deployments leading to better ROI generation.\nTurn your apps into revenue engines with Snowflake Marketplace.\n ... \nSection Title: ... > Real-World Implementation: Compliance Reporting Tool for Finance\nContent:\n| Reduced regulatory risk | Faster time-to-report |\n| Lower operational costs | Scalability across regions |\n| Monetization for providers |  |\n ... \nSection Title: Snowflake Native App Framework: A Smart Guide for Building Native Apps > FAQs\nContent:\n**What is a Snowflake Native App?** A Snowflake Native App is an application installed directly in a consumer’s Snowflake account, comprising data, logic, and UI (e.g., via Streamlit), packaged and shared by a provider.\n**How does cross-account deployment work?** Providers create an application package (with setup script and manifest), publish it via Marketplace or private listing, and consumers install it into their account.\n**How are app upgrades and versioning handled?** Providers manage versions and patches in the package, use versioned schemas for stateless objects, and issue release directives; Snowflake handles the rollout or consumers can manually upgrade.\n**How can app performance and issues be monitored?** Providers configure log and trace levels in the manifest and enable event sharing; this allows both structured logs and metrics to flow from consumer to provider.\n ... \nSection Title: Snowflake Native App Framework: A Smart Guide for Building Native Apps > FAQs > Format\nContent:\nSpecialized in:\n[**B** usiness So **lu** tions for Digital Transformation](https://www.bluent.com/)\n[**En** gineering Design & Development](https://www.bluentcad.com/)\n[**T** echnology Application & Consulting](https://www.bluent.net/)\n**Trending**\n[AI Governance Framework: Build Responsible and Scalable Enterprise AI](https://www.bluent.com/blog/ai-governance-framework)\n[Snowflake Security & Compliance: A CXO Guide to Trusted Enterprise Data](https://www.bluent.com/blog/snowflake-security-compliance)\n[The CXO’s Guide to Microsoft Fabric: Unifying Data, Analytics, and AI Governance](https://www.bluent.com/blog/microsoft-fabric-unified-data-analytics-governance)\n**Connect Now**\nΔ\nFull Name (Required)\nEmail (Required)\nPhone\nMessage (Required)\nRelated items\nSection Title: Snowflake Native App Framework: A Smart Guide for Building Native Apps > FAQs > Format\nContent:\n[Snowflake Security & Compliance: A CXO Guide to Trusted Enterprise Data 19/02/2026](https://www.bluent.com/blog/snowflake-security-compliance)\n[The CXO’s Guide to Microsoft Fabric: Unifying Data, Analytics, and AI Governance 18/02/2026](https://www.bluent.com/blog/microsoft-fabric-unified-data-analytics-governance)\n[Hidden Challenges of Snowflake Data Migration: Best Practices to Overcome Them 13/02/2026](https://www.bluent.com/blog/snowflake-data-migration-best-practices)\n[Snowflake to Databricks Migration Strategy: When & Why Enterprises Should Switch 11/02/2026](https://www.bluent.com/blog/snowflake-to-databricks-migration-strategy)\nSubscribe\nNotify of\nnew follow-up comments new replies to my comments\nLabel\nName*\nEmail*\nWebsite\nΔ\nLabel\nName*\nEmail*\nWebsite\nΔ\n0 Comments\nOldest\nNewest Most Voted\nInline Feedbacks\nView all comments\n ... \nSection Title: Snowflake Native App Framework: A Smart Guide for Building Native Apps > FAQs > Our Expertise\nContent:\n[Data Strategy & Consulting](https://www.bluent.com/data-strategy-consulting)\n[Enterprise Data Cloud Services](https://www.bluent.com/enterprise-data-cloud-services)\n[Data Engineering & Modernization](https://www.bluent.com/data-engineering-modernization)\n[Advanced Analytics & AI](https://www.bluent.com/advanced-analytics-ai)\n[Data Governance & Compliance](https://www.bluent.com/data-it-security)\n[Digital Services & Development](https://www.bluent.com/digital-services-and-solutions)\nSection Title: Snowflake Native App Framework: A Smart Guide for Building Native Apps > FAQs > Related Links\nContent:\n[Data Analytics](https://www.bluent.com/business-services/data-analytics)\n[Gift Card Analytics](https://www.bluent.com/business-services/gift-card-analytics)\n[Fintech Solutions](https://www.bluent.com/business-services/fintech-solutions)\n[Salesforce](https://www.bluent.com/business-services/salesforce)\n[Platforms](https://www.bluent.com/platforms)\n[Web Producer for Marketing](https://bluent.net)"]},{"url":"https://docs.snowflake.com/en/sql-reference/functions/system_create_billing_event","title":"SYSTEM$CREATE_BILLING_EVENT | Snowflake Documentation","excerpts":["Reference Function and stored procedure reference System SYSTEM$CREATE_BILLING_EVENT\nCategories:\nSystem functions (System Control)\nSection Title: SYSTEM$CREATE_BILLING_EVENT ¶\nContent:\nPreview Feature — Open\nAvailable to all accounts.\nCreates a billable event that tracks consumer usage of an installed\nmonetized application. If you need to exceed the one event per minute frequency limitation of this system function, use SYSTEM$CREATE_BILLING_EVENTS . This system function can only be called from an application installed in a consumer account.\nSection Title: SYSTEM$CREATE_BILLING_EVENT ¶ > Syntax ¶\nContent:\n```\nSYSTEM$CREATE_BILLING_EVENT ( \n ' <class> ', \n ' <subclass> ', \n <start_timestamp> , \n <timestamp> , \n <base_charge> , \n ' <objects> ', \n ' <additional_info> ' \n )\n```\nCopy\nSection Title: SYSTEM$CREATE_BILLING_EVENT ¶ > Arguments ¶\nContent:\n**Required:**\n`' _class_ '`\nIdentifier for the custom event class.\nType: STRING\nThe identifier has the following requirements:\nMust start with a letter (A-Z) or an underscore (“_”).\nMust contain only letters, underscores, decimal digits (0-9), and dollar signs (“$”).\nLength cannot exceed 64 characters.\nMust not start with `SNOWFLAKE_` . `SNOWFLAKE_` is reserved for internal identifiers.\nThe class name is stored and resolved as uppercase characters. Class name comparisons are case-insensitive. `_timestamp_`\nSpecifies the timestamp (UTC) when the event was created as a Unix timestamp in milliseconds.\nType: Integer `_base_charge_`\nSpecifies the amount in US dollars to charge for the billable event. The value must be greater than zero, less than 99,999.99, and must not exceed two decimal places of precision. For example, `1.00` or `0.07` .\nType: DOUBLE\n**Optional:**\nSection Title: SYSTEM$CREATE_BILLING_EVENT ¶ > Arguments ¶\nContent:\n`' _subclass_ '`\nIdentifier for the custom event subclass. This field is only used by the provider.\nType: STRING\nThe identifier has the same naming requirements as the `_class_` argument. `_start_timestamp_`\nSpecifies the start time (UTC) of the event as a Unix timestamp in milliseconds.\nType: INTEGER\nUse to set the start time in cases where providers want to emit an event based on a time range; otherwise set to the same\nvalue used for the `TIMESTAMP` argument. `' _objects_ '`\nA JSON string array containing fully qualified object names that apply to this event.\nType: STRING\nThe maximum size is 4 KB. `' _additional_info_ '`\nA JSON string of key-value pairs the provider can use to send additional info.\nType: STRINGThe maximum size is 4 KB.\n ... \nSection Title: SYSTEM$CREATE_BILLING_EVENT ¶ > Usage notes ¶\nContent:\nThis system function can only be called from within a stored procedure in the setup script of an application created using the\nSnowflake Native App Framework.\nSection Title: SYSTEM$CREATE_BILLING_EVENT ¶ > Examples ¶\nContent:\nSee Billable event examples .\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nSyntax\nArguments\nReturns\nUsage notes\nExamples\nRelated content\nAdd billable events to an application package\nSYSTEM$CREATE_BILLING_EVENTS\nLanguage: **English**\nEnglish\nFrançais\nDeutsch\n日本語\n한국어\nPortuguês"]},{"url":"https://medium.com/snowflake/finops-cost-management-in-snowflake-insights-from-a-streamlit-app-92d74477c137","title":"FinOps Cost Management in Snowflake — Insights from a Streamlit App | by Eladio Rincón Herrera | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2025-10-17","excerpts":["Sitemap\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nWrite\nSearch\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-92d74477c137---------------------------------------)\n·\nFollow publication\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-92d74477c137---------------------------------------)\nBest practices, tips & tricks from Snowflake experts and community\nFollow publication\nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App\nContent:\nEladio Rincón Herrera\nFollow\n8 min read\n·\nOct 17, 2025\n12\nListen\nShare\nCloud costs are no longer just a finance line item — they shape strategy, influence product decisions, and even impact company culture. As organizations scale across multiple clouds and SaaS providers, the challenge isn’t simply “how much are we spending?” but rather “are we spending in the right places, at the right time, for the right outcomes?”\nThis is where FinOps comes in: a practice that blends finance, engineering, and product disciplines to create shared accountability for cloud spending. And Snowflake is not just another platform in this story — it’s a proving ground for how FinOps can be embedded directly into daily workflows. By surfacing cost data natively and aligning with open standards like FOCUS, Snowflake helps teams turn raw credit consumption into meaningful business insights.\nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App\nContent:\nIn this post, we’ll explore how you can stand up a cost analytics dashboard in minutes, map Snowflake usage data to FinOps practices, and bridge the gap between strategic governance and hands-on engineering. Think of this as your playbook for making Snowflake not only performant, but also financially accountable.\nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > TL;DR\nContent:\nThis post covers how to:\nDeploy a cost analytics dashboard in Snowflake using Streamlit.\nAnalyze credits by service and warehouse.\nTrack usage trends over time.\nDrill into query-level cost drivers.\nApply a cost-aware design (snapshots + caching) to minimize overhead.\nStart with a minimal role & app, then expand to org-level views for company-wide FinOps visibility.\nDemo Full Detail of Costs:\nPress enter or click to view image in full size\nDaily Trend Press enter or click to view image in full size\nDashboard\nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > Why FinOps Matters\nContent:\nMost cloud providers actively support FinOps practices through the [FinOps Foundation](http://www.finops.org) , helping customers optimize their cloud bills in line with open standards like FOCUS. The FOCUS specification is an open schema maintained by the Foundation where providers and vendors can map their cost and usage data to a consistent model. This makes it easier for enterprises to aggregate, analyze, and act on cost data across multi-cloud and SaaS ecosystems.\nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > Why FinOps Matters\nContent:\nIn the case of Snowflake, it provides native transparency through `ACCOUNT_USAGE` and `ORGANIZATION_USAGE` views, giving teams near real-time insights into where credits are being consumed—whether in warehouses, queries, or storage. Its credit-based billing model is also directly recognized in the FOCUS specification, ensuring alignment with the industry’s standard for cloud cost and usage data. You can see the details of the FOCUS v1.2 spec [here](https://focus.finops.org/focus-specification/v1-2/) .\n*Note: At the time of writing, FOCUS v2.1 is the latest version. Snowflake mappings are still primarily aligned with v1.2, which means some newer dimensions (e.g., richer invoice identifiers, commitments) require external enrichment pipelines.*\n ... \nSection Title: ... > What is Possible in the Snowflake Portal\nContent:\nThe Snowflake web portal provides native visibility into cost and usage, enabling FinOps practices directly within the platform. From the Billing & Usage section, customers can download detailed usage statements that include credits consumed, billing currency, and invoiced amounts. These statements serve as the source of truth for financial reconciliation and can be mapped into the FOCUS specification for standardized reporting.\nIn addition, the portal exposes Snowsight dashboards for monitoring warehouse utilization, query activity, and service-level costs. Finance and engineering teams can leverage these dashboards to track trends, attribute spend, and spot anomalies without external tools. While deeper FinOps workflows may require exporting data to FOCUS-compliant pipelines, the portal ensures that organizations always have a clear, auditable view of Snowflake consumption and spend.\nSection Title: ... > Common Metrics Often Analyzed in Snowflake Portal\nContent:\n**Monthly Usage Statements & Billing Reconciliation\n** Snowflake allows downloading [monthly usage statements](https://docs.snowflake.com/en/user-guide/billing-usage-statement) that detail credits consumed and costs in currency. These are used to reconcile what appears in usage views with the official invoice. **Overall Cost & Spend Breakdown (Account / Organization Level)\n** Views like [Organization Overview and Account Overview](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) provide spend history, capacity contract consumption, and which accounts, warehouses, or services are driving the most cost. **Warehouse Compute & Credit Utilization Metrics\n** Metrics for warehouse usage (credits consumed by compute), load, latency, utilization vs capacity, and queued queries help optimize sizing, avoid spikes, and tune auto-resume/suspend policies.\nSection Title: ... > Common Metrics Often Analyzed in Snowflake Portal\nContent:\nSee the official [Snowflake docs](https://docs.snowflake.com/en/user-guide/warehouses-overview?utm_source=chatgpt.com) and [Datadog’s Snowflake metrics guide](https://www.datadoghq.com/blog/snowflake-metrics) . **Storage & Data Usage Metrics\n** [Storage metrics](https://select.dev/posts/snowflake-storage?utm_source=chatgpt.com) include costs tied to staging, Time Travel retention, table sizes, and stage usage. These help identify underutilized storage and fine-tune retention policies.\nSection Title: ... > Why Build a Custom Cost Analytics App?\nContent:\nSnowflake’s native portal and Snowsight dashboards provide useful insights into usage, spend, and warehouse performance — great for quick checks and financial reconciliation. But when teams need interactive, FinOps-oriented analysis, a custom Streamlit app inside Snowflake delivers more. By combining live telemetry ( `ACCOUNT_USAGE` ) with pre-aggregated snapshots, applying cost-aware caching, and drilling into query-level cost drivers, the app goes beyond static reporting. It enforces permission checks, adds filters by service or warehouse, and surfaces the underlying SQL so Finance and Engineering share the same transparent view. In short, it transforms Snowflake telemetry into a collaborative cockpit for FinOps, aligning data with FOCUS dimensions and enabling actionable insights, accountability, and optimization at scale.\nSection Title: ... > Get Eladio Rincón Herrera’s stories in your inbox\nContent:\nJoin Medium for free to get updates from this writer.\nSubscribe\nSubscribe\nDemo Environment:\nPress enter or click to view image in full size\nDaily Trend\n ... \nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > A Few Useful Query Patterns\nContent:\nawd.WAREHOUSE_NAME  \nAND  q.USAGE_DATE     =  awd.USAGE_DATE  \nORDER BY  q.USAGE_DATE  DESC , EST_TOTAL_CREDITS  DESC ;\n```\nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > A Few Useful Query Patterns\nContent:\nIn the further reading section, you can find links to more detailed articles on Snowflake cost analysis, warehouse optimization, and storage management.\nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > 📸 Screen Captures & Use Case\nContent:\nThe following screenshots show how the Snowflake Cost Management Streamlit App supports interactive FinOps analysis:\n ... \nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > ... > 2. Daily Trends\nContent:\nUsers can drill into daily consumption trends, spotting anomalies or spikes in credit usage. The line charts update dynamically based on filters applied.\nPress enter or click to view image in full size\nDaily trend\nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > ... > 3. Query-Level Insights\nContent:\nClicking into a specific period reveals a table with aggregated query details, including:\nUser\nWarehouse\nQuery date\nCredits consumed (Cloud Services + Compute)\nQuery count\nGB scanned\nThis view makes it easy to identify high-cost or noisy queries.\nPress enter or click to view image in full size\nQuery details\nSection Title: ... > 4. Expanded Query Details\nContent:\nFrom the aggregated table, users can expand into detailed query execution metrics:\nDatabase, Schema, Warehouse\nStart/end time\nCredits consumed\nRows produced & execution status\nFull SQL text\nThis allows Finance, Engineering, and Product to align on query-level accountability.\nPress enter or click to view image in full size\nExpanded queries details Press enter or click to view image in full size\nSingle query details\n ... \nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > Summary\nContent:\nSnowflake already gives organizations powerful cost visibility through its portal, usage views, and Snowsight dashboards. But for teams adopting FinOps at scale, native reporting is just the starting point. By aligning Snowflake’s credit-based model with the FOCUS specification, and extending telemetry with a custom Streamlit app, companies can bridge the gap between financial governance and engineering workflows.\nThe result: a collaborative, cost-aware cockpit that empowers Finance, Engineering, and Product teams to share the same transparent view of usage, detect cost drivers early, and keep cloud spending predictable without slowing innovation.\n ... \nSection Title: FinOps Cost Management in Snowflake — Insights from a Streamlit App > 📚 Further Reading\nContent:\n[](https://medium.com/snowflake?source=post_page---post_publication_info--92d74477c137---------------------------------------)\nFollow\n[## Published in Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---post_publication_info--92d74477c137---------------------------------------)\n11K followers\n· Last published 23 hours ago\nBest practices, tips & tricks from Snowflake experts and community\nFollow\nFollow"]},{"url":"https://docs.snowflake.com/en/developer-guide/native-apps/ui-consumer-enable-logging","title":"Set up event tracing for an app | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg™\nApache Iceberg™ Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Listings Snowflake Marketplace listings Use applications as a consumer Set up event tracing for an app\nSection Title: Set up event tracing for an app ¶\nContent:\nFeature — Generally Available\nThe Snowflake Native App Framework is generally available on supported cloud platforms. For additional information, see Support for private connectivity, VPS, and government regions .\nThis topic describes how to set up use event tracing to capture the log messages and trace events\nemitted by an app. It also describes how to enable event sharing to share log messages and trace events\nwith providers.\n ... \nSection Title: Set up event tracing for an app ¶ > About event sharing ¶\nContent:\nConsumers can also enable event sharing to share event data with providers. When a provider enables\nevent sharing, the log messages and trace events that are inserted into the event table in\nthe consumer account are also inserted into an event table in provider account.\nEvent sharing allows the provider to collect information about the app’s performance and behavior. See About event sharing for an app for more information.\nSection Title: Set up event tracing for an app ¶ > About event sharing ¶ > About event definitions ¶\nContent:\nEvent definitions specify how an app shares log messages and trace events with the provider.\nEvent definitions act as filters on the log message and trace event levels set by the provider.\nA provider specifies the event definitions for an app when a new version or patch is published.\nNote\nEvent definitions are not required. If a provider does not specify event definitions for an app\nconsumers can enable or disable event sharing as required.\nProviders can set an event definition to be required or optional:\nSection Title: Set up event tracing for an app ¶ > About event sharing ¶ > About event definitions ¶\nContent:\nRequired event definitions are enabled automatically when the app is installed. To collect the\nevent definitions emitted by an app, a consumer should create an event table and set it as\nthe active event table for their account.\nOptional event definitions can be enabled or disable by the consumer as necessary. Optional event\ndefinitions require an active event table, but they are not required to install or use the app.\nCaution\nEvent definitions are not the same as the log and tracing levels set by the provider. The log and\ntracing levels determine the information that is inserted into the consumer event table.\nEvent definitions are filters that act on the log messages and trace events. They determine what\ninformation is inserted in the provider event table when event sharing is enabled.\n ... \nSection Title: Set up event tracing for an app ¶ > About event sharing ¶ > Supported event definitions ¶\nContent:\nIf a provider does not configure the app to use event definitions, Snowsight displays only the All type.\nSection Title: ... > Considerations for consumers when using event definitions ¶\nContent:\nConsumers can continue to use the existing SHARE_EVENTS_WITH_PROVIDER property, however there\nare limitations:\nIf an app only uses the OPTIONAL ALL event definition, setting the SHARE_EVENTS_WITH_PROVIDER property\nto `true` enables event sharing and setting it to `false` disables event sharing.This is applicable when a provider explicitly adds the OPTIONAL ALL event definition to the manifest\nfile or an app was migrated from the existing event sharing functionality.\nIf a provider adds mandatory and optional event definitions to the manifest file, setting the\nSHARE_EVENTS_WITH_PROVIDER property to `true` enables all event definitions. In contrast, the\nSHARE_EVENTS_WITH_PROVIDER property can only be set to `false` if the provider adds only\noptional event definitions.SHARE_EVENTS_WITH_PROVIDER is TRUE only when all event definitions are enabled, otherwise it is FALSE.\n ... \nSection Title: Set up event tracing for an app ¶ > Considerations when using event tracing ¶\nContent:\nBefore setting up event tracing for an app, you must consider the following:\nSection Title: Set up event tracing for an app ¶ > Considerations when using event tracing ¶\nContent:\nThis feature requires you to set up an event table in\nyour account. After you enable event sharing , a masked and redacted\ncopy of the trace events and logs messages is automatically inserted in the event table of the designated\nprovider account. Snowflake does not charge you to enable event sharing. However, you are responsible for the\ncost of ingesting trace events and log message in the event table as well as storage\ncosts for the event table. After enabling event sharing with a provider, you cannot revoke access to shared\ntrace events and log messages. You cannot share historical events using event sharing. Snowflake sends the shared events to a designated provider account within the same region as your account. This feature does not share data across different regions. You cannot change the logging or tracing levels for an app. The app provider sets these levels\nwhen publishing the app.\nSection Title: Set up event tracing for an app ¶ > Considerations when using event tracing ¶\nContent:\nSnowflake recommends reviewing the trace events and log messages in the event table before enabling\nevent sharing. Snowflake recommends disabling event sharing if you do not need to troubleshoot the app.\nSection Title: Set up event tracing for an app ¶ > Set up an event table ¶\nContent:\nTo collect the log messages and trace events emitted by the app, consumers must create an event table to\nstore the information.\nNote\nIF the consumer does not set up an event table and make it the active event table before installing the\napp, trace event and log data is discarded.\nIf a provider includes required event definitions in the app, they are enabled by default during\ninstallation. However, if the consumer does not have an active event table, the log messages and\ntrace events emitted by the app are discarded.\nAn account can have multiple event tables, but only one of them can be set as the active event table in a\nSnowflake account at a time. Without an active event table, log messages and trace events that the app emits\nare not captured. This is true even if the functions and procedures in an app call the logging and trace\nevent APIs directly.\n ... \nSection Title: Set up event tracing for an app ¶ > Enable event sharing for an app ¶\nContent:\nThe Snowflake Native App Framework supports sharing log messages and trace events stored in the consumer event table with the\napp provider. To share logs and event information with a provider, the consumer must enable event\nsharing for an app.\n ... \nSection Title: Set up event tracing for an app ¶ > ... > Enable event sharing using Snowsight ¶\nContent:\nNote\nIf the provider includes required event definitions in the app, event sharing and the required event definitions are enabled during installation and\ncannot be disabled later.\nSection Title: Set up event tracing for an app ¶ > ... > Enable event sharing using Snowsight ¶\nContent:\nIn the navigation menu, select Catalog » Apps .\nSelect the app.\nSelect the Settings icon in the toolbar.\nSelect the Events and logs tab.\nUnder the Events and logs sharing area, move the slider for the events you want to capture.\nIf the provider has defined event definitions for the app:\nUse the slider to enable optional event definitions. By default, all event types are enabled.\nSelect Save .\nIf no event table is currently selected, select the event table from the list\nunder Event table location .CautionUse caution when changing the event table in Snowsight. Each Snowflake account\nuses a single event table for all events generated within the account. Changing the event\ntable causes all events generated in the account to be stored in the new location.\n ... \nSection Title: ... > Determine if a log message or trace event is shared with the provider ¶\nContent:\nThe RECORD_ATTRIBUTES column contains the `snow.application.shared` field. If the value of\nthis field is TRUE, the log message or trace event is shared with the provider. Otherwise,\nthe log message or event is not shared.\nSection Title: Set up event tracing for an app ¶ > View the log and trace levels for an app ¶\nContent:\nThe log and trace level of an app are defined by the provider before publishing an app.\nConsumers cannot change the log and trace levels for an app.\nHowever, before setting up event tracing or enabling event sharing for an app, Snowflake\nrecommends verifying the log level to understand the type of information that collected\nand shared with the provider.\nTo view the log and trace level of an app, run the following command:\n```\nDESC APPLICATION HelloSnowflake ;\n```\nCopy\nThis command displays information about the `HelloSnowflake` app, including the following information\nabout the log and trace level set for the app:\nlog_level: The log level set by the provider.\ntrace_level: The trace level set by the provider.\nmetric_level: The metric data level set by the provider.\neffective_log_level: The log level set for the app.\neffective_trace_level: The trace level enabled for the app.\nSection Title: Set up event tracing for an app ¶ > View the log and trace levels for an app ¶\nContent:\nThe effective log and trace levels are determined by the event definitions the consumer enables for the app.\nFor example, if the provider defines the log level as OFF, but consumer enables the ERROR_AND_WARNING event\ndefinition, the app dynamically changes log level to WARN so that ERROR_AND_WARNING events can be collected. The app\nemits events that are less or equally as verbose as WARN and shares those error and warning events with the provider.\nThe values of `log_level` would be OFF and the value of `effective_log_level` would be WARN.\nIn contrast, if the provider defines the log level as TRACE, but the consumer enables the ERROR_AND_WARNING\nevent definition, the app emits events that are less or equally as verbose as trace, but only error and warning\nmessages are shared with the provider. The value of both log_level and effective_log_level would be TRACE.\nWas this page helpful?\nYes No\n ... \nSection Title: Set up event tracing for an app ¶ > Privacy Preference Center > Your Privacy > Functional Cookies\nContent:\nFunctional Cookies\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\nCookies Details‎"]},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization - Snowflake","excerpts":["Section Title: Cost Optimization\nContent:\nWell Architected Framework Team\n[fork repo](https://github.com/Snowflake-Labs/sfquickstarts/tree/master/site/sfguides/src/well-architected-framework-cost-optimization-and-finops)\nSection Title: Cost Optimization > Overview\nContent:\nThe Cost Optimization Pillar focuses on integrating financial\naccountability and cost awareness throughout the cloud platform\nlifecycle. It involves establishing principles, gaining visibility into\nspending, implementing controls, and continuously optimizing resources\nto align cloud costs with business value. This pillar is essential for\nfinancial stakeholders, cloud architects, and engineering teams seeking\nto maximize return on investment in cloud infrastructure.\nSection Title: Cost Optimization > Principles > Business impact: Align cost with business value\nContent:\nConnect cloud spending directly to business outcomes, ensuring that every dollar spent in the cloud contributes meaningfully to strategic objectives and demonstrable value. Embed cost considerations directly into platform planning and architecture considerations.\n ... \nSection Title: Cost Optimization > Recommendations\nContent:\nFurthermore, you should establish a regular cadence for refreshing\nthis measurement to ensure value realization is in line with\nexpectations and business goals. **Visibility**\n**Understand Snowflake’s resource billing models:** Review\nSnowflake’s billing models to align technical and non-technical\nresources on financial drivers and consumption terminology. **Establish a consistent and granular cost attribution strategy:** Implement robust and organizationally consistent tagging and\nlabeling strategies across all resources (storage objects,\nwarehouses, accounts, queries) to accurately allocate costs to\nspecific teams, products, or initiatives. **Embed cost accountability into your organization's DNA:** Implement a feedback system to transparently show or charge back\ncloud costs to relevant business units or teams, increasing\naccountability.\n ... \nSection Title: Cost Optimization > Business Impact > Overview\nContent:\nTo maximize organizational outcomes, Snowflake consumption must be\nexplicitly tied to business value. While cost optimization ensures\nefficiency, it does not guarantee that spend is aligned with outcomes\nthat matter to stakeholders. Alignment of business value to cost ensures\nworkloads, pipelines, dashboards, and advanced analytics are\ncontinuously evaluated not only for cost but also for the value they\ndeliver. This approach ensures Snowflake delivers as a strategic\nbusiness platform rather than a technical expense.\n ... \nSection Title: Cost Optimization > Business Impact > Overview > Consider cost as a design constraint\nContent:\nAt the ingestion layer, best practices include balancing latency versus\ncost by selecting appropriate services (e.g., Snowpipe, Snowpipe\nStreaming, or third-party tools) and choosing the right storage format\n(e.g., native tables, Iceberg). For transformations, design with\nfrequency versus SLA in mind to ensure data freshness matches the\nbusiness need. For analytics, apply schema design best practices such as\nthoughtful clustering key choices and pruning strategies to reduce\nconsumed credits. In distribution, optimize data transfer by monitoring\negress patterns and applying cost-saving practices like the [Snowflake Data Transfer Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) .\n ... \nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\nIt is essential to review Snowflake's billing models to align technical\nand non-technical resources on financial drivers and consumption\nterminology. Snowflake's elastic, credit-based consumption model charges\nseparately for compute (Virtual Warehouses, Compute Pools, etc),\nstorage, data transfer, and various serverless features (e.g., Snowpipe,\nAutomatic Clustering, Search Optimization, Replication/Failover, AI\nServices). Understanding the interplay of these billing types ensures\nyou can attribute costs associated with each category’s unique usage\nparameters. High-level categories are below.\nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\n**Compute (Virtual Warehouses, Snowpark Container Services, Openflow):** This is often the most dynamic and largest portion of\nSnowflake spend. Virtual Warehouses are billed per-second after an\ninitial 60-second minimum when active, with credit consumption\ndirectly proportional to warehouse size (e.g., an “X-Small” Gen1\nwarehouse consumes one credit per hour, a 'Small' consumes two credits\nper hour, doubling with each size increase) while SPCS (via [compute pools](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) )\nare billed for all uptime with a minimum of five minutes. [Openflow](https://docs.snowflake.com/en/user-guide/data-integration/openflow/cost) is billed per second of runtime with a 60-second minimum.\nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\n**Storage:** Costs are based on the average monthly compressed data\nvolume stored, including active data, Time Travel (data retention),\nand Fail-safe (disaster recovery) data. The price per terabyte (TB)\nvaries by cloud provider and region. **Serverless features:** Snowflake Serverless features use resources\nmanaged by Snowflake, not the user, which automatically scale to meet\nthe needs of a workload. This allows Snowflake to pass on efficiencies\nand reduce platform administration while providing increased\nperformance to customers. The cost varies by feature and is outlined\nin Snowflake’s Credit Consumption Document . **Cloud services layer:** This encompasses essential background\nservices, including query compilation, metadata management,\ninformation schema access, access controls, and authentication.\nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\nUsage\nfor cloud services is only charged if the daily consumption of cloud\nservices exceeds 10% of the daily usage of virtual warehouses. **AI features:** Snowflake additionally offers artificial intelligence\nfeatures that run on Snowflake-managed compute resources, including\nCortex AISQL functions (e.g. COMPLETE, CLASSIFY, etc. ), Cortex\nAnalyst, Cortex Search, Fine Tuning, and Document AI. The usage of\nthese features, often with tokens, are converted to credits to unify\nwith the rest of Snowflake’s billing model. Details are listed in the\nCredit Consumption Document. **Data transfer:** Data transfer is the process of moving data into\n(ingress) and out of (egress) Snowflake.\nSection Title: Cost Optimization > Visibility > Overview > Understand Snowflake’s resource billing models\nContent:\nThis generally happens via\negress on cross-region [data replication](https://docs.snowflake.com/en/user-guide/account-replication-cost) , [copying into/out of stage, function calls](https://docs.snowflake.com/user-guide/cost-understanding-data-transfer) ,\nand cross/same region [SPCS data transfer](https://docs.snowflake.com/en/developer-guide/snowpark-container-services/accounts-orgs-usage-views) . Depending on the cloud provider and the region used during data\ntransfer, charges vary. **Data sharing & rebates:** Snowflake offers an opt-out Data\nCollaboration rebate program that allows customers to offset credits\nby data consumed with shared outside organizations. This rebate is\nproportional to the consumption of your shared data by consumer\nSnowflake accounts. See the latest terms and more details here .\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Track usage data for all platform resources**\nTo deliver clear and actionable consumption insights, it is essential to\nleverage the rich usage data that Snowflake natively provides. The\nfoundation for all cost visibility is the **SNOWFLAKE** database, which\ncontains two key schemas for this purpose: [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) (for granular, account-level data) and [ORGANIZATION_USAGE](https://docs.snowflake.com/en/sql-reference/organization-usage) (for a consolidated view across all accounts).\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Snowsight's built-in cost management capabilities:** Snowsight\nprovides pre-built visuals for usage and credit monitoring directly\nwithin the [Snowflake Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) . It allows filtering by tags (e.g., view cost by department tag),\ncredit consumption by object types, and cost insights to optimize the\nplatform. **Creating custom dashboards or Streamlit apps for different stakeholder groups:** Snowsight facilitates the creation of custom\ndashboards using ACCOUNT_USAGE and ORGANIZATION_USAGE views. Custom\ncharts in the Dashboards feature and Streamlit apps can both be easily\nshared. Combined with cost allocation and tagging, this allows for\ntailored views for finance managers (aggregated spend), engineering\nmanagers (warehouse utilization), or data analysts (query\nperformance).\nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Integrating with third-party BI tools for advanced analytics:** Connecting to Snowflake from tools like Tableau, Power BI, Looker, or\ncustom applications offers highly customizable and extensive control\nover cost data visualization. Cloud-specific third-party data programs\n(FinOps platforms) offer easier setup and more out-of-the-box\nSnowflake cost optimization insights. **Leverage Cortex Code (In Preview):** This AI Assistant capability\nallows users to query cost and usage data in ACCOUNT_USAGE views using\nnatural language natively in the Snowsight UI.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Limit data transfer\nContent:\nData egress, the transfer of data from one cloud provider or region to\nanother, can incur substantial costs, particularly when handling large\ndata volumes. Implementing appropriate tools and best practices is\nessential to minimize these data transfer expenses and maximize business\nvalue when data egress is necessary.\n**Tooling: Enable proactive cost management**\nLeverage Snowflake's native features to gain visibility and control over\ndata transfer costs before they become a significant expense."]}],"usage":[{"name":"sku_search","count":1}]}
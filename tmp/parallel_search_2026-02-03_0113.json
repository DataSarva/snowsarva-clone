{"search_id":"search_585c03f31b3b416eb254b57602b5d700","results":[{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization","excerpts":["Section Title: Cost Optimization > Business Impact > Overview > Consider cost as a design constraint\nContent:\nAt the ingestion layer, best practices include balancing latency versus\ncost by selecting appropriate services (e.g., Snowpipe, Snowpipe\nStreaming, or third-party tools) and choosing the right storage format\n(e.g., native tables, Iceberg). For transformations, design with\nfrequency versus SLA in mind to ensure data freshness matches the\nbusiness need. For analytics, apply schema design best practices such as\nthoughtful clustering key choices and pruning strategies to reduce\nconsumed credits. In distribution, optimize data transfer by monitoring\negress patterns and applying cost-saving practices like the [Snowflake Data Transfer Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) .\n ... \nSection Title: Cost Optimization > Business Impact > Overview > Measure business value KPIs baseline\nContent:\nBest practices include measuring technical unit economic metrics (e.g.\ncredits per 1K queries, credits per 1 TB scanned), warehouse efficiency\nand utilization by workload type, and business unit economics (e.g.\ncredits per customer acquired, credits per partner onboarded, or credits\nper data product-specific KPIs). This provides a more comprehensive\npicture of consumption in relation to cost and value. Outliers should be\nhighlighted in executive communications as either success stories or\ncautionary examples. Benchmarking should be embedded in a continuous\nimprovement loop, where insights drive action, action improves\nefficiency, and those improvements are effectively measured.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Recommendations\nContent:\nImplementing a robust FinOps visibility framework in Snowflake,\nsupported by cross-functional collaboration, enables each business\nfunction to access timely and relevant usage and cost data. This\nempowers them to understand the business impact of their consumption and\ntake prompt action when anomalies arise. To meet this vision, consider\nthe following recommendations based on industry best practices and\nSnowflake's capabilities:\n ... \nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nThis\nsignificantly reduces the manual effort required for tagging and\nensures that new objects created within a tagged schema or propagated\nworkflow automatically inherit the correct cost attribution. Snowflake\nstrongly recommends tags for warehouses, databases, tables, and users\nto enable granular cost breakdowns. You can use the [TAG_REFERENCES view](https://docs.snowflake.com/sql-reference/account-usage/tag_references) in SNOWFLAKE.ACCOUNT_USAGE to combine with common usage views like\nWAREHOUSE_METERING_HISTORY and TABLE_STORAGE_METRICS to allocate usage\nto relevant business groups. Object Tags are best utilized when\nSnowflake objects are not shared across cost owners.\nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\n**Query tags for granular workload attribution:** [Query tags](https://docs.snowflake.com/en/user-guide/cost-attributing) can be set via session parameters (e.g., ALTER SESSION SET QUERY_TAG =\n'your_tag';) or directly within SQL clients or ETL tools. This\nassociates individual queries with specific departments, projects, or\napplications, even when using shared warehouses. This is extremely\nvaluable for shared warehouses where multiple teams or applications\nuse the same compute resource, allowing for granular showback. It is\nalso easy to programmatically make changes to query tags within\nscripts or processes to allocate costs appropriately. Query tags can\nbe found in the QUERY_HISTORY view of the SNOWFLAKE.ACCOUNT_USAGE\nschema.\n ... \nSection Title: Cost Optimization > ... > Overview > Establish a consistent and granular cost attribution strategy\nContent:\nFor best-in-class visibility, it is recommended to have a tagging\nstrategy and tag all resources in an organization to allocate costs to\nrelevant owners.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\n**Snowsight's built-in cost management capabilities:** Snowsight\nprovides pre-built visuals for usage and credit monitoring directly\nwithin the [Snowflake Cost Management UI](https://docs.snowflake.com/en/user-guide/cost-exploring-overall) . It allows filtering by tags (e.g., view cost by department tag),\ncredit consumption by object types, and cost insights to optimize the\nplatform. **Creating custom dashboards or Streamlit apps for different stakeholder groups:** Snowsight facilitates the creation of custom\ndashboards using ACCOUNT_USAGE and ORGANIZATION_USAGE views. Custom\ncharts in the Dashboards feature and Streamlit apps can both be easily\nshared. Combined with cost allocation and tagging, this allows for\ntailored views for finance managers (aggregated spend), engineering\nmanagers (warehouse utilization), or data analysts (query\nperformance).\n ... \nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nCost Anomaly Detection is a critical component of visibility that\nleverages machine learning to continuously monitor credit consumption\nagainst historical spending patterns, automatically flagging significant\ndeviations from the established baseline. This proactive monitoring is\nessential for preventing budget overruns and identifying inefficiencies,\nshifting the organization from a reactive to a proactive cost management\nposture to mitigate financial risk. As a best practice, you should\ninitially review anomaly detection on the entire account to gain a broad\nview, then dive deeper into a more granular review for individual\nhigh-spend warehouses. [This approach](https://docs.snowflake.com/en/user-guide/cost-anomalies) allows for more targeted analysis and assigns clear ownership for\ninvestigating any flagged anomalies. There are several methods for\nanomaly detection supported by Snowflake:\n ... \nSection Title: Cost Optimization > Control > Overview > Recommendations\nContent:\nImplementing a comprehensive control framework, supported by features\nsuch as Resource Monitors, Budgets, and Tagging Policies, empowers\norganizations to enforce financial accountability and maintain budget\npredictability. By adopting these controls, teams can actively manage\nspend, quickly and automatically mitigate cost inefficiencies, and\nensure the disciplined, cost-effective utilization of the entire\nSnowflake environment. The culmination of all of these controls leads to\ngreater platform ROI and minimized financial risk. To meet this goal,\nconsider the following recommendations based on industry best practices\nand Snowflake's capabilities:\n ... \nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\n**Identify demand drivers and unit economics:** To understand what\ndrives Snowflake spend, correlate historical credit, storage, and data\ntransfer usage with key business metrics like cost per customer or per\ntransaction. Use Snowflake's [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) schema, including the WAREHOUSE_METERING_HISTORY and QUERY_HISTORY\nviews, as the primary data sources for this analysis.\n**Granular cost attribution:** Accurately tie costs back to business\nteams or workloads by implementing a mandatory tagging strategy for\nall warehouses and queries. Align these tags with your organization's\nfinancial structure to provide clear cost segmentation.\n**Build the predictive model**\nThis phase integrates historical trends with strategic business inputs\nto create forward-looking projections.\n ... \nSection Title: Cost Optimization > Control > Overview > Govern resource creation and administration\nContent:\n**Centralized management:** A dedicated team, such as a platform\nCenter of Excellence (CoE), handles resource creation and\nadministration policies. This ensures consistency, adheres to best\npractices, and facilitates robust cost control. This model is ideal\nfor large enterprises where strict governance and chargeback are\nparamount.\n**Decentralized management:** Individual business units or teams\nmanage their own resources. This provides greater autonomy and speed\nbut can lead to resource sprawl, inconsistent practices, and\nsignificant cost inefficiencies if not properly governed.\n**Striking the balance: the federated model**\n ... \nSection Title: Cost Optimization > Optimize > Overview > Limit data transfer\nContent:\n**Architectural best practices: Design for minimal data movement**\nMinimizing data transfer costs for your workloads heavily depends on the\narchitecture of your data pipelines and applications. Adhere to the\nfollowing best practices to achieve this:\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nThe COPY INTO\n( [table](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table) or [location](https://docs.snowflake.com/en/sql-reference/sql/copy-into-location) )\ncommand is a foundational and flexible method for bulk data loading from\nan external stage into a Snowflake table. Its importance lies in its\nrole as a powerful, built-in tool for migrating large volumes of\nhistorical data or loading scheduled batch files. The best practice is\nto use COPY INTO for one-time or large batch data loading jobs, which\ncan then be supplemented with more continuous ingestion methods like\nSnowpipe for incremental data. Additional information regarding COPY\nINTO and general data loading best practices can be found in our\ndocumentation [here](https://docs.snowflake.com/user-guide/data-load-considerations) .\nSome additional best practices are outlined below.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nThe [key concepts of dynamic tables](https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh) are defined in our documentation. However, best practices and\ndetermining when to use DTs versus other methods of pipeline tooling in\nSnowflake still warrant discussion, and are compared in Snowflake’s [documentation](https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison) .\nIn addition to Snowflake’s published [best practices](https://docs.snowflake.com/en/user-guide/dynamic-table-performance-guide) ,\nconsider the following\n ... \nSection Title: Cost Optimization > Optimize > Overview > Workload optimization\nContent:\nTo determine tables that can most benefit from re-ordering how data is\nstored, you can review Snowflake’s [best practice](https://medium.com/snowflake/supercharging-snowflake-pruning-using-new-account-usage-views-52530b24bf2e) on how to analyze the TABLE_QUERY_PRUNING_HISTORY and\nCOLUMN_QUERY_PRUNING_HISTORY account usage views. Fundamentally,\nreducing the percentage of partitions in each table pruned to the\npercentage of rows returned in a query will lead to the most optimized\ncost and performance for any given workload.\nA table’s Ideal pruning state is scanning the same % of rows matched as\npartitions read, minimizing unused read rows.\n**Warehouse optimization**\nWarehouse concurrency, type, and sizing can impact the execution\nperformance and cost of queries within Snowflake. Review the compute\noptimization section for more information into the tuning of the\nwarehouse and its effect on cost and performance."]},{"url":"https://www.snowflake.com/en/pricing-options/cost-and-performance-optimization/","title":"FinOps on Snowflake: Built-In Cost and Performance Control","excerpts":["[Skip to content]()\n\nSnowflake Connect: AI on January 27\n\nUnlock the full potential of data and AI with Snowflake’s latest innovations.\n\n[register now](https://www.snowflake.com/events/snowflake-connect-ai/)\n\n[pricing options](/en/pricing-options/)\n\n[overview](/en/pricing-options/)\n\n[cost & performance optimization](/en/pricing-options/cost-and-performance-optimization/)\n\n[pricing calculator](/en/pricing-options/calculator/)\n\n###### Resources\n\n[Pricing calculator overview](https://www.snowflake.com/en/pricing-options/calculator/overview/) [Pricing calculator FAQs](https://www.snowflake.com/en/pricing-options/calculator/feedback-faqs/) [Snowflake Performance Index](https://www.snowflake.com/en/pricing-options/performance-index/)\n\n[_Image Source_](https://squadrondata.com/org-impact-comparison-spark-based-saas-vs-snowflake/)\n\n###### COST MANAGEMENT AND PERFORMANCE OPTIMIZATION\n\n# FinOps on Snowflake\n\nTime is money – save both with Snowflake.\n\n[explore the economic impact of snowflake](https://www.snowflake.com/resource/forrester-the-total-economic-impact-of-the-snowflake-ai-data-cloud/?utm_cta=website-cost-and-performance-forrester-tei)\n\n## Save time on platform management. Spend it on what matters more.\n\n## Go from painstaking configurations to a proven, fully-managed service\n\nSince its founding in 2012, Snowflake has provided automated cluster management, maintenance and upgrades — all without downtime — so you can spend time on valuable data projects\n\nGet **[out-of-the-box governance and security through Snowflake Horizon Catalog](https://www.snowflake.com/en/data-cloud/horizon/)** without extra configurations or protocols\n\n## Go from piecemeal dashboards to built-in cost & performance management\n\n* Get granular visibility, control and optimization of Snowflake spend through a unified [Cost Management Interface](https://docs.snowflake.com/en/guides-overview-cost) .\n* Check query performance easily to proactively save on costs.\n* Automatically benefit from [regular rollouts of performance improvements](https://docs.snowflake.com/en/release-notes/performance-improvements) across all workloads.\n\n## Maximize your Snowflake spend\n\n* Add flexibility in how you use funds committed in your Snowflake Capacity contract.\n* Deploy partner solutions faster by simplifying finance and procurement processes.\n* Bundle your spend to increase your buying power with Snowflake and partners.\n\n[learn more](/en/product/features/marketplace/marketplace-capacity-drawdown-program/)\n\n###### OUR CUSTOMERS\n\n## Saving time on platform admin. Getting to market faster.\n\nTravelpass CTC Natwest\n\n[Travel and Hospitality](https://www.snowflake.com/en/solutions/industries/travel-hospitality/) “Now, we aren’t so focused on how to build things. We are focused more on what to build.” Dan Shah  \nManager of Data Science [Read the story](https://www.snowflake.com/en/customers/all-customers/case-study/travelpass/) * **1 week** for 130 Dynamic Tables to be in production after migration\n* **65%** cost savings switching from Databricks to Snowflake\n\n[Read the case study](/en/customers/all-customers/case-study/pfizer/) [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/) “Now with fewer ephemeral failures and higher visibility in Snowflake, we have a platform that’s much easier and cost-effective to operate than managed Spark.” David Trumbell  \nHead of Data Engineering, CTC [Read the story](https://www.snowflake.com/en/customers/all-customers/case-study/chicago-trading-company/) * **1st** data availability deadline was hit everyday for the 1st time\n* **54%** cost savings switching from managed Spark to Snowflake\n\n[Read the case study](/en/customers/all-customers/case-study/pfizer/) [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/) “The speed at which we’ve delivered wouldn’t have been possible with other providers.” Kaushik Ghosh Dastidar  \nHead of ESG Cloud Solutions, NatWest [Read the story](https://www.snowflake.com/en/customers/all-customers/case-study/natwest/) * **6x** reduction in onboarding time from 3 months to 2 weeks\n* **$750K** saved in salaries & staff training costs\n\n[Read the case study](/en/customers/all-customers/case-study/pfizer/)\n\n[Resource #### Snowflake Joins the FinOps Foundation Snowflake joins The FinOps Foundation as a Premier Enterprise Member to provide thought leadership and set industry financial best practices. Read more](https://www.finops.org/members/snowflake/)\n\n[Resource #### Snowflake Pricing Calculator Curious about Snowflake pricing? Our Snowflake pricing calculator shows credit usage, warehouse costs, and total expenses. Access calculator](/en/pricing-options/calculator/)\n\n[Guide #### Definitive Guide to Managing Spend in Snowflake Learn about considerations for consumption models such as Snowflake's, frameworks for better managing spend, and more. Get the guide](https://www.snowflake.com/en/resources/white-paper/definitive-guide-to-managing-spend-in-snowflake/)\n\n## Even More To Explore\n\n#### Snowflake Documentation\n\nAccess documentation on Managing Costs and Optimizing Performance in Snowflake.\n\n[Read about Managing Costs](https://docs.snowflake.com/en/user-guide/cost-management-overview)\n\n[Read about Optimizing Performance](https://docs.snowflake.com/en/guides-overview-performance)\n\n#### On-Demand Cost Governance Training\n\nLearn how to successfully examine, control, and optimize Snowflake costs.\n\n[Register Now](https://learn.snowflake.com/en/courses/OD-FINOPS/)\n\n#### Professional Services\n\nEngage Snowflake’s Professional Services for expert advice on optimizing your use of Snowflake.\n\n[Discover Professional Services](https://www.snowflake.com/snowflake-professional-services/)\n\n#### Priority Support\n\nLearn more about how our Priority Support team can help you reduce consumption spend through performance monitoring, observability, and management.\n\n[Learn about Priority Support](https://www.snowflake.com/support/priority-support/)\n\n## Where Data Does More\n\n* 30-day free trial\n* No credit card required\n* Cancel anytime\n\n[start for free](https://signup.snowflake.com/)\n\n[watch a demo](/en/webinars/demo/)\n\n**Subscribe to our monthly newsletter** Stay up to date on Snowflake’s latest products, expert insights and resources—right in your inbox!\n\nIndustries * [Advertising, Media & Entertainment](https://www.snowflake.com/en/solutions/industries/advertising-media-entertainment/)\n* [Financial Services](https://www.snowflake.com/en/solutions/industries/financial-services/)\n* [Healthcare & Life Sciences](https://www.snowflake.com/en/solutions/industries/healthcare-and-life-sciences/)\n* [Manufacturing](https://www.snowflake.com/en/solutions/industries/manufacturing/)\n* [Public Sector](https://www.snowflake.com/en/solutions/industries/public-sector/)\n* [Retail & Consumer Goods](https://www.snowflake.com/en/solutions/industries/retail-consumer-goods/)\n* [Technology](https://www.snowflake.com/en/solutions/industries/technology/)\n\nLearn * [Resource Library](https://snowflake.com/en/resources/)\n* [Live Demos](/en/webinars/demo/)\n* [Fundamentals](https://www.snowflake.com/en/fundamentals/)\n* [Training](https://www.snowflake.com/en/resources/learn/training/)\n* [Certifications](https://www.snowflake.com/en/resources/learn/certifications/)\n* [Snowflake University](https://learn.snowflake.com/en/)\n* [Developer Guides](https://www.snowflake.com/en/developers/guides)\n* [Documentation](https://docs.snowflake.com/)\n\n[](/en/)\n\n* © 2026 Snowflake Inc. All Rights Reserved\n* [Privacy Policy](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Site Terms](https://snowflake.com/en/legal/snowflake-site-terms/)\n* [Communication Preferences](https://info.snowflake.com/2024-Preference-center.html)\n* Cookie Settings\n* [Do Not Share My Personal Information](https://www.snowflake.com/en/legal/privacy/privacy-policy/)\n* [Legal](https://www.snowflake.com/en/legal/)\n\n[](https://x.com/Snowflake \"X (Twitter)\")\n\n[](https://www.linkedin.com/company/3653845 \"LinkedIn\")\n\n[](https://www.facebook.com/Snowflake-Computing-709171695819345/ \"Facebook\")\n\n[](https://www.youtube.com/user/snowflakecomputing \"YouTube\")\n\n\\* Private preview, <sup>†</sup> Public preview, <sup>‡</sup> Coming soon\n"]},{"url":"https://yukidata.com/blog/snowflake-finops-guide/","title":"Snowflake FinOps: Complete Guide to Automated Cost Optimization | Yuki","publish_date":"2025-09-19","excerpts":["[Skip to content]()\n[](https://yukidata.com)\nSolutionsClose Solutions Open Solutions\nResourcesClose Resources Open ResourcesResources[Blog](https://yukidata.com/blog/) [Customers](https://yukidata.com/customers/) [News](https://yukidata.com/news/) [Documentation](https://docs.yukidata.com/)[](https://yukidata.com/snowflake-gen-2-warehouses-yuki-optimization/)[Gen-2 Warehouses Are Here, Yuki Makes Them Pay Off](https://yukidata.com/snowflake-gen-2-warehouses-yuki-optimization/)[Learn More >](https://yukidata.com/snowflake-gen-2-warehouses-yuki-optimization/)[](https://yukidata.com/qwilt/)[How Qwilt Cut 63% of Snowflake Costs in Days](https://yukidata.com/qwilt/)[Learn More >](https://yukidata.com/qwilt/)\n[About Us](https://yukidata.com/about/)\n[Contact Us](https://yukidata.com/contact/)\n[Get Started](https://yukidata.com/request-demo/)\n[Free Trial](https://yukidata.com/free-trial)\n[](https://yukidata.com)\nSolutionsClose Solutions Open Solutions\nResourcesClose Resources Open Resources[Blog](https://yukidata.com/blog/) [Customers](https://yukidata.com/customers/) [News](https://yukidata.com/news/) [Documentation](https://yukidata.com/customers/)\n[About Us](https://yukidata.com/about/)\n[Contact Us](https://yukidata.com/contact/)\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization\nContent:\nBy Perry Tapiero\nSeptember 19, 2025 | 5 min read\nYour Snowflake bill increased 40% last quarter, but query performance actually got worse.\nSound familiar?\nAll FinOps organizations eventually run into this wall when they outgrow Snowflake’s basic auto-suspend and resource monitors. While Snowflake’s per-second billing offers flexibility, it also means a single efficient query can eat through hundreds of dollars – which is why manual warehouse management can’t keep pace with enterprise-sized workloads.\nThe bright side: modern Snowflake FinOps fixes this with automated systems that allow you to optimize spend and performance in real time.\nHow do you get to that point? Read on. We’ll share all of our FinOps best practices,e automation strategies, and real-world tactics that have helped enterprises cut monthly Snowflake costs by 30% or more.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > What is FinOps for Snowflake?\nContent:\nFinOps is the marriage of engineering, finance, and data teams in the cloud. When it comes to Snowflake, that means balancing performance, cost, and quality across your data platform – all while maintaining business agility\nUnlike your traditional cloud FinOps that focus on infrastructure, Snowflake FinOps requires managing:\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > What is FinOps for Snowflake?\nContent:\n**Credit-based consumption:** At $2-4 per credit (depending on your region and edition) with cost scaling linearly from X-Small (1 credit/hour) to 6X-Large (512 credits/hour).\n**Query-level optimization:** Using QUERY_HISTORY and WAREHOUSE_METERING_HISTORY views to identify expensive queries before they ruin your budget.\n**Dynamic warehouse scaling:** Beyond basic auto-suspend, implementing intelligent scaling based on queue depth and query complexity\n**Storage and** [**data transfer**](https://yukidata.com/blog/snowflake-data-transfer-costs-complete-guide/) **:** Long-term storage and cross-regional replication can add significant costs if not managed with lifestyle policies and governance.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > What is FinOps for Snowflake?\nContent:\nThe big challenge here? Manual management. That same hands-on approach that worked so well for traditional infrastructure of the past breaks down when you apply the same method to millions of queries and dozens of ever-changing warehouses.\n*With Yuki, warehouse optimization is automated with a single toggle – no more manual resizing or monitoring.*\nSection Title: ... > Why Manually Managing Snowflake For FinOps Is So Expensive\nContent:\nBefore we get into the “how to fix it” section of the article, let’s address the real cost of manual Snowflake management. These are the expenses that don’t show up on your bill, but completely ruin your ROI.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > The Engineering Time Tax\nContent:\nYour data engineers aren’t working for you to babysit warehouses. Yet many teams still find themselves spending hours each week manually resizing, suspending, and monitoring clusters. Even if that’s just 5-10 hours per engineer, that’s *thousands* of dollars lost in productivity per month – before you even add in wasted compute.\nManual management also results in issues like:\n**Overprovisioned warehouses** kept large *just in case*\n**Idle compute** burning credits overnight\n**Compliance gaps** from inconsistent chargeback and lack of audit trails\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > The Speed vs. Cost Dilemma\nContent:\nFast-growing companies always run into this growing pain: should they optimize for speed or for cost? Most choose speed, leaving them with:\n**Overprovisioned warehouses** to avoid performance bottlenecks\n**Warehouse sprawl** as teams created dedicated resources for SLAs\n**Idle compute** running 24/7 “just in case”\nThe result? Snowflake costs outgrowing revenue and eating into margin and ROI.\nSection Title: Snowflake FinOps: Complete Guide to Automated Cost Optimization > ... > The Compliance Complexity\nContent:\nEnterprise FinOps needs control, not just cost reduction. You’ll find manual approaches falling short of this:\n**Granular spend attribution** across cost centers and teams\n**Real-time budget enforcement** to prevent runaway costs\n**Audit trails** for compliance and chargeback scenarios\n**Role-based access** maintaining security while enabling autonomy\nThese challenges only continue to compound as you scale, making manual management not just expensive, but impossible to maintain.\n ... \nSection Title: ... > : Implement Granular Cost Attribution\nContent:\n**The problem:** Snowflake’s native cost reporting only shows warehouse-level spending. You need query- and team-level attribution for effective chargeback.\n**The solution:** Build automated tagging and attribution using Snowflake’s metadata like this:\n```\n-- Set up cost attribution table\nCREATE TABLE cost_attribution AS\nSELECT\n  qh.query_id,\n  qh.user_name,\n  qh.warehouse_name,\n  qh.database_name,\n  qh.schema_name,\n  qh.credits_used_cloud_services,\n  -- Extract team from username pattern or use session context\n  CASE\n    WHEN qh.user_name ILIKE '%analytics%' THEN 'Analytics Team'\n    WHEN qh.user_name ILIKE '%eng%' THEN 'Engineering Team'\n    ELSE 'General'\n  END as team_attribution,\n  qh.start_time\nFROM snowflake.account_usage.query_history qh\nWHERE qh.start_time >= dateadd(day, -30, current_timestamp());\n```\nYou can use this to pull key metrics like:\nSection Title: ... > : Implement Granular Cost Attribution\nContent:\nCost per team per month: SUM(credits_used * $3) GROUP BY team_attribution\nMost expensive users: SUM(credits_used) GROUP BY user_name\nWarehouse efficiency: credits_used / execution_time ratio\nSection Title: ... > : Automated Chargeback and Showback Models\nContent:\n**The problem:** No clear cost attribution means that teams often treat Snowflake as if it were “free,” leading to wasteful usage and budget overruns.\n**The solution:** Automated cost attribution that gets you accountability without expensive administration overhead:\n**Tag-based attribution** that automatically assigns costs to projects and business units\n**Usage-based chargeback** for shared warehouses using per-query\n**Predictive showback** forecasting team spend based on current trends\n**ROI tracking** connecting data platform costs to business outcomes\nManual tagging doesn’t scale. You need systems that automatically attribute costs based on your usage patterns, user roles, and business context.\nSection Title: ... > : Proactive Budget Management and Alerts\nContent:\n**The problem:** Reactive alerts document overspend after it happens. They don’t prevent it.\n**The solution:** Intelligent budget management that actually prevents overruns before they occur:\n**Predictive alerting** based on usage trends and historical patterns\n**Automated spend controls** scaling resources down when budgets risk\n**Multi-tiered notifications** that escalates teams to finance as spending approaches limits\n**Business-context budgeting** adjusting limits based on revenue cycles\nEffective budget management isn’t about saying “no”. It’s saying “yes” to the right workloads while automating the rest.\nSection Title: ... > : Automated Warehouse Scaling\nContent:\n**The problem:** Snowflake’s auto-suspend helps with idle time, but it doesn’t actually optimize warehouse size when it comes to workload complexity.\n**The solution:** Implement workload-aware scaling. For example, use Snowflake’s WAREHOUSE_LOAD_HISTORY to track query depth and concurrency, then programmatically adjust sizes via Snowflake Python Connector or Snowpark API:\n```\nimport snowflake.connector\n\nconn = snowflake.connector.connect(\n    user='USER',\n    password='PASSWORD',\n    account='ACCOUNT'\n)\n\ncur = conn.cursor()\ncur.execute(\"\"\"\nSELECT AVG(avg_queued_load)\nFROM snowflake.account_usage.warehouse_load_history\nWHERE warehouse_name='COMPUTE_WH'\nAND start_time >= dateadd(minute, -5, current_timestamp());\n\"\"\")\n\nqueue_depth = cur.fetchone()[0]\n\nif queue_depth > 50:\n    cur.execute(\"ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = 'LARGE'\")\nelif queue_depth < 10:\n    cur.execute(\"ALTER WAREHOUSE COMPUTE_WH SET WAREHOUSE_SIZE = 'SMALL'\n```\nSection Title: ... > : Automated Warehouse Scaling\nContent:\nThis approach can help you [reduce warehouse costs](https://yukidata.com/blog/snowflake-warehouse-optimization-guide/) by 20-40% because it lets you match your compute size to actual workload requirements.\n ... \nSection Title: ... > Fix #2: Identifying Cost Drivers\nContent:\nBefore you dig into automation, you need visibility. This query helps you quickly find your top cost drivers (i.e., those users and queries that are burning through the bulk of your credits) so you can decide if you need to optimize, reclassify, or reallocate workloads.\n```\n-- Find your most expensive queries from the last 30 days\nSELECT\n  query_text,\n  warehouse_name,\n  total_elapsed_time/1000 as seconds,\n  credits_used_cloud_services,\n  (credits_used_cloud_services * 3) as estimated_cost_usd\nFROM snowflake.account_usage.query_history\nWHERE start_time >= dateadd(day, -30, current_timestamp())\nORDER BY credits_used_cloud_services DESC\nLIMIT 10;\n```\nSection Title: ... > Fix #3: Optimizing Warehouse Sizing\nContent:\nThis lets you align your warehouse size with actual usage so you can avoid the trap of running a Large warehouse for a workload that only needs a Small.\n```\nSELECT\n  warehouse_name,\n  avg(avg_running) as avg_concurrent_queries,\n  avg(avg_queued_load) as avg_queue_depth\nFROM snowflake.account_usage.warehouse_load_history\nWHERE start_time >= dateadd(day, -7, current_timestamp())\nGROUP BY warehouse_name;\n```\nIf your avg_queue_depth > 100: Scale up your warehouse\nIf avg_concurrent_queries < 1: Consider smaller warehouses or using a longer auto-suspend\n ... \nSection Title: ... > Multi-Region Compliance & Data Residency\nContent:\nGlobal enterprises have to maintain data residency requirements while continuing to optimize costs across different regions. Doing this manually means having to carefully balance regulatory compliance and [cost optimization](https://yukidata.com/blog/snowflake-optimization-guide/) across multiple geographic regions and regulatory frameworks.\n**The solution:** Use automated, region-aware optimization. This lets you maintain compliance while minimizing cross-region data movement costs. Think intelligent query routing that processes all you need within boundaries, optimizing for cost and performance and getting you the best of both worlds."]},{"url":"https://docs.snowflake.com/en/user-guide/cost-attributing","title":"Attributing cost | Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\n[Get started](/en/user-guide-getting-started)\n[Guides](/en/guides)\n[Developer](/en/developer)\n[Reference](/en/reference)\n[Release notes](/en/release-notes/overview)\n[Tutorials](/en/tutorials)\n[Status](https://status.snowflake.com)\n[Overview](/en/guides \"Overview\")\n[Snowflake Horizon Catalog](/en/user-guide/snowflake-horizon \"Snowflake Horizon Catalog\")\n[Applications and tools for connecting to Snowflake](/en/guides-overview-connecting \"Applications and tools for connecting to Snowflake\")\n[Virtual warehouses](/en/user-guide/warehouses \"Virtual warehouses\")\n[Databases, Tables, & Views](/en/guides-overview-db \"Databases, Tables, & Views\")\n[Data types](/en/data-types \"Data types\")\nData Integration\n[Snowflake Openflow](/en/user-guide/data-integration/openflow/about \"Snowflake Openflow\")\nApache Iceberg™\n[Apache Iceberg™ Tables](/en/user-guide/tables-iceberg \"Apache Iceberg™ Tables\")\n[Snowflake Open Catalog](/en/user-guide/opencatalog/overview \"Snowflake Open Catalog\")\nData engineering\n[Data loading](/en/guides-overview-loading-data \"Data loading\")\n[Dynamic Tables](/en/user-guide/dynamic-tables-about \"Dynamic Tables\")\n[Streams and Tasks](/en/user-guide/data-pipelines-intro \"Streams and Tasks\")\n[dbt Projects on\nSnowflake](/en/user-guide/data-engineering/dbt-projects-on-snowflake \"dbt Projects on Snowflake\")\n[Data Unloading](/en/guides-overview-unloading-data \"Data Unloading\")\n[Storage Lifecycle Policies](/en/user-guide/storage-management/storage-lifecycle-policies \"Storage Lifecycle Policies\")\n[Migrations](/en/migrations/README \"Migrations\")\n[Queries](/en/guides-overview-queries \"Queries\")\n[Listings](/en/collaboration/collaboration-listings-about \"Listings\")\n[Collaboration](/en/guides-overview-sharing \"Collaboration\")\n[Snowflake AI & ML](/en/guides-overview-ai-features \"Snowflake AI & ML\")\n[Snowflake Postgres](/en/user-guide/snowflake-postgres/about \"Snowflake Postgres\")\n[Alerts & Notifications](/en/guides-overview-alerts \"Alerts & Notifications\")\n[Security](/en/guides-overview-secure \"Security\")\n[Data Governance](/en/guides-overview-govern \"Data Governance\")\n[Privacy](/en/guides-overview-privacy \"Privacy\")\n[Organizations & Accounts](/en/guides-overview-manage \"Organizations &\n ... \nSection Title: Attributing cost [¶]( \"Link to this heading\")\nContent:\nAn organization can apportion the cost of using Snowflake to logical units within the organization (for example, to different\ndepartments, environments, or projects). This chargeback or showback model is useful for accounting purposes and pinpoints\nareas of the organization that could benefit from controls and optimizations that can reduce costs.\nTo attribute costs to different groups like departments or projects, use the following recommended approach:\nUse [object tags](object-tagging/introduction) to associate resources and users with departments or projects.\nUse [query tags](../sql-reference/parameters.html) to associate individual queries with departments or projects when the queries are\nmade by the same application on behalf of users belonging to multiple departments.\n ... \nSection Title: ... > Types of cost attribution scenarios [¶]( \"Link to this heading\")\nContent:\n**Resources used exclusively by a single cost center or department:** An example of this is using object tags to associate\nwarehouses with a department. You can use these object tags to attribute the costs incurred by those warehouses to that\ndepartment entirely. [](../_images/cost-attribute-non-shared.png)\n**Resources that are shared by users from multiple departments:** An example of this is a warehouse shared by users from\ndifferent departments. In this case, you use object tags to associate each user with a department. The costs of queries are\nattributed to the users. Using the object tags assigned to users, you can break down the costs by department. [](../_images/cost-attribute-user-level-share.png)\n**Applications or workflows shared by users from different departments:** An example of this is an application that issues\nqueries on behalf of its users.\n ... \nSection Title: ... > Setting up object tags for cost attribution [¶]( \"Link to this heading\")\nContent:\nThe examples in these sections use the custom role `tag_admin` , which is assumed to have been granted the privileges to\ncreate and manage tags. Within your organization, you can use more granular [privileges for object tagging](object-tagging/work.html) to develop a secure tagging strategy.\n ... \nSection Title: ... > Tagging the resources and users [¶]( \"Link to this heading\")\nContent:\nAfter creating and replicating the tags, you can use these tags to identify the warehouses and users belonging to each\ndepartment. For example, because the sales department uses both `warehouse1` and `warehouse2` , you can set the `cost_center` tag to `'SALES'` for both warehouses.\nTip\nIdeally, you should have workflows that automate the process of applying these tags when you create resources and users.\n```\nUSE ROLE tag_admin ; \n\n ALTER WAREHOUSE warehouse1 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse2 SET TAG cost_management . tags . cost_center = 'SALES' ; \n ALTER WAREHOUSE warehouse3 SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n\n ALTER USER finance_user SET TAG cost_management . tags . cost_center = 'FINANCE' ; \n ALTER USER sales_user SET TAG cost_management . tags . cost_center = 'SALES' ;\n```\nCopy\n ... \nSection Title: ... > Viewing cost by tag in SQL [¶]( \"Link to this heading\")\nContent:\n**Attributing costs within an account**You can attribute costs within an account by querying the following views in the [ACCOUNT_USAGE](../sql-reference/account-usage) schema:\n[TAG_REFERENCES view](../sql-reference/account-usage/tag_references) : Identifies objects (for example, warehouses and users) that have tags. [WAREHOUSE_METERING_HISTORY view](../sql-reference/account-usage/warehouse_metering_history) : Provides credit usage for warehouses. [QUERY_ATTRIBUTION_HISTORY view](../sql-reference/account-usage/query_attribution_history) : Provides the compute costs for queries. The cost per query is\nthe warehouse credit usage for executing the query.For more information on using this view, see [About the QUERY_ATTRIBUTION_HISTORY view]() .\nSection Title: ... > Viewing cost by tag in SQL [¶]( \"Link to this heading\")\nContent:\n**Attributing costs across accounts in an organization**Within an organization, you can also attribute costs for resources that are used **exclusively by a single department** by\nquerying views in the [ORGANIZATION_USAGE](../sql-reference/organization-usage) schema from the [organization account](organization-accounts) .Note\nIn the ORGANIZATION_USAGE schema, the TAG_REFERENCES view is only available in the organization account. The QUERY_ATTRIBUTION_HISTORY view is only available in the ACCOUNT_USAGE schema for an account. There is no\norganization-wide equivalent of the view.\n ... \nSection Title: ... > Resources not shared by departments [¶]( \"Link to this heading\")\nContent:\nSuppose that you want to attribute costs by department and that each department uses a set of dedicated warehouses.\nIf you tag warehouses with a `cost_center` tag to identify the department that owns the warehouse, you can join the\nACCOUNT_USAGE [TAG_REFERENCES view](../sql-reference/account-usage/tag_references) with the [WAREHOUSE_METERING_HISTORY view](../sql-reference/account-usage/warehouse_metering_history) on the `object_id` and `warehouse_id` columns to get usage\ninformation by warehouse, and you can use the `tag_value` column to identify the departments that own those warehouses.\nThe following SQL statement performs this join:\nSection Title: ... > Resources not shared by departments [¶]( \"Link to this heading\")\nContent:\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\nCopy\n ... \nSection Title: ... > Resources shared by users from different departments [¶]( \"Link to this heading\")\nContent:\nSuppose that users in different departments share the same warehouses and you want to break down the credits used by each\ndepartment. You can tag the users with a `cost_center` tag to identify the department that they belong to, and you can join\nthe [TAG_REFERENCES view](../sql-reference/account-usage/tag_references) with the [QUERY_ATTRIBUTION_HISTORY view](../sql-reference/account-usage/query_attribution_history) .\nNote\nYou can only get this data for a single account at a time. You cannot execute a query that retrieves this data across\naccounts in an organization.\nThe next sections provide examples of SQL statements for attributing costs for shared resources.\n ... \nSection Title: ... > Viewing cost by tag in Snowsight [¶]( \"Link to this heading\")\nContent:\nYou can attribute costs by reporting on the use of resources that have the `cost_center` tag. You can access this data in [Snowsight](ui-snowsight-gs.html) .\nSwitch to a role that has [access to the ACCOUNT_USAGE schema](../sql-reference/account-usage.html) .\nIn the navigation menu, select Admin » Cost management .\nSelect Consumption .\nFrom the Tags drop-down, select the `cost_center` tag.\nTo focus on a specific cost center, select a value from the list of the tag’s values.\nSelect Apply .\nFor more details about filtering in Snowsight, see [Filter by tag](cost-exploring-compute.html) .\nSection Title: ... > About the QUERY_ATTRIBUTION_HISTORY view [¶]( \"Link to this heading\")\nContent:\nYou can use the [QUERY_ATTRIBUTION_HISTORY view](../sql-reference/account-usage/query_attribution_history) to attribute cost based on queries. The cost per\nquery is the warehouse credit usage for executing the query. This cost does not include any other credit usage that is incurred\nas a result of query execution. For example, the following are not included in the query cost:\nData transfer costs\nStorage costs\nCloud services costs\nCosts for serverless features\nCosts for tokens processed by AI services\nFor queries that are executed concurrently, the cost of the warehouse is attributed to individual queries based on the weighted\naverage of their resource consumption during a given time interval.\nThe cost per query does not include warehouse *idle time* . Idle time is a period of time in which no queries are running in the\nwarehouse and can be measured at the warehouse level.\n ... \nSection Title: ... > Attributing costs of hierarchical queries [¶]( \"Link to this heading\")\nContent:\nRelated content\n[Managing cost in Snowflake](/user-guide/cost-management-overview)\nLanguage: **English**"]},{"url":"https://www.unraveldata.com/insights/snowflake-cost-management/","title":"What are best practices for Snowflake cost management? - Unravel Data","publish_date":"2025-08-14","excerpts":["[WHY OUR AI](/platform/purpose-built-ai/)\n[PLATFORM](#)DATA OBSERVABILITY FORPLATFORM & PRICINGUSE CASESFeatured[](/health-check-demo/)Get a Free Health Check Report[For Databricks](/databricks-health-check-request/) [For Snowflake](/snowflake-health-check-request/)[](/self-guided-product-tour/health-check-dbx/)Tour Unravel’s Key Product Features for Yourself[Explore Now](/self-guided-product-tour/health-check-dbx/)\n[Databricks](/solutions/technologies/databricks/)\n[Snowflake](/solutions/technologies/snowflake/)\n[Google Cloud BigQuery](/solutions/technologies/google-cloud-bigquery/)\n[Amazon EMR](/solutions/technologies/amazon-emr/)\n[Cloudera](/solutions/technologies/cloudera/)\n[Platform Overview](/platform/platform-overview/)\n[All Integrations & APIs](/platform/integrations-and-apis/)\n[Plans & Pricing](/platform/pricing-options/)\n[Cloud Cost Management & FinOps](/solutions/use-cases/cloud-cost-management/)\n[Operations & Troubleshooting](/solutions/use-cases/troubleshooting/)\n[Pipeline\n& App Optimization](/solutions/use-cases/optimization/)\n[Autofix & Prevention](/solutions/use-cases/autofix-prevention/)\n[Data Quality & Reliability](/solutions/use-cases/data-quality/)\n[Cloud Migration](/solutions/use-cases/cloud-migration/)\n[RESOURCES](#)TOP TOPICSTOP TECHNOLOGIESTOP CONTENT TYPESFeatured[](/resources/ai-agents-empower-data-teams-with-actionability-for-transformative-results/)AI Agents: Empower Data Teams With Actionability TM[Read More](/resources/ai-agents-empower-data-teams-with-actionability-for-transformative-results/)[](/resources/data-actionability-webinar/)Data Actionability TM : Empower Your Team[Read More](/resources/data-actionability-webinar/)\n[AI & Automation](/resources/ai-automation/)\n[Cost Optimization & FinOps](/resources/cost-optimization/)\n[Troubleshooting & DataOps](/resources/troubleshooting/)\n[Performance & Data Eng](/resources/application-performance/)\n[Cloud Migration](/resources/cloud-migration/)\n[CI/CD](/resources/ci-cd/)\n[Explore\n ... \nSection Title: What are best practices for Snowflake cost management?\nContent:\nImplement automated resource scaling, set up granular monitoring, and establish clear governance policies to control spending while maintaining performance Managing costs in Snowflake isn’t just about turning off warehouses when you’re done. It’s about understanding how […]\n8 min read\nSection Title: ... > **Implement automated resource scaling, set up granular monitoring, and establish clear gov...\nContent:\nManaging costs in Snowflake isn’t just about turning off warehouses when you’re done. It’s about understanding how your data platform actually consumes resources and building systems that prevent runaway spending before it happens. Organizations consistently underestimate how quickly costs can spiral without proper oversight and automated controls in place.\n**TL;DR:** Effective cost control requires three foundational elements: automated warehouse scaling and suspension policies, granular cost monitoring with department-level attribution, and governance frameworks that prevent unauthorized resource consumption. Companies that implement these practices typically reduce their Snowflake spending by 30-50% within the first quarter.\n ... \nSection Title: ... > **Resource monitoring creates actionable intelligence**\nContent:\nYou can’t optimize what you can’t measure. Snowflake provides detailed resource consumption data, but most teams don’t know how to interpret it effectively.\nAccount usage views reveal everything. The WAREHOUSE_METERING_HISTORY view shows exact credit consumption by warehouse and time period. QUERY_HISTORY provides per-query resource usage. AUTOMATIC_CLUSTERING_HISTORY tracks clustering maintenance costs.\nSmart cost management requires connecting resource consumption to business context. Which departments drive the highest costs? What time patterns emerge in your usage data? How do seasonal business cycles affect your spending?\nSection Title: ... > **Building department-level cost attribution**\nContent:\nHere’s where most cost initiatives fail: they track technical metrics without connecting them to business accountability. Database administrators can see warehouse utilization. They can’t see that Marketing’s new campaign analysis is driving 40% of monthly compute costs.\nImplement resource tagging from day one:\n**Warehouse naming conventions:** Include department, workload type, and environment indicators\n**Query labeling:** Use comment tags to identify business processes and responsible teams\n**Role-based access tracking:** Monitor which roles consume the most resources over time\nThis attribution data transforms cost management from a technical exercise into a business conversation. When Marketing sees their queries cost $15,000 last month, they start asking different questions about data requirements and analysis frequency.\n ... \nSection Title: ... > **Implementation roadmap for sustainable cost management**\nContent:\nStart with quick wins that deliver immediate impact, then build toward comprehensive optimization capabilities over time.\n**Week 1-2: Foundation setup**\nConfigure auto-suspend on all warehouses (60-second timeout)\nImplement basic resource monitors with suspend actions\nSet up cost monitoring dashboards using account usage views\nReview and optimize the 10 most expensive queries\n**Month 1: Governance and attribution**\nEstablish warehouse naming conventions and role-based access controls\nImplement department-level cost attribution through tagging\nConfigure multi-cluster warehouses for variable workloads\nSet up automated alerting for cost threshold breaches\n**Month 2-3: Advanced optimization**\nAnalyze clustering effectiveness and optimize table structures\nImplement workload-specific warehouse strategies\nDeploy query result caching optimization\nEstablish seasonal scaling schedules based on usage patterns\n ... \nSection Title: What are best practices for Snowflake cost management? > **Measuring success in cost management**\nContent:\nEffective measurement requires both technical metrics and business outcomes. Track cost per query, warehouse utilization rates, and resource consumption trends. But also measure business impact: faster analysis delivery, improved data access, reduced time-to-insight.\nKey performance indicators for cost optimization:\nMonth-over-month cost growth rate compared to data volume growth\nAverage cost per business user or department\nQuery performance improvements alongside cost reductions\nWarehouse utilization efficiency (active time vs total provisioned time)\nThe most successful cost management initiatives achieve 30-50% cost reductions while maintaining or improving query performance. This requires sustained attention and continuous optimization rather than one-time configuration changes.\nSection Title: What are best practices for Snowflake cost management? > **Measuring success in cost management**\nContent:\nYour cost optimization journey starts with understanding current spending patterns and implementing automated controls. Focus on the biggest cost drivers first: warehouse management, query optimization, and resource governance. Build measurement systems that connect technical optimizations to business value.\nThe organizations that excel at cost management treat it as an ongoing capability, not a periodic project. They invest in tools, training, and processes that scale with their data platform growth. Most importantly, they make cost optimization a shared responsibility across technical and business teams.\nReady to transform your approach? Start with automated warehouse controls and granular monitoring. The foundation you build today determines your optimization success tomorrow.\nSection Title: What are best practices for Snowflake cost management? > **Measuring success in cost management**\nContent:\nPublishedAugust 14 2025AuthorUnravel DataRelated PostsExplore Other Insights By\n[What Snowflake cost management tool features are essential for automation?](https://www.unraveldata.com/insights/snowflake-cost-management-tool-features/)\n[How can I tie Snowflake cost monitoring to budget forecasting and planning?](https://www.unraveldata.com/insights/snowflake-cost-monitoring/)\n[What are the top Snowflake cost management tool evaluation criteria?](https://www.unraveldata.com/insights/snowflake-cost-management-tool-evaluation-criteria/)\n[Snowflake](https://www.unraveldata.com/insight/snowflake-data-observability-insights/)\n[](/)\nDetect, fix, and prevent data issues with the most advanced AI-native data observability and FinOps platform on the market. Powerful automated action you fully control.\nCOMPANY\nSection Title: What are best practices for Snowflake cost management? > **Measuring success in cost management**\nContent:\n[About Unravel](/company/)\n[Customers](/customers/)\n[Careers](/company/careers/)\n[Partners](/company/partners/)\n[Events](/events/)\n[News & Press](/company/news-and-press/)\nSUPPORT\n[FAQ](/platform/frequently-asked-questions/)\n[Customer Portal](https://customers.unraveldata.com/login?ec=302&startURL=%2Fs%2F)\n[Documentation](https://docs.unraveldata.com/?lang=en)\n[Contact Us](/company/contact-us/)\n[LINKEDIN](https://www.linkedin.com/company/unravel-data)\n[X](https://x.com/unraveldata)\n[YOUTUBE](https://www.youtube.com/c/UnravelData)\n[FACEBOOK](https://www.facebook.com/unraveldata)\n[INSTAGRAM](https://www.instagram.com/unraveldata/)\nCopyright © 2026 Unravel Data. All rights reserved.\n[Terms of Service](/terms-of-service/)\n[Privacy Policy](/privacy-policy/)\n[Cookies Settings](#)\nGet Started\nSection Title: What are best practices for Snowflake cost management? > **Measuring success in cost management**\nContent:\n[Free Health Check](/health-check-demo/)\n[Self-Guided Tour](/self-guided-product-tour/health-check-dbx/)\n[Book a Demo](/demo/)\n[Try for Free](/health-check-demo/)\n[Attend an Event](/events/)\nPRODUCT\n ... \nSection Title: What are best practices for Snowflake cost management? > **Measuring success in cost management**\nContent:\n[AI & Automation](/resources/ai-automation/)\n[Cost Optimization & FinOps](/resources/cost-optimization/)\n[Troubleshooting & DataOps](/resources/troubleshooting/)\n[Performance & Data Eng](/resources/application-performance/)\n[Cloud Migration](/resources/cloud-migration/)\n[CI/CD](/resources/ci-cd/)\n[Databricks](/resources/databricks/)\n[Snowflake](/resources/snowflake/)\n[BigQuery](/resources/bigquery/)\n[Customer Stories](/resource-type/customer-stories/)\n[Product Docs](/resource-type/product-docs/)\n[Research & Reports](/resource-type/research-and-reports/)\n[Webinars & Podcasts](/resource-type/webinars-and-events/)\n[Insights](/insights/)\n[Explore All](/resources/)\nCOMPANY\nSection Title: What are best practices for Snowflake cost management? > **Measuring success in cost management**\nContent:\n[About Unravel](/company/)\n[Customers](/customers/)\n[Careers](/company/careers/)\n[Partners](/company/partners/)\n[Events](/events/)\n[News & Press](/company/news-and-press/)\n[FAQ](/platform/frequently-asked-questions/)\n[Customer Portal](https://customers.unraveldata.com/login?ec=302&startURL=%2Fs%2F)\n[Documentation](https://docs.unraveldata.com/?lang=en)\n[Contact Us](/company/contact-us/)\n[](/)\nDetect, fix, and prevent data issues with the most advanced AI-native data observability and FinOps platform on the market. Powerful automated action you fully control."]},{"url":"https://docs.snowflake.com/en/sql-reference/account-usage","title":"Account Usage - Snowflake Documentation","excerpts":["Section Title: Account Usage [¶]( \"Link to this heading\") > ACCOUNT_USAGE views [¶]( \"Link to this heading\")\nContent:\n| View | Type | Latency [1] | Edition [3] | Notes |\n| [ACCESS_HISTORY](account-usage/access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_ACCESS_HISTORY](account-usage/aggregate_access_history) | Historical | 3 hours | Enterprise Edition (or higher) | Data retained for 1 year. |\n| [AGGREGATE_QUERY_HISTORY](account-usage/aggregate_query_history) | Historical | 3 hours |  |  |\n| [AGGREGATION_POLICIES](account-usage/aggregation_policies) | Object | 2 hours |  |  |\n| [TABLE_STORAGE_METRICS](account-usage/table_storage_metrics) | Object | 90 minutes |  |  |\n| [TAG_REFERENCES](account-usage/tag_references) | Object | 2 hours |  |  |\n| [TAGS](account-usage/tags) | Object | 2 hours |  |  |\n| [TASK_HISTORY](account-usage/task_history) | Historical | 45 minutes |  |  |\n| [TASK_VERSIONS](account-usage/task_versions) | Object | 3 hours |  |  |\n ... \nSection Title: ... > Reconciling cost views [¶]( \"Link to this heading\")\nContent:\nThere are several Account Usage views that contain data related to the cost of compute resources, storage, and data transfers. If you are trying to reconcile these views against a corresponding view in the [ORGANIZATION_USAGE schema](organization-usage) , you must first set the timezone of the session to UTC.\nFor example, if you are trying to reconcile ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY to the account’s data in ORGANIZATION_USAGE.WAREHOUSE_METERING_HISTORY, you must run the following command before querying the Account Usage view:\n```\nALTER SESSION SET TIMEZONE = UTC ;\n```\nCopy\n ... \nSection Title: ... > Examples: Warehouse performance [¶]( \"Link to this heading\")\nContent:\nThis query calculates virtual warehouse performance metrics such as throughput and latency for 15-minute time intervals over the course of\none day.\nIn the code sample below, you can replace `CURRENT_WAREHOUSE()` with the name of a warehouse to calculate metrics for that warehouse. In\naddition, change the `time_from` and `time_to` dates in the WITH clause to specify the time period.\nCopyNoteAnalyze different statement types separately (e.g., SELECT statements independent of INSERT or DELETE or other statements).\nSection Title: ... > Examples: Warehouse performance [¶]( \"Link to this heading\")\nContent:\nThe NUMJOBS value represents the throughput for that time interval.\nThe P50_TOTAL_DURATION (median) and P95_TOTAL_DURATION (peak) values represent latency.\nThe SUM_TOTAL_DURATION is the sum of the SUM_<job_stage>_TIME values for the different job stages (COMPILATION_AND_SCHEDULING, QUEUED,\nBLOCKED, EXECUTION).\nAnalyze the <job_stage>_RATIO values when the load (NUMJOBS) increases. Look for ratio changes or deviations from the average.\nIf the QUEUED_RATIO is high, there might not be sufficient capacity in the warehouse. Add more clusters or increase the warehouse size.\nSection Title: ... > Examples: Warehouse credit usage [¶]( \"Link to this heading\")\nContent:\nCredits used by each warehouse in your account (month-to-date):\nCopy\nCredits used over time by each warehouse in your account (month-to-date):\nCopy"]},{"url":"https://docs.snowflake.com/en/user-guide/warehouses-considerations","title":"Warehouse considerations | Snowflake Documentation","excerpts":["Section Title: Warehouse considerations [¶]( \"Link to this heading\")\nContent:\nThis topic provides general guidelines and best practices for using virtual warehouses in Snowflake to process queries. It does not provide specific or absolute numbers, values, or recommendations because every query scenario is different and is affected by\nnumerous factors, including number of concurrent users/queries, number of tables being queried, and data size and composition, as well as\nyour specific requirements for warehouse availability, latency, and cost.\nIt also does not cover warehouse considerations for data loading, which are covered in another topic (see the sidebar).\nThe keys to using warehouses effectively and efficiently are:\nSection Title: Warehouse considerations [¶]( \"Link to this heading\")\nContent:\nExperiment with different types of queries and different warehouse sizes to determine the combinations that best meet your\nspecific query needs and workload.\nDon’t focus on warehouse size. Snowflake utilizes per-second billing, so you can run larger warehouses (Large, X-Large,\n2X-Large, etc.) and simply suspend them when not in use.\nNote\nThese guidelines and best practices apply to both single-cluster warehouses, which are standard for all accounts, and [multi-cluster warehouses](warehouses-multicluster) , which are available in [Snowflake Enterprise Edition](intro-editions) (and higher).\n ... \nSection Title: ... > How does query composition impact warehouse processing? [¶]( \"Link to this heading\")\nContent:\nThe compute resources required to process a query depend on the size and complexity of the query. For the most part, queries scale\nlinearly with respect to warehouse size, particularly for larger, more complex queries. When considering factors that impact query\nprocessing, consider the following:\nThe overall size of the tables being queried has more impact than the number of rows.\nQuery filtering using predicates has an impact on processing, as does the number of joins/tables in the query.\nTip\nTo achieve the best results, try to execute relatively homogeneous queries (complexity, data sets, etc.) on the same warehouse;\nexecuting queries of widely varying complexity on the same warehouse makes it more difficult to analyze warehouse load,\nwhich can make it more difficult to select the best warehouse size to match the complexity, composition, and number of queries in your\nworkload.\n ... \nSection Title: ... > Creating a warehouse [¶]( \"Link to this heading\")\nContent:\nWhen creating a warehouse, the two most critical factors to consider, from a cost and performance perspective, are:\nWarehouse size (that is, available compute resources)\nManual vs automated management (for starting/resuming and suspending warehouses).\nThe number of clusters in a warehouse is also important if you are using [Snowflake Enterprise Edition](intro-editions) (or higher) and [multi-cluster warehouses](warehouses-multicluster) . For more details, see [Scaling Up vs Scaling Out]() (in this topic).\n ... \nSection Title: ... > Selecting a warehouse for Snowsight [¶]( \"Link to this heading\")\nContent:\nSnowsight performance can be affected if the warehouse is temporarily overloaded and UI queries are queued behind other active\nworkloads. If you notice inconsistent Snowsight performance, Snowflake recommends that you review the selected warehouse for\noverload and consider using one with lower utilization. Large accounts with many active users might benefit from a dedicated X-Small\nwarehouse for UI-related tasks.\nYou can view which Snowsight queries have been running on the currently selected warehouse and when they ran. To monitor these\nqueries, follow these steps:\nIn the navigation menu, select Monitoring » Query History .\nSelect the Filters drop-down list.\nSelect the Client-generated statements checkbox to view internal queries run by a client, driver, or library, including the web interface.\nSelect Apply Filters .\nFor information about cost governance, see [Exploring compute cost](cost-exploring-compute) ."]},{"url":"https://docs.snowflake.com/en/sql-reference/account-usage/views","title":"VIEWS view - Snowflake Documentation","excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\n[Get started](/en/user-guide-getting-started)\n[Guides](/en/guides)\n[Developer](/en/developer)\n[Reference](/en/reference)\n[Release notes](/en/release-notes/overview)\n[Tutorials](/en/tutorials)\n[Status](https://status.snowflake.com)\n[Reference](/en/reference) [General reference](/en/sql-reference) [SNOWFLAKE database](/en/sql-reference/snowflake-db) [Account Usage](/en/sql-reference/account-usage) VIEWS\nSchema:\n[ACCOUNT_USAGE](../account-usage.html)\nSection Title: VIEWS view [¶]( \"Link to this heading\")\nContent:\nThis Account Usage view displays a row for each view in the account, not including the views in the ACCOUNT_USAGE, READER_ACCOUNT_USAGE, and INFORMATION_SCHEMA schemas.\nSee also:\n[TABLES view](tables)\nSection Title: VIEWS view [¶]( \"Link to this heading\") > Columns [¶]( \"Link to this heading\")\nContent:\n| Column Name | Data Type | Description |\n| TABLE_ID | NUMBER | Internal/system-generated identifier for the view. |\n| TABLE_NAME | VARCHAR | Name of the view. |\n| TABLE_SCHEMA_ID | NUMBER | Internal/system-generated identifier for the schema that the view belongs to. |\n| TABLE_SCHEMA | VARCHAR | Schema that the view belongs to. |\n| TABLE_CATALOG_ID | NUMBER | Internal/system-generated identifier for the database that the view belongs to. |\n| TABLE_CATALOG | VARCHAR | Database that the view belongs to. |\n| TABLE_OWNER | VARCHAR | Name of the role that owns the view. |\n| VIEW_DEFINITION | VARCHAR | Text of the query expression for the view. |\n| CHECK_OPTION | VARCHAR | Not applicable for Snowflake. |\n| IS_UPDATABLE | VARCHAR | Not applicable for Snowflake. |\n| INSERTABLE_INTO | VARCHAR | Not applicable for Snowflake. |\n| IS_SECURE | VARCHAR | Specifies whether the view is secure. |\n ... \nSection Title: VIEWS view [¶]( \"Link to this heading\") > Columns [¶]( \"Link to this heading\")\nContent:\nFor dropped users, you can join the `<id>` with the USER_ID column in the USERS view of the ACCOUNT_USAGE or ORGANIZATION_USAGE schema. |\n|DELETED |TIMESTAMP_LTZ |Date and time when the view was deleted. |\n|COMMENT |VARCHAR |Comment for the view. |\n|INSTANCE_ID |NUMBER |Internal/system-generated identifier for the instance which the object belongs to. |\n|OWNER_ROLE_TYPE |VARCHAR |The type of role that owns the object, for example `ROLE` . . If a Snowflake Native App owns the object, the value is `APPLICATION` . . Snowflake returns NULL if you delete the object because a deleted object does not have an owner role. |\nSection Title: VIEWS view [¶]( \"Link to this heading\") > Usage notes [¶]( \"Link to this heading\")\nContent:\nLatency for the view may be up to 90 minutes.\nThe view does not recognize the MANAGE GRANTS privilege and consequently may show less information compared to a SHOW command\nexecuted by a user who holds the MANAGE GRANTS privilege.\nThe LAST_ALTERED column is updated when the following operations are performed on an object:For views and tables, use the LAST_DDL column for the last modification time for an object.\nDDL operations.\nDML operations (for tables only). This column is updated even when no rows are affected by the DML statement.\nBackground maintenance operations on metadata performed by Snowflake.\nThe value in the LAST_DDL column is updated as follows:\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\n[Share your feedback](/feedback)"]},{"url":"https://medium.com/@vuppala.venkat/snowflake-warehouse-sizing-best-practices-27b3063b6261","title":"Snowflake warehouse sizing best practices | by Vuppala Venkat - Medium","publish_date":"2024-06-30","excerpts":["Section Title: Snowflake warehouse sizing best practices\nContent:\nIn the world of data management, Snowflake has established itself as a powerful player, known for its scalability and flexibility. However, one of the primary concerns for many users is managing costs, with compute resources being a significant factor.\n ... \nSection Title: Snowflake warehouse sizing best practices > Understanding Snowflake Costs > Optimize Warehouse Size\nContent:\nThe first and most effective step in reducing Snowflake costs is optimizing the size of your warehouses. Here’s why this matters:\n**Cost Impact** : Larger warehouses consume more credits per hour, which can quickly add up, especially in environments with high query loads.\n**Performance Balance** : While larger warehouses can offer better performance for complex queries, they may not always be necessary for smaller or less demanding workloads.\nSection Title: Snowflake warehouse sizing best practices > ... > Best Practices for Optimization\nContent:\n**Right-Sizing** : Assess your workload requirements and choose a warehouse size that balances cost and performance. This may involve experimenting with different sizes to find the most efficient configuration.\n**Scaling** : Take advantage of Snowflake’s ability to scale up or down based on demand. Use auto-suspend and auto-resume features to minimize costs during low-usage periods.\n**Monitoring** : Regularly review your usage patterns and adjust warehouse sizes as necessary. Snowflake provides detailed usage reports that can help you identify optimization opportunities.\nBy focusing on the size and configuration of your Snowflake warehouses, you can significantly reduce costs while maintaining the performance levels needed for your business operations. Remember, the key is to balance cost efficiency with the performance demands of your data environment.\n ... \nSection Title: Snowflake warehouse sizing best practices > ... > The Impact of Warehouse Start and Stop Cycles\nContent:\nIn Snowflake, customers are charged for at least 60 seconds each time a warehouse is started, regardless of the query length. Additionally, costs may increase based on the `AUTO_SUSPEND` time setting.\n**Understanding the Cost Implications**\nIf you frequently start and stop a warehouse for small queries, this can lead to unnecessary wastage. Each cycle incurs charges that, over time, significantly impact overall costs.\n**Optimizing Usage**\nTo minimize expenses, it’s essential to manage the frequency of warehouse start and stop cycles effectively. Consider adjusting your `AUTO_SUSPEND` settings and grouping queries where possible to maximize efficiency and reduce costs.\nBy being mindful of these factors, you can optimize your Snowflake usage, preventing cost overruns while maintaining performance.\nThe following query helps to identify warehouse utilization.\n ... \nSection Title: Snowflake warehouse sizing best practices > **Monitoring**\nContent:\nImplement resource monitors on warehouses to track the warehouse usage over time and track anomalies. You can use the following query to track warehouses without resource monitors.\n```\nSHOW WAREHOUSES;  \nSELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID()))  \nWHERE \"resource_monitor\" = 'null';\n```\n[Snowflake](/tag/snowflake?source=post_page-----27b3063b6261---------------------------------------)\n[Snowflake Cost Management](/tag/snowflake-cost-management?source=post_page-----27b3063b6261---------------------------------------)\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F27b3063b6261&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40vuppala.venkat%2Fsnowflake-warehouse-sizing-best-practices-27b3063b6261&user=Vuppala+Venkat&userId=40f1b9d0f236&source=---footer_actions--27b3063b6261---------------------clap_footer------------------)"]},{"url":"https://medium.com/@kenny.nagano/cost-attribution-tagging-in-snowflake-6eceeea1c4b6","title":"Cost Attribution Tagging in Snowflake | by Kenny Nagano | Medium","publish_date":"2024-04-12","excerpts":["Section Title: Cost Attribution Tagging in Snowflake > What is a Tag?\nContent:\n```\nselect system $get_tag_allowed_values( 'tagging_db.tagging_schema.cost_center' );  \n  \n+ ------------------------------------------------------------------------+  \n| SYSTEM $GET_TAG_ALLOWED_VALUES( 'tagging_db.tagging_schema.COST_CENTER' )  |  \n| ------------------------------------------------------------------------|  \n|  [\"finance\",\"engineering\",\"sales\"]                                       |  \n+ ------------------------------------------------------------------------+\n```\nThe next step is to start tagging warehouses or the object of your choice.\n```\nALTER  WAREHOUSE  < WH_NAME >  \nSET  TAG TAGGING_DB.TAGGING_SCH.COST_CENTER = 'FINANCE' ;\n```\nThere is a wide list of snowflake objects that supports tagging. To see a full list you can go to this section of the [documentation](https://docs.snowflake.com/en/user-guide/object-tagging) .\nSection Title: Cost Attribution Tagging in Snowflake > What is a Tag?\nContent:\nAfter you have tagged your objects, you can now create a view leveraging your snowflake data share and account_usage view. This example query below will give you a summary of the past 7 days . These next 2 queries might have to be slightly adjusted but is a great starting off point.\n```\nCREATE OR  REPLACE  VIEW  TAGGING_DB.TAGGING_SCHEMA.V_COMPUTE_CREDITS_PER_COST_CENTER   \nAS  \nSELECT  \n    TAG_VALUE  AS  COST_CENTER,  \nSUM (NVL(CREDITS_USED,  0 ))  AS  CREDITS   \nFROM  \n    SNOWFLAKE.ACCOUNT_USAGE.TAG_REFERENCES   \nLEFT JOIN  \n    SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY  ON  WAREHOUSE_NAME = OBJECT_NAME   \nWHERE  \n    TAG_NAME = 'COST_CENTER'  \nAND  TAG_DATABASE = 'TAGGING_DB'  \nAND  TAG_SCHEMA = 'TAGGING_SCHEMA'  \nAND  START_TIME >= DATEADD( 'DAYS' , -7 , CURRENT_DATE ())  \nGROUP BY 1  \nORDER BY 2 DESC ; and  you should  get  an output that looks  like  this\n```\n ... \nSection Title: Cost Attribution Tagging in Snowflake > What is a Tag?\nContent:\nAbove are two basic queries to create 2 different views, but you can explore more detailed historical cost using by writing queries against views in the [ACCOUNT_USAGE](https://docs.snowflake.com/en/sql-reference/account-usage) and [ORGANIZATION_USAGE](https://docs.snowflake.com/en/sql-reference/organization-usage) schemas. You may not immediately receive data from the views you created earlier. The metadata loading into account_usage can be delayed, depending on the tagged objects. Depending on the view you used, there could be a delay of up to three hours. For the most accurate timeframe, please refer to the documentation.\n ... \nSection Title: Cost Attribution Tagging in Snowflake > Get Kenny Nagano’s stories in your inbox\nContent:\nJoin Medium for free to get updates from this writer.\nSubscribe\nSubscribe\n1. Sign in to Snowsight: Sign in to Snowsight, Snowflake’s web-based SQL development environment.\n2. Switch to the ACCOUNTADMIN Role: If you are not the account administrator, switch to a role with access to cost and usage data.\n3. Navigate to Cost Management: Select `Admin` » `Cost Management` from the Snowsight menu.\n4. Select a Warehouse: Choose a warehouse to use to view the usage data. Snowflake recommends using an X-Small warehouse for this purpose.\n5. Select Consumption: Click on the `Consumption` tab to view the consumption data. This will default to an Org level View. If you change to a specific account you can now drill into a specific tag. Change the useage type to compute or storage. You can now drill down into specific tags within the account."]}],"usage":[{"name":"sku_search","count":1}]}
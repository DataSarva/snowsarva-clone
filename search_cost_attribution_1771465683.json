{
  "search_id": "search_631fd50b108c4a54aa0a8dee271bfe21",
  "results": [
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-attributing",
      "title": "Attributing cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Visibility Attributing cost\nSection Title: Attributing cost \u00b6\nContent:\nAn organization can apportion the cost of using Snowflake to logical units within the organization (for example, to different\ndepartments, environments, or projects). This chargeback or showback model is useful for accounting purposes and pinpoints\nareas of the organization that could benefit from controls and optimizations that can reduce costs.\nTo attribute costs to different groups like departments or projects, use the following recommended approach:\nUse object tags to associate resources and users with departments or projects.\nUse query tags to associate individual queries with departments or projects when the queries are\nmade by the same application on behalf of users belonging to multiple departments.\nSection Title: Attributing cost \u00b6 > Types of cost attribution scenarios \u00b6\nContent:\nThe following cost attribution scenarios are the most commonly encountered. In these scenarios, warehouses are used as an\nexample of a resource that incurs costs.\n ... \nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6\nContent:\n**Attributing costs within an account**You can attribute costs within an account by querying the following views in the ACCOUNT_USAGE schema:\nTAG_REFERENCES view : Identifies objects (for example, warehouses and users) that have tags. WAREHOUSE_METERING_HISTORY view : Provides credit usage for warehouses. QUERY_ATTRIBUTION_HISTORY view : Provides the compute costs for queries. The cost per query is\nthe warehouse credit usage for executing the query.For more information on using this view, see About the QUERY_ATTRIBUTION_HISTORY view . **Attributing costs across accounts in an organization**Within an organization, you can also attribute costs for resources that are used **exclusively by a single department** by\nquerying views in the ORGANIZATION_USAGE schema from the organization account .Note\nIn the ORGANIZATION_USAGE schema, the TAG_REFERENCES view is only available in the organization account.\n ... \nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6 > Resources not shared by departments \u00b6\nContent:\nSuppose that you want to attribute costs by department and that each department uses a set of dedicated warehouses.\nIf you tag warehouses with a `cost_center` tag to identify the department that owns the warehouse, you can join the\nACCOUNT_USAGE TAG_REFERENCES view with the WAREHOUSE_METERING_HISTORY view on the `object_id` and `warehouse_id` columns to get usage\ninformation by warehouse, and you can use the `tag_value` column to identify the departments that own those warehouses.\nThe following SQL statement performs this join:\nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6 > Resources not shared by departments \u00b6\nContent:\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > Viewing cost by tag in SQL \u00b6 > Resources not shared by departments \u00b6\nContent:\n```\nSELECT \n    TAG_REFERENCES . tag_name , \n    COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) AS tag_value , \n    SUM ( WAREHOUSE_METERING_HISTORY . credits_used_compute ) AS total_credits \n  FROM \n    SNOWFLAKE . ORGANIZATION_USAGE . WAREHOUSE_METERING_HISTORY \n      LEFT JOIN SNOWFLAKE . ORGANIZATION_USAGE . TAG_REFERENCES \n        ON WAREHOUSE_METERING_HISTORY . warehouse_id = TAG_REFERENCES . object_id \n          AND TAG_REFERENCES . domain = 'WAREHOUSE' \n          AND tag_database = 'COST_MANAGEMENT' AND tag_schema = 'TAGS' \n  WHERE \n    WAREHOUSE_METERING_HISTORY . start_time >= DATE_TRUNC ( 'MONTH' , DATEADD ( MONTH , - 1 , CURRENT_DATE )) \n      AND WAREHOUSE_METERING_HISTORY . start_time < DATE_TRUNC ( 'MONTH' ,  CURRENT_DATE ) \n  GROUP BY TAG_REFERENCES . tag_name , COALESCE ( TAG_REFERENCES . tag_value , 'untagged' ) \n  ORDER BY total_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of user queries for the last month \u00b6\nContent:\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n  ), \n  user_credits AS ( \n    SELECT user_name , SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n        AND start_time < CURRENT_DATE \n      GROUP BY user_name \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n    FROM user_credits \n  ) \n SELECT \n    u . user_name , \n    u . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM user_credits u , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of user queries by department without idle time \u00b6\nContent:\nThe following example attributes the compute cost to each department through the queries executed by users in that department.\nThis query depends on the user objects having a tag that identifies their department.\n```\nWITH joined_data AS ( \n  SELECT \n      tr . tag_name , \n      tr . tag_value , \n      qah . credits_attributed_compute , \n      qah . start_time \n    FROM SNOWFLAKE . ACCOUNT_USAGE . TAG_REFERENCES tr \n      JOIN SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n        ON tr . domain = 'USER' AND tr . object_name = qah . user_name \n ) \n SELECT \n    tag_name , \n    tag_value , \n    SUM ( credits_attributed_compute ) AS total_credits \n  FROM joined_data \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n    AND start_time < CURRENT_DATE \n  GROUP BY tag_name , tag_value \n  ORDER BY tag_name , tag_value ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries by users without tags \u00b6\nContent:\nThe following example calculates the cost of queries by users who are not tagged. You can use this to verify that tags are\nbeing applied consistently to users.\n```\nSELECT qah . user_name , SUM ( qah . credits_attributed_compute ) as total_credits \n  FROM \n    SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY qah \n    LEFT JOIN snowflake . account_usage . tag_references tr \n    ON qah . user_name = tr . object_name AND tr . DOMAIN = 'USER' \n  WHERE \n    start_time >= dateadd ( month , - 1 , current_date ) \n    AND qah . user_name IS NULL OR tr . object_name IS NULL \n  GROUP BY qah . user_name \n  ORDER BY total_credits DESC ;\n```\nCopy\n```\n+------------+---------------+ \n | USER_NAME  | TOTAL_CREDITS | \n |------------+---------------| \n | RSMITH     |  0.1830555556 | \n +------------+---------------+\n```\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries by department \u00b6\nContent:\nThe following example calculates the compute credits and the credits used for the query acceleration service for the finance department. This depends on the `COST_CENTER=finance` query tag being applied to the original queries that were executed.\nNote that the costs exclude idle time.\n```\nSELECT \n    query_tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE query_tag = 'COST_CENTER=finance' \n  GROUP BY query_tag ;\n```\nCopy\n```\n+---------------------+-----------------+------+ \n | QUERY_TAG           | COMPUTE_CREDITS | QAS  | \n |---------------------+-----------------|------| \n | COST_CENTER=finance |      0.00576115 | null | \n +---------------------+-----------------+------+\n```\nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (excluding idle time) by query tag \u00b6\nContent:\nThe following example calculates the cost of queries by query tag and includes queries without tags (identified as \u201cuntagged\u201d).\n```\nSELECT \n    COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n    SUM ( credits_attributed_compute ) AS compute_credits , \n    SUM ( credits_used_query_acceleration ) AS qas \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n  GROUP BY tag \n  ORDER BY compute_credits DESC ;\n```\nCopy\n```\n+-------------------------+-----------------+------+ \n | TAG                     | COMPUTE_CREDITS | QAS  | \n |-------------------------+-----------------+------+ \n | untagged                | 3.623173449     | null | \n | COST_CENTER=engineering | 0.531431948     | null | \n |-------------------------+-----------------+------+\n```\n ... \nSection Title: Attributing cost \u00b6 > ... > Calculating the cost of queries (including idle time) by query tag \u00b6\nContent:\n```\nWITH \n  wh_bill AS ( \n    SELECT SUM ( credits_used_compute ) AS compute_credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n      WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n      AND start_time < CURRENT_DATE \n  ), \n  tag_credits AS ( \n    SELECT \n        COALESCE ( NULLIF ( query_tag , '' ), 'untagged' ) AS tag , \n        SUM ( credits_attributed_compute ) AS credits \n      FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n      WHERE start_time >= DATEADD ( MONTH , - 1 , CURRENT_DATE ) \n      GROUP BY tag \n  ), \n  total_credit AS ( \n    SELECT SUM ( credits ) AS sum_all_credits \n      FROM tag_credits \n  ) \n SELECT \n    tc . tag , \n    tc . credits / t . sum_all_credits * w . compute_credits AS attributed_credits \n  FROM tag_credits tc , total_credit t , wh_bill w \n  ORDER BY attributed_credits DESC ;\n```\nCopy\n ... \nSection Title: Attributing cost \u00b6 > Additional examples of queries \u00b6 > Grouping similar queries \u00b6\nContent:\nFor recurrent or similar queries, use the `query_hash` or `query_parameterized_hash` to group costs\nby query.\nTo find the most expensive recurrent queries for the current month, execute the following statement:\n```\nSELECT query_parameterized_hash , \n       COUNT (*) AS query_count , \n       SUM ( credits_attributed_compute ) AS total_credits \n  FROM SNOWFLAKE . ACCOUNT_USAGE . QUERY_ATTRIBUTION_HISTORY \n  WHERE start_time >= DATE_TRUNC ( 'MONTH' , CURRENT_DATE ) \n  AND start_time < CURRENT_DATE \n  GROUP BY query_parameterized_hash \n  ORDER BY total_credits DESC \n  LIMIT 20 ;\n```\nCopy\nFor an additional query based on query ID, see Examples ."
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history",
      "title": "WAREHOUSE_METERING_HISTORY view | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nReference General reference SNOWFLAKE database Account Usage WAREHOUSE\\_METERING\\_HISTORY\n\nSchemas:\n    ACCOUNT\\_USAGE , READER\\_ACCOUNT\\_USAGE\n\n# WAREHOUSE\\_ METERING\\_ HISTORY view \u00b6\n\nThis Account Usage view can be used to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within the last 365 days (1 year).\n\n## Columns \u00b6\n\n|Column Name |Data Type |Description |\n| --- | --- | --- |\n|READER\\_ACCOUNT\\_NAME |VARCHAR |Name of the reader account where the warehouse usage took place. Column only included in view in READER\\_ACCOUNT\\_USAGE schema. |\n|START\\_TIME |TIMESTAMP\\_LTZ |The date and beginning of the hour (in the local time zone) in which the warehouse usage took place. |\n|END\\_TIME |TIMESTAMP\\_LTZ |The date and end of the hour (in the local time zone) in which the warehouse usage took place. |\n|WAREHOUSE\\_ID |NUMBER |Internal/system-generated identifier for the warehouse. |\n|WAREHOUSE\\_NAME |VARCHAR |Name of the warehouse. |\n|CREDITS\\_USED |NUMBER |Total number of credits used for the warehouse in the hour. This is a sum of CREDITS\\_USED\\_COMPUTE and CREDITS\\_USED\\_CLOUD\\_SERVICES. This value does not take into account the adjustment for cloud services , and may therefore be greater than the credits that are billed. To determine how many credits were actually billed, run queries against the METERING\\_DAILY\\_HISTORY view . |\n|CREDITS\\_USED\\_COMPUTE |NUMBER |Number of credits used for the warehouse in the hour. |\n|CREDITS\\_USED\\_CLOUD\\_SERVICES |NUMBER |Number of credits used for cloud services in the hour. |\n|CREDITS\\_ATTRIBUTED\\_COMPUTE\\_QUERIES |NUMBER |Number of credits attributed to queries in the hour. . . Includes only the credit usage for query execution and doesn\u2019t include warehouse idle time usage. |\n\n## Usage notes \u00b6\n\n* In the ACCOUNT\\_USAGE schema, latency for the view is up to 180 minutes (3 hours), except for the CREDITS\\_USED\\_CLOUD\\_SERVICES column. Latency for\n  CREDITS\\_USED\\_CLOUD\\_SERVICES is up to 6 hours.\n* In the READER\\_ACCOUNT\\_USAGE schema, latency for the view is up to 24 hours.\n* Warehouse idle time is not included in the CREDITS\\_ATTRIBUTED\\_COMPUTE\\_QUERIES column.\n  \n  See Examples for a query that calculates the cost of idle time.\n\n* If you want to reconcile the data in this view with a corresponding view in the ORGANIZATION USAGE schema , you must first set the timezone of the session to UTC. Before querying the Account Usage view, execute:\n  \n  > ```\n  > ALTER SESSION SET TIMEZONE = UTC ;\n  > ```\n  > \n  > Copy\n  > \n  >\n\n## Examples \u00b6\n\nFor example, to determine the cost of idle time for each warehouse for the last 10 days, execute the following statement:\n\n```\nSELECT \n  ( SUM ( credits_used_compute ) - \n    SUM ( credits_attributed_compute_queries )) AS idle_cost , \n  warehouse_name \n FROM SNOWFLAKE . ACCOUNT_USAGE . WAREHOUSE_METERING_HISTORY \n WHERE start_time >= DATEADD ( 'days' , - 10 , CURRENT_DATE ()) \n  AND end_time < CURRENT_DATE () \n GROUP BY warehouse_name ;\n```\n\nCopy\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Columns\n2. Usage notes\n3. Examples\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas\n\n## Snowflake's Use of Cookies\n\n## Privacy Preference Center\n\nYour Opt Out Preference Signal is Honored\n\n* ### Your Privacy\n* ### Strictly Necessary Cookies\n* ### Performance Cookies\n* ### Functional Cookies\n* ### Targeting Cookies\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. This information might be about you, your preferences or your device and is mostly used to make the site work as you expect it to. The information does not usually directly identify you, but it can give you a more personalized web experience. Because we respect your right to privacy, you can choose not to allow some types of cookies. Click on the different category headings to find out more and change our default settings. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.  \n[More information](https://cookiepedia.co.uk/giving-consent-to-cookies)\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\n\nCookies Details\u200e\n\n#### Performance Cookies\n\nPerformance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site.    All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\n\nCookies Details\u200e\n\n#### Functional Cookies\n\nFunctional Cookies\n\nThese cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages.    If you do not allow these cookies then some or all of these services may not function properly.\n\nCookies Details\u200e\n\n#### Targeting Cookies\n\nTargeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly identifiable personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\n\nCookies Details\u200e\n\n### Cookie List\n\nConsent Leg.Interest\n\ncheckbox label label\n\ncheckbox label label\n\ncheckbox label label\n\nClear\n\ncheckbox label label\n\nApply Cancel\n\nConfirm My Choices\n\nAllow All\n\n[](https://www.onetrust.com/products/cookie-consent/)"
      ]
    },
    {
      "url": "https://medium.com/snowflake/granular-cost-attribution-and-chargeback-for-warehouse-costs-on-snowflake-cf96fb690967",
      "title": "Granular cost attribution and chargeback for warehouse costs on Snowflake | by Kaushal Jain | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium",
      "publish_date": "2024-09-03",
      "excerpts": [
        "Section Title: Granular cost attribution and chargeback for warehouse costs on Snowflake\nContent:\nKaushal Jain\n6 min read\n\u00b7\nSep 3, 2024\n--\nListen\nShare\nPress enter or click to view image in full size\nImagine a Snowflake warehouse as a virtual processing facility. While Snowflake has a separate storage layer for data, the virtual [warehouse](https://docs.snowflake.com/en/user-guide/warehouses) is primarily responsible for the compute tasks. The size and activity level of the warehouse determine the costs, similar to how a busier processing facility would incur higher expenses.\nA common request by Snowflake customers is how to attribute costs to individual queries at a finer grain than a whole warehouse. This blog post will explore various methods to allocate compute costs and introduce the [per query cost attribution](https://docs.snowflake.com/LIMITEDACCESS/query_attribution_history) feature for more granular cost attribution and analysis.\nSection Title: ... > Warehouse-Based Cost Allocation\nContent:\nWhen warehouses are dedicated to a specific business unit, cost allocation is straightforward. There are two common scenarios:\n**Scenario 1: Object Tagging** ( *recommended* ) Warehouses are tagged using [object tagging](https://docs.snowflake.com/en/user-guide/object-tagging) to logically group cost centers. The cost attribution to a business unit for a given month can be computed using the following query. Note that this requires compliance, and a unique tag attributed to every warehouse. For reconciliation, the following example query also groups the untagged warehouses into the *untagged* bucket.\nSection Title: ... > Warehouse-Based Cost Allocation\nContent:\n```\nSELECT COALESCE(tag_references.tag_value, 'untagged') AS tag_value,   \n       SUM(warehouse_metering_history.credits_used_compute) AS total_credits  \nFROM snowflake.account_usage.warehouse_metering_history  \nLEFT JOIN snowflake.account_usage.tag_references  \nON warehouse_metering_history.warehouse_id = tag_references.object_id  \nWHERE warehouse_metering_history.start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))   \n  AND warehouse_metering_history.start_time < DATE_TRUNC('MONTH',  CURRENT_DATE)  \nGROUP BY COALESCE(tag_references.tag_value, 'untagged')  \nORDER BY total_credits DESC;\n```\nPress enter or click to view image in full size\nSection Title: ... > Warehouse-Based Cost Allocation\nContent:\n**Scenario 2: Warehouse Naming Convention** Warehouses follow a naming convention such as *teamname_details* or *workload_details* . This allows for direct mapping of costs to the respective business units or workloads. Here is an example query that gives a split of the costs across teams assuming they follow a naming convention. The *others* bucket captures the ones not following the convention for reconciliation.\nSection Title: ... > Warehouse-Based Cost Allocation\nContent:\n```\nSELECT   \n    CASE   \n        WHEN POSITION('_' IN warehouse_metering_history.warehouse_name) > 0   \n             THEN SPLIT_PART(warehouse_metering_history.warehouse_name, '_', 1)  \n        ELSE 'others'  \n    END AS team_name,  \n    SUM(warehouse_metering_history.credits_used_compute) AS total_credits  \nFROM snowflake.account_usage.warehouse_metering_history  \nWHERE warehouse_metering_history.start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))   \n  AND warehouse_metering_history.start_time < DATE_TRUNC('MONTH',  CURRENT_DATE)  \nGROUP BY team_name  \nORDER BY total_credits DESC;\n```\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\nWhen multiple business units or workloads share the same warehouse, cost allocation becomes more complex as the warehouse-level granularity is not sufficient enough. Even in cases where a warehouse may be tied to a given business unit, the unit may want to understand the costs in a more granular manner. The per query cost attribution [feature](https://docs.snowflake.com/LIMITEDACCESS/query_attribution_history) , now **generally available** , helps in these scenarios. This feature allows for warehouse cost attribution at the query level.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\nThe per query cost attribution feature provides the portion of the warehouse cost that can be attributed to a given query. If there is only a single query running in a warehouse, the entire cost of the warehouse is attributed to that query. If multiple simultaneous queries are running, the cost of the warehouse during the overlapping periods is attributed to the queries based on their relative resource consumption. During the times when there is no query running and the warehouse is idle (not suspended), the costs are not attributed to any particular query but can be easily determined and spread across the queries as needed for reconciliation purposes.\n**Common Methods to Allocate Costs Using Per Query Cost Attribution**\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n**1. User-Based Costs** Understanding costs associated with a given user can be useful for accountability, budgeting and more granular forecasting. It can also provide hints on which users are most effective in their use of Snowflake. Chargeback of Snowflake costs at a user level can be achieved using the USER_NAME column in the QUERY_ATTRIBUTION_HISTORY view. The following sample query provides a way to attribute monthly warehouse costs across users. The idle time costs are proportionately attributed based on the relative spend by users.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n```\nWITH wh_bill AS (  \n   SELECT SUM(credits_used_compute) AS compute_credits  \n   FROM snowflake.account_usage.warehouse_metering_history  \n   WHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n   AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \n),  \nuser_credits AS (  \n   SELECT user_name, SUM(credits_attributed_compute) AS credits  \n   FROM snowflake.account_usage.query_attribution_history  \n   WHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n   AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \n   GROUP BY user_name  \n),  \ntotal_credit AS (  \n   SELECT SUM(credits) AS sum_all_credits  \n   FROM user_credits  \n)  \nSELECT u.user_name, u.credits / t.sum_all_credits * w.compute_credits AS attributed_credits  \nFROM user_credits u, total_credit t, wh_bill w;\n```\nPress enter or click to view image in full size\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\nUsers can be mapped to business units / cost centers in a custom view, which can be used along with the user-based costs to roll up the spend across business units. Alternatively, the ROLE_NAME (available in QUERY_HISTORY view) can be used to allocate costs for a given project, if several developers use a given role for a specific project. This method does not require manual tagging, thereby reducing the operational overhead to enforce tagging policies.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n**2. Query Tag-Based Costs** [Query tags](https://docs.snowflake.com/en/sql-reference/parameters) can be used to allocate costs across different workloads. Customers can assign query tags to specific workloads or a set of queries to track them together. These can be set at a session level (Account->User->Session) and can also be configured when issuing snowflake queries from tools such as [dbt](https://docs.getdbt.com/reference/resource-configs/snowflake-configs) , [airflow](https://airflow.apache.org/docs/apache-airflow-providers-snowflake/stable/_api/airflow/providers/snowflake/hooks/snowflake_sql_api/index.html) , etc. The following example query helps attribute the warehouse compute costs from the previous month to different workloads, assuming that workloads are tagged, and also allocates the idle costs proportionately across workloads for reconciliation purposes.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n```\nWITH wh_bill AS (  \n   SELECT SUM(credits_used_compute) AS compute_credits  \n   FROM snowflake.account_usage.warehouse_metering_history  \n   WHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n   AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \n),  \ntag_credits AS (  \n   SELECT COALESCE(NULLIF(query_tag, ''), 'untagged') AS tag, SUM(credits_attributed_compute) AS credits  \n   FROM snowflake.account_usage.query_attribution_history  \n   WHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n   AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \n   GROUP BY tag  \n),  \ntotal_credit AS (  \n   SELECT SUM(credits) AS sum_all_credits  \n   FROM tag_credits  \n)  \nSELECT tc.tag, tc.credits / t.sum_all_credits * w.compute_credits AS attributed_credits  \nFROM tag_credits tc, total_credit t, wh_bill w;\n```\nPress enter or click to view image in full size\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n**3. Recurrent Query Costs** For recurrent or similar queries, costs can be attributed using QUERY_HASH and QUERY_PARAMETERIZED_HASH. If there are several recurrent / similar queries and want to understand costs attributable to such queries over a period, you can use the following example query to group the costs based on the recurrent queries and identify the top drivers of cost for the past month (most expensive recurrent queries). Note that the attributed costs of a query changes depending on the environment (size of a warehouse, utilization of warehouse, other queries running during that time etc.).\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n```\nSELECT query_parameterized_hash,   \n       COUNT(*) AS query_count,   \n       SUM(credits_attributed_compute) AS total_credits  \nFROM snowflake.account_usage.query_attribution_history  \nWHERE start_time >= DATE_TRUNC('MONTH', DATEADD(MONTH, -1, CURRENT_DATE))  \n  AND start_time < DATE_TRUNC('MONTH', CURRENT_DATE)  \nGROUP BY query_parameterized_hash  \nORDER BY total_credits DESC  \nLIMIT 20;\n```\nPress enter or click to view image in full size\nAdditionally, for stored procedures that often issue multiple hierarchical queries, you can compute the attributed costs for the entire procedure using the following example query, as the parent and the root query ids are available for such procedures in the view.\nSection Title: ... > Introducing Per Query Cost Attribution\nContent:\n```\nSET query_id = '<query_id>'; // root query id for the stored procedure  \n  \nSELECT SUM(credits_attributed_compute) AS total_attributed_credits  \n  FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY  \n  WHERE (root_query_id = $query_id  \n         OR query_id = $query_id);\n```\nSection Title: Granular cost attribution and chargeback for warehouse costs on Snowflake > ... > Conclusion\nContent:\nSnowflake\u2019s per query cost attribution feature, along with existing cost allocation mechanisms, provides powerful tools for granular cost management. By leveraging these features, organizations can enhance transparency, optimize resource usage, and ensure fair cost allocation. For more details, visit Snowflake\u2019s [per query cost attribution](https://docs.snowflake.com/LIMITEDACCESS/query_attribution_history) page.\nMaximize your cost efficiency with Snowflake and take control of your data platform expenses today!\nOptimization\nCost Management\nSnowflake\nWarehouse Management\n--\n--\n[](https://medium.com/snowflake?source=post_page---post_publication_info--cf96fb690967---------------------------------------)\n[](https://medium.com/snowflake?source=post_page---post_publication_info--cf96fb690967---------------------------------------)\n ... \nSection Title: Granular cost attribution and chargeback for warehouse costs on Snowflake > Written by Kaushal Jain\nContent:\n27 followers\n\u00b7 7 following"
      ]
    },
    {
      "url": "https://blog.greybeam.ai/snowflake-cost-per-query/",
      "title": "Deep Dive: Snowflake's Query Cost and Idle Time Attribution",
      "publish_date": "2024-10-22",
      "excerpts": [
        "[](https://www.greybeam.ai/)\n[Blog](https://blog.greybeam.ai/)\n[Waitlist](https://greybeam.ai)\n[Customer Stories](https://blog.greybeam.ai/tag/customer-story/)\nSubscribe\nSep 9, 2024 13 min read How-To\nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query\nContent:\nSnowflake's new QUERY_ATTRIBUTION_HISTORY view\nSnowflake recently released a new feature for granular cost attribution down to individual queries through the `QUERY_ATTRIBUTION_HISTORY` view in `ACCOUNT_USAGE` . As a company focused on SQL optimization, we at Greybeam were eager to dive in and see how this new capability compares to our own custom cost attribution logic. What we found was surprising - and it led us down a rabbit hole of query cost analysis.\nSection Title: ... > The Promise and Limitations of QUERY_ATTRIBUTION_HISTORY\nContent:\nThe new view aims to provide visibility into the compute costs associated with each query. Some key things to note:\nData is only available from July 1, 2024 onwards\nShort queries (<100ms) are excluded\nIdle time is not included in the attributed costs\nThere can be up to a 6 hour delay in data appearing\nThere's also a `WAREHOUSE_UTILIZATION` view that displays cost of idle time. At the time of writing, this must be enabled by your Snowflake support team.\n ... \nSection Title: ... > Our Initial Findings\nContent:\n```\nWITH query_execution AS (\n    SELECT\n        qa.query_id\n        , TIMEADD(\n                'millisecond',\n                qh.queued_overload_time + qh.compilation_time +\n                qh.queued_provisioning_time + qh.queued_repair_time +\n                qh.list_external_files_time,\n                qh.start_time\n            ) AS execution_start_time\n        , qh.end_time::timestamp AS end_time\n        , DATEDIFF('MILLISECOND', execution_start_time, qh.end_time)*0.001 as execution_time_secs\n        , qa.credits_attributed_compute\n        , DATE_TRUNC('HOUR', execution_start_time) as execution_start_hour\n        , w.credits_used_compute\n    FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_ATTRIBUTION_HISTORY AS qa\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS qh\n        ON qa.query_id = qh.query_id\n    JOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS w\n        ON execution_start_hour = w.start_time\n        AND qh.warehouse_id = w.warehouse_id\n    WHERE\n ... \nSection Title: A Deep Dive into Snowflake's Query Cost Attribution: Finding Cost per Query > ... > Digging Deeper\nContent:\nTo investigate further, we compared the results to our own custom cost attribution logic that accounts for idle time. Here\u2019s a snippet of what we found for the same hour:\nGreybeam\u2019s internal query cost attribution results\nAs you can see, our calculations show much smaller fractions of credits attributed to the actual query runtimes for the first hour, with the bulk going to idle periods. This aligns much more closely with our expectations given the warehouse configuration, and it works historically!\n ... \nSection Title: ... > Our Approach to Query Cost Attribution\nContent:\nGiven the discrepancies we\u2019ve found, we wanted to share our methodology for calculating per-query costs, including idle time. Here\u2019s an overview of our process:\nGather warehouse suspend events\nEnrich query data with execution times and idle periods\nCreate a timeline of all events (queries and idle periods)\nJoin with `WAREHOUSE_METERING_HISTORY` to attribute costs\nBefore we dive in, let\u2019s cover a few basics:\nSnippet of WAREHOUSE_METERING_HISTORY\nSection Title: ... > Our Approach to Query Cost Attribution\nContent:\nWe use `WAREHOUSE_METERING_HISTORY` as our source of truth for warehouse compute credits. The credits billed here will reconcile with Snowflake\u2019s cost management dashboards.\nCredits here are represented on an hourly grain. We like to refer to this as *credits metered* , analogous to how most homes in North America are metered for their electricity. In our solution, we\u2019ll need to allocate queries and idle times into their metered hours.\nWe use a weighted time-based approach to attribute costs within the metered hour. In reality, Snowflake\u2019s credit attribution is likely much more complex, especially in situations with more clusters or warehouse scaling.\nHow we need to break down our queries and idle times.\nThe full SQL query will be available at the end of this blog.\nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\n\u2757\nWe've updated this article with an optimization using `ASOF JOIN` . Check out how to use ASOF JOINs [here](https://blog.greybeam.ai/snowflake-asof-join/) .\nFirst, we need to know when warehouses are suspended, this is pulled from [`WAREHOUSE_EVENTS_HISTORY`](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_events_history?ref=blog.greybeam.ai) .\n```\nWITH warehouse_events AS (\n    SELECT\n        warehouse_id\n        , timestamp\n        , LAG(timestamp) OVER (PARTITION BY warehouse_id ORDER BY timestamp) as lag_timestamp\n    FROM snowflake.account_usage.warehouse_events_history\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'\n        AND DATEADD('DAY', 15, timestamp) >= current_date\n)\n```\n ... \nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\nIn addition, warehouse suspension doesn\u2019t actually occur during the `SUSPEND_WAREHOUSE` event. Technically, it happens when the `WAREHOUSE_CONSISTENT` event is logged. The `WAREHOUSE_CONSISTENT` event indicates that all compute resources associated with the warehouse have been fully released. You can find more information about this event in the [Snowflake documentation](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_events_history?ref=blog.greybeam.ai) .\nFor the sake of simplicity (and because the time difference is usually negligible), we\u2019re sticking with the `SUSPEND_WAREHOUSE` event in our analysis. This approach gives us a good balance between accuracy and complexity in our cost attribution model.\nBefore moving onto enriching query data, we want to apply filters to reduce the load from table scans. Feel free to adjust the dates as you see fit.\nSection Title: ... > Step 1: Gather Warehouse Suspend Events\nContent:\n```\nWITH warehouse_list AS (\n    SELECT \n        DISTINCT warehouse_name,\n        warehouse_id\n    FROM warehouse_metering_history\n    WHERE \n        warehouse_name IS NOT NULL\n        AND start_time >= $startDate\n),\n\nwarehouse_events AS (\n    SELECT\n        weh.warehouse_id\n        , weh.timestamp\n    FROM warehouse_events_history as weh\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'        \n),\n\nqueries_filtered AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.warehouse_name\n        , q.warehouse_size\n        , q.role_name\n        , q.user_name\n        , q.query_text\n        , q.query_hash\n        , q.queued_overload_time\n        , q.compilation_time\n        , q.queued_provisioning_time\n        , q.queued_repair_time\n        , q.list_external_files_time\n        , q.start_time\n        , TIMEADD(\n                'millisecond',\n                q.queued_overload_time + q.compilation_time +\n ... \nSection Title: ... > Step 2: Enrich Query Data\nContent:\nIn this step, we take the raw query data and enrich it with additional information that will allow us to breakdown query and idle times into their hourly components. We choose hourly slots because the source of truth for credits comes from `WAREHOUSE_METERING_HISTORY` , which is on an hourly grain.\n ... \nSection Title: ... > Step 3: Create Timeline of All Events\nContent:\nWe now need to create an hourly timeline of all events so that we can reconcile our credits with `WAREHOUSE_METERING_HISTORY` . The timeline of all events can be broken down into 4 components:\nA query executed and ended in the same hour\nIdle time started and ended in the same hour\nA query executed and ended in a different hour\nIdle time started and ended in a different hour\n1 and 2 are straight forward since they don\u2019t cross any hourly boundaries we can simply select from the dataset and join directly to `WAREHOUSE_METERING_HISTORY` .\n ... \nSection Title: ... > Step 3: Create Timeline of All Events\nContent:\nFor 3 and 4, we need a record for each hour that the queries and idle times ran within. For example, if a query ran from 7:55PM to 10:40PM, we\u2019d need a record for 7, 8, 9, and 10PM.\nA query that executed across 4 hourly slots (including 0).\nOriginally we used a slightly more complicated join:\n```\nFROM queries_enriched AS q\nJOIN SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY AS m\n    ON q.warehouse_id = m.warehouse_id\n    AND m.start_time >= q.meter_start_time\n    AND m.start_time < q.end_time\n```\nThis took forever to run on a large account. Instead, we first create records for each hour so that the join to `WAREHOUSE_METERING_HISTORY` is a direct join in the next step.\n ... \nSection Title: ... > Step 4: Attribute Costs\nContent:\nFinally, with each query and idle period properly allocated to their hourly slots, we can directly join to `WAREHOUSE_METERING_HISTORY` and calculate our credits used.\n```\nmetered AS (\n    SELECT\n        m.query_id\n        , m.warehouse_id\n        , m.type\n        , m.event_start_at\n        , m.event_end_at\n        , m.meter_start_hour\n        , m.meter_start_at\n        , m.meter_end_at\n        , m.meter_time_secs\n        , SUM(m.meter_time_secs) OVER (PARTITION BY m.warehouse_id, m.meter_start_hour) AS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN snowflake.account_usage.warehouse_metering_history AS w -- inner join because both tables have different delays\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time -- we can directly join now since we used our numgen method\n)\n```\n ... \nSection Title: ... > Step 4: Attribute Costs\nContent:\nOne important note: This approach assumes that all time within an hour is equally valuable in terms of credit consumption. In reality, Snowflake may have more complex internal algorithms for credit attribution, especially for multi-cluster warehouses or warehouses that change size within an hour. However, this weighted time-based approach provides a reasonable and transparent method for cost attribution that aligns well with Snowflake\u2019s consumption-based billing model.\n ... \nSection Title: ... > Full SQL Cost Attribution\nContent:\n```\nSET startDate = DATEADD('DAY', -15, current_date);\nWITH warehouse_list AS (\n    SELECT \n        DISTINCT warehouse_name,\n        warehouse_id\n    FROM warehouse_metering_history\n    WHERE \n        warehouse_name IS NOT NULL\n        AND start_time >= $startDate\n),\n\nwarehouse_events AS (\n    SELECT\n        weh.warehouse_id\n        , weh.timestamp\n    FROM warehouse_events_history as weh\n    WHERE\n        event_name = 'SUSPEND_WAREHOUSE'        \n),\n\nqueries_filtered AS (\n    SELECT\n        q.query_id\n        , q.warehouse_id\n        , q.warehouse_name\n        , q.warehouse_size\n        , q.role_name\n        , q.user_name\n        , q.query_text\n        , q.query_hash\n        , q.queued_overload_time\n        , q.compilation_time\n        , q.queued_provisioning_time\n        , q.queued_repair_time\n        , q.list_external_files_time\n        , q.start_time\n        , TIMEADD(\n                'millisecond',\n                q.queued_overload_time +\n ... \nSection Title: ... > Full SQL Cost Attribution\nContent:\nAS total_meter_time_secs\n        , (m.meter_time_secs / total_meter_time_secs) * w.credits_used_compute AS credits_used\n    FROM mega_timeline AS m\n    JOIN warehouse_metering_history AS w\n        ON m.warehouse_id = w.warehouse_id\n        AND m.meter_start_hour = w.start_time\n),\n\nfinal AS (\n    SELECT\n        m.* EXCLUDE total_meter_time_secs, meter_end_at, original_query_id\n        , q.query_text\n        , q.query_hash\n        , q.warehouse_size\n        , q.warehouse_name\n        , q.role_name\n        , q.user_name\n    FROM metered AS m\n    JOIN queries_filtered AS q\n        ON m.original_query_id = q.query_id\n)\nSELECT\n    *\nFROM final\n;\n```"
      ]
    },
    {
      "url": "https://www.reddit.com/r/snowflake/comments/1k8nmt7/finding_cost_without_creating_multiple_warehouse/",
      "title": "Finding Cost without creating multiple warehouse - snowflake",
      "excerpts": [
        "ACCOUNT_USAGE schema has a query_history view and a warehouse_metering_history which you can combine to get rough estimates. The method I'm\u00a0... Read more Apr 26, 2025 \u00b7 The SNOWFLAKE.ACCOUNT_USAGE schema has a query_history view and a warehouse_metering_history which you can combine to get rough estimates. The\u00a0..."
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-understanding-overall",
      "title": "Understanding overall cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nGuides Cost & Billing Understanding cost\nSection Title: Understanding overall cost \u00b6\nContent:\nNote\nThis topic describes foundational costs associated with using Snowflake (compute costs, storage costs, and data transfer costs).\nSpecific Snowflake features (for example, Snowflake Cortex and Snowpark Container Services) incur costs in unique ways, and are not\ndiscussed in this topic.\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\nThe total cost of using Snowflake is the aggregate of the cost of using data transfer, storage, and compute resources. Snowflake\u2019s\ninnovative cloud architecture separates the cost of accomplishing any task into one of these\nusage types.\nCompute Resources\nUsing compute resources within Snowflake consumes Snowflake credits. The billed cost of using compute resources is\ncalculated by multiplying the number of consumed credits by the price of a credit. For the current price of a credit, see the [Snowflake Pricing Guide](https://www.snowflake.com/pricing/pricing-guide/) .\nThere are three types of compute resources that consume credits within Snowflake:\nSection Title: Understanding overall cost \u00b6 > How are costs incurred? \u00b6\nContent:\n**Virtual Warehouse Compute** : Virtual warehouses are user-managed compute resources that consume\ncredits when loading data, executing queries, and performing other DML operations. Because Snowflake utilizes per-second billing (with a\n60-second minimum each time the warehouse starts), warehouses are billed only for the credits they actually consume when they are\nactively working. **Serverless Compute** : There are Snowflake features such as Search Optimization and Snowpipe that use Snowflake-managed compute\nresources rather than virtual warehouses. To minimize cost, these serverless compute resources are automatically resized and scaled\nup or down by Snowflake as required for each workload. **Cloud Services Compute** : The cloud services layer of the Snowflake architecture consumes credits as it performs behind-the-scenes\ntasks such as authentication, metadata management, and access control.\n ... \nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\nThe following example provides insight into the total cost in Snowflake to load and query data.\nSuppose an organization loads data constantly, 24x7. It has two different groups of users (Finance and Sales) using the database in\noverlapping, but different times of the day. It also runs a weekly batch report. This organization:\n ... \nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n| Parameter | Customer Requirement | Configuration | Cost |\n| Finance Users | 5 Users, 8am-5pm (9 hours) | Large Standard Virtual Warehouse (8 credits/hr) | 1,440 credits (8 credits/hr x 9 hours per day x 20 days per month) |\n| Sales Users | 12 Users, 16 hour time slot | Medium Standard Virtual Warehouse (4 credits/hr) | 1,280 (4 credits/hr x 16 hours per day x 20 days per month) |\n| Complex Query Users | 1 User, 2 hours/day | 2X Standard Virtual Warehouse (32 credits/hr) | 256 (32 credits/hr x 2 hours per day x 4 days per month) |\n ... \nSection Title: Understanding overall cost \u00b6 > Total cost example \u00b6\nContent:\n**Next Topics**\nUnderstanding compute cost\nUnderstanding storage cost\nUnderstanding data transfer cost\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nHow are costs incurred?\nTotal cost example\nRelated content\nExploring overall cost\nManaging cost in Snowflake\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/user-guide/cost-exploring-compute",
      "title": "Exploring compute cost | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n/\nGet started\nGuides\nDeveloper\nReference\nRelease notes\nTutorials\n[Status](https://status.snowflake.com)\nOverview\nSnowflake Horizon Catalog\nApplications and tools for connecting to Snowflake\nVirtual warehouses\nDatabases, Tables, & Views\nData types\nData Integration\nSnowflake Openflow\nApache Iceberg\u2122\nApache Iceberg\u2122 Tables\nSnowflake Open Catalog\nData engineering\nData loading\nDynamic Tables\nStreams and Tasks\ndbt Projects on Snowflake\nData Unloading\nStorage Lifecycle Policies\nMigrations\nQueries\nListings\nCollaboration\nSnowflake AI & ML\nSnowflake Postgres\nAlerts & Notifications\nSecurity\nData Governance\nPrivacy\nOrganizations & Accounts\nBusiness continuity & data recovery\nPerformance optimization\nCost & Billing\nGuides Cost & Billing Visibility Exploring cost Exploring compute cost\nSection Title: Exploring compute cost \u00b6\nContent:\nTotal compute cost consists of the overall use of:\nVirtual warehouses (user-managed compute resources)\nServerless features such as Automatic Clustering and Snowpipe that use Snowflake-managed compute resources\nCloud services layer of the Snowflake architecture\nvCPU usage for Openflow BYOC cost and scaling considerations and Openflow Snowflake Deployment cost and scaling considerations .\nSee Openflow components for more information about Openflow components including runtimes.\nThis topic describes how to gain insight into historical compute costs using Snowsight , or by writing queries against views in\nthe ACCOUNT_USAGE and ORGANIZATION_USAGE schemas.\nSnowsight allows you to quickly and easily obtain information about cost from a visual dashboard. Queries against the usage views\nallow you to drill down into cost data and can help generate custom reports and dashboards.\nSection Title: Exploring compute cost \u00b6\nContent:\nIf you need more information about how compute costs are incurred, refer to Understanding compute cost .\nNote\nThe cloud services layer consumes credits, but not all of those credits are actually billed. Usage for cloud services is charged only if\nthe daily consumption of cloud services exceeds 10% of the daily usage of virtual warehouses. Snowsight and a majority of views\nshow the total number of credits consumed by warehouses, serverless features, and cloud services without accounting for this daily\nadjustment to cloud services.\nTo determine how many credits were actually billed for compute costs, run queries against the METERING_DAILY_HISTORY view .\n ... \nSection Title: Exploring compute cost \u00b6 > Querying data for compute cost \u00b6 > Example queries \u00b6\nContent:\nThe following queries drill-down into data in ACCOUNT_USAGE views to gain insight into compute costs.\nNote\nQueries executed against views in the Account Usage schema can be modified to gain insight into cost for the entire organization by\nusing the corresponding view in the Organization Usage schema. For example, both schemas include a WAREHOUSE_METERING_HISTORY view.\nClick the name of a query below to see the full SQL example.\nCompute for Warehouses :\n* Query: Average hour-by-hour Snowflake spend (across all warehouses) over the past m days\nQuery: Credit consumption by warehouse over specific time period\nQuery: Warehouse usage over m-day average\nQuery: Warehouse cost attribution by query tag .\nQuery: Warehouse cost attribution by user .\nCompute for Cloud Services :\n* Query: Billed cloud services\n ... \nSection Title: Exploring compute cost \u00b6 > ... > Example queries \u00b6 > Compute for Snowflake Notebooks \u00b6\nContent:\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\nShare your feedback\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\nOn this page\nViewing credit usage\nQuerying data for compute cost\nRelated content\nUnderstanding compute cost\nExploring overall cost\nAttributing cost\nLanguage: **English**\nEnglish\nFran\u00e7ais\nDeutsch\n\u65e5\u672c\u8a9e\n\ud55c\uad6d\uc5b4\nPortugu\u00eas"
      ]
    },
    {
      "url": "https://docs.snowflake.com/en/sql-reference/functions/warehouse_metering_history",
      "title": "WAREHOUSE_METERING_HISTORY | Snowflake Documentation",
      "excerpts": [
        "[DOCUMENTATION](https://docs.snowflake.com)\n\n/\n\nGet started\n\nGuides\n\nDeveloper\n\nReference\n\nRelease notes\n\nTutorials\n\n[Status](https://status.snowflake.com)\n\nReference Function and stored procedure reference Table WAREHOUSE\\_METERING\\_HISTORY\n\nCategories:\n    Information Schema , Table functions\n\n# WAREHOUSE\\_METERING\\_HISTORY \u00b6\n\nThis table function can be used in queries to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within a specified date range.\n\nNote\n\nThis function returns credit usage within the last 6 months. However, if you are querying multiple warehouses over a lengthy time period,\nit might not return a complete data set. To obtain a complete data set, use the ACCOUNT\\_USAGE view instead.\n\nSee also:\n    WAREHOUSE\\_LOAD\\_HISTORY\n\n## Syntax \u00b6\n\n```\nWAREHOUSE_METERING_HISTORY ( \n      DATE_RANGE_START => <constant_expr> \n      [ , DATE_RANGE_END => <constant_expr> ] \n      [ , WAREHOUSE_NAME => '<string>' ] )\n```\n\nCopy\n\n## Arguments \u00b6\n\n**Required:**\n\n`DATE_RANGE_START => _constant_expr_`\n    The starting date, within the last 6 months, for which warehouse usage is returned.\n\n**Optional:**\n\n`DATE_RANGE_END => _constant_expr_`\n    The ending date, within the last 6 months, for which warehouse usage is returned.\n\nDefault: CURRENT\\_DATE is used. `WAREHOUSE_NAME => ' _string_ '`\n    The name of the warehouse to retrieve credit usage for. Note that the warehouse name must be enclosed in single quotes. Also, if the warehouse name any spaces, mixed-case characters,\nor special characters, the name must be double-quoted within the single quotes (e.g. `'\"My Warehouse\"'` vs `'mywarehouse'` ).\n\nDefault: All warehouses that ran during the specified date range.\n\n## Usage notes \u00b6\n\n* Returns results only for the ACCOUNTADMIN role or any role that has been explicitly granted the MONITOR USAGE global privilege.\n* When calling an Information Schema table function, the session must have an INFORMATION\\_SCHEMA schema in use or the function name must be fully-qualified. For more details, see Snowflake Information Schema .\n* The order and structure of the arguments depends on whether the argument keywords (e.g. `DATE_RANGE_START` ) are included:\n  \n    + The keywords are not required if the arguments are specified in order.\n    + If the argument keywords are included, the arguments can be specified in any order.\n\n## Output \u00b6\n\nThe function returns the following columns, ordered by WAREHOUSE\\_NAME and START\\_TIME:\n\n|Column Name |Data Type |Description |\n| --- | --- | --- |\n|START\\_TIME |TIMESTAMP\\_LTZ |The beginning of the hour in which this warehouse usage took place. |\n|END\\_TIME |TIMESTAMP\\_LTZ |The end of the hour in which this warehouse usage took place. |\n|WAREHOUSE\\_NAME |VARCHAR |Name of the warehouse. |\n|CREDITS\\_USED |NUMBER |Number of credits billed for this warehouse in this hour. |\n|CREDITS\\_USED\\_COMPUTE |NUMBER |Number of credits used for the warehouse in the hour. |\n|CREDITS\\_USED\\_CLOUD\\_SERVICES |NUMBER |Number of credits used for cloud services in the hour. |\n\n## Examples \u00b6\n\nRetrieve hourly warehouse usage over the past 10 days for all warehouses that ran during this time period:\n\n> ```\n> select * \n>  from table ( information_schema . warehouse_metering_history ( dateadd ( 'days' ,- 10 , current_date ())));\n> ```\n> \n> Copy\n> \n> \n\nRetrieve hourly warehouse usage for the `testingwh` warehouse on a specified date:\n\n> ```\n> select * \n>  from table ( information_schema . warehouse_metering_history ( '2017-10-23' , '2017-10-23' , 'testingwh' ));\n> ```\n> \n> Copy\n> \n>\n\nWas this page helpful?\n\nYes No\n\n[Visit Snowflake](https://www.snowflake.com)\n\n[Join the conversation](https://community.snowflake.com/s/)\n\n[Develop with Snowflake](https://developers.snowflake.com)\n\nShare your feedback\n\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n\n[Get your own certification](https://learn.snowflake.com)\n\nOn this page\n\n1. Syntax\n2. Arguments\n3. Usage notes\n4. Output\n5. Examples\n\nLanguage: **English**\n\n* English\n* Fran\u00e7ais\n* Deutsch\n* \u65e5\u672c\u8a9e\n* \ud55c\uad6d\uc5b4\n* Portugu\u00eas"
      ]
    },
    {
      "url": "https://diggrowth.com/blogs/marketing-attribution/snowflake-cost-attribution/",
      "title": "Snowflake Cost Attribution For B2B: Assign, Track, And Optimize Usage",
      "excerpts": [
        "Section Title: How B2B Companies Can Master Snowflake Cost Attribution > How Snowflake Charges You: A Quick Recap\nContent:\nBefore you can attribute costs effectively, you need to understand how Snowflake structures its pricing. Snowflake operates on a pay-per-use model, where you are charged based on actual consumption across several key areas. This model is flexible and scalable, but it can quickly lead to unexpected costs if not properly managed.\nSection Title: How B2B Companies Can Master Snowflake Cost Attribution > ... > Key Cost Components in Snowflake:\nContent:\nCompute Costs:\nSnowflake charges for the compute power you use. This is based on **virtual warehouse size** and the **amount of time** the warehouse is active. Larger warehouses and longer-running queries will naturally incur higher costs. You are billed for compute resources even when the warehouse is idle, so proper management of warehouse scaling is crucial.\nStorage Costs:\nSnowflake charges for both **data storage** and **backup storage.** The cost is calculated based on the amount of data you store in your databases and stages.\n**A. Active Storage:** Data that you use regularly.\n**B. Historical Storage:** Data that is infrequently accessed but still stored in Snowflake.\nManaging the growth of your data and archiving older data efficiently can help control these costs.\nServerless Features:\nSection Title: How B2B Companies Can Master Snowflake Cost Attribution > ... > Key Cost Components in Snowflake:\nContent:\nSnowflake offers serverless features such as **Snowpipe** for continuous data loading and **Materialized Views** for optimized query performance. These services are charged based on the amount of data processed, so frequent use of Snowpipe or large data transformations can add up quickly.\nData Transfer Costs:\nSnowflake charges for data transfer between regions and cloud providers, as well as for loading and unloading data from external sources. If your workflows involve **frequent data movement** , this can become a significant portion of your monthly bill.\nCloud Services Costs:\n ... \nSection Title: ... > 2. Enforce Tagging Standards Across the Snowflake Environment\nContent:\nStandardized tagging is what enables precise cost segmentation. However, tagging only works if it is used consistently across users, teams, and workloads.\n ... \nSection Title: How B2B Companies Can Master Snowflake Cost Attribution > ... > Automate these processes using:\nContent:\nScheduled SQL jobs that extract data from Snowflake\u2019s ACCOUNT_USAGE views\nJoin logic to combine usage metrics with tags and user metadata\nData pipelines (e.g., dbt, Airflow) to normalize, enrich, and store attribution-ready tables\nUsage data should include compute credits consumed (WAREHOUSE_METERING_HISTORY), query-level metadata (QUERY_HISTORY), and storage trends (STORAGE_USAGE). Incorporating login and role history adds a layer of accountability for user behavior.\n ... \nSection Title: How B2B Companies Can Master Snowflake Cost Attribution > ... > Sample SQL: Warehouse Usage by Tag\nContent:\nThis query uses Snowflake\u2019s ACCOUNT_USAGE views to attribute warehouse credit consumption based on object tags.\n**SELECT**\nwmh.WAREHOUSE_NAME,\nwmh.START_TIME::DATE AS usage_date,\nwt.TAG_VALUE AS team,\nSUM(wmh.CREDITS_USED) AS total_credits\n**FROM**\nSNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_METERING_HISTORY wmh\n**JOIN**\nSNOWFLAKE.ACCOUNT_USAGE.TAG_REFERENCES wt\nON wmh.WAREHOUSE_ID = wt.OBJECT_ID\n**WHERE**\nwt.TAG_NAME = \u2018team\u2019\nAND wmh.START_TIME >= DATEADD(\u2018day\u2019, -30, CURRENT_DATE)\n**GROUP BY**\nwmh.WAREHOUSE_NAME,\nwmh.START_TIME::DATE,\nwt.TAG_VALUE\n**ORDER BY**\nusage_date DESC,\ntotal_credits DESC;\nThis query aggregates compute usage by warehouse and team, enabling a direct line of sight into how different business groups consume Snowflake resources."
      ]
    },
    {
      "url": "https://www.getbluesky.io/blog/how-to-use-query-cost-attribution",
      "title": "How to Use Query Cost Attribution to Optimize Snowflake Data Cloud Cost  Sep 09, 2022",
      "excerpts": [
        "Section Title: How to Use Query Cost Attribution to Optimize Snowflake Data Cloud Cost\nContent:\nVinoo Ganesh\n[@vinooganesh](https://www.linkedin.com/in/vinoo-ganesh/)\nMaking any kind of informed business decision requires a solid understanding of the data. Whether you\u2019re creating a new business initiative, developing a new product line, or understanding the current financials, having visibility into the underlying data is key. In the data cloud space, that generally means visibility into how efficiently your data cloud is performing from both a workload and cost perspective. As such, in any prospect meeting, our first question is always, \u201cwhat kind of visibility do you have into your workload?\u201d We\u2019re particularly curious about visibility across key dimensions such as individual users and business units.\nSection Title: How to Use Query Cost Attribution to Optimize Snowflake Data Cloud Cost\nContent:\nUnfortunately, the most common response we hear is, \u201cwe don\u2019t.\u201d This is unsurprising given the speed at which users ramp up their use of data cloud. Cost management and visibility are generally considered lower priority than generating business insights and value. Furthermore, the metrics provided by tools such as Snowflake (e.g. from [query history table](https://docs.snowflake.com/en/sql-reference/functions/query_history.html) ) are difficult to understand and operationalize. This combination makes attributing Snowflake data cloud cost a somewhat challenging and complex problem.\nSection Title: How to Use Query Cost Attribution to Optimize Snowflake Data Cloud Cost\nContent:\nLet\u2019s talk a bit about pricing. Snowflake\u2019s pricing model generally relies on users paying for the uptime of their virtual warehouses. When a virtual warehouse is up, users are charged at a consistent rate, regardless of whether the warehouse has queries actively running or whether the warehouse sits idle. The longer the warehouse is up, the more query credits are consumed.\nSection Title: How to Use Query Cost Attribution to Optimize Snowflake Data Cloud Cost > **The Cost of A Query**\nContent:\nOptimizing data cloud spend starts with understanding the components of that spend. On this journey, the most rational place to start is the queries themselves.\nSQL queries are powerful tools in the toolkit of data practitioners. Their flexibility and expressive nature, however, is also their downside. Queries that generate the same results can be written in a variety of ways - some significantly more performant than others. Scoring and providing additional cost insight into queries is a powerful tool in understanding the effectiveness of your queries.\nQuery cost attribution also enables you to calculate both aggregated Snowflake usage and cost per user and business unit. This provides both visibility and accountability on the organization and individual level so you can properly forecast and set reasonable quotas for future usage.\nSection Title: How to Use Query Cost Attribution to Optimize Snowflake Data Cloud Cost > **The Cost of A Query**\nContent:\n**When it comes to cost optimization, rallying and making the organization committed to the cause is half of the battle.** Some of our users refer to such visibility to cost efficiencies as the \u201cWall of Shame.\u201d At Bluesky we like to call it \u201cWorkload of Significance.\u201d :-)\n\u200d\nWe believe that query cost attribution and cost optimization efforts should be done in an ongoing and automated manner instead of one-off clean up exercises in finding and killing bad queries, so companies can achieve long term, sustainable growth.\n ... \nSection Title: ... > **Tackling The Challenges of \u201cPricing\u201d Queries**\nContent:\nQuery cost attribution is the first step in getting your data cloud under control. We\u2019d love to hear about your experience and help you along this journey.  Alternatively, join other Snowflake users today for a free trial of , a SaaS product that finds areas of cost optimization for your Snowflake workload.\n\u200d"
      ]
    }
  ],
  "usage": [
    {
      "name": "sku_search",
      "count": 1
    }
  ]
}

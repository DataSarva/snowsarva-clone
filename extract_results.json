{"extract_id":"extract_1168caa64cad453393f2bdecb8b96fb3","results":[{"url":"https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection","title":"Anomaly Detection (Snowflake ML Functions) | Snowflake Documentation","publish_date":"2020-01-01","excerpts":["Section Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > Overview [¶]( \"Link to this heading\")\nContent:\nAnomaly detection is the process of identifying outliers in data. The anomaly detection function lets you train a model\nto detect outliers in your time-series data. Outliers, which are data points that deviate from the expected range, can\nhave an outsized impact on statistics and models derived from your data. Spotting and removing outliers can therefore\nhelp improve the quality of your results.\nNote\nAnomaly Detection is part of Snowflake’s suite of business analysis tools powered by machine learning.\nDetecting outliers can also be useful in pinpointing the origin of problems or deviations in processes when there is no\nobvious cause. For example:\nDetermining when a problem started to occur with your logging pipeline.\nIdentifying the days when your Snowflake compute costs are higher than expected.\nAnomaly detection works with either single-series or multi-series data. Multi-series data represents multiple\nindependent threads of events. For example, if you have sales data for multiple stores, each store’s sales can be\nchecked separately by a single model based on the store identifier.\nThe data must include:\nA timestamp column.\nA target column representing some quantity of interest at each timestamp.\nNote\nSection Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > Overview [¶]( \"Link to this heading\")\nContent:\nIdeally, the training data for an Anomaly Detection model has time steps at equally spaced intervals (for example,\ndaily). However, model training can handle real-world data that has missing, duplicate, or misaligned time steps.\nFor more information, see [Dealing with real-world data in Time-Series Forecasting](preprocessing) .\nTo detect outliers in time-series data, use the Snowflake built-in class [ANOMALY_DETECTION (SNOWFLAKE.ML)](../../sql-reference/classes/anomaly_detection) ,\nand follow these steps:\n[Create an anomaly detection object](../../sql-reference/classes/anomaly-detection/commands/create-anomaly-detection.html) ,\npassing in a reference to the training data.This object fits a model to the training data that you provide. The model is a schema-level object.\nUsing this anomaly detection model object, call the [<model_name>!DETECT_ANOMALIES](../../sql-reference/classes/anomaly-detection/methods/detect_anomalies) method to\ndetect anomalies, passing in a reference to the data to analyze.The method uses the model to identify outliers in the data.\nAnomaly detection is closely related to [Forecasting](forecasting) . An anomaly detection model\nproduces a forecast for the same time period as the data you’re checking for anomalies, then compares the actual data to\nthe forecast to identify outliers.\nImportant\n ... \nSection Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > ... > Limitations [¶]( \"Link to this heading\")\nContent:\nThis feature only detects anomalies in the test data; it cannot detect anomalies in the training data. Furthermore,\ntimestamps in the test data must all be greater than timestamps in the training data. Ensure that the training data\ncovers a typical period free of actual outliers, or label known outliers in a Boolean column. You cannot clone models or share models across roles or accounts. When cloning a schema or database, model objects are skipped. You cannot [replicate](../account-replication-intro) an instance of the ANOMALY_DETECTION\nclass.\n ... \nSection Title: ... > Training, Using, Viewing, Deleting, and Updating Models [¶]( \"Link to this heading\")\nContent:\nUse [CREATE SNOWFLAKE.ML.ANOMALY_DETECTION](../../sql-reference/classes/anomaly-detection/commands/create-anomaly-detection.html) to create and train a model. The model is trained on the dataset you\nprovide.\n```\nCREATE SNOWFLAKE.ML.ANOMALY_DETECTION mydetector (...);\n```\nCopy\nSee [ANOMALY_DETECTION (SNOWFLAKE.ML)](../../sql-reference/classes/anomaly_detection) for complete details about the SNOWFLAKE.ML.ANOMALY_DETECTION\nconstructor. For examples of creating a model, see [Detecting Anomalies]() .\nNote\nSNOWFLAKE.ML.ANOMALY_DETECTION runs using limited privileges, so by default it does not have access to your data. You must\ntherefore pass tables and views as [references](../../developer-guide/stored-procedure/stored-procedures-calling-references) , which pass along the\ncaller’s privileges. You can also provide a [query reference](../../developer-guide/stored-procedure/stored-procedures-calling-references.html) instead of a\nreference to a table or a view.\nTo create this reference, you can use the [TABLE keyword](../../sql-reference/snowflake-db-classes.html) with the table name, view name,\nor query, or you can call the [SYSTEM$REFERENCE](../../sql-reference/functions/system_reference) or [SYSTEM$QUERY_REFERENCE](../../sql-reference/functions/system_query_reference) function.\nSection Title: ... > Training, Using, Viewing, Deleting, and Updating Models [¶]( \"Link to this heading\")\nContent:\nTo detect anomalies, call the model’s [<model_name>!DETECT_ANOMALIES](../../sql-reference/classes/anomaly-detection/methods/detect_anomalies.html) method:\n```\nCALL mydetector ! DETECT_ANOMALIES (...);\n```\nCopy\nTo select columns from the tabular output of the method, you can [call the method in the FROM clause](../../sql-reference/snowflake-db-classes.html) :\n```\nSELECT ts , forecast FROM TABLE ( mydetector ! DETECT_ANOMALIES (...));\n```\nCopy\nTo view a list of your models, use the [SHOW SNOWFLAKE.ML.ANOMALY_DETECTION](../../sql-reference/classes/anomaly-detection/commands/show-anomaly-detection.html) command:\n```\nSHOW SNOWFLAKE.ML.ANOMALY_DETECTION ;\n```\nCopy\nTo remove a model, use the [DROP SNOWFLAKE.ML.ANOMALY_DETECTION](../../sql-reference/classes/anomaly-detection/commands/drop-anomaly-detection.html) command:\n```\nDROP SNOWFLAKE.ML.ANOMALY_DETECTION < name >;\n```\nCopy\nTo update a model, delete it and train a new one. Models are immutable and cannot be updated in place.\n ... \nSection Title: ... > Detecting Anomalies for a Single Time Series (Unsupervised) [¶]( \"Link to this heading\")\nContent:\nTo detect anomalies in your data:\nTrain an anomaly detection model using historical data.\nUse the trained anomaly detection model to detect anomalies in historical or projected data. The timestamps in the test data\nmust chronologically follow the timestamps in the training data. You need at least 2 data points to train a model, at least\n12 for non-naive results, and at least 60 for non-linear results.\nSee [ANOMALY_DETECTION (SNOWFLAKE.ML)](../../sql-reference/classes/anomaly_detection) for information on the parameters used in creating and using a model.\nSection Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > ... > Training an Anomaly Detection Model [¶]( \"Link to this heading\")\nContent:\nTo create an anomaly detection model object, execute the [CREATE SNOWFLAKE.ML.ANOMALY_DETECTION](../../sql-reference/classes/anomaly-detection/commands/create-anomaly-detection.html) command.\nFor example, suppose that you want to analyze the sales for jackets in the store with the `store_id` of 1:\nSection Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > ... > Training an Anomaly Detection Model [¶]( \"Link to this heading\")\nContent:\nCreate a view or design a query that returns the data for training the model for anomaly detection.For this example, execute the [CREATE VIEW](../../sql-reference/sql/create-view) command to create a view named `view_with_training_data` that contains the date and sales information:Copy\nCreate an anomaly detection object, and train its model on the data in that view.For this example, execute the [CREATE SNOWFLAKE.ML.ANOMALY_DETECTION](../../sql-reference/classes/anomaly-detection/commands/create-anomaly-detection.html) command to create an anomaly detection object named `basic_model` . Pass in the following arguments:CopyThis example passes in a reference to a view as the INPUT_DATA argument. The example [uses the TABLE keyword to create the reference](../../developer-guide/stored-procedure/stored-procedures-calling-references.html) . As an alternative, you can call [SYSTEM$REFERENCE](../../sql-reference/functions/system_reference) to create the reference.The purpose of the label column is to tell the model which rows are known anomalies. Because this example uses\nunsupervised training, you do not need to use the label column.\n ... \nSection Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > ... > Including Additional Columns for Analysis [¶]( \"Link to this heading\")\nContent:\nYou can include additional columns in the data (for example, `temperature` , `weather` , `is_black_friday` ) in the data for training\nand analysis, if these columns can help you improve the identification of true anomalies.\nTo include new columns for analysis:\nFor the training data, create a view or design a query that includes the new columns, and create a new anomaly detection object,\npassing in a reference to that view or query.\nFor the data to analyze, create a view or design a query that includes the new columns, and pass a reference to that\nview or query to the [<model_name>!DETECT_ANOMALIES](../../sql-reference/classes/anomaly-detection/methods/detect_anomalies.html) method.\nThe anomaly detection model detects and uses the additional columns automatically.\nNote\nYou must provide a view or query with the same set of additional columns when executing the [CREATE SNOWFLAKE.ML.ANOMALY_DETECTION](../../sql-reference/classes/anomaly-detection/commands/create-anomaly-detection.html) command and when calling the [<model_name>!DETECT_ANOMALIES](../../sql-reference/classes/anomaly-detection/methods/detect_anomalies.html) method. If there is a mismatch between the columns in the training data\npassed to the command and the columns in the data for analysis passed to the function, an error occurs.\n ... \nSection Title: ... > Automate Anomaly Detection with Snowflake Tasks and Alerts [¶]( \"Link to this heading\")\nContent:\nYou can create an automated anomaly detection pipeline, both for retraining the model and for monitoring your data for anomalies, by using Anomaly Detection functions within Snowflake Tasks or Alerts.\n[Recurring Training with a Snowflake Task]()\n[Monitoring with a Snowflake Task]()\n[Monitoring with a Snowflake Alert]()\n ... \nSection Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > ... > Monitoring with a Snowflake Alert [¶]( \"Link to this heading\")\nContent:\nYou can also use [Snowflake Alerts](../alerts) to monitor your data at a given frequency and send you\nemail with detected anomalies. The following statements create an alert that detects anomalies every minute. First you\ndefine a [stored procedure](../../developer-guide/stored-procedure/stored-procedures-overview) to detect anomalies, then create an alert\nthat uses that stored procedure.\nNote\nYou must set up email integration to send mail from a stored procedure; see [Notifications in Snowflake](../notifications/about-notifications) .\nSection Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > ... > Monitoring with a Snowflake Alert [¶]( \"Link to this heading\")\nContent:\n```\nCREATE OR REPLACE PROCEDURE extract_anomalies () \n  RETURNS TABLE () \n  LANGUAGE SQL \n  AS \n  $$ \n    BEGIN \n      let res RESULTSET := ( SELECT * FROM TABLE ( \n        model_trained_with_labeled_data ! DETECT_ANOMALIES ( \n          INPUT_DATA => TABLE ( view_with_data_to_analyze ), \n          TIMESTAMP_COLNAME => 'date' , \n          TARGET_COLNAME => 'sales' , \n          CONFIG_OBJECT => { 'prediction_interval' :0 . 99 } \n        )) \n        WHERE is_anomaly = TRUE \n      ); \n      RETURN TABLE ( res ); \n    END ; \n  $$ \n  ; \n\n CREATE OR REPLACE ALERT sample_sales_alert \n WAREHOUSE = < your_warehouse_name > \n SCHEDULE = '1 MINUTE' \n IF ( EXISTS ( CALL extract_anomalies ())) \n THEN \n CALL SYSTEM$SEND_EMAIL ( \n  'sales_email_alert' , \n  'your_email@snowflake.com' , \n  'Anomalous Sales Data Detected in data stream' , \n  CONCAT ( \n    'Anomalous Sales Data Detected in data stream \\n' , \n    'Value outside of prediction interval detected in the most recent run at ' , \n    current_timestamp ( 1 ) \n  ));\n```\nCopy\nTo start or resume the alert, execute the [ALTER ALERT … RESUME](../../sql-reference/sql/alter-alert) command:\n```\nALTER ALERT sample_sales_alert RESUME ;\n```\nCopy\nTo pause the alert, execute the [ALTER ALERT … SUSPEND](../../sql-reference/sql/alter-alert) command:\n```\nALTER ALERT sample_sales_alert SUSPEND ;\n```\nCopy\n ... \nSection Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > ... > Example [¶]( \"Link to this heading\")\nContent:\n0 , 'B' ), \n  ( TO_TIMESTAMP_NTZ ( '2020-01-06' ), 12 . 0 , 'B' ); \n\n CREATE SNOWFLAKE.ML.ANOMALY_DETECTION model ( \n  INPUT_DATA => TABLE ( SELECT date , sales , series FROM t_error ), \n  SERIES_COLNAME => 'series' , \n  TIMESTAMP_COLNAME => 'date' , \n  TARGET_COLNAME => 'sales' , \n  LABEL_COLNAME => '' , \n  CONFIG_OBJECT => { 'ON_ERROR' : 'SKIP' } \n ); \n\n CALL model ! SHOW_TRAINING_LOGS ();\n```\n ... \nSection Title: Anomaly Detection (Snowflake ML Functions) [¶]( \"Link to this heading\") > Cost Considerations [¶]( \"Link to this heading\")\nContent:\nFor details on costs for using ML functions, see [Cost Considerations](../../guides-overview-ml-functions.html) in the ML functions overview.\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\n[Share your feedback](/feedback)\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\n[Privacy Notice](https://www.snowflake.com/privacy-policy/) [Site Terms](https://www.snowflake.com/legal/snowflake-site-terms/) Cookies Settings © 2026 Snowflake, Inc. All Rights Reserved.\nLanguage: **English**\n[English](/en/user-guide/ml-functions/anomaly-detection)\n[Français](/fr/user-guide/ml-functions/anomaly-detection)\n[Deutsch](/de/user-guide/ml-functions/anomaly-detection)\n[日本語](/ja/user-guide/ml-functions/anomaly-detection)\n[한국어](/ko/user-guide/ml-functions/anomaly-detection)\n[Português](/pt/user-guide/ml-functions/anomaly-detection)"],"full_content":null},{"url":"https://docs.snowflake.com/en/user-guide/cost-insights","title":"Using cost insights to save | Snowflake Documentation","publish_date":null,"excerpts":["[DOCUMENTATION](https://docs.snowflake.com)\n/\n[Get started](/en/user-guide-getting-started)\n[Guides](/en/guides)\n[Developer](/en/developer)\n[Reference](/en/reference)\n[Release notes](/en/release-notes/overview)\n[Tutorials](/en/tutorials)\n[Status](https://status.snowflake.com)\n[Guides](/en/guides) [Cost & Billing](/en/guides-overview-cost) [Optimization](/en/user-guide/cost-optimize) Cost insights\nSection Title: Using cost insights to save [¶]( \"Link to this heading\")\nContent:\nSnowflake provides cost insights that identify opportunities to optimize Snowflake for cost within a particular account. These insights are\ncalculated and refreshed weekly.\nEach insight indicates how many credits or terabytes could be saved by optimizing Snowflake.\nTo access the Cost Insights tile:\nSign in to [Snowsight](ui-snowsight-gs.html) .\nSwitch to a role with [access to cost-related features](cost-access-control) .\nIn the navigation menu, select Admin » Cost management .\nSelect the Account Overview tab.\nFind the Cost insights tile.\nEach of the following insights includes suggestions on how to optimize your spend.\n[Insight: Rarely used tables with automatic clustering]()\n[Insight: Rarely used materialized views]()\n[Insight: Rarely used search optimization paths]()\n[Insight: Large tables that are never queried]()\n[Insight: Tables over 100 GB from which data is written but not read]()\n[Insight: Short-lived permanent tables]()\n[Insight: Inefficient usage of multi-cluster warehouses]()\nInsight: Rarely used tables with automatic clustering\nThis insight identifies tables with [automatic clustering](tables-auto-reclustering) that are queried fewer than 100\ntimes per week by this account.\nSection Title: Using cost insights to save [¶]( \"Link to this heading\")\nContent:\nEnabling automatic clustering for a table can significantly improve the performance of queries against that table. However, as the table\nchanges, Snowflake must use serverless compute resources to keep it in a well-clustered state. If the number of queries executed against\nthe table is minimal, the cost incurred might not justify the performance improvements.\n**Recommendation:** Consider disabling automatic clustering on these tables. Before you turn off automatic clustering, determine whether the table exists\nsolely for disaster recovery purposes or for use by other Snowflake accounts through data sharing, which might explain why it isn’t\naccessed frequently.\nFor example, to disable automatic clustering for a table named `t1` , execute the following command:\n```\nALTER TABLE t1 SUSPEND RECLUSTER ;\n```\nCopy\nInsight: Rarely used materialized views\nThis insight identifies [materialized views](views-materialized) that are queried fewer than 10 times per week by this\naccount.\nCreating a materialized view can significantly improve performance for certain query patterns. However, materialized views incur\nadditional storage costs as well as serverless compute costs associated with keeping the materialized view up to date with new data. If\nthe number of queries executed against the materialized view is minimal, the cost incurred might not justify the performance improvements.\nSection Title: Using cost insights to save [¶]( \"Link to this heading\")\nContent:\n**Recommendation:** Consider removing or suspending updates to the materialized views. Before you drop a materialized view, determine whether the table exists\nsolely for disaster recovery purposes or for use by other Snowflake accounts through data sharing, which might explain why it isn’t\naccessed frequently.\nFor example, to delete a materialized view named `mv1` , execute the following command:\n```\nDROP MATERIALIZED VIEW mv1 ;\n```\nCopy\nInsight: Rarely used search optimization paths\nThis insight identifies [search optimization](search-optimization-service) access paths that are used fewer than\n10 times per week by this account.\nSearch optimization uses search access paths to improve the performance of certain types of point lookup and analytical queries. Adding\nsearch optimization to a table can significantly improve performance for these queries. However, search optimization incurs additional\nstorage costs as well as serverless compute costs associated with keeping that storage up to date. If the number of queries that use the\nsearch access path created by search optimization is minimal, the cost incurred might not justify the performance improvements.\nSection Title: Using cost insights to save [¶]( \"Link to this heading\")\nContent:\n**Recommendation:** Consider removing search optimization from the table. Before you remove search optimization, determine whether the table exists solely\nfor disaster recovery purposes or for use by other Snowflake accounts through data sharing, which might explain why it isn’t accessed\nfrequently.\nFor example, to completely remove search optimization from a table named `t1` , execute the following command:\n```\nALTER TABLE t1 DROP SEARCH OPTIMIZATION ;\n```\nCopy\nInsight: Large tables that are never queried\nThis insight identifies large tables that have not been queried in the last week by this account.\n**Recommendation:** Consider deleting unused tables, which can reduce storage costs without impacting any workloads. Before you drop the tables, determine\nwhether the table exists solely for disaster recovery purposes or for use by other Snowflake accounts through data sharing, which might\nexplain why it isn’t accessed frequently.\nFor example, to delete a table name `t1` , execute the following command:\n```\nDROP TABLE t1 ;\n```\nCopy\nInsight: Tables over 100 GB from which data is written but not read\nThis insight identifies tables where data is written but never read by this account.\n ... \nSection Title: Using cost insights to save [¶]( \"Link to this heading\")\nContent:\nInsight: Inefficient usage of multi-cluster warehouses\nThis insight identifies when you have the minimum and maximum cluster count set to the same value for a multi-cluster warehouse, which\nprevents the warehouse from scaling up or down to respond to demand. If your multi-cluster warehouse can scale down during periods of\nlighter usage, it can save credits.\n**Recommendation:** Consider lowering the minimum cluster count to allow the multi-cluster warehouse to scale down during periods of\nlighter usage.\nFor example, to set the minimum cluster count to 1 for a warehouse named `wh1` , execute the following command:\n```\nALTER WAREHOUSE wh1 SET MIN_CLUSTER_COUNT = 1 ;\n```\nCopy\nWas this page helpful?\nYes No\n[Visit Snowflake](https://www.snowflake.com)\n[Join the conversation](https://community.snowflake.com/s/)\n[Develop with Snowflake](https://developers.snowflake.com)\n[Share your feedback](/feedback)\n[Read the latest on our blog](https://www.snowflake.com/blog/)\n[Get your own certification](https://learn.snowflake.com)\n[Privacy Notice](https://www.snowflake.com/privacy-policy/) [Site Terms](https://www.snowflake.com/legal/snowflake-site-terms/) Cookies Settings © 2026 Snowflake, Inc. All Rights Reserved.\nRelated content\n[Managing cost in Snowflake](/user-guide/cost-management-overview)\n[Optimizing cost](/user-guide/cost-optimize)\nLanguage: **English**\nSection Title: Using cost insights to save [¶]( \"Link to this heading\")\nContent:\n[English](/en/user-guide/cost-insights)\n[Français](/fr/user-guide/cost-insights)\n[Deutsch](/de/user-guide/cost-insights)\n[日本語](/ja/user-guide/cost-insights)\n[한국어](/ko/user-guide/cost-insights)\n[Português](/pt/user-guide/cost-insights)"],"full_content":null},{"url":"https://medium.com/snowflake/machine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee","title":"Machine Learning-Based Alerts for Snowflake FinOps | by Piotr Paczewski | Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science | Medium","publish_date":"2023-12-04","excerpts":["[Sitemap](/sitemap/sitemap.xml)\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fsnowflake%2Fmachine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[](/?source=post_page---top_nav_layout_nav-----------------------------------------)\n[Write](/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n[Search](/search?source=post_page---top_nav_layout_nav-----------------------------------------)\nSign up\n[Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2Fsnowflake%2Fmachine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n[## Snowflake Builders Blog: Data Engineers, App Developers, AI, & Data Science](https://medium.com/snowflake?source=post_page---publication_nav-34b6daafc07-8ec640fb1cee---------------------------------------)\n·\n[](https://medium.com/snowflake?source=post_page---post_publication_sidebar-34b6daafc07-8ec640fb1cee---------------------------------------)\nBest practices, tips & tricks from Snowflake experts and community\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps\nContent:\n[](/@piotrpaczewski?source=post_page---byline--8ec640fb1cee---------------------------------------)\n[Piotr Paczewski](/@piotrpaczewski?source=post_page---byline--8ec640fb1cee---------------------------------------)\n5 min read\n·\nDec 4, 2023\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fsnowflake%2F8ec640fb1cee&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fsnowflake%2Fmachine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee&user=Piotr+Paczewski&userId=f82630acd7c5&source=---header_actions--8ec640fb1cee---------------------clap_footer------------------)\n--\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ec640fb1cee&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fsnowflake%2Fmachine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee&source=---header_actions--8ec640fb1cee---------------------bookmark_footer------------------)\nListen\nShare\nPress enter or click to view image in full size\nOverview of GenAI and LLM capabilities in Snowflake\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > Overview\nContent:\nCost management is a key component of every successful cloud strategy. Snowflake provides a range of built-in tools to effectively manage cost, including:\n[Cost exploration using Snowsight & account usage](https://docs.snowflake.com/en/user-guide/cost-exploring-overall)\n[Resource monitors](https://docs.snowflake.com/en/user-guide/resource-monitors)\n[Alerts & notifications](https://docs.snowflake.com/en/guides-overview-alerts)\n[Budgets](https://docs.snowflake.com/en/user-guide/budgets)\n[Object tagging](https://docs.snowflake.com/en/user-guide/object-tagging)\nThis article will demonstrate how you can leverage Snowflake alerts to automatically receive email notifications for anomalies detected in virtual warehouse compute usage using [Snowflake Cortex ML-Based Functions](https://docs.snowflake.com/en/guides-overview-ml-powered-functions) .\nThe cost of using Snowflake platform can be broken down into:\nCompute resources (virtual warehouse compute, serverless compute, cloud service compute)\nStorage\nData transfer\nVirtual warehouse compute usage can be monitored using [warehouse_metering_history](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history) view in [account usage schema](https://docs.snowflake.com/en/sql-reference/account-usage) . It shows hourly credit usage for virtual warehouses in your account within the last 365 days.\nPress enter or click to view image in full size\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > Overview\nContent:\nSample result from querying [warehouse_metering_history](https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history)\nTo automatically detect anomalies in virtual warehouse compute usage, Snowflake users can leverage machine learning. Snowflake Cortex ML-based functions simplify the complexities associated with using ML models and enable organizations to quickly gain insights into their data.\nTo train a machine learning using Snowflake Cortex ML-based functions, let’s start with splitting the data into a training dataset and a test dataset. Data from the last 12 months of virtual warehouse compute usage will be used, with the test dataset comprising the most recent 2 months and the remaining 10 months forming the training dataset.\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > Overview > Build ML model using Snowflake Cortex ML-based functions\nContent:\nStep 1: Create the training dataset:\n```\ncreate or replace view warehouse_compute_usage_train as  \n  select  \n    to_timestamp_ntz(to_date(start_time)) as timestamp,  \n    sum(credits_used_compute) as credits_used  \n  from snowflake.account_usage.warehouse_metering_history  \n  where timestamp between dateadd(day,-365,current_date()) and dateadd(day,-61,current_date())  \n  group by all;\n```\nStep 2: Create the test dataset:\n```\ncreate or replace view warehouse_compute_usage_test as  \n  select  \n    to_timestamp_ntz(to_date(start_time)) as timestamp,  \n    sum(credits_used_compute) as credits_used  \n  from snowflake.account_usage.warehouse_metering_history  \n  where timestamp between dateadd(day,-60,current_date()) and current_date()  \n  group by all;\n```\nStep 3: Train an anomaly detection model using Snowflake Cortex ML-based functions:\n```\ncreate or replace snowflake.ml.anomaly_detection warehouse_usage_analysis(  \n  input_data => system$reference('view', 'warehouse_compute_usage_train'),  \n  timestamp_colname => 'timestamp',  \n  target_colname => 'credits_used',  \n  label_colname => ''  \n  );\n```\nStep 4: Run model inference using the trained model:\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > Overview > Build ML model using Snowflake Cortex ML-based functions\nContent:\n```\ncall warehouse_usage_analysis!detect_anomalies(  \n  input_data => system$reference('view','warehouse_compute_usage_test')  \n  , timestamp_colname => 'timestamp'  \n  , target_colname => 'credits_used'  \n  );\n```\nPress enter or click to view image in full size\nResults of model inference\nStep 5: Visualize model results using Snowsight:\nPress enter or click to view image in full size\nVisualization of ML model inference using selected parameters.\nThe forecasted values are indicated by the red line, while the lower bounds and upper bounds of the forecasts are respectively represented by light blue and yellow lines.\nThe actual (observed) values are displayed using a dark blue line. Every point on this line that is not within the lower and upper bounds of the forecasts is marked an anomaly.\nBy running the SQL below, I am able to identify that they are 6anomalies detected in the test dataset.\n```\ncreate table warehouse_usage_anomalies   \n  as select * from table(result_scan(last_query_id()));  \n  \nselect * from warehouse_usage_anomalies   \n  where is_anomaly = true;\n```\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > Overview > Build ML model using Snowflake Cortex ML-based functions\nContent:\nIn the code used to call the model inference, the *prediction_interval* parameter value was not specified, therefore, the default value 0.99 was used. To mark more observations as anomalies, reduce the value of *prediction_interval* and set it to, for instance, 0.9. On the other hand, to mark fewer observations as anomalies, the value of *prediction_interval* parameter should be increased.\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > ... > Create email notifications for newly detected anomalies using Snowflake Alerts and Tasks\nContent:\nTo create automatic email notifications for new anomalies detected in virtual warehouse compute usage Snowflake Tasks & Alerts can be used.\nStep 1: Create a task to retrain ML model on a weekly basis at 5 AM every Sunday LA time:\n```\ncreate or replace task train_warehouse_usage_anomaly_task  \nwarehouse = demo_wh  \nschedule = 'USING CRON 0 5 * * 0 America/Los_Angeles'  \nas  \nexecute immediate  \n$$  \nbegin  \n  create or replace snowflake.ml.anomaly_detection warehouse_usage_analysis(  \n    input_data => system$reference('view', 'warehouse_compute_usage_train'),  \n    timestamp_colname => 'timestamp',  \n    target_colname => 'credits_used',  \n    label_colname => ''  \n    );  \nend;  \n$$;\n```\nStep 2: Create a task to call the anomaly detection model on a daily basis at 7 AM LA time and insert the result into warehouse_usage_anomalies table:\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > ... > Create email notifications for newly detected anomalies using Snowflake Alerts and Tasks\nContent:\n```\ncreate or replace task inference_warehouse_usage_anomaly_task  \nwarehouse = demo_wh  \nschedule = 'USING CRON 0 7 * * * America/Los_Angeles'   \nas  \nexecute immediate  \n$$  \nbegin  \n  call warehouse_usage_analysis!detect_anomalies(  \n    input_data => system$reference('view','warehouse_compute_usage_test')  \n    , timestamp_colname => 'timestamp'  \n    , target_colname => 'credits_used'  \n    );  \ninsert into warehouse_usage_anomalies  \n  select * from table(result_scan(last_query_id()));  \nend;  \n$$;\n```\nStep 3: Set up an alert to check every day at 8 AM LA time if any new anomalies have been detected in warehouse compute usage:\n```\ncreate or replace alert warehouse_usage_anomaly_alert  \n  warehouse = demo_wh  \n  schedule = 'USING CRON 0 8 * * * America/Los_Angeles'  \n  if (exists (select * from warehouse_usage_anomalies where is_anomaly=True and ts > dateadd('day',-1,current_timestamp())))  \n  then  \n  call system$send_email(  \n    'warehouse_email_alert',  \n    'test@domain.com',  \n    'Warehouse compute usage anomaly detected',  \n    concat(  \n      'Anomaly detected in the warehouse compute usage. ',  \n      'Value outside of confidence interval detected.'  \n    )  \n  );\n```\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > ... > Create email notifications for newly detected anomalies using Snowflake Alerts and Tasks\nContent:\nSimilarly to tasks, alerts are created in suspended state and need to be resumed in order to start running. After resuming the alert, this is the email I have received after some time:\nPress enter or click to view image in full size\nSample email notification received\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > Overview > Final note\nContent:\nAnomaly detection using Snowflake Cortex ML-based functions works with both single-series and multi-series data. While this article focuses on single-series data, it’s important to note that it’s also possible to use anomaly detection functions with multi-series data to build separate, independent ML models, for example, for each virtual warehouse object.\nThere are various approaches that can be taken to anomaly detection in Snowflake. The decision to implement ML-based anomaly detection should be assessed on case-by-case basis. In certain scenarios, simple approaches, such as threshold-based anomaly detection, might be more practical than implementing ML-based methods.\n*I am currently a Snowflake Solutions Consultant at Snowflake. Opinions expressed in this post are solely my own and do not represent the views or opinions of my employer.*\n[Snowflake](/tag/snowflake?source=post_page-----8ec640fb1cee---------------------------------------)\n[Mlops](/tag/mlops?source=post_page-----8ec640fb1cee---------------------------------------)\n[Finops](/tag/finops?source=post_page-----8ec640fb1cee---------------------------------------)\n[Cost Optimization](/tag/cost-optimization?source=post_page-----8ec640fb1cee---------------------------------------)\n[AI](/tag/ai?source=post_page-----8ec640fb1cee---------------------------------------)\nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > Overview > Final note\nContent:\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fsnowflake%2F8ec640fb1cee&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fsnowflake%2Fmachine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee&user=Piotr+Paczewski&userId=f82630acd7c5&source=---footer_actions--8ec640fb1cee---------------------clap_footer------------------)\n-- [](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fsnowflake%2F8ec640fb1cee&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fsnowflake%2Fmachine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee&user=Piotr+Paczewski&userId=f82630acd7c5&source=---footer_actions--8ec640fb1cee---------------------clap_footer------------------)\n--\n[](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ec640fb1cee&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fsnowflake%2Fmachine-learning-based-alerts-for-snowflake-finops-8ec640fb1cee&source=---footer_actions--8ec640fb1cee---------------------bookmark_footer------------------)\n[](https://medium.com/snowflake?source=post_page---post_publication_info--8ec640fb1cee---------------------------------------)\n[](https://medium.com/snowflake?source=post_page---post_publication_info--8ec640fb1cee---------------------------------------)\n ... \nSection Title: Machine Learning-Based Alerts for Snowflake FinOps > No responses yet\nContent:\n[](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page---post_responses--8ec640fb1cee---------------------------------------)"],"full_content":null},{"url":"https://www.snowflake.com/en/developers/guides/well-architected-framework-cost-optimization-and-finops/","title":"Cost Optimization","publish_date":null,"excerpts":["Section Title: Cost Optimization > Recommendations\nContent:\n**Establish a consistent and granular cost attribution strategy:** Implement robust and organizationally consistent tagging and\nlabeling strategies across all resources (storage objects,\nwarehouses, accounts, queries) to accurately allocate costs to\nspecific teams, products, or initiatives. **Embed cost accountability into your organization's DNA:** Implement a feedback system to transparently show or charge back\ncloud costs to relevant business units or teams, increasing\naccountability. **Deliver clear, historical consumption insights:** Utilize\nconsistent in-tool visualizations or custom dashboards to monitor\nconsumption and contextualize spend on the platform with unit\neconomics. **Investigate anomalous consumption activity:** Review anomaly\ndetection to identify unforeseen cost anomalies and investigate\ncause and effect trends. **Control**\n**Proactively monitor all platform usage:** Define and enforce\nbudgets for projects and services, setting soft quotas to limit\nresource consumption and prevent runaway spending. **Forecast consumption based on business needs:** Establish a\nforecast process to project future spend needs based on business and\ntechnical needs. **Enforce cost guardrails for organizational resources:** Set up\nautomated checks (e.g., Tasks, query insights) and resource\nguardrails (e.g., warehouse timeout, storage policies, resource\nmonitors) to identify unusual usage patterns and potential\noverspending as they occur.\n ... \nSection Title: Cost Optimization > Visibility > Overview > Recommendations\nContent:\nImplementing a robust FinOps visibility framework in Snowflake,\nsupported by cross-functional collaboration, enables each business\nfunction to access timely and relevant usage and cost data. This\nempowers them to understand the business impact of their consumption and\ntake prompt action when anomalies arise. To meet this vision, consider\nthe following recommendations based on industry best practices and\nSnowflake's capabilities:\n ... \nSection Title: Cost Optimization > Visibility > Overview > Deliver clear historical consumption insights\nContent:\nThe most mature FinOps customers are those who programmatically and\nstrategically drive consumption insights across the business. This\ninvolves three core elements:\n**Platform cost tracking:** Pinpoint specific Snowflake credit\nconsumption (compute, storage, serverless, AI, and data transfer),\nusage patterns, and efficiency opportunities to deconstruct credit\nusage, understand drivers, identify anomalies, and (eventually) drive\nforecasting operations.\n**Normalization of consumption:** Once consumption has been attributed\nand aggregated to meaningful levels, normalizing it against relevant\nbusiness and technical metrics contextualizes it in relation to\norganizational goals. It allows for the natural growth and seasonality\nof platform usage to be put into context with business and technical\ndemand drivers.\n**Clear reporting:** Presenting Snowflake cost data in an\nunderstandable format for various stakeholders is vital. This enables\nbudgeting, forecasting, KPIs, and business value metrics directly tied\nto Snowflake credit consumption.\n**Track usage data for all platform resources**\n ... \nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nCost Anomaly Detection is a critical component of visibility that\nleverages machine learning to continuously monitor credit consumption\nagainst historical spending patterns, automatically flagging significant\ndeviations from the established baseline. This proactive monitoring is\nessential for preventing budget overruns and identifying inefficiencies,\nshifting the organization from a reactive to a proactive cost management\nposture to mitigate financial risk. As a best practice, you should\ninitially review anomaly detection on the entire account to gain a broad\nview, then dive deeper into a more granular review for individual\nhigh-spend warehouses. [This approach](https://docs.snowflake.com/en/user-guide/cost-anomalies) allows for more targeted analysis and assigns clear ownership for\ninvestigating any flagged anomalies. There are several methods for\nanomaly detection supported by Snowflake:\n**Cost Anomalies in Snowsight**\nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nSnowsight, Snowflake's primary web interface, offers a dedicated Cost\nManagement UI that allows users to visually identify and analyze the\ndetails of any detected cost anomaly. The importance of this intuitive\nvisual interface lies in its ability to make complex cost data\naccessible to a wide range of stakeholders, enabling rapid root cause\nanalysis by correlating a cost spike with specific query history or user\nactivity. One of the tabs in this UI is the Cost Anomaly Detection tab,\nwhich enables you to view cost anomalies at the organization or account\nlevel and explore the top warehouses or accounts driving this change. To\nfoster a culture of cost awareness and accountability, it is a best\npractice to ensure there is an owner for an anomaly detected in the\naccount and set up a [notification (via email)](https://docs.snowflake.com/en/user-guide/cost-anomalies-ui) in the UI itself to ensure that cost anomalies are quickly and\naccurately investigated.\n**Programmatic Cost Anomaly Detection**\nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nFor deeper integration and automation, organizations can review [anomalies programmatically](https://docs.snowflake.com/en/user-guide/cost-anomalies-class) using the SQL functions and views available within the SNOWFLAKE.LOCAL\nschema. This approach is important for enabling automation and\nscalability, allowing cost governance to be embedded directly into\noperational workflows, such as feeding anomaly data into third-party\nobservability tools or triggering automated incident response playbooks.\nA key best practice is to utilize this programmatic access to build\ncustom reports and dashboards that align with specific financial\nreporting needs and to create advanced, automated alerting mechanisms\nthat pipe anomaly data into established operational channels, such as\nSlack or PagerDuty.\n**Custom Anomaly Detection & Notification**\nSection Title: Cost Optimization > Visibility > Overview > Investigate anomalous consumption activity\nContent:\nAlthough anomalies are detected at the account and organization level,\nif you desire to detect anomalies at lower levels (e.g. warehouse or\ntable), it is recommended to leverage Snowflake’s [Anomaly Detection](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) ML class and pair it with a Snowflake [alert](https://docs.snowflake.com/en/user-guide/ml-functions/anomaly-detection) to notify owners of more granular anomalies that occur within the\necosystem. This ensures all levels of Snowflake cost can be monitored in\na proactive and effective way. As a best practice, [notifications](https://docs.snowflake.com/en/user-guide/notifications/about-notifications) should be configured for a targeted distribution list that includes the\nbudget owner, the FinOps team, and the technical lead responsible for\nthe associated Snowflake resources, ensuring all stakeholders are\nimmediately aware of a potential cost overrun and can coordinate a swift\nresponse.\n ... \nSection Title: Cost Optimization > Control > Overview > Proactively monitor all platform usage\nContent:\nCategorizing costs is fundamental for granular budget management. You\ncan establish budgets based on the [account](https://docs.snowflake.com/en/user-guide/budgets/account-budget) or create [custom categories](https://docs.snowflake.com/en/user-guide/budgets/custom-budget) using Object Tags. Custom tags, such as those for a data product or cost\ncenter, are critical for accurately apportioning costs across different\ndepartments, lines of business, or specific projects. This granular\napproach provides a detailed breakdown of where spending occurs,\nenabling more precise control and informed decision-making regarding\nresource allocation. Implementing robust tagging policies and naming\nconventions ensures consistency and facilitates the interpretation of\ncost data. Because budgets are soft limit objects, objects can be part\nof more than one budget if different perspectives need to be tracked for\ncost (e.g., cost center & workload level budgeting).\n**Implement a notification strategy**\nSection Title: Cost Optimization > Control > Overview > Proactively monitor all platform usage\nContent:\nEffective budget management relies on timely communication. Setting up\nalerting through emails or webhooks to collaboration tools like Slack\nand Microsoft Teams provides proactive [notification](https://docs.snowflake.com/en/user-guide/budgets/notifications) to key stakeholders when spending approaches or exceeds a defined\nthreshold. These alerts provide teams with an opportunity to review and\nadjust their usage before it leads to significant cost overruns. This\ncapability positions organizations for security success by mitigating\npotential threats through comprehensive monitoring and detection.\nNotifications are not limited to just budgets; [Snowflake alerts](https://docs.snowflake.com/en/user-guide/alerts) can also be\nconfigured to systematically notify administrators of unusual or costly\npatterns, such as those listed in the Control and Optimize sections of\nthe Cost Pillar. This ensures that key drivers of Snowflake consumption\ncan be tracked and remediated proactively, even as the platform’s usage\ngrows.\n ... \nSection Title: Cost Optimization > Control > Overview > Forecast consumption based on business needs\nContent:\n**Build the predictive model**\nThis phase integrates historical trends with strategic business inputs\nto create forward-looking projections.\n**Historical trend analysis:** Analyze past usage for trends,\nseasonality, and outliers to inform future projections. Start with\nsimple trend-based forecasting and progressively move to more\nsophisticated models, leveraging Snowflake’s built-in [SNOWFLAKE.ML.FORECAST function](https://docs.snowflake.com/en/user-guide/ml-functions/forecasting) for time-series forecasting.\n**Driver-based forecasting:** Integrate planned business initiatives\nand new projects directly into the model. Collaborate with business\nleaders to gather strategic inputs such as projected customer growth,\nnew product launches, or increased data ingestion from marketing\ncampaigns.\n**Scenario modeling:** Develop multiple forecast scenarios (e.g.,\n\"conservative,\" \"base case,\" \"aggressive\") by applying varied growth\nfactors to key business drivers. This enables flexible planning and\nhelps mitigate financial risk.\n**Operationalize and optimize**\nThis phase links the forecast to continuous monitoring, governance, and\nproactive cost controls.\n ... \nSection Title: Cost Optimization > Optimize > Overview > Limit data transfer\nContent:\n**Egress Cost Optimizer (ECO):** For providers of data products on the\nSnowflake Marketplace or private listings, enabling the [Egress Cost Optimizer](https://docs.snowflake.com/en/collaboration/provider-listings-auto-fulfillment-eco) (ECO) at the organizational level is a critical best practice. ECO is\nan automated feature for listings with Cross-Cloud Auto-Fulfillment.\nIt intelligently routes data through a Snowflake-managed cache,\nallowing you to pay a one-time egress cost for the initial data\ntransfer. After that, expanding to new regions incurs zero additional\negress costs for the same dataset. This is a powerful tool for scaling\nyour data sharing without compounding data transfer fees.\n**Monitoring and alerts:** To effectively manage data transfer costs,\nutilize Snowflake's [DATA_TRANSFER_HISTORY](https://docs.snowflake.com/en/sql-reference/organization-usage/data_transfer_history) telemetry view. This view provides detailed insights into data\nmovement between different regions and clouds. Establish dashboards\nand alerts to meticulously track this usage, enabling prompt detection\nof any unexpected cost increases.\n**Architectural best practices: Design for minimal data movement**\nMinimizing data transfer costs for your workloads heavily depends on the\narchitecture of your data pipelines and applications. Adhere to the\nfollowing best practices to achieve this:\n ... \nSection Title: Cost Optimization > Optimize > Overview > Improve continually\nContent:\nOptimization is a continuous process that ensures all workloads not only\ndrive maximum business value but also do so in an optimal manner. By\nregularly reviewing, analyzing, and refining your Snowflake environment,\nyou can identify inefficiencies, implement improvements, and adapt your\nplatform to the ever-evolving business needs. The following set of steps\nwill help you continue to improve your environment as you grow:\n**Step 1: Identify & investigate workloads to improve**\nBegin by regularly reviewing (usually on a weekly, bi-weekly, or monthly\ncadence) workloads that could benefit from optimization, using\nSnowflake's [Cost Insights](https://docs.snowflake.com/en/user-guide/cost-insights) ,\ndeviations in unit economics or health metrics (from the Visibility\nprinciple), or objects hitting control limits (e.g., queries hitting\nwarehouse timeouts from the Control principle). Once identified,\ninvestigate these findings through the Cost Management UI, Cost Anomaly\ndetection, Query History, or custom dashboards with Account Usage Views\nto pinpoint the root cause. Then, using the recommendations in the\nOptimize Pillar, make improvements to the workload or object.\n**Step 2: Estimate & test**"],"full_content":null}],"errors":[],"warnings":null,"usage":[{"name":"sku_extract_excerpts","count":4}]}
